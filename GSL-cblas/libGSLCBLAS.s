	.file	"libGSLCBLAS.bc"


	.text
	.align	16
	.globl	cblas_caxpy
	.type	cblas_caxpy,@function
cblas_caxpy:
	pushq	%rbx
	testl	%ecx, %ecx
	jg	.LBB1_9	# entry.bb2_crit_edge
.LBB1_1:	# bb
	movl	$1, %eax
	subl	%edi, %eax
	imull	%ecx, %eax
.LBB1_2:	# bb2
	testl	%r9d, %r9d
	jg	.LBB1_10	# bb2.bb5_crit_edge
.LBB1_3:	# bb3
	movl	$1, %r10d
	subl	%edi, %r10d
	imull	%r9d, %r10d
.LBB1_4:	# bb5
	movss	(%rsi), %xmm0
	pxor	%xmm1, %xmm1
	ucomiss	%xmm1, %xmm0
	setnp	%r11b
	sete	%bl
	andb	%r11b, %bl
	movss	4(%rsi), %xmm2
	ucomiss	%xmm1, %xmm2
	setnp	%sil
	sete	%r11b
	andb	%sil, %r11b
	testb	%bl, %r11b
	jne	.LBB1_8	# return
.LBB1_5:	# bb5
	testl	%edi, %edi
	jle	.LBB1_8	# return
.LBB1_6:	# bb.nph
	addl	%r9d, %r9d
	addl	%r10d, %r10d
	addl	%ecx, %ecx
	addl	%eax, %eax
	xorl	%esi, %esi
	.align	16
.LBB1_7:	# bb8
	movslq	%eax, %r11
	movss	(%rdx,%r11,4), %xmm1
	movaps	%xmm0, %xmm3
	mulss	%xmm1, %xmm3
	leal	1(%rax), %r11d
	movslq	%r11d, %r11
	movss	(%rdx,%r11,4), %xmm4
	movaps	%xmm2, %xmm5
	mulss	%xmm4, %xmm5
	subss	%xmm5, %xmm3
	movslq	%r10d, %r11
	addss	(%r8,%r11,4), %xmm3
	movss	%xmm3, (%r8,%r11,4)
	mulss	%xmm2, %xmm1
	mulss	%xmm0, %xmm4
	addss	%xmm1, %xmm4
	leal	1(%r10), %r11d
	movslq	%r11d, %r11
	addss	(%r8,%r11,4), %xmm4
	movss	%xmm4, (%r8,%r11,4)
	addl	%r9d, %r10d
	addl	%ecx, %eax
	incl	%esi
	cmpl	%edi, %esi
	jne	.LBB1_7	# bb8
.LBB1_8:	# return
	popq	%rbx
	ret
.LBB1_9:	# entry.bb2_crit_edge
	xorl	%eax, %eax
	jmp	.LBB1_2	# bb2
.LBB1_10:	# bb2.bb5_crit_edge
	xorl	%r10d, %r10d
	jmp	.LBB1_4	# bb5
	.size	cblas_caxpy, .-cblas_caxpy


	.align	16
	.globl	cblas_ccopy
	.type	cblas_ccopy,@function
cblas_ccopy:
	testl	%edx, %edx
	jg	.LBB2_8	# entry.bb2_crit_edge
.LBB2_1:	# bb
	movl	$1, %eax
	subl	%edi, %eax
	imull	%edx, %eax
.LBB2_2:	# bb2
	testl	%r8d, %r8d
	jg	.LBB2_9	# bb2.bb7.preheader_crit_edge
.LBB2_3:	# bb3
	movl	$1, %r9d
	subl	%edi, %r9d
	imull	%r8d, %r9d
.LBB2_4:	# bb7.preheader
	testl	%edi, %edi
	jle	.LBB2_7	# return
.LBB2_5:	# bb.nph
	addl	%r8d, %r8d
	addl	%r9d, %r9d
	addl	%edx, %edx
	addl	%eax, %eax
	xorl	%r10d, %r10d
	.align	16
.LBB2_6:	# bb6
	movslq	%eax, %r11
	movss	(%rsi,%r11,4), %xmm0
	movslq	%r9d, %r11
	movss	%xmm0, (%rcx,%r11,4)
	leal	1(%rax), %r11d
	movslq	%r11d, %r11
	movss	(%rsi,%r11,4), %xmm0
	leal	(%r8,%r9), %r11d
	incl	%r9d
	movslq	%r9d, %r9
	movss	%xmm0, (%rcx,%r9,4)
	addl	%edx, %eax
	incl	%r10d
	cmpl	%edi, %r10d
	movl	%r11d, %r9d
	jne	.LBB2_6	# bb6
.LBB2_7:	# return
	ret
.LBB2_8:	# entry.bb2_crit_edge
	xorl	%eax, %eax
	jmp	.LBB2_2	# bb2
.LBB2_9:	# bb2.bb7.preheader_crit_edge
	xorl	%r9d, %r9d
	jmp	.LBB2_4	# bb7.preheader
	.size	cblas_ccopy, .-cblas_ccopy


	.align	16
	.globl	cblas_cdotc_sub
	.type	cblas_cdotc_sub,@function
cblas_cdotc_sub:
	pushq	%r14
	pushq	%rbx
	testl	%edx, %edx
	jg	.LBB3_8	# entry.bb2_crit_edge
.LBB3_1:	# bb
	movl	$1, %eax
	subl	%edi, %eax
	imull	%edx, %eax
.LBB3_2:	# bb2
	testl	%r8d, %r8d
	jg	.LBB3_9	# bb2.bb7.preheader_crit_edge
.LBB3_3:	# bb3
	movl	$1, %r10d
	subl	%edi, %r10d
	imull	%r8d, %r10d
.LBB3_4:	# bb7.preheader
	testl	%edi, %edi
	jle	.LBB3_10	# bb7.preheader.bb8_crit_edge
.LBB3_5:	# bb.nph
	addl	%r8d, %r8d
	addl	%r10d, %r10d
	addl	%edx, %edx
	addl	%eax, %eax
	pxor	%xmm0, %xmm0
	xorl	%r11d, %r11d
	movaps	%xmm0, %xmm1
	.align	16
.LBB3_6:	# bb6
	movslq	%eax, %rbx
	leal	1(%rax), %r14d
	movslq	%r14d, %r14
	cvtss2sd	(%rsi,%r14,4), %xmm2
	movslq	%r10d, %r14
	movss	(%rcx,%r14,4), %xmm3
	cvtss2sd	%xmm3, %xmm4
	mulsd	%xmm2, %xmm4
	movss	(%rsi,%rbx,4), %xmm5
	leal	1(%r10), %ebx
	movslq	%ebx, %rbx
	movss	(%rcx,%rbx,4), %xmm6
	movaps	%xmm5, %xmm7
	mulss	%xmm6, %xmm7
	cvtss2sd	%xmm7, %xmm7
	subsd	%xmm4, %xmm7
	cvtss2sd	%xmm1, %xmm1
	addsd	%xmm7, %xmm1
	cvtsd2ss	%xmm1, %xmm1
	cvtss2sd	%xmm6, %xmm4
	mulsd	%xmm2, %xmm4
	mulss	%xmm3, %xmm5
	cvtss2sd	%xmm5, %xmm2
	addsd	%xmm4, %xmm2
	cvtss2sd	%xmm0, %xmm0
	addsd	%xmm2, %xmm0
	cvtsd2ss	%xmm0, %xmm0
	addl	%r8d, %r10d
	addl	%edx, %eax
	incl	%r11d
	cmpl	%edi, %r11d
	jne	.LBB3_6	# bb6
.LBB3_7:	# bb8
	movss	%xmm0, (%r9)
	movss	%xmm1, 4(%r9)
	popq	%rbx
	popq	%r14
	ret
.LBB3_8:	# entry.bb2_crit_edge
	xorl	%eax, %eax
	jmp	.LBB3_2	# bb2
.LBB3_9:	# bb2.bb7.preheader_crit_edge
	xorl	%r10d, %r10d
	jmp	.LBB3_4	# bb7.preheader
.LBB3_10:	# bb7.preheader.bb8_crit_edge
	pxor	%xmm0, %xmm0
	movaps	%xmm0, %xmm1
	jmp	.LBB3_7	# bb8
	.size	cblas_cdotc_sub, .-cblas_cdotc_sub


	.align	16
	.globl	cblas_cdotu_sub
	.type	cblas_cdotu_sub,@function
cblas_cdotu_sub:
	pushq	%r14
	pushq	%rbx
	testl	%edx, %edx
	jg	.LBB4_8	# entry.bb2_crit_edge
.LBB4_1:	# bb
	movl	$1, %eax
	subl	%edi, %eax
	imull	%edx, %eax
.LBB4_2:	# bb2
	testl	%r8d, %r8d
	jg	.LBB4_9	# bb2.bb7.preheader_crit_edge
.LBB4_3:	# bb3
	movl	$1, %r10d
	subl	%edi, %r10d
	imull	%r8d, %r10d
.LBB4_4:	# bb7.preheader
	testl	%edi, %edi
	jle	.LBB4_10	# bb7.preheader.bb8_crit_edge
.LBB4_5:	# bb.nph
	addl	%r8d, %r8d
	addl	%r10d, %r10d
	addl	%edx, %edx
	addl	%eax, %eax
	pxor	%xmm0, %xmm0
	xorl	%r11d, %r11d
	movaps	%xmm0, %xmm1
	.align	16
.LBB4_6:	# bb6
	movslq	%eax, %rbx
	leal	1(%rax), %r14d
	movslq	%r14d, %r14
	cvtss2sd	(%rsi,%r14,4), %xmm2
	movslq	%r10d, %r14
	movss	(%rcx,%r14,4), %xmm3
	cvtss2sd	%xmm3, %xmm4
	mulsd	%xmm2, %xmm4
	movss	(%rsi,%rbx,4), %xmm5
	leal	1(%r10), %ebx
	movslq	%ebx, %rbx
	movss	(%rcx,%rbx,4), %xmm6
	movaps	%xmm5, %xmm7
	mulss	%xmm6, %xmm7
	cvtss2sd	%xmm7, %xmm7
	addsd	%xmm4, %xmm7
	cvtss2sd	%xmm1, %xmm1
	addsd	%xmm7, %xmm1
	cvtsd2ss	%xmm1, %xmm1
	cvtss2sd	%xmm6, %xmm4
	mulsd	%xmm2, %xmm4
	mulss	%xmm3, %xmm5
	cvtss2sd	%xmm5, %xmm2
	subsd	%xmm4, %xmm2
	cvtss2sd	%xmm0, %xmm0
	addsd	%xmm2, %xmm0
	cvtsd2ss	%xmm0, %xmm0
	addl	%r8d, %r10d
	addl	%edx, %eax
	incl	%r11d
	cmpl	%edi, %r11d
	jne	.LBB4_6	# bb6
.LBB4_7:	# bb8
	movss	%xmm0, (%r9)
	movss	%xmm1, 4(%r9)
	popq	%rbx
	popq	%r14
	ret
.LBB4_8:	# entry.bb2_crit_edge
	xorl	%eax, %eax
	jmp	.LBB4_2	# bb2
.LBB4_9:	# bb2.bb7.preheader_crit_edge
	xorl	%r10d, %r10d
	jmp	.LBB4_4	# bb7.preheader
.LBB4_10:	# bb7.preheader.bb8_crit_edge
	pxor	%xmm0, %xmm0
	movaps	%xmm0, %xmm1
	jmp	.LBB4_7	# bb8
	.size	cblas_cdotu_sub, .-cblas_cdotu_sub


	.section	.rodata.cst4,"aM",@progbits,4
	.align	4
.LCPI5_0:					
	.long	1065353216	# float 1.000000e+00
	.text
	.align	16
	.globl	cblas_cgbmv
	.type	cblas_cgbmv,@function
cblas_cgbmv:
.Leh_func_begin1:
.Llabel1:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	testl	%ecx, %ecx
	movq	136(%rsp), %rax
	movss	4(%rax), %xmm0
	movss	(%rax), %xmm1
	movq	96(%rsp), %rax
	movss	4(%rax), %xmm2
	movss	(%rax), %xmm3
	movl	152(%rsp), %eax
	movq	144(%rsp), %r10
	movl	128(%rsp), %r11d
	movq	120(%rsp), %rbx
	movq	104(%rsp), %r14
	movl	%edi, 16(%rsp)
	je	.LBB5_69	# return
.LBB5_1:	# entry
	testl	%edx, %edx
	je	.LBB5_69	# return
.LBB5_2:	# bb
	pxor	%xmm4, %xmm4
	ucomiss	%xmm4, %xmm3
	setp	36(%rsp)
	setne	%dil
	setnp	%r15b
	sete	%r12b
	andb	%r15b, %r12b
	ucomiss	%xmm4, %xmm2
	setp	%r15b
	setnp	%r13b
	sete	12(%rsp)
	setne	%bpl
	andb	%r13b, 12(%rsp)
	andb	%r12b, 12(%rsp)
	orb	36(%rsp), %dil
	orb	%r15b, %bpl
	orb	%dil, %bpl
	testb	%bpl, %bpl
	jne	.LBB5_5	# bb36
.LBB5_3:	# bb
	ucomiss	.LCPI5_0(%rip), %xmm1
	jne	.LBB5_5	# bb36
	jp	.LBB5_5	# bb36
.LBB5_4:	# bb
	pxor	%xmm4, %xmm4
	ucomiss	%xmm4, %xmm0
	setnp	%dil
	sete	%r15b
	testb	%dil, %r15b
	jne	.LBB5_69	# return
.LBB5_5:	# bb36
	cmpl	$111, %esi
	je	.LBB5_70	# bb36.bb39_crit_edge
.LBB5_6:	# bb38
	movl	%r8d, 28(%rsp)
	movl	%r9d, %r8d
	movl	%ecx, 36(%rsp)
	movl	%edx, %ecx
.LBB5_7:	# bb39
	movl	%r8d, 24(%rsp)
	movl	%ecx, 32(%rsp)
	pxor	%xmm4, %xmm4
	ucomiss	%xmm4, %xmm0
	jne	.LBB5_14	# bb47
	jp	.LBB5_14	# bb47
.LBB5_8:	# bb39
	pxor	%xmm4, %xmm4
	ucomiss	%xmm4, %xmm1
	jne	.LBB5_14	# bb47
	jp	.LBB5_14	# bb47
.LBB5_9:	# bb41
	testl	%eax, %eax
	jg	.LBB5_71	# bb41.bb46.preheader_crit_edge
.LBB5_10:	# bb42
	movl	$1, %ecx
	subl	36(%rsp), %ecx
	imull	%eax, %ecx
.LBB5_11:	# bb46.preheader
	cmpl	$0, 36(%rsp)
	jle	.LBB5_21	# bb55
.LBB5_12:	# bb.nph
	leal	(%rax,%rax), %edx
	addl	%ecx, %ecx
	xorl	%edi, %edi
	.align	16
.LBB5_13:	# bb45
	movslq	%ecx, %r8
	movl	$0, (%r10,%r8,4)
	leal	(%rdx,%rcx), %r8d
	incl	%ecx
	movslq	%ecx, %rcx
	movl	$0, (%r10,%rcx,4)
	incl	%edi
	cmpl	36(%rsp), %edi
	movl	%r8d, %ecx
	jne	.LBB5_13	# bb45
	jmp	.LBB5_21	# bb55
.LBB5_14:	# bb47
	pxor	%xmm4, %xmm4
	ucomiss	%xmm4, %xmm0
	jne	.LBB5_16	# bb49
	jp	.LBB5_16	# bb49
.LBB5_15:	# bb47
	ucomiss	.LCPI5_0(%rip), %xmm1
	setnp	%cl
	sete	%dl
	testb	%cl, %dl
	jne	.LBB5_21	# bb55
.LBB5_16:	# bb49
	testl	%eax, %eax
	jg	.LBB5_72	# bb49.bb54.preheader_crit_edge
.LBB5_17:	# bb50
	movl	$1, %ecx
	subl	36(%rsp), %ecx
	imull	%eax, %ecx
.LBB5_18:	# bb54.preheader
	cmpl	$0, 36(%rsp)
	jle	.LBB5_21	# bb55
.LBB5_19:	# bb.nph216
	leal	(%rax,%rax), %edx
	addl	%ecx, %ecx
	xorl	%edi, %edi
	.align	16
.LBB5_20:	# bb53
	movslq	%ecx, %r8
	movss	(%r10,%r8,4), %xmm4
	movaps	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	leal	1(%rcx), %r9d
	movslq	%r9d, %r9
	movss	(%r10,%r9,4), %xmm6
	movaps	%xmm6, %xmm7
	mulss	%xmm0, %xmm7
	subss	%xmm7, %xmm5
	movss	%xmm5, (%r10,%r8,4)
	mulss	%xmm0, %xmm4
	mulss	%xmm1, %xmm6
	addss	%xmm4, %xmm6
	movss	%xmm6, (%r10,%r9,4)
	addl	%edx, %ecx
	incl	%edi
	cmpl	36(%rsp), %edi
	jne	.LBB5_20	# bb53
.LBB5_21:	# bb55
	testb	$1, 12(%rsp)
	jne	.LBB5_69	# return
.LBB5_22:	# bb57
	cmpl	$111, %esi
	jne	.LBB5_24	# bb61
.LBB5_23:	# bb57
	cmpl	$101, 16(%rsp)
	je	.LBB5_26	# bb65
.LBB5_24:	# bb61
	cmpl	$112, %esi
	jne	.LBB5_34	# bb80
.LBB5_25:	# bb61
	cmpl	$102, 16(%rsp)
	jne	.LBB5_34	# bb80
.LBB5_26:	# bb65
	testl	%eax, %eax
	jg	.LBB5_73	# bb65.bb79.preheader_crit_edge
.LBB5_27:	# bb66
	movl	$1, %ecx
	subl	36(%rsp), %ecx
	imull	%eax, %ecx
.LBB5_28:	# bb79.preheader
	cmpl	$0, 36(%rsp)
	jle	.LBB5_69	# return
.LBB5_29:	# bb.nph213
	movl	$1, %edx
	subl	32(%rsp), %edx
	imull	%r11d, %edx
	movl	%edx, 8(%rsp)
	movl	$4294967294, %edx
	movl	28(%rsp), %esi
	subl	%esi, %edx
	addl	%eax, %eax
	addl	%ecx, %ecx
	movl	112(%rsp), %edi
	decl	%edi
	movl	%edi, 20(%rsp)
	incl	%esi
	movl	%esi, 28(%rsp)
	movl	24(%rsp), %esi
	movl	%esi, %edi
	negl	%edi
	movl	%edi, 12(%rsp)
	xorl	%edi, %edi
	movl	%esi, 16(%rsp)
	.align	16
.LBB5_30:	# bb69
	xorl	%esi, %esi
	testl	%r11d, %r11d
	movl	8(%rsp), %r8d
	cmovg	%esi, %r8d
	movl	12(%rsp), %r9d
	leal	(%r9,%rdi), %r9d
	cmpl	24(%rsp), %edi
	cmovle	%esi, %r9d
	movl	%r9d, %esi
	imull	%r11d, %esi
	movl	28(%rsp), %r15d
	leal	(%r15,%rdi), %r15d
	movl	32(%rsp), %r12d
	cmpl	%r12d, %r15d
	cmovg	%r12d, %r15d
	cmpl	%r15d, %r9d
	jge	.LBB5_74	# bb69.bb78_crit_edge
.LBB5_31:	# bb.nph208
	movl	16(%rsp), %r15d
	leal	(%r9,%r15), %r15d
	addl	%r15d, %r15d
	movl	32(%rsp), %r12d
	notl	%r12d
	cmpl	%r12d, %edx
	cmovge	%edx, %r12d
	addl	%r9d, %r12d
	notl	%r12d
	pxor	%xmm0, %xmm0
	xorl	%r9d, %r9d
	movaps	%xmm0, %xmm1
	.align	16
.LBB5_32:	# bb76
	movslq	%r15d, %r13
	leal	1(%r15), %ebp
	movslq	%ebp, %rbp
	movss	(%r14,%rbp,4), %xmm4
	addl	%r8d, %esi
	leal	1(,%rsi,2), %r8d
	leal	(%rsi,%rsi), %ebp
	movslq	%ebp, %rbp
	movss	(%rbx,%rbp,4), %xmm5
	movaps	%xmm4, %xmm6
	mulss	%xmm5, %xmm6
	movss	(%r14,%r13,4), %xmm7
	movslq	%r8d, %r8
	movss	(%rbx,%r8,4), %xmm8
	movaps	%xmm7, %xmm9
	mulss	%xmm8, %xmm9
	addss	%xmm6, %xmm9
	addss	%xmm9, %xmm1
	mulss	%xmm8, %xmm4
	mulss	%xmm5, %xmm7
	subss	%xmm4, %xmm7
	addss	%xmm7, %xmm0
	addl	$2, %r15d
	incl	%r9d
	cmpl	%r12d, %r9d
	movl	%r11d, %r8d
	jne	.LBB5_32	# bb76
.LBB5_33:	# bb78
	movaps	%xmm2, %xmm4
	mulss	%xmm1, %xmm4
	movaps	%xmm3, %xmm5
	mulss	%xmm0, %xmm5
	subss	%xmm4, %xmm5
	movslq	%ecx, %rsi
	addss	(%r10,%rsi,4), %xmm5
	movss	%xmm5, (%r10,%rsi,4)
	mulss	%xmm2, %xmm0
	mulss	%xmm3, %xmm1
	addss	%xmm0, %xmm1
	leal	1(%rcx), %esi
	movslq	%esi, %rsi
	addss	(%r10,%rsi,4), %xmm1
	movss	%xmm1, (%r10,%rsi,4)
	addl	%eax, %ecx
	movl	16(%rsp), %esi
	addl	20(%rsp), %esi
	movl	%esi, 16(%rsp)
	decl	%edx
	incl	%edi
	cmpl	36(%rsp), %edi
	jne	.LBB5_30	# bb69
	jmp	.LBB5_69	# return
.LBB5_34:	# bb80
	movl	16(%rsp), %ecx
	cmpl	$102, %ecx
	sete	%dl
	cmpl	$111, %esi
	sete	%dil
	andb	%dl, %dil
	cmpl	$101, %ecx
	sete	%cl
	cmpl	$112, %esi
	sete	%dl
	testb	%cl, %dl
	jne	.LBB5_36	# bb88
.LBB5_35:	# bb80
	notb	%dil
	testb	$1, %dil
	jne	.LBB5_46	# bb106
.LBB5_36:	# bb88
	testl	%r11d, %r11d
	jg	.LBB5_75	# bb88.bb105.preheader_crit_edge
.LBB5_37:	# bb89
	movl	$1, %ecx
	subl	32(%rsp), %ecx
	imull	%r11d, %ecx
.LBB5_38:	# bb105.preheader
	cmpl	$0, 32(%rsp)
	jle	.LBB5_69	# return
.LBB5_39:	# bb.nph200
	movl	$1, %edx
	subl	36(%rsp), %edx
	imull	%eax, %edx
	movl	%edx, 12(%rsp)
	movl	$4294967294, %edx
	movl	24(%rsp), %esi
	subl	%esi, %edx
	addl	%r11d, %r11d
	addl	%ecx, %ecx
	movl	112(%rsp), %edi
	decl	%edi
	movl	%edi, 20(%rsp)
	incl	%esi
	movl	%esi, 24(%rsp)
	movl	28(%rsp), %esi
	movl	%esi, %edi
	negl	%edi
	movl	%edi, 16(%rsp)
	xorl	%edi, %edi
	.align	16
.LBB5_40:	# bb92
	movslq	%ecx, %r8
	movss	(%rbx,%r8,4), %xmm0
	movaps	%xmm3, %xmm1
	mulss	%xmm0, %xmm1
	leal	1(%rcx), %r8d
	movslq	%r8d, %r8
	movss	(%rbx,%r8,4), %xmm4
	movaps	%xmm2, %xmm5
	mulss	%xmm4, %xmm5
	subss	%xmm5, %xmm1
	pxor	%xmm5, %xmm5
	ucomiss	%xmm5, %xmm1
	mulss	%xmm2, %xmm0
	mulss	%xmm3, %xmm4
	addss	%xmm0, %xmm4
	jne	.LBB5_42	# bb94
	jp	.LBB5_42	# bb94
.LBB5_41:	# bb92
	pxor	%xmm0, %xmm0
	ucomiss	%xmm0, %xmm4
	setnp	%r8b
	sete	%r9b
	testb	%r8b, %r9b
	jne	.LBB5_45	# bb104
.LBB5_42:	# bb94
	xorl	%r8d, %r8d
	testl	%eax, %eax
	movl	12(%rsp), %r9d
	cmovg	%r8d, %r9d
	movl	16(%rsp), %r15d
	leal	(%r15,%rdi), %r15d
	cmpl	28(%rsp), %edi
	cmovle	%r8d, %r15d
	movl	%r15d, %r8d
	imull	%eax, %r8d
	movl	24(%rsp), %r12d
	leal	(%r12,%rdi), %r12d
	movl	36(%rsp), %r13d
	cmpl	%r13d, %r12d
	cmovg	%r13d, %r12d
	cmpl	%r12d, %r15d
	jge	.LBB5_45	# bb104
.LBB5_43:	# bb.nph197
	leal	(%r15,%rsi), %r12d
	addl	%r12d, %r12d
	movl	36(%rsp), %r13d
	notl	%r13d
	cmpl	%r13d, %edx
	cmovge	%edx, %r13d
	addl	%r15d, %r13d
	notl	%r13d
	xorl	%r15d, %r15d
	.align	16
.LBB5_44:	# bb102
	movslq	%r12d, %rbp
	movss	(%r14,%rbp,4), %xmm0
	movaps	%xmm0, %xmm5
	mulss	%xmm1, %xmm5
	leal	1(%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r14,%rbp,4), %xmm6
	movaps	%xmm6, %xmm7
	mulss	%xmm4, %xmm7
	subss	%xmm7, %xmm5
	addl	%r9d, %r8d
	leal	(%r8,%r8), %r9d
	movslq	%r9d, %r9
	addss	(%r10,%r9,4), %xmm5
	movss	%xmm5, (%r10,%r9,4)
	mulss	%xmm4, %xmm0
	mulss	%xmm1, %xmm6
	addss	%xmm0, %xmm6
	leal	1(,%r8,2), %r9d
	movslq	%r9d, %r9
	addss	(%r10,%r9,4), %xmm6
	movss	%xmm6, (%r10,%r9,4)
	addl	$2, %r12d
	incl	%r15d
	cmpl	%r13d, %r15d
	movl	%eax, %r9d
	jne	.LBB5_44	# bb102
.LBB5_45:	# bb104
	addl	%r11d, %ecx
	addl	20(%rsp), %esi
	decl	%edx
	incl	%edi
	cmpl	32(%rsp), %edi
	jne	.LBB5_40	# bb92
	jmp	.LBB5_69	# return
.LBB5_46:	# bb106
	cmpl	$113, %esi
	jne	.LBB5_58	# bb128
.LBB5_47:	# bb106
	cmpl	$101, 16(%rsp)
	jne	.LBB5_58	# bb128
.LBB5_48:	# bb110
	testl	%r11d, %r11d
	jg	.LBB5_76	# bb110.bb127.preheader_crit_edge
.LBB5_49:	# bb111
	movl	$1, %ecx
	subl	32(%rsp), %ecx
	imull	%r11d, %ecx
.LBB5_50:	# bb127.preheader
	cmpl	$0, 32(%rsp)
	jle	.LBB5_69	# return
.LBB5_51:	# bb.nph191
	movl	$1, %edx
	subl	36(%rsp), %edx
	imull	%eax, %edx
	movl	%edx, 12(%rsp)
	movl	$4294967294, %edx
	movl	24(%rsp), %esi
	subl	%esi, %edx
	addl	%r11d, %r11d
	addl	%ecx, %ecx
	movl	112(%rsp), %edi
	decl	%edi
	movl	%edi, 20(%rsp)
	incl	%esi
	movl	%esi, 24(%rsp)
	movl	28(%rsp), %esi
	movl	%esi, %edi
	negl	%edi
	movl	%edi, 16(%rsp)
	xorl	%edi, %edi
	.align	16
.LBB5_52:	# bb114
	movslq	%ecx, %r8
	movss	(%rbx,%r8,4), %xmm0
	movaps	%xmm3, %xmm1
	mulss	%xmm0, %xmm1
	leal	1(%rcx), %r8d
	movslq	%r8d, %r8
	movss	(%rbx,%r8,4), %xmm4
	movaps	%xmm2, %xmm5
	mulss	%xmm4, %xmm5
	subss	%xmm5, %xmm1
	pxor	%xmm5, %xmm5
	ucomiss	%xmm5, %xmm1
	mulss	%xmm2, %xmm0
	mulss	%xmm3, %xmm4
	addss	%xmm0, %xmm4
	jne	.LBB5_54	# bb116
	jp	.LBB5_54	# bb116
.LBB5_53:	# bb114
	pxor	%xmm0, %xmm0
	ucomiss	%xmm0, %xmm4
	setnp	%r8b
	sete	%r9b
	testb	%r8b, %r9b
	jne	.LBB5_57	# bb126
.LBB5_54:	# bb116
	xorl	%r8d, %r8d
	testl	%eax, %eax
	movl	12(%rsp), %r9d
	cmovg	%r8d, %r9d
	movl	16(%rsp), %r15d
	leal	(%r15,%rdi), %r15d
	cmpl	28(%rsp), %edi
	cmovle	%r8d, %r15d
	movl	%r15d, %r8d
	imull	%eax, %r8d
	movl	24(%rsp), %r12d
	leal	(%r12,%rdi), %r12d
	movl	36(%rsp), %r13d
	cmpl	%r13d, %r12d
	cmovg	%r13d, %r12d
	cmpl	%r12d, %r15d
	jge	.LBB5_57	# bb126
.LBB5_55:	# bb.nph188
	leal	(%r15,%rsi), %r12d
	addl	%r12d, %r12d
	movl	36(%rsp), %r13d
	notl	%r13d
	cmpl	%r13d, %edx
	cmovge	%edx, %r13d
	addl	%r15d, %r13d
	notl	%r13d
	xorl	%r15d, %r15d
	.align	16
.LBB5_56:	# bb124
	movslq	%r12d, %rbp
	movss	(%r14,%rbp,4), %xmm0
	movaps	%xmm0, %xmm5
	mulss	%xmm1, %xmm5
	leal	1(%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r14,%rbp,4), %xmm6
	movaps	%xmm6, %xmm7
	mulss	%xmm4, %xmm7
	addss	%xmm5, %xmm7
	addl	%r9d, %r8d
	leal	(%r8,%r8), %r9d
	movslq	%r9d, %r9
	addss	(%r10,%r9,4), %xmm7
	movss	%xmm7, (%r10,%r9,4)
	mulss	%xmm4, %xmm0
	mulss	%xmm1, %xmm6
	subss	%xmm6, %xmm0
	leal	1(,%r8,2), %r9d
	movslq	%r9d, %r9
	addss	(%r10,%r9,4), %xmm0
	movss	%xmm0, (%r10,%r9,4)
	addl	$2, %r12d
	incl	%r15d
	cmpl	%r13d, %r15d
	movl	%eax, %r9d
	jne	.LBB5_56	# bb124
.LBB5_57:	# bb126
	addl	%r11d, %ecx
	addl	20(%rsp), %esi
	decl	%edx
	incl	%edi
	cmpl	32(%rsp), %edi
	jne	.LBB5_52	# bb114
	jmp	.LBB5_69	# return
.LBB5_58:	# bb128
	cmpl	$113, %esi
	jne	.LBB5_68	# bb148
.LBB5_59:	# bb128
	cmpl	$102, 16(%rsp)
	jne	.LBB5_68	# bb148
.LBB5_60:	# bb132
	testl	%eax, %eax
	jg	.LBB5_77	# bb132.bb147.preheader_crit_edge
.LBB5_61:	# bb133
	movl	$1, %ecx
	subl	36(%rsp), %ecx
	imull	%eax, %ecx
.LBB5_62:	# bb147.preheader
	cmpl	$0, 36(%rsp)
	jle	.LBB5_69	# return
.LBB5_63:	# bb.nph182
	movl	$1, %edx
	subl	32(%rsp), %edx
	imull	%r11d, %edx
	movl	%edx, 12(%rsp)
	movl	$4294967294, %edx
	movl	28(%rsp), %esi
	subl	%esi, %edx
	addl	%eax, %eax
	addl	%ecx, %ecx
	movl	112(%rsp), %edi
	decl	%edi
	movl	%edi, 20(%rsp)
	incl	%esi
	movl	%esi, 28(%rsp)
	movl	24(%rsp), %esi
	movl	%esi, %edi
	negl	%edi
	movl	%edi, 16(%rsp)
	xorl	%edi, %edi
	.align	16
.LBB5_64:	# bb136
	xorl	%r8d, %r8d
	testl	%r11d, %r11d
	movl	12(%rsp), %r9d
	cmovg	%r8d, %r9d
	movl	16(%rsp), %r15d
	leal	(%r15,%rdi), %r15d
	cmpl	24(%rsp), %edi
	cmovle	%r8d, %r15d
	movl	%r15d, %r8d
	imull	%r11d, %r8d
	movl	28(%rsp), %r12d
	leal	(%r12,%rdi), %r12d
	movl	32(%rsp), %r13d
	cmpl	%r13d, %r12d
	cmovg	%r13d, %r12d
	cmpl	%r12d, %r15d
	jge	.LBB5_78	# bb136.bb146_crit_edge
.LBB5_65:	# bb.nph177
	leal	(%r15,%rsi), %r12d
	addl	%r12d, %r12d
	movl	32(%rsp), %r13d
	notl	%r13d
	cmpl	%r13d, %edx
	cmovge	%edx, %r13d
	addl	%r15d, %r13d
	notl	%r13d
	pxor	%xmm0, %xmm0
	xorl	%r15d, %r15d
	movaps	%xmm0, %xmm1
	.align	16
.LBB5_66:	# bb144
	movslq	%r12d, %rbp
	movss	(%r14,%rbp,4), %xmm4
	addl	%r9d, %r8d
	leal	1(,%r8,2), %r9d
	leal	(%r8,%r8), %ebp
	movslq	%ebp, %rbp
	movss	(%rbx,%rbp,4), %xmm5
	movaps	%xmm4, %xmm6
	mulss	%xmm5, %xmm6
	leal	1(%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r14,%rbp,4), %xmm7
	mulss	%xmm7, %xmm5
	movslq	%r9d, %r9
	movss	(%rbx,%r9,4), %xmm8
	mulss	%xmm8, %xmm4
	subss	%xmm5, %xmm4
	addss	%xmm4, %xmm0
	mulss	%xmm8, %xmm7
	addss	%xmm6, %xmm7
	addss	%xmm7, %xmm1
	addl	$2, %r12d
	incl	%r15d
	cmpl	%r13d, %r15d
	movl	%r11d, %r9d
	jne	.LBB5_66	# bb144
.LBB5_67:	# bb146
	movaps	%xmm2, %xmm4
	mulss	%xmm0, %xmm4
	movaps	%xmm3, %xmm5
	mulss	%xmm1, %xmm5
	subss	%xmm4, %xmm5
	movslq	%ecx, %r8
	addss	(%r10,%r8,4), %xmm5
	movss	%xmm5, (%r10,%r8,4)
	mulss	%xmm2, %xmm1
	mulss	%xmm3, %xmm0
	addss	%xmm1, %xmm0
	leal	1(%rcx), %r8d
	movslq	%r8d, %r8
	addss	(%r10,%r8,4), %xmm0
	movss	%xmm0, (%r10,%r8,4)
	addl	%eax, %ecx
	addl	20(%rsp), %esi
	decl	%edx
	incl	%edi
	cmpl	36(%rsp), %edi
	jne	.LBB5_64	# bb136
	jmp	.LBB5_69	# return
.LBB5_68:	# bb148
	xorl	%edi, %edi
	leaq	.str, %rsi
	leaq	.str1, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB5_69:	# return
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB5_70:	# bb36.bb39_crit_edge
	movl	%r9d, 28(%rsp)
	movl	%edx, 36(%rsp)
	jmp	.LBB5_7	# bb39
.LBB5_71:	# bb41.bb46.preheader_crit_edge
	xorl	%ecx, %ecx
	jmp	.LBB5_11	# bb46.preheader
.LBB5_72:	# bb49.bb54.preheader_crit_edge
	xorl	%ecx, %ecx
	jmp	.LBB5_18	# bb54.preheader
.LBB5_73:	# bb65.bb79.preheader_crit_edge
	xorl	%ecx, %ecx
	jmp	.LBB5_28	# bb79.preheader
.LBB5_74:	# bb69.bb78_crit_edge
	pxor	%xmm0, %xmm0
	movaps	%xmm0, %xmm1
	jmp	.LBB5_33	# bb78
.LBB5_75:	# bb88.bb105.preheader_crit_edge
	xorl	%ecx, %ecx
	jmp	.LBB5_38	# bb105.preheader
.LBB5_76:	# bb110.bb127.preheader_crit_edge
	xorl	%ecx, %ecx
	jmp	.LBB5_50	# bb127.preheader
.LBB5_77:	# bb132.bb147.preheader_crit_edge
	xorl	%ecx, %ecx
	jmp	.LBB5_62	# bb147.preheader
.LBB5_78:	# bb136.bb146_crit_edge
	pxor	%xmm0, %xmm0
	movaps	%xmm0, %xmm1
	jmp	.LBB5_67	# bb146
	.size	cblas_cgbmv, .-cblas_cgbmv
.Leh_func_end1:


	.section	.rodata.cst4,"aM",@progbits,4
	.align	4
.LCPI6_0:					
	.long	1065353216	# float 1.000000e+00
	.text
	.align	16
	.globl	cblas_cgemm
	.type	cblas_cgemm,@function
cblas_cgemm:
.Leh_func_begin2:
.Llabel2:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	movl	%ecx, 36(%rsp)
	movq	96(%rsp), %rax
	movss	(%rax), %xmm0
	pxor	%xmm1, %xmm1
	ucomiss	%xmm1, %xmm0
	setp	%cl
	setne	%r10b
	setnp	%r11b
	sete	%bl
	andb	%r11b, %bl
	movss	4(%rax), %xmm2
	ucomiss	%xmm1, %xmm2
	setp	%al
	setnp	%r11b
	sete	%r14b
	setne	%r15b
	andb	%r11b, %r14b
	andb	%bl, %r14b
	movb	%r14b, 7(%rsp)
	orb	%cl, %r10b
	orb	%al, %r15b
	orb	%r10b, %r15b
	testb	%r15b, %r15b
	movq	136(%rsp), %rax
	movss	4(%rax), %xmm1
	movss	(%rax), %xmm3
	movq	144(%rsp), %rax
	movq	120(%rsp), %rcx
	movl	112(%rsp), %r10d
	movl	%r10d, 32(%rsp)
	movq	104(%rsp), %r10
	jne	.LBB6_3	# bb17
.LBB6_1:	# entry
	ucomiss	.LCPI6_0(%rip), %xmm3
	jne	.LBB6_3	# bb17
	jp	.LBB6_3	# bb17
.LBB6_2:	# entry
	pxor	%xmm4, %xmm4
	ucomiss	%xmm4, %xmm1
	setnp	%r11b
	sete	%bl
	testb	%r11b, %bl
	jne	.LBB6_70	# return
.LBB6_3:	# bb17
	cmpl	$101, %edi
	je	.LBB6_71	# bb18
.LBB6_4:	# bb31
	cmpl	$111, %esi
	movl	$111, %edi
	movl	$112, %r11d
	cmove	%edi, %r11d
	movl	%r11d, 16(%rsp)
	cmpl	$113, %esi
	movl	$4294967295, %esi
	movl	$1, %r11d
	cmove	%esi, %r11d
	movl	%r11d, 28(%rsp)
	cmpl	$111, %edx
	movl	$112, %r11d
	cmove	%edi, %r11d
	movl	%r11d, 8(%rsp)
	cmpl	$113, %edx
	movl	$1, %edx
	cmove	%esi, %edx
	movl	%edx, 20(%rsp)
	movq	%r10, %rdx
	movq	%rcx, %r10
	movl	112(%rsp), %ecx
	movl	%ecx, 24(%rsp)
	movl	128(%rsp), %ecx
	movl	%ecx, 32(%rsp)
	movl	36(%rsp), %ecx
	movl	%r8d, 36(%rsp)
.LBB6_5:	# bb44
	pxor	%xmm4, %xmm4
	ucomiss	%xmm4, %xmm1
	jne	.LBB6_13	# bb52
	jp	.LBB6_13	# bb52
.LBB6_6:	# bb44
	pxor	%xmm4, %xmm4
	ucomiss	%xmm4, %xmm3
	jne	.LBB6_13	# bb52
	jp	.LBB6_13	# bb52
.LBB6_7:	# bb51.preheader
	cmpl	$0, 36(%rsp)
	jle	.LBB6_20	# bb60
.LBB6_8:	# bb.nph132
	testl	%ecx, %ecx
	jle	.LBB6_20	# bb60
.LBB6_9:	# bb49.preheader.preheader
	movl	152(%rsp), %esi
	leal	(%rsi,%rsi), %r8d
	xorl	%r11d, %r11d
	movl	%r11d, %ebx
	jmp	.LBB6_12	# bb49.preheader
	.align	16
.LBB6_10:	# bb48
	movslq	%edi, %r14
	movl	$0, (%rax,%r14,4)
	leal	1(%rdi), %r14d
	movslq	%r14d, %r14
	movl	$0, (%rax,%r14,4)
	addl	$2, %edi
	incl	%esi
	cmpl	%ecx, %esi
	jne	.LBB6_10	# bb48
.LBB6_11:	# bb50
	addl	%r8d, %r11d
	incl	%ebx
	cmpl	36(%rsp), %ebx
	je	.LBB6_20	# bb60
.LBB6_12:	# bb49.preheader
	xorl	%esi, %esi
	movl	%r11d, %edi
	jmp	.LBB6_10	# bb48
.LBB6_13:	# bb52
	pxor	%xmm4, %xmm4
	ucomiss	%xmm4, %xmm1
	setnp	%sil
	sete	%dil
	andb	%sil, %dil
	ucomiss	.LCPI6_0(%rip), %xmm3
	setnp	%sil
	sete	%r8b
	andb	%sil, %r8b
	testb	%r8b, %dil
	jne	.LBB6_20	# bb60
.LBB6_14:	# bb52
	cmpl	$0, 36(%rsp)
	jle	.LBB6_20	# bb60
.LBB6_15:	# bb.nph168
	testl	%ecx, %ecx
	jle	.LBB6_20	# bb60
.LBB6_16:	# bb57.preheader.preheader
	movl	152(%rsp), %esi
	leal	(%rsi,%rsi), %r8d
	xorl	%edi, %edi
	movl	%edi, %esi
	.align	16
.LBB6_17:	# bb57.preheader
	xorl	%r11d, %r11d
	movl	%edi, %ebx
	.align	16
.LBB6_18:	# bb56
	movslq	%ebx, %r14
	movss	(%rax,%r14,4), %xmm4
	movaps	%xmm3, %xmm5
	mulss	%xmm4, %xmm5
	leal	1(%rbx), %r15d
	movslq	%r15d, %r15
	movss	(%rax,%r15,4), %xmm6
	movaps	%xmm1, %xmm7
	mulss	%xmm6, %xmm7
	subss	%xmm7, %xmm5
	movss	%xmm5, (%rax,%r14,4)
	mulss	%xmm1, %xmm4
	mulss	%xmm3, %xmm6
	addss	%xmm4, %xmm6
	movss	%xmm6, (%rax,%r15,4)
	addl	$2, %ebx
	incl	%r11d
	cmpl	%ecx, %r11d
	jne	.LBB6_18	# bb56
.LBB6_19:	# bb58
	addl	%r8d, %edi
	incl	%esi
	cmpl	36(%rsp), %esi
	jne	.LBB6_17	# bb57.preheader
.LBB6_20:	# bb60
	testb	$1, 7(%rsp)
	jne	.LBB6_70	# return
.LBB6_21:	# bb62
	cmpl	$111, 8(%rsp)
	jne	.LBB6_33	# bb76
.LBB6_22:	# bb62
	cmpl	$111, 16(%rsp)
	jne	.LBB6_33	# bb76
.LBB6_23:	# bb75.preheader
	testl	%r9d, %r9d
	jle	.LBB6_70	# return
.LBB6_24:	# bb.nph164
	cmpl	$0, 36(%rsp)
	cvtsi2ss	28(%rsp), %xmm1
	cvtsi2ss	20(%rsp), %xmm3
	jle	.LBB6_70	# return
.LBB6_25:	# bb73.preheader.preheader
	movl	24(%rsp), %esi
	addl	%esi, %esi
	movl	%esi, 24(%rsp)
	xorl	%ebx, %ebx
	movl	%ebx, 20(%rsp)
	jmp	.LBB6_32	# bb73.preheader
	.align	16
.LBB6_26:	# bb67
	movslq	%r14d, %r15
	leal	1(%r14), %r12d
	movslq	%r12d, %r12
	movaps	%xmm3, %xmm4
	mulss	(%r10,%r12,4), %xmm4
	movaps	%xmm0, %xmm5
	mulss	%xmm4, %xmm5
	movss	(%r10,%r15,4), %xmm6
	movaps	%xmm2, %xmm7
	mulss	%xmm6, %xmm7
	addss	%xmm5, %xmm7
	pxor	%xmm5, %xmm5
	ucomiss	%xmm5, %xmm7
	setnp	%r15b
	sete	%r12b
	andb	%r15b, %r12b
	mulss	%xmm2, %xmm4
	mulss	%xmm0, %xmm6
	subss	%xmm4, %xmm6
	ucomiss	%xmm5, %xmm6
	setnp	%r15b
	sete	%r13b
	andb	%r15b, %r13b
	testl	%ecx, %ecx
	setle	%r15b
	testb	%r12b, %r13b
	jne	.LBB6_30	# bb72
.LBB6_27:	# bb67
	testb	$1, %r15b
	jne	.LBB6_30	# bb72
.LBB6_28:	# bb.nph160
	leal	1(%rsi), %r15d
	xorl	%r12d, %r12d
	movl	%r12d, %r13d
	.align	16
.LBB6_29:	# bb70
	leal	(%r11,%r12), %ebp
	movslq	%ebp, %rbp
	movaps	%xmm1, %xmm4
	mulss	(%rdx,%rbp,4), %xmm4
	movaps	%xmm7, %xmm5
	mulss	%xmm4, %xmm5
	leal	(%rbx,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%rdx,%rbp,4), %xmm8
	movaps	%xmm6, %xmm9
	mulss	%xmm8, %xmm9
	subss	%xmm5, %xmm9
	leal	(%rsi,%r12), %ebp
	movslq	%ebp, %rbp
	addss	(%rax,%rbp,4), %xmm9
	movss	%xmm9, (%rax,%rbp,4)
	mulss	%xmm7, %xmm8
	mulss	%xmm6, %xmm4
	addss	%xmm8, %xmm4
	leal	(%r15,%r12), %ebp
	movslq	%ebp, %rbp
	addss	(%rax,%rbp,4), %xmm4
	movss	%xmm4, (%rax,%rbp,4)
	addl	$2, %r12d
	incl	%r13d
	cmpl	%ecx, %r13d
	jne	.LBB6_29	# bb70
.LBB6_30:	# bb72
	addl	28(%rsp), %r14d
	addl	%r8d, %esi
	incl	%edi
	cmpl	36(%rsp), %edi
	jne	.LBB6_26	# bb67
.LBB6_31:	# bb74
	addl	24(%rsp), %ebx
	movl	20(%rsp), %esi
	incl	%esi
	movl	%esi, 20(%rsp)
	cmpl	%r9d, %esi
	je	.LBB6_70	# return
.LBB6_32:	# bb73.preheader
	leal	1(%rbx), %r11d
	movl	20(%rsp), %esi
	leal	(%rsi,%rsi), %r14d
	movl	32(%rsp), %esi
	leal	(%rsi,%rsi), %esi
	movl	%esi, 28(%rsp)
	movl	152(%rsp), %esi
	leal	(%rsi,%rsi), %r8d
	xorl	%esi, %esi
	movl	%esi, %edi
	jmp	.LBB6_26	# bb67
.LBB6_33:	# bb76
	cmpl	$111, 8(%rsp)
	jne	.LBB6_45	# bb89
.LBB6_34:	# bb76
	cmpl	$112, 16(%rsp)
	jne	.LBB6_45	# bb89
.LBB6_35:	# bb88.preheader
	cmpl	$0, 36(%rsp)
	jle	.LBB6_70	# return
.LBB6_36:	# bb.nph158
	testl	%ecx, %ecx
	cvtsi2ss	28(%rsp), %xmm1
	cvtsi2ss	20(%rsp), %xmm3
	jle	.LBB6_70	# return
.LBB6_37:	# bb86.preheader.preheader
	movl	32(%rsp), %esi
	addl	%esi, %esi
	movl	%esi, 32(%rsp)
	movl	152(%rsp), %esi
	addl	%esi, %esi
	movl	%esi, 12(%rsp)
	xorl	%r11d, %r11d
	movl	%r11d, 28(%rsp)
	movl	%r11d, 20(%rsp)
	jmp	.LBB6_44	# bb86.preheader
.LBB6_38:	# bb.nph152
	leal	1(%rbx), %r15d
	pxor	%xmm4, %xmm4
	xorl	%r12d, %r12d
	movl	%r12d, %r13d
	movaps	%xmm4, %xmm5
	.align	16
.LBB6_39:	# bb83
	leal	(%rbx,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%rdx,%rbp,4), %xmm6
	leal	(%rdi,%r12), %ebp
	movslq	%ebp, %rbp
	movaps	%xmm3, %xmm7
	mulss	(%r10,%rbp,4), %xmm7
	movaps	%xmm7, %xmm8
	mulss	%xmm6, %xmm8
	leal	(%r15,%r12), %ebp
	movslq	%ebp, %rbp
	movaps	%xmm1, %xmm9
	mulss	(%rdx,%rbp,4), %xmm9
	leal	(%r11,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r10,%rbp,4), %xmm10
	movaps	%xmm10, %xmm11
	mulss	%xmm9, %xmm11
	addss	%xmm8, %xmm11
	addss	%xmm11, %xmm5
	mulss	%xmm7, %xmm9
	mulss	%xmm6, %xmm10
	subss	%xmm9, %xmm10
	addss	%xmm10, %xmm4
	addl	$2, %r12d
	incl	%r13d
	cmpl	%r9d, %r13d
	jne	.LBB6_39	# bb83
.LBB6_40:	# bb85
	movaps	%xmm2, %xmm6
	mulss	%xmm5, %xmm6
	movaps	%xmm0, %xmm7
	mulss	%xmm4, %xmm7
	subss	%xmm6, %xmm7
	movslq	%esi, %r15
	addss	(%rax,%r15,4), %xmm7
	movss	%xmm7, (%rax,%r15,4)
	mulss	%xmm2, %xmm4
	mulss	%xmm0, %xmm5
	addss	%xmm4, %xmm5
	leal	1(%rsi), %r15d
	movslq	%r15d, %r15
	addss	(%rax,%r15,4), %xmm5
	movss	%xmm5, (%rax,%r15,4)
	addl	%r8d, %ebx
	addl	$2, %esi
	incl	%r14d
	cmpl	%ecx, %r14d
	je	.LBB6_43	# bb87
.LBB6_41:	# bb84.preheader
	testl	%r9d, %r9d
	jg	.LBB6_38	# bb.nph152
.LBB6_42:	# bb84.preheader.bb85_crit_edge
	pxor	%xmm4, %xmm4
	movaps	%xmm4, %xmm5
	jmp	.LBB6_40	# bb85
.LBB6_43:	# bb87
	addl	32(%rsp), %r11d
	movl	28(%rsp), %esi
	addl	12(%rsp), %esi
	movl	%esi, 28(%rsp)
	movl	20(%rsp), %esi
	incl	%esi
	movl	%esi, 20(%rsp)
	cmpl	36(%rsp), %esi
	je	.LBB6_70	# return
.LBB6_44:	# bb86.preheader
	leal	1(%r11), %edi
	movl	24(%rsp), %esi
	leal	(%rsi,%rsi), %r8d
	xorl	%ebx, %ebx
	movl	28(%rsp), %esi
	movl	%ebx, %r14d
	jmp	.LBB6_41	# bb84.preheader
.LBB6_45:	# bb89
	cmpl	$112, 8(%rsp)
	jne	.LBB6_57	# bb104
.LBB6_46:	# bb89
	cmpl	$111, 16(%rsp)
	jne	.LBB6_57	# bb104
.LBB6_47:	# bb103.preheader
	testl	%r9d, %r9d
	jle	.LBB6_70	# return
.LBB6_48:	# bb.nph148
	cmpl	$0, 36(%rsp)
	cvtsi2ss	28(%rsp), %xmm1
	cvtsi2ss	20(%rsp), %xmm3
	jle	.LBB6_70	# return
.LBB6_49:	# bb101.preheader.preheader
	movl	24(%rsp), %esi
	addl	%esi, %esi
	movl	%esi, 24(%rsp)
	movl	32(%rsp), %esi
	addl	%esi, %esi
	movl	%esi, 32(%rsp)
	xorl	%esi, %esi
	movl	%esi, 28(%rsp)
	movl	%esi, 20(%rsp)
	jmp	.LBB6_56	# bb101.preheader
	.align	16
.LBB6_50:	# bb95
	movslq	%r8d, %r15
	leal	1(%r8), %r12d
	movslq	%r12d, %r12
	movaps	%xmm3, %xmm4
	mulss	(%r10,%r12,4), %xmm4
	movaps	%xmm0, %xmm5
	mulss	%xmm4, %xmm5
	movss	(%r10,%r15,4), %xmm6
	movaps	%xmm2, %xmm7
	mulss	%xmm6, %xmm7
	addss	%xmm5, %xmm7
	pxor	%xmm5, %xmm5
	ucomiss	%xmm5, %xmm7
	setnp	%r15b
	sete	%r12b
	andb	%r15b, %r12b
	mulss	%xmm2, %xmm4
	mulss	%xmm0, %xmm6
	subss	%xmm4, %xmm6
	ucomiss	%xmm5, %xmm6
	setnp	%r15b
	sete	%r13b
	andb	%r15b, %r13b
	testl	%ecx, %ecx
	setle	%r15b
	testb	%r12b, %r13b
	jne	.LBB6_54	# bb100
.LBB6_51:	# bb95
	testb	$1, %r15b
	jne	.LBB6_54	# bb100
.LBB6_52:	# bb.nph144
	leal	1(%r14), %r15d
	xorl	%r12d, %r12d
	movl	%r12d, %r13d
	.align	16
.LBB6_53:	# bb98
	leal	(%rbx,%r12), %ebp
	movslq	%ebp, %rbp
	movaps	%xmm1, %xmm4
	mulss	(%rdx,%rbp,4), %xmm4
	movaps	%xmm7, %xmm5
	mulss	%xmm4, %xmm5
	leal	(%rsi,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%rdx,%rbp,4), %xmm8
	movaps	%xmm6, %xmm9
	mulss	%xmm8, %xmm9
	subss	%xmm5, %xmm9
	leal	(%r14,%r12), %ebp
	movslq	%ebp, %rbp
	addss	(%rax,%rbp,4), %xmm9
	movss	%xmm9, (%rax,%rbp,4)
	mulss	%xmm7, %xmm8
	mulss	%xmm6, %xmm4
	addss	%xmm8, %xmm4
	leal	(%r15,%r12), %ebp
	movslq	%ebp, %rbp
	addss	(%rax,%rbp,4), %xmm4
	movss	%xmm4, (%rax,%rbp,4)
	addl	$2, %r12d
	incl	%r13d
	cmpl	%ecx, %r13d
	jne	.LBB6_53	# bb98
.LBB6_54:	# bb100
	addl	%r11d, %r14d
	addl	$2, %r8d
	incl	%edi
	cmpl	36(%rsp), %edi
	jne	.LBB6_50	# bb95
.LBB6_55:	# bb102
	addl	24(%rsp), %esi
	movl	28(%rsp), %edi
	addl	32(%rsp), %edi
	movl	%edi, 28(%rsp)
	movl	20(%rsp), %edi
	incl	%edi
	movl	%edi, 20(%rsp)
	cmpl	%r9d, %edi
	je	.LBB6_70	# return
.LBB6_56:	# bb101.preheader
	leal	1(%rsi), %ebx
	movl	152(%rsp), %edi
	leal	(%rdi,%rdi), %r11d
	xorl	%r14d, %r14d
	movl	28(%rsp), %r8d
	movl	%r14d, %edi
	jmp	.LBB6_50	# bb95
.LBB6_57:	# bb104
	cmpl	$112, 8(%rsp)
	jne	.LBB6_69	# bb117
.LBB6_58:	# bb104
	cmpl	$112, 16(%rsp)
	jne	.LBB6_69	# bb117
.LBB6_59:	# bb116.preheader
	cmpl	$0, 36(%rsp)
	jle	.LBB6_70	# return
.LBB6_60:	# bb.nph142
	testl	%ecx, %ecx
	cvtsi2ss	28(%rsp), %xmm3
	cvtsi2ss	20(%rsp), %xmm1
	jle	.LBB6_70	# return
.LBB6_61:	# bb114.preheader.preheader
	movl	152(%rsp), %esi
	addl	%esi, %esi
	movl	%esi, 12(%rsp)
	xorl	%esi, %esi
	movl	%esi, 16(%rsp)
	movl	%esi, 20(%rsp)
	jmp	.LBB6_68	# bb114.preheader
.LBB6_62:	# bb.nph136
	movl	32(%rsp), %ebx
	leal	(%rbx,%rbx), %ebx
	pxor	%xmm4, %xmm4
	xorl	%r14d, %r14d
	movl	28(%rsp), %r15d
	movl	%esi, %r12d
	movaps	%xmm4, %xmm5
	.align	16
.LBB6_63:	# bb111
	movslq	%r15d, %r13
	leal	1(%r15), %ebp
	movslq	%ebp, %rbp
	movaps	%xmm1, %xmm6
	mulss	(%r10,%rbp,4), %xmm6
	movslq	%r12d, %rbp
	movss	(%rdx,%rbp,4), %xmm7
	movaps	%xmm6, %xmm8
	mulss	%xmm7, %xmm8
	leal	1(%r12), %ebp
	movslq	%ebp, %rbp
	movaps	%xmm3, %xmm9
	mulss	(%rdx,%rbp,4), %xmm9
	movss	(%r10,%r13,4), %xmm10
	movaps	%xmm10, %xmm11
	mulss	%xmm9, %xmm11
	addss	%xmm8, %xmm11
	addss	%xmm11, %xmm4
	mulss	%xmm6, %xmm9
	mulss	%xmm7, %xmm10
	subss	%xmm9, %xmm10
	addss	%xmm10, %xmm5
	addl	%ebx, %r15d
	addl	$2, %r12d
	incl	%r14d
	cmpl	%r9d, %r14d
	jne	.LBB6_63	# bb111
.LBB6_64:	# bb113
	movaps	%xmm2, %xmm6
	mulss	%xmm4, %xmm6
	movaps	%xmm0, %xmm7
	mulss	%xmm5, %xmm7
	subss	%xmm6, %xmm7
	movslq	%r11d, %rbx
	addss	(%rax,%rbx,4), %xmm7
	movss	%xmm7, (%rax,%rbx,4)
	mulss	%xmm2, %xmm5
	mulss	%xmm0, %xmm4
	addss	%xmm5, %xmm4
	leal	1(%r11), %ebx
	movslq	%ebx, %rbx
	addss	(%rax,%rbx,4), %xmm4
	movss	%xmm4, (%rax,%rbx,4)
	addl	%r8d, %esi
	addl	$2, %r11d
	incl	%edi
	cmpl	%ecx, %edi
	je	.LBB6_67	# bb115
.LBB6_65:	# bb112.preheader
	testl	%r9d, %r9d
	jg	.LBB6_62	# bb.nph136
.LBB6_66:	# bb112.preheader.bb113_crit_edge
	pxor	%xmm4, %xmm4
	movaps	%xmm4, %xmm5
	jmp	.LBB6_64	# bb113
.LBB6_67:	# bb115
	movl	16(%rsp), %esi
	addl	12(%rsp), %esi
	movl	%esi, 16(%rsp)
	movl	20(%rsp), %esi
	incl	%esi
	movl	%esi, 20(%rsp)
	cmpl	36(%rsp), %esi
	je	.LBB6_70	# return
.LBB6_68:	# bb114.preheader
	movl	20(%rsp), %esi
	leal	(%rsi,%rsi), %esi
	movl	%esi, 28(%rsp)
	movl	24(%rsp), %esi
	leal	(%rsi,%rsi), %r8d
	xorl	%esi, %esi
	movl	16(%rsp), %r11d
	movl	%esi, %edi
	jmp	.LBB6_65	# bb112.preheader
.LBB6_69:	# bb117
	xorl	%edi, %edi
	leaq	.str2, %rsi
	leaq	.str13, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB6_70:	# return
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB6_71:	# bb18
	cmpl	$111, %edx
	movl	$111, %edi
	movl	$112, %r11d
	cmove	%edi, %r11d
	movl	%r11d, 16(%rsp)
	cmpl	$113, %edx
	movl	$4294967295, %edx
	movl	$1, %r11d
	cmove	%edx, %r11d
	movl	%r11d, 28(%rsp)
	cmpl	$111, %esi
	movl	$112, %r11d
	cmove	%edi, %r11d
	movl	%r11d, 8(%rsp)
	cmpl	$113, %esi
	movl	$1, %esi
	cmove	%edx, %esi
	movl	%esi, 20(%rsp)
	movq	%rcx, %rdx
	movl	128(%rsp), %ecx
	movl	%ecx, 24(%rsp)
	movl	%r8d, %ecx
	jmp	.LBB6_5	# bb44
	.size	cblas_cgemm, .-cblas_cgemm
.Leh_func_end2:


	.section	.rodata.cst4,"aM",@progbits,4
	.align	4
.LCPI7_0:					
	.long	1065353216	# float 1.000000e+00
	.text
	.align	16
	.globl	cblas_cgemv
	.type	cblas_cgemv,@function
cblas_cgemv:
.Leh_func_begin3:
.Llabel3:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$24, %rsp
	testl	%ecx, %ecx
	movq	104(%rsp), %rax
	movss	4(%rax), %xmm0
	movss	(%rax), %xmm1
	movss	4(%r8), %xmm2
	movss	(%r8), %xmm3
	movq	112(%rsp), %rax
	movq	88(%rsp), %r8
	je	.LBB7_63	# return
.LBB7_1:	# entry
	testl	%edx, %edx
	je	.LBB7_63	# return
.LBB7_2:	# bb
	pxor	%xmm4, %xmm4
	ucomiss	%xmm4, %xmm3
	setp	%r10b
	setne	%r11b
	setnp	%bl
	sete	%r14b
	andb	%bl, %r14b
	ucomiss	%xmm4, %xmm2
	setp	%bl
	setnp	%r15b
	sete	11(%rsp)
	setne	%r12b
	andb	%r15b, 11(%rsp)
	andb	%r14b, 11(%rsp)
	orb	%r10b, %r11b
	orb	%bl, %r12b
	orb	%r11b, %r12b
	testb	%r12b, %r12b
	jne	.LBB7_5	# bb32
.LBB7_3:	# bb
	ucomiss	.LCPI7_0(%rip), %xmm1
	jne	.LBB7_5	# bb32
	jp	.LBB7_5	# bb32
.LBB7_4:	# bb
	pxor	%xmm4, %xmm4
	ucomiss	%xmm4, %xmm0
	setnp	%r10b
	sete	%r11b
	testb	%r10b, %r11b
	jne	.LBB7_63	# return
.LBB7_5:	# bb32
	cmpl	$111, %esi
	movl	%edx, %r10d
	cmove	%ecx, %r10d
	cmove	%edx, %ecx
	pxor	%xmm4, %xmm4
	ucomiss	%xmm4, %xmm0
	jne	.LBB7_12	# bb43
	jp	.LBB7_12	# bb43
.LBB7_6:	# bb32
	pxor	%xmm4, %xmm4
	ucomiss	%xmm4, %xmm1
	jne	.LBB7_12	# bb43
	jp	.LBB7_12	# bb43
.LBB7_7:	# bb37
	cmpl	$0, 120(%rsp)
	jg	.LBB7_64	# bb37.bb42.preheader_crit_edge
.LBB7_8:	# bb38
	movl	$1, %edx
	subl	%ecx, %edx
	imull	120(%rsp), %edx
.LBB7_9:	# bb42.preheader
	testl	%ecx, %ecx
	jle	.LBB7_19	# bb51
.LBB7_10:	# bb.nph
	movl	120(%rsp), %r11d
	leal	(%r11,%r11), %r11d
	addl	%edx, %edx
	xorl	%ebx, %ebx
	.align	16
.LBB7_11:	# bb41
	movslq	%edx, %r14
	movl	$0, (%rax,%r14,4)
	leal	(%r11,%rdx), %r14d
	incl	%edx
	movslq	%edx, %rdx
	movl	$0, (%rax,%rdx,4)
	incl	%ebx
	cmpl	%ecx, %ebx
	movl	%r14d, %edx
	jne	.LBB7_11	# bb41
	jmp	.LBB7_19	# bb51
.LBB7_12:	# bb43
	pxor	%xmm4, %xmm4
	ucomiss	%xmm4, %xmm0
	jne	.LBB7_14	# bb45
	jp	.LBB7_14	# bb45
.LBB7_13:	# bb43
	ucomiss	.LCPI7_0(%rip), %xmm1
	setnp	%dl
	sete	%r11b
	testb	%dl, %r11b
	jne	.LBB7_19	# bb51
.LBB7_14:	# bb45
	cmpl	$0, 120(%rsp)
	jg	.LBB7_65	# bb45.bb50.preheader_crit_edge
.LBB7_15:	# bb46
	movl	$1, %edx
	subl	%ecx, %edx
	imull	120(%rsp), %edx
.LBB7_16:	# bb50.preheader
	testl	%ecx, %ecx
	jle	.LBB7_19	# bb51
.LBB7_17:	# bb.nph172
	movl	120(%rsp), %r11d
	leal	(%r11,%r11), %r11d
	addl	%edx, %edx
	xorl	%ebx, %ebx
	.align	16
.LBB7_18:	# bb49
	movslq	%edx, %r14
	movss	(%rax,%r14,4), %xmm4
	movaps	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	leal	1(%rdx), %r15d
	movslq	%r15d, %r15
	movss	(%rax,%r15,4), %xmm6
	movaps	%xmm6, %xmm7
	mulss	%xmm0, %xmm7
	subss	%xmm7, %xmm5
	movss	%xmm5, (%rax,%r14,4)
	mulss	%xmm0, %xmm4
	mulss	%xmm1, %xmm6
	addss	%xmm4, %xmm6
	movss	%xmm6, (%rax,%r15,4)
	addl	%r11d, %edx
	incl	%ebx
	cmpl	%ecx, %ebx
	jne	.LBB7_18	# bb49
.LBB7_19:	# bb51
	testb	$1, 11(%rsp)
	jne	.LBB7_63	# return
.LBB7_20:	# bb53
	cmpl	$111, %esi
	jne	.LBB7_22	# bb57
.LBB7_21:	# bb53
	cmpl	$101, %edi
	je	.LBB7_24	# bb61
.LBB7_22:	# bb57
	cmpl	$112, %esi
	jne	.LBB7_32	# bb73
.LBB7_23:	# bb57
	cmpl	$102, %edi
	jne	.LBB7_32	# bb73
.LBB7_24:	# bb61
	cmpl	$0, 120(%rsp)
	jg	.LBB7_66	# bb61.bb72.preheader_crit_edge
.LBB7_25:	# bb62
	movl	$1, %edx
	subl	%ecx, %edx
	imull	120(%rsp), %edx
.LBB7_26:	# bb72.preheader
	testl	%ecx, %ecx
	jle	.LBB7_63	# return
.LBB7_27:	# bb.nph169
	movl	$1, %esi
	subl	%r10d, %esi
	imull	96(%rsp), %esi
	movl	120(%rsp), %edi
	addl	%edi, %edi
	movl	%edi, 12(%rsp)
	addl	%edx, %edx
	movl	80(%rsp), %edi
	addl	%edi, %edi
	movl	%edi, 20(%rsp)
	xorl	%edi, %edi
	movl	%edi, %r11d
	.align	16
.LBB7_28:	# bb65
	cmpl	$0, 96(%rsp)
	movl	$0, %ebx
	cmovle	%esi, %ebx
	testl	%r10d, %r10d
	jle	.LBB7_67	# bb65.bb71_crit_edge
.LBB7_29:	# bb.nph164
	movl	96(%rsp), %r14d
	leal	(%r14,%r14), %r14d
	addl	%ebx, %ebx
	pxor	%xmm0, %xmm0
	xorl	%r15d, %r15d
	movl	%edi, %r12d
	movaps	%xmm0, %xmm1
	.align	16
.LBB7_30:	# bb69
	movslq	%ebx, %r13
	movss	(%r8,%r13,4), %xmm4
	movslq	%r12d, %r13
	leal	1(%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r9,%rbp,4), %xmm5
	movaps	%xmm5, %xmm6
	mulss	%xmm4, %xmm6
	movss	(%r9,%r13,4), %xmm7
	leal	1(%rbx), %r13d
	movslq	%r13d, %r13
	movss	(%r8,%r13,4), %xmm8
	movaps	%xmm7, %xmm9
	mulss	%xmm8, %xmm9
	addss	%xmm6, %xmm9
	addss	%xmm9, %xmm1
	mulss	%xmm4, %xmm7
	mulss	%xmm8, %xmm5
	subss	%xmm5, %xmm7
	addss	%xmm7, %xmm0
	addl	%r14d, %ebx
	addl	$2, %r12d
	incl	%r15d
	cmpl	%r10d, %r15d
	jne	.LBB7_30	# bb69
.LBB7_31:	# bb71
	movaps	%xmm2, %xmm4
	mulss	%xmm1, %xmm4
	movaps	%xmm3, %xmm5
	mulss	%xmm0, %xmm5
	subss	%xmm4, %xmm5
	movslq	%edx, %rbx
	addss	(%rax,%rbx,4), %xmm5
	movss	%xmm5, (%rax,%rbx,4)
	mulss	%xmm2, %xmm0
	mulss	%xmm3, %xmm1
	addss	%xmm0, %xmm1
	leal	1(%rdx), %ebx
	movslq	%ebx, %rbx
	addss	(%rax,%rbx,4), %xmm1
	movss	%xmm1, (%rax,%rbx,4)
	addl	12(%rsp), %edx
	addl	20(%rsp), %edi
	incl	%r11d
	cmpl	%ecx, %r11d
	jne	.LBB7_28	# bb65
	jmp	.LBB7_63	# return
.LBB7_32:	# bb73
	cmpl	$102, %edi
	sete	%dl
	cmpl	$111, %esi
	sete	%r11b
	andb	%dl, %r11b
	cmpl	$101, %edi
	sete	%dl
	cmpl	$112, %esi
	sete	%bl
	testb	%dl, %bl
	jne	.LBB7_34	# bb81
.LBB7_33:	# bb73
	notb	%r11b
	testb	$1, %r11b
	jne	.LBB7_42	# bb93
.LBB7_34:	# bb81
	cmpl	$0, 96(%rsp)
	jg	.LBB7_68	# bb81.bb92.preheader_crit_edge
.LBB7_35:	# bb82
	movl	$1, %edx
	subl	%r10d, %edx
	imull	96(%rsp), %edx
.LBB7_36:	# bb92.preheader
	testl	%r10d, %r10d
	jle	.LBB7_63	# return
.LBB7_37:	# bb.nph159
	movl	$1, %esi
	subl	%ecx, %esi
	imull	120(%rsp), %esi
	movl	96(%rsp), %edi
	addl	%edi, %edi
	movl	%edi, 16(%rsp)
	addl	%edx, %edx
	movl	80(%rsp), %edi
	addl	%edi, %edi
	movl	%edi, 20(%rsp)
	xorl	%edi, %edi
	movl	%edi, %r11d
	.align	16
.LBB7_38:	# bb85
	cmpl	$0, 120(%rsp)
	movl	$0, %ebx
	cmovle	%esi, %ebx
	movslq	%edx, %r14
	movss	(%r8,%r14,4), %xmm0
	movaps	%xmm2, %xmm1
	mulss	%xmm0, %xmm1
	leal	1(%rdx), %r14d
	movslq	%r14d, %r14
	movss	(%r8,%r14,4), %xmm4
	movaps	%xmm3, %xmm5
	mulss	%xmm4, %xmm5
	addss	%xmm1, %xmm5
	mulss	%xmm3, %xmm0
	mulss	%xmm2, %xmm4
	subss	%xmm4, %xmm0
	testl	%ecx, %ecx
	jle	.LBB7_41	# bb91
.LBB7_39:	# bb.nph156
	movl	120(%rsp), %r14d
	leal	(%r14,%r14), %r14d
	addl	%ebx, %ebx
	xorl	%r15d, %r15d
	movl	%edi, %r12d
	.align	16
.LBB7_40:	# bb89
	movslq	%r12d, %r13
	movss	(%r9,%r13,4), %xmm1
	movaps	%xmm1, %xmm4
	mulss	%xmm0, %xmm4
	leal	1(%r12), %r13d
	movslq	%r13d, %r13
	movss	(%r9,%r13,4), %xmm6
	movaps	%xmm6, %xmm7
	mulss	%xmm5, %xmm7
	subss	%xmm7, %xmm4
	movslq	%ebx, %r13
	addss	(%rax,%r13,4), %xmm4
	movss	%xmm4, (%rax,%r13,4)
	mulss	%xmm5, %xmm1
	mulss	%xmm0, %xmm6
	addss	%xmm1, %xmm6
	leal	1(%rbx), %r13d
	movslq	%r13d, %r13
	addss	(%rax,%r13,4), %xmm6
	movss	%xmm6, (%rax,%r13,4)
	addl	%r14d, %ebx
	addl	$2, %r12d
	incl	%r15d
	cmpl	%ecx, %r15d
	jne	.LBB7_40	# bb89
.LBB7_41:	# bb91
	addl	16(%rsp), %edx
	addl	20(%rsp), %edi
	incl	%r11d
	cmpl	%r10d, %r11d
	jne	.LBB7_38	# bb85
	jmp	.LBB7_63	# return
.LBB7_42:	# bb93
	cmpl	$113, %esi
	jne	.LBB7_52	# bb109
.LBB7_43:	# bb93
	cmpl	$101, %edi
	jne	.LBB7_52	# bb109
.LBB7_44:	# bb97
	cmpl	$0, 96(%rsp)
	jg	.LBB7_69	# bb97.bb108.preheader_crit_edge
.LBB7_45:	# bb98
	movl	$1, %edx
	subl	%r10d, %edx
	imull	96(%rsp), %edx
.LBB7_46:	# bb108.preheader
	testl	%r10d, %r10d
	jle	.LBB7_63	# return
.LBB7_47:	# bb.nph153
	movl	$1, %esi
	subl	%ecx, %esi
	imull	120(%rsp), %esi
	movl	96(%rsp), %edi
	addl	%edi, %edi
	movl	%edi, 16(%rsp)
	addl	%edx, %edx
	movl	80(%rsp), %edi
	addl	%edi, %edi
	movl	%edi, 20(%rsp)
	xorl	%edi, %edi
	movl	%edi, %r11d
	.align	16
.LBB7_48:	# bb101
	cmpl	$0, 120(%rsp)
	movl	$0, %ebx
	cmovle	%esi, %ebx
	movslq	%edx, %r14
	movss	(%r8,%r14,4), %xmm0
	movaps	%xmm2, %xmm1
	mulss	%xmm0, %xmm1
	leal	1(%rdx), %r14d
	movslq	%r14d, %r14
	movss	(%r8,%r14,4), %xmm4
	movaps	%xmm3, %xmm5
	mulss	%xmm4, %xmm5
	addss	%xmm1, %xmm5
	mulss	%xmm3, %xmm0
	mulss	%xmm2, %xmm4
	subss	%xmm4, %xmm0
	testl	%ecx, %ecx
	jle	.LBB7_51	# bb107
.LBB7_49:	# bb.nph150
	movl	120(%rsp), %r14d
	leal	(%r14,%r14), %r14d
	addl	%ebx, %ebx
	xorl	%r15d, %r15d
	movl	%edi, %r12d
	.align	16
.LBB7_50:	# bb105
	movslq	%r12d, %r13
	movss	(%r9,%r13,4), %xmm1
	movaps	%xmm1, %xmm4
	mulss	%xmm0, %xmm4
	leal	1(%r12), %r13d
	movslq	%r13d, %r13
	movss	(%r9,%r13,4), %xmm6
	movaps	%xmm6, %xmm7
	mulss	%xmm5, %xmm7
	addss	%xmm4, %xmm7
	movslq	%ebx, %r13
	addss	(%rax,%r13,4), %xmm7
	movss	%xmm7, (%rax,%r13,4)
	mulss	%xmm5, %xmm1
	mulss	%xmm0, %xmm6
	subss	%xmm6, %xmm1
	leal	1(%rbx), %r13d
	movslq	%r13d, %r13
	addss	(%rax,%r13,4), %xmm1
	movss	%xmm1, (%rax,%r13,4)
	addl	%r14d, %ebx
	addl	$2, %r12d
	incl	%r15d
	cmpl	%ecx, %r15d
	jne	.LBB7_50	# bb105
.LBB7_51:	# bb107
	addl	16(%rsp), %edx
	addl	20(%rsp), %edi
	incl	%r11d
	cmpl	%r10d, %r11d
	jne	.LBB7_48	# bb101
	jmp	.LBB7_63	# return
.LBB7_52:	# bb109
	cmpl	$113, %esi
	jne	.LBB7_62	# bb125
.LBB7_53:	# bb109
	cmpl	$102, %edi
	jne	.LBB7_62	# bb125
.LBB7_54:	# bb113
	cmpl	$0, 120(%rsp)
	jg	.LBB7_70	# bb113.bb124.preheader_crit_edge
.LBB7_55:	# bb114
	movl	$1, %edx
	subl	%ecx, %edx
	imull	120(%rsp), %edx
.LBB7_56:	# bb124.preheader
	testl	%ecx, %ecx
	jle	.LBB7_63	# return
.LBB7_57:	# bb.nph147
	movl	$1, %esi
	subl	%r10d, %esi
	imull	96(%rsp), %esi
	movl	120(%rsp), %edi
	addl	%edi, %edi
	movl	%edi, 12(%rsp)
	addl	%edx, %edx
	movl	80(%rsp), %edi
	addl	%edi, %edi
	movl	%edi, 20(%rsp)
	xorl	%edi, %edi
	movl	%edi, %r11d
	.align	16
.LBB7_58:	# bb117
	cmpl	$0, 96(%rsp)
	movl	$0, %ebx
	cmovle	%esi, %ebx
	testl	%r10d, %r10d
	jle	.LBB7_71	# bb117.bb123_crit_edge
.LBB7_59:	# bb.nph142
	movl	96(%rsp), %r14d
	leal	(%r14,%r14), %r14d
	addl	%ebx, %ebx
	pxor	%xmm0, %xmm0
	xorl	%r15d, %r15d
	movl	%edi, %r12d
	movaps	%xmm0, %xmm1
	.align	16
.LBB7_60:	# bb121
	movslq	%ebx, %r13
	movss	(%r8,%r13,4), %xmm4
	movslq	%r12d, %r13
	movss	(%r9,%r13,4), %xmm5
	movaps	%xmm5, %xmm6
	mulss	%xmm4, %xmm6
	leal	1(%r12), %r13d
	movslq	%r13d, %r13
	movss	(%r9,%r13,4), %xmm7
	mulss	%xmm7, %xmm4
	leal	1(%rbx), %r13d
	movslq	%r13d, %r13
	movss	(%r8,%r13,4), %xmm8
	mulss	%xmm8, %xmm5
	subss	%xmm4, %xmm5
	addss	%xmm5, %xmm0
	mulss	%xmm8, %xmm7
	addss	%xmm6, %xmm7
	addss	%xmm7, %xmm1
	addl	%r14d, %ebx
	addl	$2, %r12d
	incl	%r15d
	cmpl	%r10d, %r15d
	jne	.LBB7_60	# bb121
.LBB7_61:	# bb123
	movaps	%xmm2, %xmm4
	mulss	%xmm0, %xmm4
	movaps	%xmm3, %xmm5
	mulss	%xmm1, %xmm5
	subss	%xmm4, %xmm5
	movslq	%edx, %rbx
	addss	(%rax,%rbx,4), %xmm5
	movss	%xmm5, (%rax,%rbx,4)
	mulss	%xmm2, %xmm1
	mulss	%xmm3, %xmm0
	addss	%xmm1, %xmm0
	leal	1(%rdx), %ebx
	movslq	%ebx, %rbx
	addss	(%rax,%rbx,4), %xmm0
	movss	%xmm0, (%rax,%rbx,4)
	addl	12(%rsp), %edx
	addl	20(%rsp), %edi
	incl	%r11d
	cmpl	%ecx, %r11d
	jne	.LBB7_58	# bb117
	jmp	.LBB7_63	# return
.LBB7_62:	# bb125
	xorl	%edi, %edi
	leaq	.str4, %rsi
	leaq	.str15, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB7_63:	# return
	addq	$24, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB7_64:	# bb37.bb42.preheader_crit_edge
	xorl	%edx, %edx
	jmp	.LBB7_9	# bb42.preheader
.LBB7_65:	# bb45.bb50.preheader_crit_edge
	xorl	%edx, %edx
	jmp	.LBB7_16	# bb50.preheader
.LBB7_66:	# bb61.bb72.preheader_crit_edge
	xorl	%edx, %edx
	jmp	.LBB7_26	# bb72.preheader
.LBB7_67:	# bb65.bb71_crit_edge
	pxor	%xmm0, %xmm0
	movaps	%xmm0, %xmm1
	jmp	.LBB7_31	# bb71
.LBB7_68:	# bb81.bb92.preheader_crit_edge
	xorl	%edx, %edx
	jmp	.LBB7_36	# bb92.preheader
.LBB7_69:	# bb97.bb108.preheader_crit_edge
	xorl	%edx, %edx
	jmp	.LBB7_46	# bb108.preheader
.LBB7_70:	# bb113.bb124.preheader_crit_edge
	xorl	%edx, %edx
	jmp	.LBB7_56	# bb124.preheader
.LBB7_71:	# bb117.bb123_crit_edge
	pxor	%xmm0, %xmm0
	movaps	%xmm0, %xmm1
	jmp	.LBB7_61	# bb123
	.size	cblas_cgemv, .-cblas_cgemv
.Leh_func_end3:


	.align	16
	.globl	cblas_cgerc
	.type	cblas_cgerc,@function
cblas_cgerc:
.Leh_func_begin4:
.Llabel4:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$8, %rsp
	cmpl	$102, %edi
	movss	4(%rcx), %xmm0
	movss	(%rcx), %xmm1
	movq	80(%rsp), %rax
	movl	72(%rsp), %ecx
	movq	64(%rsp), %r10
	je	.LBB8_10	# bb21
.LBB8_1:	# entry
	cmpl	$101, %edi
	jne	.LBB8_18	# bb33
.LBB8_2:	# bb
	testl	%r9d, %r9d
	jg	.LBB8_20	# bb.bb19.preheader_crit_edge
.LBB8_3:	# bb9
	movl	$1, %edi
	subl	%esi, %edi
	imull	%r9d, %edi
.LBB8_4:	# bb19.preheader
	testl	%esi, %esi
	jle	.LBB8_19	# return
.LBB8_5:	# bb.nph46
	movl	$1, %r11d
	subl	%edx, %r11d
	imull	%ecx, %r11d
	addl	%r9d, %r9d
	movl	%r9d, (%rsp)
	addl	%edi, %edi
	movl	88(%rsp), %r9d
	addl	%r9d, %r9d
	movl	%r9d, 4(%rsp)
	xorl	%r9d, %r9d
	movl	%r9d, %ebx
	.align	16
.LBB8_6:	# bb12
	testl	%ecx, %ecx
	movl	$0, %r14d
	cmovle	%r11d, %r14d
	movslq	%edi, %r15
	movss	(%r8,%r15,4), %xmm2
	movaps	%xmm0, %xmm3
	mulss	%xmm2, %xmm3
	leal	1(%rdi), %r15d
	movslq	%r15d, %r15
	movss	(%r8,%r15,4), %xmm4
	movaps	%xmm1, %xmm5
	mulss	%xmm4, %xmm5
	addss	%xmm3, %xmm5
	mulss	%xmm1, %xmm2
	mulss	%xmm0, %xmm4
	subss	%xmm4, %xmm2
	testl	%edx, %edx
	jle	.LBB8_9	# bb18
.LBB8_7:	# bb.nph43
	leal	(%rcx,%rcx), %r15d
	addl	%r14d, %r14d
	xorl	%r12d, %r12d
	movl	%r9d, %r13d
	.align	16
.LBB8_8:	# bb16
	movslq	%r14d, %rbp
	movss	(%r10,%rbp,4), %xmm3
	movaps	%xmm3, %xmm4
	mulss	%xmm2, %xmm4
	leal	1(%r14), %ebp
	movslq	%ebp, %rbp
	movss	(%r10,%rbp,4), %xmm6
	movaps	%xmm5, %xmm7
	mulss	%xmm6, %xmm7
	addss	%xmm4, %xmm7
	movslq	%r13d, %rbp
	addss	(%rax,%rbp,4), %xmm7
	movss	%xmm7, (%rax,%rbp,4)
	mulss	%xmm5, %xmm3
	mulss	%xmm2, %xmm6
	subss	%xmm6, %xmm3
	leal	1(%r13), %ebp
	movslq	%ebp, %rbp
	addss	(%rax,%rbp,4), %xmm3
	movss	%xmm3, (%rax,%rbp,4)
	addl	%r15d, %r14d
	addl	$2, %r13d
	incl	%r12d
	cmpl	%edx, %r12d
	jne	.LBB8_8	# bb16
.LBB8_9:	# bb18
	addl	(%rsp), %edi
	addl	4(%rsp), %r9d
	incl	%ebx
	cmpl	%esi, %ebx
	jne	.LBB8_6	# bb12
	jmp	.LBB8_19	# return
.LBB8_10:	# bb21
	testl	%ecx, %ecx
	jg	.LBB8_21	# bb21.bb32.preheader_crit_edge
.LBB8_11:	# bb22
	movl	$1, %edi
	subl	%edx, %edi
	imull	%ecx, %edi
.LBB8_12:	# bb32.preheader
	testl	%edx, %edx
	jle	.LBB8_19	# return
.LBB8_13:	# bb.nph40
	movl	$1, %r11d
	subl	%esi, %r11d
	imull	%r9d, %r11d
	movl	%r11d, (%rsp)
	addl	%ecx, %ecx
	addl	%edi, %edi
	movl	88(%rsp), %r11d
	addl	%r11d, %r11d
	movl	%r11d, 4(%rsp)
	xorl	%r11d, %r11d
	movl	%r11d, %ebx
	.align	16
.LBB8_14:	# bb25
	testl	%r9d, %r9d
	movl	$0, %r14d
	cmovle	(%rsp), %r14d
	movslq	%edi, %r15
	movss	(%r10,%r15,4), %xmm2
	movaps	%xmm0, %xmm3
	mulss	%xmm2, %xmm3
	leal	1(%rdi), %r15d
	movslq	%r15d, %r15
	movss	(%r10,%r15,4), %xmm4
	movaps	%xmm1, %xmm5
	mulss	%xmm4, %xmm5
	subss	%xmm5, %xmm3
	mulss	%xmm1, %xmm2
	mulss	%xmm0, %xmm4
	addss	%xmm2, %xmm4
	testl	%esi, %esi
	jle	.LBB8_17	# bb31
.LBB8_15:	# bb.nph
	leal	(%r9,%r9), %r15d
	addl	%r14d, %r14d
	xorl	%r12d, %r12d
	movl	%r11d, %r13d
	.align	16
.LBB8_16:	# bb29
	movslq	%r14d, %rbp
	movss	(%r8,%rbp,4), %xmm2
	movaps	%xmm2, %xmm5
	mulss	%xmm4, %xmm5
	leal	1(%r14), %ebp
	movslq	%ebp, %rbp
	movss	(%r8,%rbp,4), %xmm6
	movaps	%xmm6, %xmm7
	mulss	%xmm3, %xmm7
	subss	%xmm7, %xmm5
	movslq	%r13d, %rbp
	addss	(%rax,%rbp,4), %xmm5
	movss	%xmm5, (%rax,%rbp,4)
	mulss	%xmm3, %xmm2
	mulss	%xmm4, %xmm6
	addss	%xmm2, %xmm6
	leal	1(%r13), %ebp
	movslq	%ebp, %rbp
	addss	(%rax,%rbp,4), %xmm6
	movss	%xmm6, (%rax,%rbp,4)
	addl	%r15d, %r14d
	addl	$2, %r13d
	incl	%r12d
	cmpl	%esi, %r12d
	jne	.LBB8_16	# bb29
.LBB8_17:	# bb31
	addl	%ecx, %edi
	addl	4(%rsp), %r11d
	incl	%ebx
	cmpl	%edx, %ebx
	jne	.LBB8_14	# bb25
	jmp	.LBB8_19	# return
.LBB8_18:	# bb33
	xorl	%edi, %edi
	leaq	.str6, %rsi
	leaq	.str17, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB8_19:	# return
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB8_20:	# bb.bb19.preheader_crit_edge
	xorl	%edi, %edi
	jmp	.LBB8_4	# bb19.preheader
.LBB8_21:	# bb21.bb32.preheader_crit_edge
	xorl	%edi, %edi
	jmp	.LBB8_12	# bb32.preheader
	.size	cblas_cgerc, .-cblas_cgerc
.Leh_func_end4:


	.align	16
	.globl	cblas_cgeru
	.type	cblas_cgeru,@function
cblas_cgeru:
.Leh_func_begin5:
.Llabel5:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$8, %rsp
	cmpl	$102, %edi
	movss	4(%rcx), %xmm0
	movss	(%rcx), %xmm1
	movq	80(%rsp), %rax
	movl	72(%rsp), %ecx
	movq	64(%rsp), %r10
	je	.LBB9_10	# bb21
.LBB9_1:	# entry
	cmpl	$101, %edi
	jne	.LBB9_18	# bb33
.LBB9_2:	# bb
	testl	%r9d, %r9d
	jg	.LBB9_20	# bb.bb19.preheader_crit_edge
.LBB9_3:	# bb9
	movl	$1, %edi
	subl	%esi, %edi
	imull	%r9d, %edi
.LBB9_4:	# bb19.preheader
	testl	%esi, %esi
	jle	.LBB9_19	# return
.LBB9_5:	# bb.nph46
	movl	$1, %r11d
	subl	%edx, %r11d
	imull	%ecx, %r11d
	addl	%r9d, %r9d
	movl	%r9d, (%rsp)
	addl	%edi, %edi
	movl	88(%rsp), %r9d
	addl	%r9d, %r9d
	movl	%r9d, 4(%rsp)
	xorl	%r9d, %r9d
	movl	%r9d, %ebx
	.align	16
.LBB9_6:	# bb12
	testl	%ecx, %ecx
	movl	$0, %r14d
	cmovle	%r11d, %r14d
	movslq	%edi, %r15
	movss	(%r8,%r15,4), %xmm2
	movaps	%xmm0, %xmm3
	mulss	%xmm2, %xmm3
	leal	1(%rdi), %r15d
	movslq	%r15d, %r15
	movss	(%r8,%r15,4), %xmm4
	movaps	%xmm1, %xmm5
	mulss	%xmm4, %xmm5
	addss	%xmm3, %xmm5
	mulss	%xmm1, %xmm2
	mulss	%xmm0, %xmm4
	subss	%xmm4, %xmm2
	testl	%edx, %edx
	jle	.LBB9_9	# bb18
.LBB9_7:	# bb.nph43
	leal	(%rcx,%rcx), %r15d
	addl	%r14d, %r14d
	xorl	%r12d, %r12d
	movl	%r9d, %r13d
	.align	16
.LBB9_8:	# bb16
	movslq	%r14d, %rbp
	movss	(%r10,%rbp,4), %xmm3
	movaps	%xmm3, %xmm4
	mulss	%xmm2, %xmm4
	leal	1(%r14), %ebp
	movslq	%ebp, %rbp
	movss	(%r10,%rbp,4), %xmm6
	movaps	%xmm6, %xmm7
	mulss	%xmm5, %xmm7
	subss	%xmm7, %xmm4
	movslq	%r13d, %rbp
	addss	(%rax,%rbp,4), %xmm4
	movss	%xmm4, (%rax,%rbp,4)
	mulss	%xmm5, %xmm3
	mulss	%xmm2, %xmm6
	addss	%xmm3, %xmm6
	leal	1(%r13), %ebp
	movslq	%ebp, %rbp
	addss	(%rax,%rbp,4), %xmm6
	movss	%xmm6, (%rax,%rbp,4)
	addl	%r15d, %r14d
	addl	$2, %r13d
	incl	%r12d
	cmpl	%edx, %r12d
	jne	.LBB9_8	# bb16
.LBB9_9:	# bb18
	addl	(%rsp), %edi
	addl	4(%rsp), %r9d
	incl	%ebx
	cmpl	%esi, %ebx
	jne	.LBB9_6	# bb12
	jmp	.LBB9_19	# return
.LBB9_10:	# bb21
	testl	%ecx, %ecx
	jg	.LBB9_21	# bb21.bb32.preheader_crit_edge
.LBB9_11:	# bb22
	movl	$1, %edi
	subl	%edx, %edi
	imull	%ecx, %edi
.LBB9_12:	# bb32.preheader
	testl	%edx, %edx
	jle	.LBB9_19	# return
.LBB9_13:	# bb.nph40
	movl	$1, %r11d
	subl	%esi, %r11d
	imull	%r9d, %r11d
	movl	%r11d, (%rsp)
	addl	%ecx, %ecx
	addl	%edi, %edi
	movl	88(%rsp), %r11d
	addl	%r11d, %r11d
	movl	%r11d, 4(%rsp)
	xorl	%r11d, %r11d
	movl	%r11d, %ebx
	.align	16
.LBB9_14:	# bb25
	testl	%r9d, %r9d
	movl	$0, %r14d
	cmovle	(%rsp), %r14d
	movslq	%edi, %r15
	movss	(%r10,%r15,4), %xmm2
	movaps	%xmm0, %xmm3
	mulss	%xmm2, %xmm3
	leal	1(%rdi), %r15d
	movslq	%r15d, %r15
	movss	(%r10,%r15,4), %xmm4
	movaps	%xmm1, %xmm5
	mulss	%xmm4, %xmm5
	addss	%xmm3, %xmm5
	mulss	%xmm1, %xmm2
	mulss	%xmm0, %xmm4
	subss	%xmm4, %xmm2
	testl	%esi, %esi
	jle	.LBB9_17	# bb31
.LBB9_15:	# bb.nph
	leal	(%r9,%r9), %r15d
	addl	%r14d, %r14d
	xorl	%r12d, %r12d
	movl	%r11d, %r13d
	.align	16
.LBB9_16:	# bb29
	movslq	%r14d, %rbp
	movss	(%r8,%rbp,4), %xmm3
	movaps	%xmm3, %xmm4
	mulss	%xmm2, %xmm4
	leal	1(%r14), %ebp
	movslq	%ebp, %rbp
	movss	(%r8,%rbp,4), %xmm6
	movaps	%xmm6, %xmm7
	mulss	%xmm5, %xmm7
	subss	%xmm7, %xmm4
	movslq	%r13d, %rbp
	addss	(%rax,%rbp,4), %xmm4
	movss	%xmm4, (%rax,%rbp,4)
	mulss	%xmm5, %xmm3
	mulss	%xmm2, %xmm6
	addss	%xmm3, %xmm6
	leal	1(%r13), %ebp
	movslq	%ebp, %rbp
	addss	(%rax,%rbp,4), %xmm6
	movss	%xmm6, (%rax,%rbp,4)
	addl	%r15d, %r14d
	addl	$2, %r13d
	incl	%r12d
	cmpl	%esi, %r12d
	jne	.LBB9_16	# bb29
.LBB9_17:	# bb31
	addl	%ecx, %edi
	addl	4(%rsp), %r11d
	incl	%ebx
	cmpl	%edx, %ebx
	jne	.LBB9_14	# bb25
	jmp	.LBB9_19	# return
.LBB9_18:	# bb33
	xorl	%edi, %edi
	leaq	.str8, %rsi
	leaq	.str19, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB9_19:	# return
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB9_20:	# bb.bb19.preheader_crit_edge
	xorl	%edi, %edi
	jmp	.LBB9_4	# bb19.preheader
.LBB9_21:	# bb21.bb32.preheader_crit_edge
	xorl	%edi, %edi
	jmp	.LBB9_12	# bb32.preheader
	.size	cblas_cgeru, .-cblas_cgeru
.Leh_func_end5:


	.section	.rodata.cst4,"aM",@progbits,4
	.align	4
.LCPI10_0:					
	.long	1065353216	# float 1.000000e+00
	.text
	.align	16
	.globl	cblas_chbmv
	.type	cblas_chbmv,@function
cblas_chbmv:
.Leh_func_begin6:
.Llabel6:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$56, %rsp
	cmpl	$102, %edi
	movl	$4294967295, %eax
	movl	$1, %r10d
	cmove	%eax, %r10d
	movl	%r10d, 32(%rsp)
	testl	%edx, %edx
	movq	136(%rsp), %rax
	movss	4(%rax), %xmm0
	movss	(%rax), %xmm1
	movss	4(%r8), %xmm2
	movss	(%r8), %xmm3
	movq	144(%rsp), %rax
	movq	120(%rsp), %r8
	movl	%ecx, 44(%rsp)
	movl	%esi, 52(%rsp)
	movl	%edi, 48(%rsp)
	je	.LBB10_46	# return
.LBB10_1:	# bb20
	pxor	%xmm4, %xmm4
	ucomiss	%xmm4, %xmm3
	setp	%cl
	setne	%sil
	setnp	%dil
	sete	%r10b
	andb	%dil, %r10b
	ucomiss	%xmm4, %xmm2
	setp	%dil
	setnp	%r11b
	sete	%bl
	setne	%r14b
	andb	%r11b, %bl
	andb	%r10b, %bl
	movb	%bl, 40(%rsp)
	orb	%cl, %sil
	orb	%dil, %r14b
	orb	%sil, %r14b
	testb	%r14b, %r14b
	jne	.LBB10_4	# bb24
.LBB10_2:	# bb20
	ucomiss	.LCPI10_0(%rip), %xmm1
	jne	.LBB10_4	# bb24
	jp	.LBB10_4	# bb24
.LBB10_3:	# bb20
	pxor	%xmm4, %xmm4
	ucomiss	%xmm4, %xmm0
	setnp	%cl
	sete	%sil
	testb	%cl, %sil
	jne	.LBB10_46	# return
.LBB10_4:	# bb24
	pxor	%xmm4, %xmm4
	ucomiss	%xmm4, %xmm0
	jne	.LBB10_11	# bb32
	jp	.LBB10_11	# bb32
.LBB10_5:	# bb24
	pxor	%xmm4, %xmm4
	ucomiss	%xmm4, %xmm1
	jne	.LBB10_11	# bb32
	jp	.LBB10_11	# bb32
.LBB10_6:	# bb26
	cmpl	$0, 152(%rsp)
	jg	.LBB10_47	# bb26.bb31.preheader_crit_edge
.LBB10_7:	# bb27
	movl	$1, %ecx
	subl	%edx, %ecx
	imull	152(%rsp), %ecx
.LBB10_8:	# bb31.preheader
	testl	%edx, %edx
	jle	.LBB10_18	# bb40
.LBB10_9:	# bb.nph
	movl	152(%rsp), %esi
	leal	(%rsi,%rsi), %esi
	addl	%ecx, %ecx
	xorl	%edi, %edi
	.align	16
.LBB10_10:	# bb30
	movslq	%ecx, %r10
	movl	$0, (%rax,%r10,4)
	leal	(%rsi,%rcx), %r10d
	incl	%ecx
	movslq	%ecx, %rcx
	movl	$0, (%rax,%rcx,4)
	incl	%edi
	cmpl	%edx, %edi
	movl	%r10d, %ecx
	jne	.LBB10_10	# bb30
	jmp	.LBB10_18	# bb40
.LBB10_11:	# bb32
	pxor	%xmm4, %xmm4
	ucomiss	%xmm4, %xmm0
	jne	.LBB10_13	# bb34
	jp	.LBB10_13	# bb34
.LBB10_12:	# bb32
	ucomiss	.LCPI10_0(%rip), %xmm1
	setnp	%cl
	sete	%sil
	testb	%cl, %sil
	jne	.LBB10_18	# bb40
.LBB10_13:	# bb34
	cmpl	$0, 152(%rsp)
	jg	.LBB10_48	# bb34.bb39.preheader_crit_edge
.LBB10_14:	# bb35
	movl	$1, %ecx
	subl	%edx, %ecx
	imull	152(%rsp), %ecx
.LBB10_15:	# bb39.preheader
	testl	%edx, %edx
	jle	.LBB10_18	# bb40
.LBB10_16:	# bb.nph153
	movl	152(%rsp), %esi
	leal	(%rsi,%rsi), %esi
	addl	%ecx, %ecx
	xorl	%edi, %edi
	.align	16
.LBB10_17:	# bb38
	movslq	%ecx, %r10
	movss	(%rax,%r10,4), %xmm4
	movaps	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	leal	1(%rcx), %r11d
	movslq	%r11d, %r11
	movss	(%rax,%r11,4), %xmm6
	movaps	%xmm6, %xmm7
	mulss	%xmm0, %xmm7
	subss	%xmm7, %xmm5
	movss	%xmm5, (%rax,%r10,4)
	mulss	%xmm0, %xmm4
	mulss	%xmm1, %xmm6
	addss	%xmm4, %xmm6
	movss	%xmm6, (%rax,%r11,4)
	addl	%esi, %ecx
	incl	%edi
	cmpl	%edx, %edi
	jne	.LBB10_17	# bb38
.LBB10_18:	# bb40
	testb	$1, 40(%rsp)
	jne	.LBB10_46	# return
.LBB10_19:	# bb42
	cmpl	$121, 52(%rsp)
	jne	.LBB10_21	# bb45
.LBB10_20:	# bb42
	cmpl	$101, 48(%rsp)
	je	.LBB10_23	# bb49
.LBB10_21:	# bb45
	cmpl	$122, 52(%rsp)
	jne	.LBB10_33	# bb67
.LBB10_22:	# bb45
	cmpl	$102, 48(%rsp)
	jne	.LBB10_33	# bb67
.LBB10_23:	# bb49
	cmpl	$0, 128(%rsp)
	jg	.LBB10_49	# bb49.bb52_crit_edge
.LBB10_24:	# bb50
	movl	$1, %ecx
	subl	%edx, %ecx
	imull	128(%rsp), %ecx
	movl	%ecx, 48(%rsp)
.LBB10_25:	# bb52
	cmpl	$0, 152(%rsp)
	jg	.LBB10_50	# bb52.bb66.preheader_crit_edge
.LBB10_26:	# bb53
	movl	$1, %ecx
	subl	%edx, %ecx
	imull	152(%rsp), %ecx
.LBB10_27:	# bb66.preheader
	testl	%edx, %edx
	jle	.LBB10_46	# return
.LBB10_28:	# bb.nph150
	movl	$1, %esi
	subl	%edx, %esi
	movl	%esi, %edi
	movl	128(%rsp), %r10d
	imull	%r10d, %edi
	movl	%edi, 8(%rsp)
	movl	152(%rsp), %edi
	imull	%edi, %esi
	movl	%esi, 4(%rsp)
	addl	%ecx, %ecx
	movl	48(%rsp), %esi
	addl	%esi, %esi
	movl	%esi, 48(%rsp)
	movl	112(%rsp), %esi
	addl	%esi, %esi
	movl	%esi, 28(%rsp)
	movl	44(%rsp), %esi
	leal	1(%rsi), %esi
	movl	%esi, 20(%rsp)
	cvtsi2ss	32(%rsp), %xmm0
	leal	(%rdi,%rdi), %esi
	movl	%esi, 16(%rsp)
	leal	(%r10,%r10), %esi
	movl	%esi, 12(%rsp)
	movl	$4294967294, 24(%rsp)
	xorl	%esi, %esi
	movl	%esi, 52(%rsp)
	movl	%r10d, 40(%rsp)
	movl	%edi, 36(%rsp)
	movl	%esi, 32(%rsp)
	.align	16
.LBB10_29:	# bb56
	movl	48(%rsp), %edi
	movslq	%edi, %r10
	movss	(%r8,%r10,4), %xmm1
	movaps	%xmm3, %xmm4
	mulss	%xmm1, %xmm4
	leal	1(%rdi), %edi
	movslq	%edi, %rdi
	movss	(%r8,%rdi,4), %xmm5
	movaps	%xmm2, %xmm6
	mulss	%xmm5, %xmm6
	subss	%xmm6, %xmm4
	movslq	52(%rsp), %rdi
	movss	(%r9,%rdi,4), %xmm6
	movaps	%xmm4, %xmm7
	mulss	%xmm6, %xmm7
	movslq	%ecx, %rdi
	addss	(%rax,%rdi,4), %xmm7
	movss	%xmm7, (%rax,%rdi,4)
	mulss	%xmm2, %xmm1
	mulss	%xmm3, %xmm5
	addss	%xmm1, %xmm5
	mulss	%xmm5, %xmm6
	incl	%ecx
	movslq	%ecx, %rcx
	addss	(%rax,%rcx,4), %xmm6
	movss	%xmm6, (%rax,%rcx,4)
	xorl	%r10d, %r10d
	cmpl	$0, 152(%rsp)
	movl	4(%rsp), %r11d
	cmovg	%r10d, %r11d
	cmpl	$0, 128(%rsp)
	cmovle	8(%rsp), %r10d
	movl	20(%rsp), %ebx
	leal	(%rbx,%rsi), %ebx
	cmpl	%edx, %ebx
	cmovg	%edx, %ebx
	leal	1(%rsi), %r14d
	cmpl	%ebx, %r14d
	jge	.LBB10_51	# bb56.bb65_crit_edge
.LBB10_30:	# bb.nph144
	movl	$4294967295, %ebx
	subl	%edx, %ebx
	movl	32(%rsp), %r14d
	negl	%r14d
	subl	44(%rsp), %r14d
	addl	$4294967294, %r14d
	cmpl	%r14d, %ebx
	cmovg	%ebx, %r14d
	movl	24(%rsp), %ebx
	subl	%r14d, %ebx
	pxor	%xmm1, %xmm1
	xorl	%r14d, %r14d
	movl	52(%rsp), %r15d
	movl	36(%rsp), %r12d
	movl	40(%rsp), %r13d
	movaps	%xmm1, %xmm6
	.align	16
.LBB10_31:	# bb63
	leal	3(%r15), %ebp
	movslq	%ebp, %rbp
	movaps	%xmm0, %xmm7
	mulss	(%r9,%rbp,4), %xmm7
	movaps	%xmm7, %xmm8
	mulss	%xmm5, %xmm8
	addl	$2, %r15d
	movslq	%r15d, %rbp
	movss	(%r9,%rbp,4), %xmm9
	movaps	%xmm4, %xmm10
	mulss	%xmm9, %xmm10
	addss	%xmm8, %xmm10
	addl	%r11d, %r12d
	leal	(%r12,%r12), %r11d
	movslq	%r11d, %r11
	addss	(%rax,%r11,4), %xmm10
	movss	%xmm10, (%rax,%r11,4)
	movaps	%xmm4, %xmm8
	mulss	%xmm7, %xmm8
	movaps	%xmm5, %xmm10
	mulss	%xmm9, %xmm10
	subss	%xmm8, %xmm10
	leal	1(,%r12,2), %r11d
	movslq	%r11d, %r11
	addss	(%rax,%r11,4), %xmm10
	movss	%xmm10, (%rax,%r11,4)
	addl	%r10d, %r13d
	leal	1(,%r13,2), %r10d
	movslq	%r10d, %r10
	movss	(%r8,%r10,4), %xmm8
	movaps	%xmm8, %xmm10
	mulss	%xmm9, %xmm10
	leal	(%r13,%r13), %r10d
	movslq	%r10d, %r10
	movss	(%r8,%r10,4), %xmm11
	movaps	%xmm11, %xmm12
	mulss	%xmm7, %xmm12
	addss	%xmm10, %xmm12
	addss	%xmm12, %xmm6
	mulss	%xmm8, %xmm7
	mulss	%xmm9, %xmm11
	subss	%xmm7, %xmm11
	addss	%xmm11, %xmm1
	incl	%r14d
	cmpl	%ebx, %r14d
	movl	152(%rsp), %r11d
	movl	128(%rsp), %r10d
	jne	.LBB10_31	# bb63
.LBB10_32:	# bb65
	movaps	%xmm2, %xmm4
	mulss	%xmm6, %xmm4
	movaps	%xmm3, %xmm5
	mulss	%xmm1, %xmm5
	subss	%xmm4, %xmm5
	addss	(%rax,%rdi,4), %xmm5
	movss	%xmm5, (%rax,%rdi,4)
	mulss	%xmm2, %xmm1
	mulss	%xmm3, %xmm6
	addss	%xmm1, %xmm6
	addss	(%rax,%rcx,4), %xmm6
	movss	%xmm6, (%rax,%rcx,4)
	movl	%edi, %ecx
	addl	16(%rsp), %ecx
	movl	48(%rsp), %edi
	addl	12(%rsp), %edi
	movl	%edi, 48(%rsp)
	movl	128(%rsp), %edi
	addl	%edi, 40(%rsp)
	movl	152(%rsp), %edi
	addl	%edi, 36(%rsp)
	movl	52(%rsp), %edi
	addl	28(%rsp), %edi
	movl	%edi, 52(%rsp)
	incl	32(%rsp)
	decl	24(%rsp)
	incl	%esi
	cmpl	%edx, %esi
	jne	.LBB10_29	# bb56
	jmp	.LBB10_46	# return
.LBB10_33:	# bb67
	movl	48(%rsp), %ecx
	cmpl	$102, %ecx
	sete	%sil
	movl	52(%rsp), %edi
	cmpl	$121, %edi
	sete	%r10b
	andb	%sil, %r10b
	cmpl	$101, %ecx
	sete	%cl
	cmpl	$122, %edi
	sete	%sil
	testb	%cl, %sil
	jne	.LBB10_35	# bb75
.LBB10_34:	# bb67
	notb	%r10b
	testb	$1, %r10b
	jne	.LBB10_45	# bb96
.LBB10_35:	# bb75
	cmpl	$0, 128(%rsp)
	jg	.LBB10_52	# bb75.bb78_crit_edge
.LBB10_36:	# bb76
	movl	$1, %ecx
	subl	%edx, %ecx
	imull	128(%rsp), %ecx
	movl	%ecx, 52(%rsp)
.LBB10_37:	# bb78
	cmpl	$0, 152(%rsp)
	jg	.LBB10_53	# bb78.bb95.preheader_crit_edge
.LBB10_38:	# bb79
	movl	$1, %ecx
	subl	%edx, %ecx
	imull	152(%rsp), %ecx
.LBB10_39:	# bb95.preheader
	testl	%edx, %edx
	jle	.LBB10_46	# return
.LBB10_40:	# bb.nph132
	movl	$1, %esi
	subl	%edx, %esi
	movl	%esi, %edi
	movl	152(%rsp), %r10d
	imull	%r10d, %edi
	movl	%edi, 20(%rsp)
	movl	128(%rsp), %edi
	imull	%edi, %esi
	movl	%esi, 12(%rsp)
	movl	52(%rsp), %esi
	addl	%esi, %esi
	movl	%esi, 52(%rsp)
	addl	%ecx, %ecx
	movl	112(%rsp), %esi
	leal	(%rsi,%rsi), %r11d
	movl	%r11d, 36(%rsp)
	decl	%esi
	movl	%esi, 28(%rsp)
	movl	44(%rsp), %esi
	leal	(%rsi,%rsi), %r11d
	movl	%r11d, 40(%rsp)
	movl	%esi, %r11d
	negl	%r11d
	movl	%r11d, 16(%rsp)
	cvtsi2ss	32(%rsp), %xmm0
	leal	(%rdi,%rdi), %edi
	movl	%edi, 32(%rsp)
	leal	(%r10,%r10), %edi
	movl	%edi, 24(%rsp)
	xorl	%edi, %edi
	movl	%esi, 48(%rsp)
	.align	16
.LBB10_41:	# bb82
	xorl	%esi, %esi
	movl	152(%rsp), %r10d
	testl	%r10d, %r10d
	movl	20(%rsp), %r11d
	cmovg	%esi, %r11d
	movl	128(%rsp), %ebx
	testl	%ebx, %ebx
	movl	12(%rsp), %r14d
	cmovg	%esi, %r14d
	movl	16(%rsp), %r15d
	leal	(%r15,%rdi), %r15d
	cmpl	44(%rsp), %edi
	cmovl	%esi, %r15d
	movl	%r15d, %esi
	imull	%r10d, %esi
	movl	%r15d, %r10d
	imull	%ebx, %r10d
	movl	52(%rsp), %ebx
	movslq	%ebx, %r12
	leal	1(%rbx), %ebx
	cmpl	%edi, %r15d
	movss	(%r8,%r12,4), %xmm1
	movaps	%xmm2, %xmm4
	mulss	%xmm1, %xmm4
	movslq	%ebx, %rbx
	movss	(%r8,%rbx,4), %xmm5
	movaps	%xmm3, %xmm6
	mulss	%xmm5, %xmm6
	addss	%xmm4, %xmm6
	mulss	%xmm3, %xmm1
	mulss	%xmm2, %xmm5
	subss	%xmm5, %xmm1
	jge	.LBB10_54	# bb82.bb94_crit_edge
.LBB10_42:	# bb.nph124
	movl	%edi, %ebx
	subl	%r15d, %ebx
	addl	48(%rsp), %r15d
	addl	%r15d, %r15d
	pxor	%xmm4, %xmm4
	xorl	%r12d, %r12d
	movaps	%xmm4, %xmm5
	.align	16
.LBB10_43:	# bb92
	movslq	%r15d, %r13
	leal	1(%r15), %ebp
	movslq	%ebp, %rbp
	movaps	%xmm0, %xmm7
	mulss	(%r9,%rbp,4), %xmm7
	movaps	%xmm7, %xmm8
	mulss	%xmm6, %xmm8
	movss	(%r9,%r13,4), %xmm9
	movaps	%xmm1, %xmm10
	mulss	%xmm9, %xmm10
	addss	%xmm8, %xmm10
	addl	%r11d, %esi
	leal	(%rsi,%rsi), %r11d
	movslq	%r11d, %r11
	addss	(%rax,%r11,4), %xmm10
	movss	%xmm10, (%rax,%r11,4)
	movaps	%xmm1, %xmm8
	mulss	%xmm7, %xmm8
	movaps	%xmm6, %xmm10
	mulss	%xmm9, %xmm10
	subss	%xmm8, %xmm10
	leal	1(,%rsi,2), %r11d
	movslq	%r11d, %r11
	addss	(%rax,%r11,4), %xmm10
	movss	%xmm10, (%rax,%r11,4)
	addl	%r14d, %r10d
	leal	1(,%r10,2), %r11d
	movslq	%r11d, %r11
	movss	(%r8,%r11,4), %xmm8
	movaps	%xmm8, %xmm10
	mulss	%xmm9, %xmm10
	leal	(%r10,%r10), %r11d
	movslq	%r11d, %r11
	movss	(%r8,%r11,4), %xmm11
	movaps	%xmm11, %xmm12
	mulss	%xmm7, %xmm12
	addss	%xmm10, %xmm12
	addss	%xmm12, %xmm4
	mulss	%xmm8, %xmm7
	mulss	%xmm9, %xmm11
	subss	%xmm7, %xmm11
	addss	%xmm11, %xmm5
	addl	$2, %r15d
	incl	%r12d
	cmpl	%ebx, %r12d
	movl	152(%rsp), %r11d
	movl	128(%rsp), %r14d
	jne	.LBB10_43	# bb92
.LBB10_44:	# bb94
	movl	40(%rsp), %esi
	movslq	%esi, %r10
	movss	(%r9,%r10,4), %xmm7
	mulss	%xmm7, %xmm1
	movslq	%ecx, %r10
	addss	(%rax,%r10,4), %xmm1
	movss	%xmm1, (%rax,%r10,4)
	mulss	%xmm7, %xmm6
	leal	1(%rcx), %r11d
	movslq	%r11d, %r11
	addss	(%rax,%r11,4), %xmm6
	movss	%xmm6, (%rax,%r11,4)
	movaps	%xmm2, %xmm1
	mulss	%xmm4, %xmm1
	movaps	%xmm3, %xmm6
	mulss	%xmm5, %xmm6
	subss	%xmm1, %xmm6
	addss	(%rax,%r10,4), %xmm6
	movss	%xmm6, (%rax,%r10,4)
	mulss	%xmm2, %xmm5
	mulss	%xmm3, %xmm4
	addss	%xmm5, %xmm4
	addss	(%rax,%r11,4), %xmm4
	movss	%xmm4, (%rax,%r11,4)
	movl	52(%rsp), %r10d
	addl	32(%rsp), %r10d
	movl	%r10d, 52(%rsp)
	addl	24(%rsp), %ecx
	addl	36(%rsp), %esi
	movl	%esi, 40(%rsp)
	movl	48(%rsp), %esi
	addl	28(%rsp), %esi
	movl	%esi, 48(%rsp)
	incl	%edi
	cmpl	%edx, %edi
	jne	.LBB10_41	# bb82
	jmp	.LBB10_46	# return
.LBB10_45:	# bb96
	xorl	%edi, %edi
	leaq	.str10, %rsi
	leaq	.str111, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB10_46:	# return
	addq	$56, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB10_47:	# bb26.bb31.preheader_crit_edge
	xorl	%ecx, %ecx
	jmp	.LBB10_8	# bb31.preheader
.LBB10_48:	# bb34.bb39.preheader_crit_edge
	xorl	%ecx, %ecx
	jmp	.LBB10_15	# bb39.preheader
.LBB10_49:	# bb49.bb52_crit_edge
	movl	$0, 48(%rsp)
	jmp	.LBB10_25	# bb52
.LBB10_50:	# bb52.bb66.preheader_crit_edge
	xorl	%ecx, %ecx
	jmp	.LBB10_27	# bb66.preheader
.LBB10_51:	# bb56.bb65_crit_edge
	pxor	%xmm1, %xmm1
	movaps	%xmm1, %xmm6
	jmp	.LBB10_32	# bb65
.LBB10_52:	# bb75.bb78_crit_edge
	movl	$0, 52(%rsp)
	jmp	.LBB10_37	# bb78
.LBB10_53:	# bb78.bb95.preheader_crit_edge
	xorl	%ecx, %ecx
	jmp	.LBB10_39	# bb95.preheader
.LBB10_54:	# bb82.bb94_crit_edge
	pxor	%xmm4, %xmm4
	movaps	%xmm4, %xmm5
	jmp	.LBB10_44	# bb94
	.size	cblas_chbmv, .-cblas_chbmv
.Leh_func_end6:


	.section	.rodata.cst4,"aM",@progbits,4
	.align	4
.LCPI11_0:					
	.long	1065353216	# float 1.000000e+00
	.text
	.align	16
	.globl	cblas_chemm
	.type	cblas_chemm,@function
cblas_chemm:
.Leh_func_begin7:
.Llabel7:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$88, %rsp
	movl	%ecx, 60(%rsp)
	movss	(%r9), %xmm0
	pxor	%xmm1, %xmm1
	ucomiss	%xmm1, %xmm0
	setp	%al
	setne	%cl
	setnp	%r10b
	sete	%r11b
	andb	%r10b, %r11b
	movss	4(%r9), %xmm2
	ucomiss	%xmm1, %xmm2
	setp	%r9b
	setnp	%r10b
	sete	%bl
	setne	%r14b
	andb	%r10b, %bl
	andb	%r11b, %bl
	movb	%bl, 80(%rsp)
	orb	%al, %cl
	orb	%r9b, %r14b
	orb	%cl, %r14b
	testb	%r14b, %r14b
	movq	176(%rsp), %rax
	movss	4(%rax), %xmm1
	movss	(%rax), %xmm3
	movq	184(%rsp), %rax
	movq	160(%rsp), %rcx
	jne	.LBB11_3	# bb31
.LBB11_1:	# entry
	ucomiss	.LCPI11_0(%rip), %xmm3
	jne	.LBB11_3	# bb31
	jp	.LBB11_3	# bb31
.LBB11_2:	# entry
	pxor	%xmm4, %xmm4
	ucomiss	%xmm4, %xmm1
	setnp	%r9b
	sete	%r10b
	testb	%r9b, %r10b
	jne	.LBB11_66	# return
.LBB11_3:	# bb31
	cmpl	$101, %edi
	je	.LBB11_67	# bb31.bb40_crit_edge
.LBB11_4:	# bb33
	cmpl	$141, %esi
	movl	$142, %edi
	movl	$141, %esi
	cmove	%edi, %esi
	cmpl	$121, %edx
	movl	$122, %edi
	movl	$121, %edx
	cmove	%edi, %edx
	movl	60(%rsp), %edi
	movl	%edi, 84(%rsp)
	movl	%r8d, 60(%rsp)
.LBB11_5:	# bb40
	pxor	%xmm4, %xmm4
	ucomiss	%xmm4, %xmm1
	jne	.LBB11_13	# bb48
	jp	.LBB11_13	# bb48
.LBB11_6:	# bb40
	pxor	%xmm4, %xmm4
	ucomiss	%xmm4, %xmm3
	jne	.LBB11_13	# bb48
	jp	.LBB11_13	# bb48
.LBB11_7:	# bb47.preheader
	cmpl	$0, 60(%rsp)
	jle	.LBB11_20	# bb56
.LBB11_8:	# bb.nph122
	cmpl	$0, 84(%rsp)
	jle	.LBB11_20	# bb56
.LBB11_9:	# bb45.preheader.preheader
	movl	192(%rsp), %edi
	leal	(%rdi,%rdi), %edi
	xorl	%r8d, %r8d
	movl	%r8d, %r9d
	jmp	.LBB11_12	# bb45.preheader
	.align	16
.LBB11_10:	# bb44
	movslq	%r10d, %rbx
	movl	$0, (%rax,%rbx,4)
	leal	1(%r10), %ebx
	movslq	%ebx, %rbx
	movl	$0, (%rax,%rbx,4)
	addl	$2, %r10d
	incl	%r11d
	cmpl	84(%rsp), %r11d
	jne	.LBB11_10	# bb44
.LBB11_11:	# bb46
	addl	%edi, %r8d
	incl	%r9d
	cmpl	60(%rsp), %r9d
	je	.LBB11_20	# bb56
.LBB11_12:	# bb45.preheader
	xorl	%r11d, %r11d
	movl	%r8d, %r10d
	jmp	.LBB11_10	# bb44
.LBB11_13:	# bb48
	pxor	%xmm4, %xmm4
	ucomiss	%xmm4, %xmm1
	setnp	%dil
	sete	%r8b
	andb	%dil, %r8b
	ucomiss	.LCPI11_0(%rip), %xmm3
	setnp	%dil
	sete	%r9b
	andb	%dil, %r9b
	testb	%r9b, %r8b
	jne	.LBB11_20	# bb56
.LBB11_14:	# bb48
	cmpl	$0, 60(%rsp)
	jle	.LBB11_20	# bb56
.LBB11_15:	# bb.nph173
	cmpl	$0, 84(%rsp)
	jle	.LBB11_20	# bb56
.LBB11_16:	# bb53.preheader.preheader
	movl	192(%rsp), %edi
	leal	(%rdi,%rdi), %r9d
	xorl	%r8d, %r8d
	movl	%r8d, %r11d
	.align	16
.LBB11_17:	# bb53.preheader
	xorl	%edi, %edi
	movl	%r8d, %r10d
	.align	16
.LBB11_18:	# bb52
	movslq	%r10d, %rbx
	movss	(%rax,%rbx,4), %xmm4
	movaps	%xmm3, %xmm5
	mulss	%xmm4, %xmm5
	leal	1(%r10), %r14d
	movslq	%r14d, %r14
	movss	(%rax,%r14,4), %xmm6
	movaps	%xmm1, %xmm7
	mulss	%xmm6, %xmm7
	subss	%xmm7, %xmm5
	movss	%xmm5, (%rax,%rbx,4)
	mulss	%xmm1, %xmm4
	mulss	%xmm3, %xmm6
	addss	%xmm4, %xmm6
	movss	%xmm6, (%rax,%r14,4)
	addl	$2, %r10d
	incl	%edi
	cmpl	84(%rsp), %edi
	jne	.LBB11_18	# bb52
.LBB11_19:	# bb54
	addl	%r9d, %r8d
	incl	%r11d
	cmpl	60(%rsp), %r11d
	jne	.LBB11_17	# bb53.preheader
.LBB11_20:	# bb56
	testb	$1, 80(%rsp)
	jne	.LBB11_66	# return
.LBB11_21:	# bb58
	cmpl	$121, %edx
	jne	.LBB11_32	# bb70
.LBB11_22:	# bb58
	cmpl	$141, %esi
	jne	.LBB11_32	# bb70
.LBB11_23:	# bb69.preheader
	cmpl	$0, 60(%rsp)
	jle	.LBB11_66	# return
.LBB11_24:	# bb.nph169
	cmpl	$0, 84(%rsp)
	jle	.LBB11_66	# return
.LBB11_25:	# bb67.preheader.preheader
	movl	152(%rsp), %edx
	leal	2(,%rdx,2), %edx
	movl	%edx, (%rsp)
	movl	168(%rsp), %edx
	leal	(%rdx,%rdx), %edx
	movl	%edx, 16(%rsp)
	movl	192(%rsp), %edx
	leal	(%rdx,%rdx), %edx
	movl	%edx, 12(%rsp)
	movl	60(%rsp), %edx
	leal	-1(%rdx), %esi
	xorl	%edx, %edx
	movl	%edx, 56(%rsp)
	movl	%edx, 76(%rsp)
	movl	%edx, 64(%rsp)
	movl	%edx, 20(%rsp)
	jmp	.LBB11_31	# bb67.preheader
	.align	16
.LBB11_26:	# bb63
	movl	40(%rsp), %edi
	leal	(%rdi,%rdx), %edi
	movslq	%edi, %rdi
	movss	(%rcx,%rdi,4), %xmm1
	movaps	%xmm2, %xmm3
	mulss	%xmm1, %xmm3
	movl	76(%rsp), %edi
	leal	(%rdi,%rdx), %edi
	movslq	%edi, %rdi
	movss	(%rcx,%rdi,4), %xmm4
	movaps	%xmm0, %xmm5
	mulss	%xmm4, %xmm5
	subss	%xmm3, %xmm5
	movq	144(%rsp), %rdi
	movq	24(%rsp), %r8
	movss	(%rdi,%r8,4), %xmm3
	movaps	%xmm5, %xmm6
	mulss	%xmm3, %xmm6
	movl	64(%rsp), %edi
	leal	(%rdi,%rdx), %edi
	movslq	%edi, %rdi
	addss	(%rax,%rdi,4), %xmm6
	movss	%xmm6, (%rax,%rdi,4)
	mulss	%xmm2, %xmm4
	mulss	%xmm0, %xmm1
	addss	%xmm4, %xmm1
	mulss	%xmm1, %xmm3
	movl	44(%rsp), %r8d
	leal	(%r8,%rdx), %r8d
	movslq	%r8d, %r8
	addss	(%rax,%r8,4), %xmm3
	movss	%xmm3, (%rax,%r8,4)
	movl	36(%rsp), %r9d
	cmpl	60(%rsp), %r9d
	jge	.LBB11_68	# bb63.bb66_crit_edge
.LBB11_27:	# bb.nph163
	movl	52(%rsp), %r9d
	leal	(%r9,%rdx), %r9d
	movl	48(%rsp), %r10d
	leal	(%r10,%rdx), %r10d
	movl	168(%rsp), %r11d
	leal	(%r11,%r11), %r11d
	movl	192(%rsp), %ebx
	leal	(%rbx,%rbx), %ebx
	pxor	%xmm3, %xmm3
	xorl	%r14d, %r14d
	movl	56(%rsp), %r15d
	movaps	%xmm3, %xmm4
	.align	16
.LBB11_28:	# bb64
	leal	3(%r15), %r12d
	movslq	%r12d, %r12
	movq	144(%rsp), %r13
	movss	(%r13,%r12,4), %xmm6
	movaps	%xmm6, %xmm7
	mulss	%xmm1, %xmm7
	addl	$2, %r15d
	movslq	%r15d, %r12
	movss	(%r13,%r12,4), %xmm8
	movaps	%xmm8, %xmm9
	mulss	%xmm5, %xmm9
	addss	%xmm7, %xmm9
	movslq	%r10d, %r12
	addss	(%rax,%r12,4), %xmm9
	movslq	%r9d, %r13
	movss	(%rcx,%r13,4), %xmm7
	leal	1(%r9), %r13d
	movslq	%r13d, %r13
	movss	(%rcx,%r13,4), %xmm10
	movss	%xmm9, (%rax,%r12,4)
	movaps	%xmm5, %xmm9
	mulss	%xmm6, %xmm9
	movaps	%xmm8, %xmm11
	mulss	%xmm1, %xmm11
	subss	%xmm9, %xmm11
	leal	1(%r10), %r12d
	movslq	%r12d, %r12
	addss	(%rax,%r12,4), %xmm11
	movss	%xmm11, (%rax,%r12,4)
	movaps	%xmm6, %xmm9
	mulss	%xmm7, %xmm9
	movaps	%xmm8, %xmm11
	mulss	%xmm10, %xmm11
	addss	%xmm9, %xmm11
	addss	%xmm11, %xmm4
	mulss	%xmm10, %xmm6
	mulss	%xmm7, %xmm8
	subss	%xmm6, %xmm8
	addss	%xmm8, %xmm3
	addl	%r11d, %r9d
	addl	%ebx, %r10d
	incl	%r14d
	cmpl	%esi, %r14d
	jne	.LBB11_28	# bb64
.LBB11_29:	# bb66
	movaps	%xmm2, %xmm1
	mulss	%xmm4, %xmm1
	movaps	%xmm0, %xmm5
	mulss	%xmm3, %xmm5
	subss	%xmm1, %xmm5
	addss	(%rax,%rdi,4), %xmm5
	movss	%xmm5, (%rax,%rdi,4)
	mulss	%xmm2, %xmm3
	mulss	%xmm0, %xmm4
	addss	%xmm3, %xmm4
	addss	(%rax,%r8,4), %xmm4
	movss	%xmm4, (%rax,%r8,4)
	addl	$2, %edx
	movl	80(%rsp), %edi
	incl	%edi
	movl	%edi, 80(%rsp)
	cmpl	84(%rsp), %edi
	jne	.LBB11_26	# bb63
.LBB11_30:	# bb68
	movl	56(%rsp), %edx
	addl	(%rsp), %edx
	movl	%edx, 56(%rsp)
	movl	76(%rsp), %edx
	addl	16(%rsp), %edx
	movl	%edx, 76(%rsp)
	movl	64(%rsp), %edx
	addl	12(%rsp), %edx
	movl	%edx, 64(%rsp)
	decl	%esi
	movl	20(%rsp), %edx
	incl	%edx
	movl	%edx, 20(%rsp)
	cmpl	60(%rsp), %edx
	je	.LBB11_66	# return
.LBB11_31:	# bb67.preheader
	movl	76(%rsp), %edi
	movl	16(%rsp), %edx
	leal	(%rdx,%rdi), %edx
	movl	%edx, 52(%rsp)
	movl	64(%rsp), %r8d
	movl	12(%rsp), %edx
	leal	(%rdx,%r8), %edx
	movl	%edx, 48(%rsp)
	leal	1(%r8), %edx
	movl	%edx, 44(%rsp)
	leal	1(%rdi), %edx
	movl	%edx, 40(%rsp)
	movl	20(%rsp), %edx
	leal	1(%rdx), %edx
	movl	%edx, 36(%rsp)
	movslq	56(%rsp), %rdx
	movq	%rdx, 24(%rsp)
	xorl	%edx, %edx
	movl	%edx, 80(%rsp)
	jmp	.LBB11_26	# bb63
.LBB11_32:	# bb70
	cmpl	$122, %edx
	jne	.LBB11_43	# bb83
.LBB11_33:	# bb70
	cmpl	$141, %esi
	jne	.LBB11_43	# bb83
.LBB11_34:	# bb82.preheader
	cmpl	$0, 60(%rsp)
	jle	.LBB11_66	# return
.LBB11_35:	# bb.nph157
	cmpl	$0, 84(%rsp)
	jle	.LBB11_66	# return
.LBB11_36:	# bb80.preheader.preheader
	movl	152(%rsp), %esi
	leal	2(,%rsi,2), %edi
	movl	%edi, 48(%rsp)
	addl	%esi, %esi
	movl	%esi, (%rsp)
	movl	168(%rsp), %esi
	leal	(%rsi,%rsi), %esi
	movl	%esi, 44(%rsp)
	movl	192(%rsp), %esi
	leal	(%rsi,%rsi), %esi
	movl	%esi, 40(%rsp)
	xorl	%esi, %esi
	movl	%esi, 64(%rsp)
	movl	%esi, 80(%rsp)
	movl	%esi, 76(%rsp)
	movl	%esi, %edi
	jmp	.LBB11_42	# bb80.preheader
	.align	16
.LBB11_37:	# bb76
	movl	80(%rsp), %r9d
	leal	(%r9,%r8), %r9d
	movslq	%r9d, %r9
	movss	(%rcx,%r9,4), %xmm1
	movaps	%xmm2, %xmm3
	mulss	%xmm1, %xmm3
	movl	52(%rsp), %r9d
	leal	(%r9,%r8), %r9d
	movslq	%r9d, %r9
	movss	(%rcx,%r9,4), %xmm4
	movaps	%xmm0, %xmm5
	mulss	%xmm4, %xmm5
	addss	%xmm3, %xmm5
	mulss	%xmm2, %xmm4
	mulss	%xmm0, %xmm1
	subss	%xmm4, %xmm1
	testl	%edi, %edi
	jle	.LBB11_69	# bb76.bb79_crit_edge
.LBB11_38:	# bb77.preheader
	movl	168(%rsp), %r9d
	leal	(%r9,%r9), %r9d
	movl	192(%rsp), %r10d
	leal	(%r10,%r10), %r10d
	pxor	%xmm3, %xmm3
	xorl	%r11d, %r11d
	movl	%r8d, %ebx
	movl	%r8d, %r14d
	movl	64(%rsp), %r15d
	movaps	%xmm3, %xmm4
	.align	16
.LBB11_39:	# bb77
	movslq	%r15d, %r12
	movq	144(%rsp), %r13
	movss	(%r13,%r12,4), %xmm6
	movaps	%xmm6, %xmm7
	mulss	%xmm1, %xmm7
	leal	1(%r15), %r12d
	movslq	%r12d, %r12
	movss	(%r13,%r12,4), %xmm8
	movaps	%xmm8, %xmm9
	mulss	%xmm5, %xmm9
	addss	%xmm7, %xmm9
	movslq	%r14d, %r12
	addss	(%rax,%r12,4), %xmm9
	movslq	%ebx, %r13
	movss	(%rcx,%r13,4), %xmm7
	leal	1(%rbx), %r13d
	movslq	%r13d, %r13
	movss	(%rcx,%r13,4), %xmm10
	movss	%xmm9, (%rax,%r12,4)
	movaps	%xmm6, %xmm9
	mulss	%xmm5, %xmm9
	movaps	%xmm1, %xmm11
	mulss	%xmm8, %xmm11
	subss	%xmm11, %xmm9
	leal	1(%r14), %r12d
	movslq	%r12d, %r12
	addss	(%rax,%r12,4), %xmm9
	movss	%xmm9, (%rax,%r12,4)
	movaps	%xmm8, %xmm9
	mulss	%xmm7, %xmm9
	movaps	%xmm6, %xmm11
	mulss	%xmm10, %xmm11
	addss	%xmm9, %xmm11
	addss	%xmm11, %xmm4
	mulss	%xmm7, %xmm6
	mulss	%xmm10, %xmm8
	subss	%xmm8, %xmm6
	addss	%xmm6, %xmm3
	addl	%r9d, %ebx
	addl	%r10d, %r14d
	addl	$2, %r15d
	incl	%r11d
	cmpl	%edi, %r11d
	jne	.LBB11_39	# bb77
.LBB11_40:	# bb79
	movl	76(%rsp), %r9d
	leal	(%r9,%r8), %r9d
	movslq	%r9d, %r9
	movq	144(%rsp), %r10
	movss	(%r10,%rdx,4), %xmm6
	mulss	%xmm6, %xmm1
	addss	(%rax,%r9,4), %xmm1
	movss	%xmm1, (%rax,%r9,4)
	movl	56(%rsp), %r10d
	leal	(%r10,%r8), %r10d
	movslq	%r10d, %r10
	mulss	%xmm6, %xmm5
	addss	(%rax,%r10,4), %xmm5
	movss	%xmm5, (%rax,%r10,4)
	movaps	%xmm2, %xmm1
	mulss	%xmm4, %xmm1
	movaps	%xmm0, %xmm5
	mulss	%xmm3, %xmm5
	subss	%xmm1, %xmm5
	addss	(%rax,%r9,4), %xmm5
	movss	%xmm5, (%rax,%r9,4)
	mulss	%xmm2, %xmm3
	mulss	%xmm0, %xmm4
	addss	%xmm3, %xmm4
	addss	(%rax,%r10,4), %xmm4
	movss	%xmm4, (%rax,%r10,4)
	addl	$2, %r8d
	incl	%esi
	cmpl	84(%rsp), %esi
	jne	.LBB11_37	# bb76
.LBB11_41:	# bb81
	movl	%edx, %esi
	addl	48(%rsp), %esi
	movl	64(%rsp), %r8d
	addl	(%rsp), %r8d
	movl	%r8d, 64(%rsp)
	movl	80(%rsp), %r8d
	addl	44(%rsp), %r8d
	movl	%r8d, 80(%rsp)
	movl	76(%rsp), %r8d
	addl	40(%rsp), %r8d
	movl	%r8d, 76(%rsp)
	incl	%edi
	cmpl	60(%rsp), %edi
	je	.LBB11_66	# return
.LBB11_42:	# bb80.preheader
	movl	76(%rsp), %r8d
	leal	1(%r8), %r8d
	movl	%r8d, 56(%rsp)
	movl	80(%rsp), %r8d
	leal	1(%r8), %r8d
	movl	%r8d, 52(%rsp)
	movslq	%esi, %rdx
	xorl	%r8d, %r8d
	movl	%r8d, %esi
	jmp	.LBB11_37	# bb76
.LBB11_43:	# bb83
	cmpl	$121, %edx
	jne	.LBB11_54	# bb96
.LBB11_44:	# bb83
	cmpl	$142, %esi
	jne	.LBB11_54	# bb96
.LBB11_45:	# bb95.preheader
	cmpl	$0, 60(%rsp)
	jle	.LBB11_66	# return
.LBB11_46:	# bb.nph145
	cmpl	$0, 84(%rsp)
	jle	.LBB11_66	# return
.LBB11_47:	# bb93.preheader.preheader
	movl	168(%rsp), %edx
	addl	%edx, %edx
	movl	%edx, 8(%rsp)
	movl	192(%rsp), %edx
	addl	%edx, %edx
	movl	%edx, 4(%rsp)
	xorl	%edx, %edx
	movl	%edx, 52(%rsp)
	movl	%edx, 56(%rsp)
	movl	%edx, 12(%rsp)
	jmp	.LBB11_53	# bb93.preheader
	.align	16
.LBB11_48:	# bb89
	movl	16(%rsp), %edi
	leal	(%rdi,%rsi), %edi
	movslq	%edi, %rdi
	movss	(%rcx,%rdi,4), %xmm1
	movaps	%xmm2, %xmm3
	mulss	%xmm1, %xmm3
	movl	52(%rsp), %edi
	leal	(%rdi,%rsi), %edi
	movslq	%edi, %rdi
	movss	(%rcx,%rdi,4), %xmm4
	movaps	%xmm0, %xmm5
	mulss	%xmm4, %xmm5
	subss	%xmm3, %xmm5
	movslq	80(%rsp), %rdi
	movq	144(%rsp), %r8
	movss	(%r8,%rdi,4), %xmm3
	movaps	%xmm5, %xmm6
	mulss	%xmm3, %xmm6
	movl	56(%rsp), %edi
	leal	(%rdi,%rsi), %edi
	movslq	%edi, %rdi
	movq	%rdi, 64(%rsp)
	addss	(%rax,%rdi,4), %xmm6
	movss	%xmm6, (%rax,%rdi,4)
	mulss	%xmm2, %xmm4
	mulss	%xmm0, %xmm1
	addss	%xmm4, %xmm1
	mulss	%xmm1, %xmm3
	movl	20(%rsp), %edi
	leal	(%rdi,%rsi), %edi
	movslq	%edi, %rdi
	addss	(%rax,%rdi,4), %xmm3
	movss	%xmm3, (%rax,%rdi,4)
	movl	76(%rsp), %r8d
	leal	1(%r8), %r8d
	cmpl	84(%rsp), %r8d
	jge	.LBB11_70	# bb89.bb92_crit_edge
.LBB11_49:	# bb.nph139
	movl	44(%rsp), %r8d
	leal	(%r8,%rsi), %r8d
	movl	40(%rsp), %r9d
	leal	(%r9,%rsi), %r9d
	movl	36(%rsp), %r10d
	leal	(%r10,%rsi), %r10d
	movl	24(%rsp), %r11d
	leal	(%r11,%rsi), %r11d
	movl	80(%rsp), %ebx
	leal	3(%rbx), %r14d
	leal	2(%rbx), %ebx
	pxor	%xmm3, %xmm3
	xorl	%r15d, %r15d
	movl	%r15d, %r12d
	movaps	%xmm3, %xmm4
	.align	16
.LBB11_50:	# bb90
	leal	(%r14,%r15), %r13d
	movslq	%r13d, %r13
	movq	144(%rsp), %rbp
	movss	(%rbp,%r13,4), %xmm6
	movaps	%xmm1, %xmm7
	mulss	%xmm6, %xmm7
	leal	(%rbx,%r15), %r13d
	movslq	%r13d, %r13
	movss	(%rbp,%r13,4), %xmm8
	movaps	%xmm5, %xmm9
	mulss	%xmm8, %xmm9
	subss	%xmm7, %xmm9
	leal	(%r9,%r15), %r13d
	movslq	%r13d, %r13
	addss	(%rax,%r13,4), %xmm9
	leal	(%r10,%r15), %ebp
	movslq	%ebp, %rbp
	movss	(%rcx,%rbp,4), %xmm7
	leal	(%r11,%r15), %ebp
	movslq	%ebp, %rbp
	movss	(%rcx,%rbp,4), %xmm10
	movss	%xmm9, (%rax,%r13,4)
	movaps	%xmm1, %xmm9
	mulss	%xmm8, %xmm9
	movaps	%xmm5, %xmm11
	mulss	%xmm6, %xmm11
	addss	%xmm9, %xmm11
	leal	(%r8,%r15), %r13d
	movslq	%r13d, %r13
	addss	(%rax,%r13,4), %xmm11
	movss	%xmm11, (%rax,%r13,4)
	movaps	%xmm10, %xmm9
	mulss	%xmm6, %xmm9
	mulss	%xmm7, %xmm6
	mulss	%xmm8, %xmm7
	subss	%xmm9, %xmm7
	addss	%xmm7, %xmm4
	mulss	%xmm8, %xmm10
	addss	%xmm6, %xmm10
	addss	%xmm10, %xmm3
	addl	$2, %r15d
	incl	%r12d
	cmpl	%edx, %r12d
	jne	.LBB11_50	# bb90
.LBB11_51:	# bb92
	movaps	%xmm2, %xmm1
	mulss	%xmm4, %xmm1
	movaps	%xmm0, %xmm5
	mulss	%xmm3, %xmm5
	subss	%xmm1, %xmm5
	movq	64(%rsp), %r8
	addss	(%rax,%r8,4), %xmm5
	movss	%xmm5, (%rax,%r8,4)
	mulss	%xmm2, %xmm3
	mulss	%xmm0, %xmm4
	addss	%xmm3, %xmm4
	addss	(%rax,%rdi,4), %xmm4
	movss	%xmm4, (%rax,%rdi,4)
	movl	80(%rsp), %edi
	addl	48(%rsp), %edi
	movl	%edi, 80(%rsp)
	addl	$2, %esi
	decl	%edx
	movl	76(%rsp), %edi
	incl	%edi
	movl	%edi, 76(%rsp)
	cmpl	84(%rsp), %edi
	jne	.LBB11_48	# bb89
.LBB11_52:	# bb94
	movl	52(%rsp), %edx
	addl	8(%rsp), %edx
	movl	%edx, 52(%rsp)
	movl	56(%rsp), %edx
	addl	4(%rsp), %edx
	movl	%edx, 56(%rsp)
	movl	12(%rsp), %edx
	incl	%edx
	movl	%edx, 12(%rsp)
	cmpl	60(%rsp), %edx
	je	.LBB11_66	# return
.LBB11_53:	# bb93.preheader
	movl	152(%rsp), %edx
	leal	2(,%rdx,2), %edx
	movl	%edx, 48(%rsp)
	movl	56(%rsp), %edx
	leal	3(%rdx), %esi
	movl	%esi, 44(%rsp)
	leal	2(%rdx), %esi
	movl	%esi, 40(%rsp)
	movl	52(%rsp), %esi
	leal	3(%rsi), %edi
	movl	%edi, 36(%rsp)
	leal	2(%rsi), %edi
	movl	%edi, 24(%rsp)
	leal	1(%rdx), %edx
	movl	%edx, 20(%rsp)
	leal	1(%rsi), %edx
	movl	%edx, 16(%rsp)
	movl	84(%rsp), %edx
	leal	-1(%rdx), %edx
	xorl	%edi, %edi
	movl	%edi, 80(%rsp)
	movl	%edi, %esi
	movl	%edi, 76(%rsp)
	jmp	.LBB11_48	# bb89
.LBB11_54:	# bb96
	cmpl	$122, %edx
	jne	.LBB11_65	# bb109
.LBB11_55:	# bb96
	cmpl	$142, %esi
	jne	.LBB11_65	# bb109
.LBB11_56:	# bb108.preheader
	cmpl	$0, 60(%rsp)
	jle	.LBB11_66	# return
.LBB11_57:	# bb.nph133
	cmpl	$0, 84(%rsp)
	jle	.LBB11_66	# return
.LBB11_58:	# bb106.preheader.preheader
	movl	168(%rsp), %edx
	addl	%edx, %edx
	movl	%edx, 8(%rsp)
	movl	192(%rsp), %edx
	addl	%edx, %edx
	movl	%edx, 4(%rsp)
	xorl	%r10d, %r10d
	movl	%r10d, %r9d
	movl	%r10d, 64(%rsp)
	jmp	.LBB11_64	# bb106.preheader
	.align	16
.LBB11_59:	# bb102
	leal	(%r10,%rdx), %r14d
	movslq	%r14d, %r14
	movss	(%rcx,%r14,4), %xmm1
	movaps	%xmm2, %xmm3
	mulss	%xmm1, %xmm3
	leal	(%rbx,%rdx), %r14d
	movslq	%r14d, %r14
	movss	(%rcx,%r14,4), %xmm4
	movaps	%xmm0, %xmm5
	mulss	%xmm4, %xmm5
	addss	%xmm3, %xmm5
	mulss	%xmm2, %xmm4
	mulss	%xmm0, %xmm1
	subss	%xmm4, %xmm1
	testl	%edi, %edi
	jle	.LBB11_71	# bb102.bb105_crit_edge
.LBB11_60:	# bb.nph126
	leal	1(%rsi), %r14d
	pxor	%xmm3, %xmm3
	xorl	%r15d, %r15d
	movl	%r15d, %r12d
	movaps	%xmm3, %xmm4
	.align	16
.LBB11_61:	# bb103
	leal	(%r14,%r15), %r13d
	movslq	%r13d, %r13
	movq	144(%rsp), %rbp
	movss	(%rbp,%r13,4), %xmm6
	movaps	%xmm5, %xmm7
	mulss	%xmm6, %xmm7
	leal	(%rsi,%r15), %r13d
	movslq	%r13d, %r13
	movss	(%rbp,%r13,4), %xmm8
	movaps	%xmm1, %xmm9
	mulss	%xmm8, %xmm9
	subss	%xmm7, %xmm9
	leal	(%r9,%r15), %r13d
	movslq	%r13d, %r13
	addss	(%rax,%r13,4), %xmm9
	leal	(%rbx,%r15), %ebp
	movslq	%ebp, %rbp
	movss	(%rcx,%rbp,4), %xmm7
	leal	(%r10,%r15), %ebp
	movslq	%ebp, %rbp
	movss	(%rcx,%rbp,4), %xmm10
	movss	%xmm9, (%rax,%r13,4)
	movaps	%xmm5, %xmm9
	mulss	%xmm8, %xmm9
	movaps	%xmm1, %xmm11
	mulss	%xmm6, %xmm11
	addss	%xmm9, %xmm11
	leal	(%r8,%r15), %r13d
	movslq	%r13d, %r13
	addss	(%rax,%r13,4), %xmm11
	movss	%xmm11, (%rax,%r13,4)
	movaps	%xmm10, %xmm9
	mulss	%xmm6, %xmm9
	mulss	%xmm7, %xmm6
	mulss	%xmm8, %xmm7
	subss	%xmm9, %xmm7
	addss	%xmm7, %xmm4
	mulss	%xmm8, %xmm10
	addss	%xmm6, %xmm10
	addss	%xmm10, %xmm3
	addl	$2, %r15d
	incl	%r12d
	cmpl	%edi, %r12d
	jne	.LBB11_61	# bb103
.LBB11_62:	# bb105
	leal	(%r9,%rdx), %r14d
	movslq	%r14d, %r14
	movslq	%r11d, %r15
	movq	144(%rsp), %r12
	movss	(%r12,%r15,4), %xmm6
	mulss	%xmm6, %xmm1
	addss	(%rax,%r14,4), %xmm1
	movss	%xmm1, (%rax,%r14,4)
	leal	(%r8,%rdx), %r15d
	movslq	%r15d, %r15
	mulss	%xmm6, %xmm5
	addss	(%rax,%r15,4), %xmm5
	movss	%xmm5, (%rax,%r15,4)
	movaps	%xmm2, %xmm1
	mulss	%xmm4, %xmm1
	movaps	%xmm0, %xmm5
	mulss	%xmm3, %xmm5
	subss	%xmm1, %xmm5
	addss	(%rax,%r14,4), %xmm5
	movss	%xmm5, (%rax,%r14,4)
	mulss	%xmm2, %xmm3
	mulss	%xmm0, %xmm4
	addss	%xmm3, %xmm4
	addss	(%rax,%r15,4), %xmm4
	movss	%xmm4, (%rax,%r15,4)
	addl	80(%rsp), %r11d
	addl	76(%rsp), %esi
	addl	$2, %edx
	incl	%edi
	cmpl	84(%rsp), %edi
	jne	.LBB11_59	# bb102
.LBB11_63:	# bb107
	addl	8(%rsp), %r10d
	addl	4(%rsp), %r9d
	movl	64(%rsp), %edx
	incl	%edx
	movl	%edx, 64(%rsp)
	cmpl	60(%rsp), %edx
	je	.LBB11_66	# return
.LBB11_64:	# bb106.preheader
	leal	1(%r10), %ebx
	leal	1(%r9), %r8d
	movl	152(%rsp), %edx
	leal	2(,%rdx,2), %esi
	movl	%esi, 80(%rsp)
	leal	(%rdx,%rdx), %edx
	movl	%edx, 76(%rsp)
	xorl	%r11d, %r11d
	movl	%r11d, %esi
	movl	%r11d, %edx
	movl	%r11d, %edi
	jmp	.LBB11_59	# bb102
.LBB11_65:	# bb109
	xorl	%edi, %edi
	leaq	.str12, %rsi
	leaq	.str113, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB11_66:	# return
	addq	$88, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB11_67:	# bb31.bb40_crit_edge
	movl	%r8d, 84(%rsp)
	jmp	.LBB11_5	# bb40
.LBB11_68:	# bb63.bb66_crit_edge
	pxor	%xmm3, %xmm3
	movaps	%xmm3, %xmm4
	jmp	.LBB11_29	# bb66
.LBB11_69:	# bb76.bb79_crit_edge
	pxor	%xmm3, %xmm3
	movaps	%xmm3, %xmm4
	jmp	.LBB11_40	# bb79
.LBB11_70:	# bb89.bb92_crit_edge
	pxor	%xmm3, %xmm3
	movaps	%xmm3, %xmm4
	jmp	.LBB11_51	# bb92
.LBB11_71:	# bb102.bb105_crit_edge
	pxor	%xmm3, %xmm3
	movaps	%xmm3, %xmm4
	jmp	.LBB11_62	# bb105
	.size	cblas_chemm, .-cblas_chemm
.Leh_func_end7:


	.section	.rodata.cst4,"aM",@progbits,4
	.align	4
.LCPI12_0:					
	.long	1065353216	# float 1.000000e+00
	.text
	.align	16
	.globl	cblas_chemv
	.type	cblas_chemv,@function
cblas_chemv:
.Leh_func_begin8:
.Llabel8:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	movl	%r9d, 20(%rsp)
	movss	(%rcx), %xmm0
	pxor	%xmm1, %xmm1
	ucomiss	%xmm1, %xmm0
	setp	%al
	setne	%r9b
	setnp	%r10b
	sete	%r11b
	andb	%r10b, %r11b
	movss	4(%rcx), %xmm2
	ucomiss	%xmm1, %xmm2
	setp	%cl
	setnp	%r10b
	sete	%bl
	setne	%r14b
	andb	%r10b, %bl
	andb	%r11b, %bl
	orb	%al, %r9b
	orb	%cl, %r14b
	orb	%r9b, %r14b
	cmpl	$102, %edi
	movl	$4294967295, %eax
	movl	$1, %ecx
	cmove	%eax, %ecx
	movl	%ecx, 16(%rsp)
	testb	%r14b, %r14b
	movq	112(%rsp), %rax
	movss	4(%rax), %xmm1
	movss	(%rax), %xmm3
	movq	120(%rsp), %rax
	movq	96(%rsp), %rcx
	jne	.LBB12_3	# bb23
.LBB12_1:	# entry
	ucomiss	.LCPI12_0(%rip), %xmm3
	jne	.LBB12_3	# bb23
	jp	.LBB12_3	# bb23
.LBB12_2:	# entry
	pxor	%xmm4, %xmm4
	ucomiss	%xmm4, %xmm1
	setnp	%r9b
	sete	%r10b
	testb	%r9b, %r10b
	jne	.LBB12_45	# bb95.thread
.LBB12_3:	# bb23
	pxor	%xmm4, %xmm4
	ucomiss	%xmm4, %xmm1
	jne	.LBB12_10	# bb31
	jp	.LBB12_10	# bb31
.LBB12_4:	# bb23
	pxor	%xmm4, %xmm4
	ucomiss	%xmm4, %xmm3
	jne	.LBB12_10	# bb31
	jp	.LBB12_10	# bb31
.LBB12_5:	# bb25
	cmpl	$0, 128(%rsp)
	jg	.LBB12_47	# bb25.bb30.preheader_crit_edge
.LBB12_6:	# bb26
	movl	$1, %r9d
	subl	%edx, %r9d
	imull	128(%rsp), %r9d
.LBB12_7:	# bb30.preheader
	testl	%edx, %edx
	jle	.LBB12_17	# bb39
.LBB12_8:	# bb.nph
	movl	128(%rsp), %r10d
	leal	(%r10,%r10), %r10d
	addl	%r9d, %r9d
	xorl	%r11d, %r11d
	.align	16
.LBB12_9:	# bb29
	movslq	%r9d, %r14
	movl	$0, (%rax,%r14,4)
	leal	(%r10,%r9), %r14d
	incl	%r9d
	movslq	%r9d, %r9
	movl	$0, (%rax,%r9,4)
	incl	%r11d
	cmpl	%edx, %r11d
	movl	%r14d, %r9d
	jne	.LBB12_9	# bb29
	jmp	.LBB12_17	# bb39
.LBB12_10:	# bb31
	pxor	%xmm4, %xmm4
	ucomiss	%xmm4, %xmm1
	jne	.LBB12_12	# bb33
	jp	.LBB12_12	# bb33
.LBB12_11:	# bb31
	ucomiss	.LCPI12_0(%rip), %xmm3
	setnp	%r9b
	sete	%r10b
	testb	%r9b, %r10b
	jne	.LBB12_17	# bb39
.LBB12_12:	# bb33
	cmpl	$0, 128(%rsp)
	jg	.LBB12_48	# bb33.bb38.preheader_crit_edge
.LBB12_13:	# bb34
	movl	$1, %r9d
	subl	%edx, %r9d
	imull	128(%rsp), %r9d
.LBB12_14:	# bb38.preheader
	testl	%edx, %edx
	jle	.LBB12_17	# bb39
.LBB12_15:	# bb.nph140
	movl	128(%rsp), %r10d
	leal	(%r10,%r10), %r10d
	addl	%r9d, %r9d
	xorl	%r11d, %r11d
	.align	16
.LBB12_16:	# bb37
	movslq	%r9d, %r14
	movss	(%rax,%r14,4), %xmm4
	movaps	%xmm4, %xmm5
	mulss	%xmm3, %xmm5
	leal	1(%r9), %r15d
	movslq	%r15d, %r15
	movss	(%rax,%r15,4), %xmm6
	movaps	%xmm6, %xmm7
	mulss	%xmm1, %xmm7
	subss	%xmm7, %xmm5
	movss	%xmm5, (%rax,%r14,4)
	mulss	%xmm1, %xmm4
	mulss	%xmm3, %xmm6
	addss	%xmm4, %xmm6
	movss	%xmm6, (%rax,%r15,4)
	addl	%r10d, %r9d
	incl	%r11d
	cmpl	%edx, %r11d
	jne	.LBB12_16	# bb37
.LBB12_17:	# bb39
	testb	$1, %bl
	jne	.LBB12_45	# bb95.thread
.LBB12_18:	# bb41
	cmpl	$121, %esi
	jne	.LBB12_20	# bb44
.LBB12_19:	# bb41
	cmpl	$101, %edi
	je	.LBB12_22	# bb48
.LBB12_20:	# bb44
	cmpl	$122, %esi
	jne	.LBB12_32	# bb66
.LBB12_21:	# bb44
	cmpl	$102, %edi
	jne	.LBB12_32	# bb66
.LBB12_22:	# bb48
	cmpl	$0, 104(%rsp)
	jg	.LBB12_49	# bb48.bb51_crit_edge
.LBB12_23:	# bb49
	movl	$1, %esi
	subl	%edx, %esi
	imull	104(%rsp), %esi
	movl	%esi, 32(%rsp)
.LBB12_24:	# bb51
	cmpl	$0, 128(%rsp)
	jg	.LBB12_50	# bb51.bb65.preheader_crit_edge
.LBB12_25:	# bb52
	movl	$1, %esi
	subl	%edx, %esi
	imull	128(%rsp), %esi
.LBB12_26:	# bb65.preheader
	testl	%edx, %edx
	jle	.LBB12_45	# bb95.thread
.LBB12_27:	# bb.nph137
	movl	$1, %edi
	subl	%edx, %edi
	movl	%edi, %r9d
	movl	104(%rsp), %r10d
	imull	%r10d, %r9d
	movl	%r9d, 8(%rsp)
	movl	128(%rsp), %r9d
	imull	%r9d, %edi
	movl	%edi, 4(%rsp)
	addl	%esi, %esi
	movl	32(%rsp), %edi
	addl	%edi, %edi
	movl	%edi, 32(%rsp)
	movl	20(%rsp), %edi
	leal	2(,%rdi,2), %edi
	movl	%edi, 20(%rsp)
	leal	-1(%rdx), %edi
	cvtsi2ss	16(%rsp), %xmm1
	leal	(%r9,%r9), %r11d
	movl	%r11d, 16(%rsp)
	leal	(%r10,%r10), %r11d
	movl	%r11d, 12(%rsp)
	xorl	%r11d, %r11d
	movl	%r11d, 36(%rsp)
	movl	%r10d, 28(%rsp)
	movl	%r9d, 24(%rsp)
	.align	16
.LBB12_28:	# bb55
	movl	32(%rsp), %r9d
	movslq	%r9d, %r10
	movss	(%rcx,%r10,4), %xmm3
	movaps	%xmm0, %xmm4
	mulss	%xmm3, %xmm4
	leal	1(%r9), %r9d
	movslq	%r9d, %r9
	movss	(%rcx,%r9,4), %xmm5
	movaps	%xmm2, %xmm6
	mulss	%xmm5, %xmm6
	subss	%xmm6, %xmm4
	movslq	36(%rsp), %r9
	movss	(%r8,%r9,4), %xmm6
	movaps	%xmm4, %xmm7
	mulss	%xmm6, %xmm7
	movslq	%esi, %r9
	addss	(%rax,%r9,4), %xmm7
	movss	%xmm7, (%rax,%r9,4)
	mulss	%xmm2, %xmm3
	mulss	%xmm0, %xmm5
	addss	%xmm3, %xmm5
	mulss	%xmm5, %xmm6
	incl	%esi
	movslq	%esi, %rsi
	addss	(%rax,%rsi,4), %xmm6
	movss	%xmm6, (%rax,%rsi,4)
	xorl	%r10d, %r10d
	cmpl	$0, 128(%rsp)
	movl	4(%rsp), %ebx
	cmovg	%r10d, %ebx
	cmpl	$0, 104(%rsp)
	cmovle	8(%rsp), %r10d
	leal	1(%r11), %r14d
	cmpl	%edx, %r14d
	jge	.LBB12_51	# bb55.bb64_crit_edge
.LBB12_29:	# bb55.bb62_crit_edge
	pxor	%xmm3, %xmm3
	xorl	%r14d, %r14d
	movl	36(%rsp), %r15d
	movl	24(%rsp), %r12d
	movl	28(%rsp), %r13d
	movaps	%xmm3, %xmm6
	.align	16
.LBB12_30:	# bb62
	leal	3(%r15), %ebp
	movslq	%ebp, %rbp
	movaps	%xmm1, %xmm7
	mulss	(%r8,%rbp,4), %xmm7
	movaps	%xmm7, %xmm8
	mulss	%xmm5, %xmm8
	addl	$2, %r15d
	movslq	%r15d, %rbp
	movss	(%r8,%rbp,4), %xmm9
	movaps	%xmm4, %xmm10
	mulss	%xmm9, %xmm10
	addss	%xmm8, %xmm10
	addl	%ebx, %r12d
	leal	(%r12,%r12), %ebx
	movslq	%ebx, %rbx
	addss	(%rax,%rbx,4), %xmm10
	movss	%xmm10, (%rax,%rbx,4)
	movaps	%xmm4, %xmm8
	mulss	%xmm7, %xmm8
	movaps	%xmm5, %xmm10
	mulss	%xmm9, %xmm10
	subss	%xmm8, %xmm10
	leal	1(,%r12,2), %ebx
	movslq	%ebx, %rbx
	addss	(%rax,%rbx,4), %xmm10
	movss	%xmm10, (%rax,%rbx,4)
	addl	%r10d, %r13d
	leal	1(,%r13,2), %r10d
	movslq	%r10d, %r10
	movss	(%rcx,%r10,4), %xmm8
	movaps	%xmm8, %xmm10
	mulss	%xmm9, %xmm10
	leal	(%r13,%r13), %r10d
	movslq	%r10d, %r10
	movss	(%rcx,%r10,4), %xmm11
	movaps	%xmm11, %xmm12
	mulss	%xmm7, %xmm12
	addss	%xmm10, %xmm12
	addss	%xmm12, %xmm6
	mulss	%xmm8, %xmm7
	mulss	%xmm9, %xmm11
	subss	%xmm7, %xmm11
	addss	%xmm11, %xmm3
	incl	%r14d
	cmpl	%edi, %r14d
	movl	128(%rsp), %ebx
	movl	104(%rsp), %r10d
	jne	.LBB12_30	# bb62
.LBB12_31:	# bb64
	movaps	%xmm2, %xmm4
	mulss	%xmm6, %xmm4
	movaps	%xmm0, %xmm5
	mulss	%xmm3, %xmm5
	subss	%xmm4, %xmm5
	addss	(%rax,%r9,4), %xmm5
	movss	%xmm5, (%rax,%r9,4)
	mulss	%xmm2, %xmm3
	mulss	%xmm0, %xmm6
	addss	%xmm3, %xmm6
	addss	(%rax,%rsi,4), %xmm6
	movss	%xmm6, (%rax,%rsi,4)
	movl	%r9d, %esi
	addl	16(%rsp), %esi
	movl	32(%rsp), %r9d
	addl	12(%rsp), %r9d
	movl	%r9d, 32(%rsp)
	movl	104(%rsp), %r9d
	addl	%r9d, 28(%rsp)
	movl	128(%rsp), %r9d
	addl	%r9d, 24(%rsp)
	movl	36(%rsp), %r9d
	addl	20(%rsp), %r9d
	movl	%r9d, 36(%rsp)
	decl	%edi
	incl	%r11d
	cmpl	%edx, %r11d
	jne	.LBB12_28	# bb55
	jmp	.LBB12_45	# bb95.thread
.LBB12_32:	# bb66
	cmpl	$102, %edi
	sete	%r9b
	cmpl	$121, %esi
	sete	%r10b
	andb	%r9b, %r10b
	cmpl	$101, %edi
	sete	%dil
	cmpl	$122, %esi
	sete	%sil
	testb	%dil, %sil
	jne	.LBB12_34	# bb74
.LBB12_33:	# bb66
	notb	%r10b
	testb	$1, %r10b
	jne	.LBB12_46	# bb97
.LBB12_34:	# bb74
	cmpl	$0, 104(%rsp)
	jg	.LBB12_52	# bb74.bb77_crit_edge
.LBB12_35:	# bb75
	movl	$1, %esi
	subl	%edx, %esi
	imull	104(%rsp), %esi
	movl	%esi, 36(%rsp)
.LBB12_36:	# bb77
	cmpl	$0, 128(%rsp)
	jg	.LBB12_53	# bb77.bb80_crit_edge
.LBB12_37:	# bb78
	movl	$1, %esi
	subl	%edx, %esi
	imull	128(%rsp), %esi
.LBB12_38:	# bb80
	leal	-1(%rdx), %edi
	movl	20(%rsp), %r9d
	leal	2(,%r9,2), %r10d
	movl	%r9d, %r11d
	imull	%edi, %r11d
	movl	104(%rsp), %ebx
	movl	%ebx, %r14d
	imull	%edi, %r14d
	movl	128(%rsp), %r15d
	movl	%r15d, %r12d
	imull	%edi, %r12d
	imull	%edi, %r10d
	movl	%r10d, 32(%rsp)
	addl	%r9d, %r9d
	movl	%r9d, 20(%rsp)
	movl	$4294967294, %edi
	subl	%r9d, %edi
	movl	%edi, 4(%rsp)
	addl	%r11d, %r11d
	movl	%r11d, 28(%rsp)
	addl	%esi, %r12d
	addl	%r12d, %r12d
	movl	%r12d, 24(%rsp)
	movl	36(%rsp), %esi
	addl	%r14d, %esi
	addl	%esi, %esi
	movl	%esi, 36(%rsp)
	movl	$1, %esi
	subl	%edx, %esi
	movl	%esi, %edi
	imull	%ebx, %edi
	movl	%edi, 8(%rsp)
	imull	%r15d, %esi
	movl	%esi, (%rsp)
	cvtsi2ss	16(%rsp), %xmm1
	leal	(%r15,%r15), %esi
	movl	%esi, 16(%rsp)
	leal	(%rbx,%rbx), %esi
	movl	%esi, 12(%rsp)
	jmp	.LBB12_43	# bb91
.LBB12_39:	# bb81
	movl	36(%rsp), %edi
	movslq	%edi, %r9
	movss	(%rcx,%r9,4), %xmm3
	movaps	%xmm0, %xmm4
	mulss	%xmm3, %xmm4
	leal	1(%rdi), %edi
	movslq	%edi, %rdi
	movss	(%rcx,%rdi,4), %xmm5
	movaps	%xmm2, %xmm6
	mulss	%xmm5, %xmm6
	subss	%xmm6, %xmm4
	movslq	32(%rsp), %rdi
	movss	(%r8,%rdi,4), %xmm6
	movaps	%xmm4, %xmm7
	mulss	%xmm6, %xmm7
	movl	24(%rsp), %edi
	movslq	%edi, %r9
	addss	(%rax,%r9,4), %xmm7
	movss	%xmm7, (%rax,%r9,4)
	mulss	%xmm2, %xmm3
	mulss	%xmm0, %xmm5
	addss	%xmm3, %xmm5
	mulss	%xmm5, %xmm6
	leal	1(%rdi), %edi
	movslq	%edi, %rdi
	addss	(%rax,%rdi,4), %xmm6
	movss	%xmm6, (%rax,%rdi,4)
	xorl	%r10d, %r10d
	cmpl	$0, 128(%rsp)
	movl	(%rsp), %r11d
	cmovg	%r10d, %r11d
	cmpl	$0, 104(%rsp)
	cmovle	8(%rsp), %r10d
	testl	%esi, %esi
	jle	.LBB12_54	# bb81.bb90_crit_edge
.LBB12_40:	# bb.nph117
	movl	104(%rsp), %esi
	leal	(%rsi,%rsi), %esi
	addl	%r10d, %r10d
	movl	128(%rsp), %ebx
	leal	(%rbx,%rbx), %ebx
	addl	%r11d, %r11d
	leal	-1(%rdx), %r14d
	pxor	%xmm3, %xmm3
	xorl	%r15d, %r15d
	movl	28(%rsp), %r12d
	movaps	%xmm3, %xmm6
	.align	16
.LBB12_41:	# bb88
	movslq	%r12d, %r13
	leal	1(%r12), %ebp
	movslq	%ebp, %rbp
	movaps	%xmm1, %xmm7
	mulss	(%r8,%rbp,4), %xmm7
	movaps	%xmm7, %xmm8
	mulss	%xmm5, %xmm8
	movss	(%r8,%r13,4), %xmm9
	movaps	%xmm4, %xmm10
	mulss	%xmm9, %xmm10
	addss	%xmm8, %xmm10
	movslq	%r11d, %r13
	addss	(%rax,%r13,4), %xmm10
	movss	%xmm10, (%rax,%r13,4)
	movaps	%xmm4, %xmm8
	mulss	%xmm7, %xmm8
	movaps	%xmm5, %xmm10
	mulss	%xmm9, %xmm10
	subss	%xmm8, %xmm10
	leal	1(%r11), %r13d
	movslq	%r13d, %r13
	addss	(%rax,%r13,4), %xmm10
	movss	%xmm10, (%rax,%r13,4)
	movslq	%r10d, %r13
	leal	1(%r10), %ebp
	movslq	%ebp, %rbp
	movss	(%rcx,%rbp,4), %xmm8
	movaps	%xmm8, %xmm10
	mulss	%xmm9, %xmm10
	movss	(%rcx,%r13,4), %xmm11
	movaps	%xmm11, %xmm12
	mulss	%xmm7, %xmm12
	addss	%xmm10, %xmm12
	addss	%xmm12, %xmm3
	mulss	%xmm7, %xmm8
	mulss	%xmm9, %xmm11
	subss	%xmm8, %xmm11
	addss	%xmm11, %xmm6
	addl	%esi, %r10d
	addl	%ebx, %r11d
	addl	$2, %r12d
	incl	%r15d
	cmpl	%r14d, %r15d
	jne	.LBB12_41	# bb88
.LBB12_42:	# bb90
	movaps	%xmm2, %xmm4
	mulss	%xmm3, %xmm4
	movaps	%xmm0, %xmm5
	mulss	%xmm6, %xmm5
	subss	%xmm4, %xmm5
	addss	(%rax,%r9,4), %xmm5
	movss	%xmm5, (%rax,%r9,4)
	mulss	%xmm2, %xmm6
	mulss	%xmm0, %xmm3
	addss	%xmm6, %xmm3
	addss	(%rax,%rdi,4), %xmm3
	movss	%xmm3, (%rax,%rdi,4)
	movl	32(%rsp), %esi
	addl	4(%rsp), %esi
	movl	%esi, 32(%rsp)
	movl	24(%rsp), %esi
	subl	16(%rsp), %esi
	movl	%esi, 24(%rsp)
	movl	36(%rsp), %esi
	subl	12(%rsp), %esi
	movl	%esi, 36(%rsp)
	movl	28(%rsp), %esi
	subl	20(%rsp), %esi
	movl	%esi, 28(%rsp)
	decl	%edx
.LBB12_43:	# bb91
	testl	%edx, %edx
	jle	.LBB12_45	# bb95.thread
.LBB12_44:	# bb92
	leal	-1(%rdx), %esi
	testl	%edx, %edx
	jne	.LBB12_39	# bb81
.LBB12_45:	# bb95.thread
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB12_46:	# bb97
	xorl	%edi, %edi
	leaq	.str14, %rsi
	leaq	.str115, %rdx
	xorb	%al, %al
	call	cblas_xerbla
	jmp	.LBB12_45	# bb95.thread
.LBB12_47:	# bb25.bb30.preheader_crit_edge
	xorl	%r9d, %r9d
	jmp	.LBB12_7	# bb30.preheader
.LBB12_48:	# bb33.bb38.preheader_crit_edge
	xorl	%r9d, %r9d
	jmp	.LBB12_14	# bb38.preheader
.LBB12_49:	# bb48.bb51_crit_edge
	movl	$0, 32(%rsp)
	jmp	.LBB12_24	# bb51
.LBB12_50:	# bb51.bb65.preheader_crit_edge
	xorl	%esi, %esi
	jmp	.LBB12_26	# bb65.preheader
.LBB12_51:	# bb55.bb64_crit_edge
	pxor	%xmm3, %xmm3
	movaps	%xmm3, %xmm6
	jmp	.LBB12_31	# bb64
.LBB12_52:	# bb74.bb77_crit_edge
	movl	$0, 36(%rsp)
	jmp	.LBB12_36	# bb77
.LBB12_53:	# bb77.bb80_crit_edge
	xorl	%esi, %esi
	jmp	.LBB12_38	# bb80
.LBB12_54:	# bb81.bb90_crit_edge
	pxor	%xmm3, %xmm3
	movaps	%xmm3, %xmm6
	jmp	.LBB12_42	# bb90
	.size	cblas_chemv, .-cblas_chemv
.Leh_func_end8:


	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI13_0:					
	.long	2147483648	# float -0.000000e+00
	.long	2147483648	# float -0.000000e+00
	.long	2147483648	# float -0.000000e+00
	.long	2147483648	# float -0.000000e+00
	.text
	.align	16
	.globl	cblas_cher2
	.type	cblas_cher2,@function
cblas_cher2:
.Leh_func_begin9:
.Llabel9:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$56, %rsp
	cmpl	$102, %edi
	movl	$4294967295, %eax
	movl	$1, %r10d
	cmove	%eax, %r10d
	movl	%r10d, 52(%rsp)
	movss	4(%rcx), %xmm0
	pxor	%xmm1, %xmm1
	ucomiss	%xmm1, %xmm0
	movss	(%rcx), %xmm1
	movq	128(%rsp), %rax
	movq	112(%rsp), %rcx
	movl	%edx, 48(%rsp)
	jne	.LBB13_2	# bb20
	jp	.LBB13_2	# bb20
.LBB13_1:	# entry
	pxor	%xmm2, %xmm2
	ucomiss	%xmm2, %xmm1
	setnp	%dl
	sete	%r10b
	testb	%dl, %r10b
	jne	.LBB13_29	# return
.LBB13_2:	# bb20
	cmpl	$121, %esi
	jne	.LBB13_4	# bb23
.LBB13_3:	# bb20
	cmpl	$101, %edi
	je	.LBB13_6	# bb27
.LBB13_4:	# bb23
	cmpl	$122, %esi
	jne	.LBB13_16	# bb39
.LBB13_5:	# bb23
	cmpl	$102, %edi
	jne	.LBB13_16	# bb39
.LBB13_6:	# bb27
	testl	%r9d, %r9d
	jg	.LBB13_30	# bb27.bb30_crit_edge
.LBB13_7:	# bb28
	movl	$1, %edx
	subl	48(%rsp), %edx
	imull	%r9d, %edx
	movl	%edx, 40(%rsp)
.LBB13_8:	# bb30
	cmpl	$0, 120(%rsp)
	jg	.LBB13_31	# bb30.bb38.preheader_crit_edge
.LBB13_9:	# bb31
	movl	$1, %edx
	subl	48(%rsp), %edx
	imull	120(%rsp), %edx
	movl	%edx, 36(%rsp)
.LBB13_10:	# bb38.preheader
	cmpl	$0, 48(%rsp)
	jle	.LBB13_29	# return
.LBB13_11:	# bb.nph88
	movl	40(%rsp), %edx
	leal	(%r9,%rdx), %esi
	addl	%esi, %esi
	movl	%esi, 12(%rsp)
	movl	36(%rsp), %esi
	movl	120(%rsp), %edi
	leal	(%rdi,%rsi), %r10d
	addl	%r10d, %r10d
	movl	%r10d, 8(%rsp)
	movl	136(%rsp), %r10d
	leal	2(,%r10,2), %r10d
	movl	%r10d, 32(%rsp)
	leal	1(,%rdx,2), %r10d
	movl	%r10d, 28(%rsp)
	addl	%edx, %edx
	movl	%edx, 40(%rsp)
	leal	(%r9,%r9), %edx
	movl	%edx, 24(%rsp)
	leal	1(,%rsi,2), %edx
	movl	%edx, 20(%rsp)
	addl	%esi, %esi
	movl	%esi, 36(%rsp)
	leal	(%rdi,%rdi), %edx
	movl	%edx, 16(%rsp)
	movl	48(%rsp), %edx
	leal	-1(%rdx), %edx
	cvtsi2ss	52(%rsp), %xmm2
	movaps	%xmm0, %xmm3
	xorps	.LCPI13_0(%rip), %xmm3
	xorl	%esi, %esi
	movl	%esi, 52(%rsp)
	movl	%esi, %edi
	movl	%esi, 44(%rsp)
	.align	16
.LBB13_12:	# bb34
	movl	52(%rsp), %r10d
	movl	28(%rsp), %r11d
	leal	(%r11,%r10), %r11d
	movslq	%r11d, %r11
	movss	(%r8,%r11,4), %xmm4
	movaps	%xmm1, %xmm5
	mulss	%xmm4, %xmm5
	movl	40(%rsp), %r11d
	leal	(%r11,%r10), %r10d
	movslq	%r10d, %r10
	movss	(%r8,%r10,4), %xmm6
	movaps	%xmm0, %xmm7
	mulss	%xmm6, %xmm7
	addss	%xmm5, %xmm7
	movl	20(%rsp), %r10d
	leal	(%r10,%rdi), %r10d
	movslq	%r10d, %r10
	movss	(%rcx,%r10,4), %xmm5
	movaps	%xmm7, %xmm8
	mulss	%xmm5, %xmm8
	mulss	%xmm0, %xmm4
	mulss	%xmm1, %xmm6
	subss	%xmm4, %xmm6
	movl	36(%rsp), %r10d
	leal	(%r10,%rdi), %r10d
	movslq	%r10d, %r10
	movss	(%rcx,%r10,4), %xmm4
	movaps	%xmm6, %xmm9
	mulss	%xmm4, %xmm9
	addss	%xmm8, %xmm9
	addss	%xmm9, %xmm9
	movslq	%esi, %r10
	addss	(%rax,%r10,4), %xmm9
	movss	%xmm9, (%rax,%r10,4)
	leal	1(%rsi), %r10d
	movslq	%r10d, %r10
	movl	$0, (%rax,%r10,4)
	movaps	%xmm0, %xmm8
	mulss	%xmm5, %xmm8
	movaps	%xmm1, %xmm9
	mulss	%xmm4, %xmm9
	addss	%xmm8, %xmm9
	mulss	%xmm1, %xmm5
	mulss	%xmm3, %xmm4
	addss	%xmm5, %xmm4
	movl	44(%rsp), %r10d
	leal	1(%r10), %r10d
	cmpl	48(%rsp), %r10d
	jge	.LBB13_15	# bb37
.LBB13_13:	# bb.nph84
	movl	52(%rsp), %r11d
	movl	12(%rsp), %r10d
	leal	(%r10,%r11), %r10d
	movl	8(%rsp), %r11d
	leal	(%r11,%rdi), %r11d
	movl	120(%rsp), %ebx
	leal	(%rbx,%rbx), %ebx
	leal	(%r9,%r9), %r14d
	xorl	%r15d, %r15d
	movl	%esi, %r12d
	.align	16
.LBB13_14:	# bb35
	movslq	%r10d, %r13
	movss	(%r8,%r13,4), %xmm5
	movaps	%xmm9, %xmm8
	mulss	%xmm5, %xmm8
	leal	1(%r10), %r13d
	movslq	%r13d, %r13
	movss	(%r8,%r13,4), %xmm10
	movaps	%xmm4, %xmm11
	mulss	%xmm10, %xmm11
	addss	%xmm8, %xmm11
	movslq	%r11d, %r13
	movss	(%rcx,%r13,4), %xmm8
	movaps	%xmm6, %xmm12
	mulss	%xmm8, %xmm12
	leal	1(%r11), %r13d
	movslq	%r13d, %r13
	movss	(%rcx,%r13,4), %xmm13
	movaps	%xmm7, %xmm14
	mulss	%xmm13, %xmm14
	addss	%xmm12, %xmm14
	addss	%xmm11, %xmm14
	leal	2(%r12), %r13d
	movslq	%r13d, %rbp
	addss	(%rax,%rbp,4), %xmm14
	movss	%xmm14, (%rax,%rbp,4)
	mulss	%xmm4, %xmm5
	mulss	%xmm9, %xmm10
	subss	%xmm10, %xmm5
	mulss	%xmm7, %xmm8
	mulss	%xmm6, %xmm13
	subss	%xmm13, %xmm8
	addss	%xmm5, %xmm8
	mulss	%xmm2, %xmm8
	addl	$3, %r12d
	movslq	%r12d, %r12
	addss	(%rax,%r12,4), %xmm8
	movss	%xmm8, (%rax,%r12,4)
	addl	%ebx, %r11d
	addl	%r14d, %r10d
	incl	%r15d
	cmpl	%edx, %r15d
	movl	%r13d, %r12d
	jne	.LBB13_14	# bb35
.LBB13_15:	# bb37
	addl	32(%rsp), %esi
	movl	52(%rsp), %r10d
	addl	24(%rsp), %r10d
	movl	%r10d, 52(%rsp)
	addl	16(%rsp), %edi
	decl	%edx
	movl	44(%rsp), %r10d
	incl	%r10d
	movl	%r10d, 44(%rsp)
	cmpl	48(%rsp), %r10d
	jne	.LBB13_12	# bb34
	jmp	.LBB13_29	# return
.LBB13_16:	# bb39
	cmpl	$102, %edi
	sete	%dl
	cmpl	$121, %esi
	sete	%r10b
	andb	%dl, %r10b
	cmpl	$101, %edi
	sete	%dl
	cmpl	$122, %esi
	sete	%sil
	testb	%dl, %sil
	jne	.LBB13_18	# bb47
.LBB13_17:	# bb39
	notb	%r10b
	testb	$1, %r10b
	jne	.LBB13_28	# bb65
.LBB13_18:	# bb47
	testl	%r9d, %r9d
	jg	.LBB13_32	# bb47.bb50_crit_edge
.LBB13_19:	# bb48
	movl	$1, %edx
	subl	48(%rsp), %edx
	imull	%r9d, %edx
.LBB13_20:	# bb50
	cmpl	$0, 120(%rsp)
	jg	.LBB13_33	# bb50.bb64.preheader_crit_edge
.LBB13_21:	# bb51
	movl	$1, %esi
	subl	48(%rsp), %esi
	imull	120(%rsp), %esi
.LBB13_22:	# bb64.preheader
	cmpl	$0, 48(%rsp)
	jle	.LBB13_29	# return
.LBB13_23:	# bb.nph74
	movl	$1, %edi
	subl	48(%rsp), %edi
	movl	%edi, %r10d
	imull	%r9d, %r10d
	movl	%r10d, 36(%rsp)
	movl	120(%rsp), %r10d
	imull	%r10d, %edi
	movl	%edi, 20(%rsp)
	addl	%esi, %esi
	addl	%edx, %edx
	movl	136(%rsp), %edi
	leal	2(,%rdi,2), %r11d
	movl	%r11d, 44(%rsp)
	addl	%edi, %edi
	movl	%edi, 24(%rsp)
	cvtsi2ss	52(%rsp), %xmm2
	movaps	%xmm0, %xmm3
	xorps	.LCPI13_0(%rip), %xmm3
	movss	%xmm3, 28(%rsp)
	leal	(%r10,%r10), %edi
	movl	%edi, 40(%rsp)
	leal	(%r9,%r9), %edi
	movl	%edi, 32(%rsp)
	xorl	%edi, %edi
	movl	%edi, 52(%rsp)
	movl	%edi, %r10d
	.align	16
.LBB13_24:	# bb54
	xorl	%r11d, %r11d
	cmpl	$0, 120(%rsp)
	movl	20(%rsp), %ebx
	cmovg	%r11d, %ebx
	testl	%r9d, %r9d
	cmovle	36(%rsp), %r11d
	movslq	%esi, %r14
	movss	(%rcx,%r14,4), %xmm3
	movaps	%xmm1, %xmm4
	mulss	%xmm3, %xmm4
	movaps	%xmm3, %xmm5
	mulss	28(%rsp), %xmm5
	leal	1(%rsi), %r14d
	movslq	%r14d, %r14
	movss	(%rcx,%r14,4), %xmm6
	movaps	%xmm1, %xmm7
	mulss	%xmm6, %xmm7
	addss	%xmm5, %xmm7
	movaps	%xmm0, %xmm5
	mulss	%xmm6, %xmm5
	addss	%xmm4, %xmm5
	movslq	%edx, %r14
	movss	(%r8,%r14,4), %xmm4
	movaps	%xmm0, %xmm8
	mulss	%xmm4, %xmm8
	leal	1(%rdx), %r14d
	movslq	%r14d, %r14
	movss	(%r8,%r14,4), %xmm9
	movaps	%xmm1, %xmm10
	mulss	%xmm9, %xmm10
	addss	%xmm8, %xmm10
	mulss	%xmm1, %xmm4
	mulss	%xmm0, %xmm9
	subss	%xmm9, %xmm4
	testl	%r10d, %r10d
	jle	.LBB13_27	# bb63
.LBB13_25:	# bb.nph
	leal	(%r9,%r9), %r14d
	addl	%r11d, %r11d
	movl	120(%rsp), %r15d
	leal	(%r15,%r15), %r15d
	addl	%ebx, %ebx
	xorl	%r12d, %r12d
	movl	52(%rsp), %r13d
	.align	16
.LBB13_26:	# bb61
	movslq	%r11d, %rbp
	movss	(%r8,%rbp,4), %xmm8
	movaps	%xmm5, %xmm9
	mulss	%xmm8, %xmm9
	leal	1(%r11), %ebp
	movslq	%ebp, %rbp
	movss	(%r8,%rbp,4), %xmm11
	movaps	%xmm7, %xmm12
	mulss	%xmm11, %xmm12
	addss	%xmm9, %xmm12
	movslq	%ebx, %rbp
	movss	(%rcx,%rbp,4), %xmm9
	movaps	%xmm4, %xmm13
	mulss	%xmm9, %xmm13
	leal	1(%rbx), %ebp
	movslq	%ebp, %rbp
	movss	(%rcx,%rbp,4), %xmm14
	movaps	%xmm10, %xmm15
	mulss	%xmm14, %xmm15
	addss	%xmm13, %xmm15
	addss	%xmm12, %xmm15
	movslq	%r13d, %rbp
	addss	(%rax,%rbp,4), %xmm15
	movss	%xmm15, (%rax,%rbp,4)
	mulss	%xmm7, %xmm8
	mulss	%xmm5, %xmm11
	subss	%xmm11, %xmm8
	mulss	%xmm10, %xmm9
	mulss	%xmm4, %xmm14
	subss	%xmm14, %xmm9
	addss	%xmm8, %xmm9
	mulss	%xmm2, %xmm9
	leal	1(%r13), %ebp
	movslq	%ebp, %rbp
	addss	(%rax,%rbp,4), %xmm9
	movss	%xmm9, (%rax,%rbp,4)
	addl	%r14d, %r11d
	addl	%r15d, %ebx
	addl	$2, %r13d
	incl	%r12d
	cmpl	%r10d, %r12d
	jne	.LBB13_26	# bb61
.LBB13_27:	# bb63
	mulss	%xmm6, %xmm10
	mulss	%xmm3, %xmm4
	addss	%xmm10, %xmm4
	addss	%xmm4, %xmm4
	movslq	%edi, %r11
	addss	(%rax,%r11,4), %xmm4
	movss	%xmm4, (%rax,%r11,4)
	movl	44(%rsp), %r11d
	leal	(%r11,%rdi), %r11d
	incl	%edi
	movslq	%edi, %rdi
	movl	$0, (%rax,%rdi,4)
	addl	40(%rsp), %esi
	addl	32(%rsp), %edx
	movl	52(%rsp), %edi
	addl	24(%rsp), %edi
	movl	%edi, 52(%rsp)
	incl	%r10d
	cmpl	48(%rsp), %r10d
	movl	%r11d, %edi
	jne	.LBB13_24	# bb54
	jmp	.LBB13_29	# return
.LBB13_28:	# bb65
	xorl	%edi, %edi
	leaq	.str16, %rsi
	leaq	.str117, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB13_29:	# return
	addq	$56, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB13_30:	# bb27.bb30_crit_edge
	movl	$0, 40(%rsp)
	jmp	.LBB13_8	# bb30
.LBB13_31:	# bb30.bb38.preheader_crit_edge
	movl	$0, 36(%rsp)
	jmp	.LBB13_10	# bb38.preheader
.LBB13_32:	# bb47.bb50_crit_edge
	xorl	%edx, %edx
	jmp	.LBB13_20	# bb50
.LBB13_33:	# bb50.bb64.preheader_crit_edge
	xorl	%esi, %esi
	jmp	.LBB13_22	# bb64.preheader
	.size	cblas_cher2, .-cblas_cher2
.Leh_func_end9:


	.section	.rodata.cst4,"aM",@progbits,4
	.align	16
.LCPI14_0:					
	.long	1065353216	# float 1.000000e+00
	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI14_1:					
	.long	2147483648	# float -0.000000e+00
	.long	2147483648	# float -0.000000e+00
	.long	2147483648	# float -0.000000e+00
	.long	2147483648	# float -0.000000e+00
	.text
	.align	16
	.globl	cblas_cher2k
	.type	cblas_cher2k,@function
cblas_cher2k:
.Leh_func_begin10:
.Llabel10:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$72, %rsp
	ucomiss	.LCPI14_0(%rip), %xmm0
	movss	4(%r9), %xmm1
	movss	(%r9), %xmm2
	movq	160(%rsp), %rax
	movq	144(%rsp), %r9
	movq	128(%rsp), %r10
	movl	%r8d, 68(%rsp)
	movl	%ecx, 64(%rsp)
	jne	.LBB14_3	# bb47
	jp	.LBB14_3	# bb47
.LBB14_1:	# bb
	pxor	%xmm3, %xmm3
	ucomiss	%xmm3, %xmm2
	setnp	%cl
	sete	%r8b
	andb	%cl, %r8b
	ucomiss	%xmm3, %xmm1
	setnp	%cl
	sete	%r11b
	andb	%cl, %r11b
	testb	%r8b, %r11b
	jne	.LBB14_93	# return
.LBB14_2:	# bb
	cmpl	$0, 68(%rsp)
	je	.LBB14_93	# return
.LBB14_3:	# bb47
	cmpl	$101, %edi
	je	.LBB14_5	# bb56
.LBB14_4:	# bb49
	cmpl	$111, %edx
	movl	$113, %ecx
	movl	$111, %edx
	cmove	%ecx, %edx
	cmpl	$121, %esi
	movl	$122, %ecx
	movl	$121, %esi
	cmove	%ecx, %esi
	xorps	.LCPI14_1(%rip), %xmm1
.LBB14_5:	# bb56
	movl	%edx, 56(%rsp)
	movl	%esi, 60(%rsp)
	pxor	%xmm3, %xmm3
	ucomiss	%xmm3, %xmm0
	jne	.LBB14_25	# bb70
	jp	.LBB14_25	# bb70
.LBB14_6:	# bb57
	cmpl	$121, 60(%rsp)
	je	.LBB14_17	# bb63.preheader
.LBB14_7:	# bb69.preheader
	cmpl	$0, 64(%rsp)
	jle	.LBB14_11	# bb87
.LBB14_8:	# bb.nph172
	movl	168(%rsp), %ecx
	leal	(%rcx,%rcx), %r8d
	xorl	%esi, %esi
	movl	%esi, %edx
	jmp	.LBB14_23	# bb67.preheader
	.align	16
.LBB14_9:	# bb60
	movslq	%ecx, %rbx
	movl	$0, (%rax,%rbx,4)
	leal	1(%rcx), %ebx
	movslq	%ebx, %rbx
	movl	$0, (%rax,%rbx,4)
	addl	$2, %ecx
	incl	%edx
	cmpl	%esi, %edx
	jne	.LBB14_9	# bb60
.LBB14_10:	# bb62
	addl	%edi, %r8d
	decl	%esi
	incl	%r11d
	cmpl	64(%rsp), %r11d
	jne	.LBB14_19	# bb61.preheader
.LBB14_11:	# bb87
	pxor	%xmm0, %xmm0
	ucomiss	%xmm0, %xmm1
	jne	.LBB14_13	# bb89
	jp	.LBB14_13	# bb89
.LBB14_12:	# bb87
	pxor	%xmm0, %xmm0
	ucomiss	%xmm0, %xmm2
	setnp	%cl
	sete	%dl
	testb	%cl, %dl
	jne	.LBB14_93	# return
.LBB14_13:	# bb89
	cmpl	$121, 60(%rsp)
	jne	.LBB14_53	# bb104
.LBB14_14:	# bb89
	cmpl	$111, 56(%rsp)
	jne	.LBB14_53	# bb104
.LBB14_15:	# bb103.preheader
	cmpl	$0, 64(%rsp)
	jle	.LBB14_93	# return
.LBB14_16:	# bb.nph216
	movl	168(%rsp), %ecx
	leal	2(,%rcx,2), %ecx
	movl	%ecx, 28(%rsp)
	movl	136(%rsp), %ecx
	leal	(%rcx,%rcx), %ecx
	movl	%ecx, 36(%rsp)
	movl	152(%rsp), %ecx
	leal	(%rcx,%rcx), %ecx
	movl	%ecx, 32(%rsp)
	movl	64(%rsp), %ecx
	leal	-1(%rcx), %ecx
	movl	%ecx, 56(%rsp)
	xorl	%esi, %esi
	movl	%esi, 44(%rsp)
	movl	%esi, %ecx
	movl	%esi, %edx
	movl	%esi, 40(%rsp)
	jmp	.LBB14_51	# bb95.preheader
.LBB14_17:	# bb63.preheader
	cmpl	$0, 64(%rsp)
	jle	.LBB14_11	# bb87
.LBB14_18:	# bb.nph220
	movl	168(%rsp), %ecx
	leal	2(,%rcx,2), %edi
	xorl	%r8d, %r8d
	movl	64(%rsp), %esi
	movl	%r8d, %r11d
	.align	16
.LBB14_19:	# bb61.preheader
	cmpl	64(%rsp), %r11d
	jge	.LBB14_10	# bb62
.LBB14_20:	# bb61.preheader.bb60_crit_edge
	xorl	%edx, %edx
	movl	%r8d, %ecx
	jmp	.LBB14_9	# bb60
	.align	16
.LBB14_21:	# bb66
	movslq	%ecx, %r11
	movl	$0, (%rax,%r11,4)
	leal	1(%rcx), %r11d
	movslq	%r11d, %r11
	movl	$0, (%rax,%r11,4)
	addl	$2, %ecx
	incl	%edi
	cmpl	%edx, %edi
	jle	.LBB14_21	# bb66
.LBB14_22:	# bb68
	addl	%r8d, %esi
	incl	%edx
	cmpl	64(%rsp), %edx
	je	.LBB14_11	# bb87
.LBB14_23:	# bb67.preheader
	testl	%edx, %edx
	js	.LBB14_22	# bb68
.LBB14_24:	# bb67.preheader.bb66_crit_edge
	xorl	%edi, %edi
	movl	%esi, %ecx
	jmp	.LBB14_21	# bb66
.LBB14_25:	# bb70
	ucomiss	.LCPI14_0(%rip), %xmm0
	jne	.LBB14_29	# bb71
	jp	.LBB14_29	# bb71
.LBB14_26:	# bb86.preheader
	cmpl	$0, 64(%rsp)
	jle	.LBB14_11	# bb87
.LBB14_27:	# bb.nph
	movl	168(%rsp), %ecx
	leal	2(,%rcx,2), %ecx
	xorl	%edx, %edx
	movl	$1, %esi
	.align	16
.LBB14_28:	# bb85
	movslq	%esi, %rdi
	movl	$0, (%rax,%rdi,4)
	addl	%ecx, %esi
	incl	%edx
	cmpl	64(%rsp), %edx
	jne	.LBB14_28	# bb85
	jmp	.LBB14_11	# bb87
.LBB14_29:	# bb71
	cmpl	$121, 60(%rsp)
	jne	.LBB14_94	# bb83.preheader
.LBB14_30:	# bb77.preheader
	cmpl	$0, 64(%rsp)
	jle	.LBB14_11	# bb87
.LBB14_31:	# bb.nph168
	movl	168(%rsp), %ecx
	leal	2(,%rcx,2), %ecx
	movl	64(%rsp), %edx
	leal	-1(%rdx), %edx
	xorl	%esi, %esi
	movl	%esi, %edi
	.align	16
.LBB14_32:	# bb73
	movslq	%esi, %r8
	movaps	%xmm0, %xmm3
	mulss	(%rax,%r8,4), %xmm3
	movss	%xmm3, (%rax,%r8,4)
	leal	1(%rsi), %r8d
	movslq	%r8d, %r8
	movl	$0, (%rax,%r8,4)
	leal	1(%rdi), %r8d
	cmpl	64(%rsp), %r8d
	jge	.LBB14_35	# bb76
.LBB14_33:	# bb73.bb74_crit_edge
	xorl	%r8d, %r8d
	movl	%esi, %r11d
	.align	16
.LBB14_34:	# bb74
	leal	2(%r11), %ebx
	movslq	%ebx, %r14
	movaps	%xmm0, %xmm3
	mulss	(%rax,%r14,4), %xmm3
	movss	%xmm3, (%rax,%r14,4)
	addl	$3, %r11d
	movslq	%r11d, %r11
	movaps	%xmm0, %xmm3
	mulss	(%rax,%r11,4), %xmm3
	movss	%xmm3, (%rax,%r11,4)
	incl	%r8d
	cmpl	%edx, %r8d
	movl	%ebx, %r11d
	jne	.LBB14_34	# bb74
.LBB14_35:	# bb76
	addl	%ecx, %esi
	decl	%edx
	incl	%edi
	cmpl	64(%rsp), %edi
	je	.LBB14_11	# bb87
	jmp	.LBB14_32	# bb73
	.align	16
.LBB14_36:	# bb80
	movslq	%edi, %r14
	movaps	%xmm0, %xmm3
	mulss	(%rax,%r14,4), %xmm3
	movss	%xmm3, (%rax,%r14,4)
	leal	1(%rdi), %r14d
	movslq	%r14d, %r14
	movaps	%xmm0, %xmm3
	mulss	(%rax,%r14,4), %xmm3
	movss	%xmm3, (%rax,%r14,4)
	addl	$2, %edi
	incl	%ebx
	cmpl	%edx, %ebx
	jne	.LBB14_36	# bb80
.LBB14_37:	# bb82
	movslq	%r8d, %rdi
	movaps	%xmm0, %xmm3
	mulss	(%rax,%rdi,4), %xmm3
	movss	%xmm3, (%rax,%rdi,4)
	leal	(%r11,%r8), %edi
	incl	%r8d
	movslq	%r8d, %r8
	movl	$0, (%rax,%r8,4)
	addl	%ecx, %esi
	incl	%edx
	cmpl	64(%rsp), %edx
	je	.LBB14_11	# bb87
.LBB14_38:	# bb81.preheader
	movl	%edi, %r8d
	testl	%edx, %edx
	jle	.LBB14_37	# bb82
.LBB14_39:	# bb81.preheader.bb80_crit_edge
	xorl	%ebx, %ebx
	movl	%esi, %edi
	jmp	.LBB14_36	# bb80
.LBB14_40:	# bb.nph203
	leal	1(%rdx), %esi
	leal	1(%rcx), %edi
	pxor	%xmm0, %xmm0
	xorl	%r8d, %r8d
	movl	%r8d, %r11d
	.align	16
.LBB14_41:	# bb94
	leal	(%rcx,%r8), %ebx
	movslq	%ebx, %rbx
	movss	(%r10,%rbx,4), %xmm3
	movaps	%xmm1, %xmm4
	mulss	%xmm3, %xmm4
	leal	(%rdi,%r8), %ebx
	movslq	%ebx, %rbx
	movss	(%r10,%rbx,4), %xmm5
	movaps	%xmm2, %xmm6
	mulss	%xmm5, %xmm6
	addss	%xmm4, %xmm6
	leal	(%rsi,%r8), %ebx
	movslq	%ebx, %rbx
	mulss	(%r9,%rbx,4), %xmm6
	mulss	%xmm1, %xmm5
	mulss	%xmm2, %xmm3
	subss	%xmm5, %xmm3
	leal	(%rdx,%r8), %ebx
	movslq	%ebx, %rbx
	mulss	(%r9,%rbx,4), %xmm3
	addss	%xmm6, %xmm3
	addss	%xmm3, %xmm0
	addl	$2, %r8d
	incl	%r11d
	cmpl	68(%rsp), %r11d
	jne	.LBB14_41	# bb94
.LBB14_42:	# bb95.bb96_crit_edge
	addss	%xmm0, %xmm0
.LBB14_43:	# bb96
	movl	44(%rsp), %esi
	movslq	%esi, %rdi
	addss	(%rax,%rdi,4), %xmm0
	movss	%xmm0, (%rax,%rdi,4)
	leal	1(%rsi), %esi
	movslq	%esi, %rsi
	movl	$0, (%rax,%rsi,4)
	movl	40(%rsp), %esi
	leal	1(%rsi), %esi
	cmpl	64(%rsp), %esi
	jge	.LBB14_50	# bb102
.LBB14_44:	# bb.nph214
	movl	32(%rsp), %esi
	leal	(%rsi,%rdx), %edi
	movl	36(%rsp), %esi
	leal	(%rsi,%rcx), %esi
	leal	1(%rcx), %r11d
	leal	1(%rdx), %r8d
	movl	136(%rsp), %ebx
	leal	(%rbx,%rbx), %ebx
	movl	%ebx, 52(%rsp)
	movl	152(%rsp), %ebx
	leal	(%rbx,%rbx), %ebx
	movl	%ebx, 48(%rsp)
	movl	$0, 60(%rsp)
	movl	44(%rsp), %r14d
	jmp	.LBB14_48	# bb99.preheader
.LBB14_45:	# bb.nph208
	leal	1(%rdi), %r14d
	leal	1(%rsi), %r15d
	pxor	%xmm0, %xmm0
	xorl	%r12d, %r12d
	movl	%r12d, %r13d
	movaps	%xmm0, %xmm3
	.align	16
.LBB14_46:	# bb98
	leal	(%rsi,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r10,%rbp,4), %xmm4
	movaps	%xmm1, %xmm5
	mulss	%xmm4, %xmm5
	leal	(%r15,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r10,%rbp,4), %xmm6
	movaps	%xmm2, %xmm7
	mulss	%xmm6, %xmm7
	addss	%xmm5, %xmm7
	leal	(%rdx,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r9,%rbp,4), %xmm5
	movaps	%xmm5, %xmm8
	mulss	%xmm7, %xmm8
	mulss	%xmm1, %xmm6
	mulss	%xmm2, %xmm4
	subss	%xmm6, %xmm4
	leal	(%r8,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r9,%rbp,4), %xmm6
	movaps	%xmm6, %xmm9
	mulss	%xmm4, %xmm9
	subss	%xmm8, %xmm9
	leal	(%r11,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r10,%rbp,4), %xmm8
	movaps	%xmm1, %xmm10
	mulss	%xmm8, %xmm10
	leal	(%rcx,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r10,%rbp,4), %xmm11
	movaps	%xmm2, %xmm12
	mulss	%xmm11, %xmm12
	subss	%xmm10, %xmm12
	leal	(%r14,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r9,%rbp,4), %xmm10
	movaps	%xmm12, %xmm13
	mulss	%xmm10, %xmm13
	mulss	%xmm1, %xmm11
	mulss	%xmm2, %xmm8
	addss	%xmm11, %xmm8
	leal	(%rdi,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r9,%rbp,4), %xmm11
	movaps	%xmm8, %xmm14
	mulss	%xmm11, %xmm14
	subss	%xmm13, %xmm14
	addss	%xmm9, %xmm14
	addss	%xmm14, %xmm3
	mulss	%xmm7, %xmm6
	mulss	%xmm5, %xmm4
	addss	%xmm6, %xmm4
	mulss	%xmm10, %xmm8
	mulss	%xmm11, %xmm12
	addss	%xmm8, %xmm12
	addss	%xmm4, %xmm12
	addss	%xmm12, %xmm0
	addl	$2, %r12d
	incl	%r13d
	cmpl	68(%rsp), %r13d
	jne	.LBB14_46	# bb98
.LBB14_47:	# bb100
	leal	2(%rbx), %r14d
	movslq	%r14d, %r15
	addss	(%rax,%r15,4), %xmm0
	movss	%xmm0, (%rax,%r15,4)
	addl	$3, %ebx
	movslq	%ebx, %rbx
	addss	(%rax,%rbx,4), %xmm3
	movss	%xmm3, (%rax,%rbx,4)
	addl	52(%rsp), %esi
	addl	48(%rsp), %edi
	movl	60(%rsp), %ebx
	incl	%ebx
	movl	%ebx, 60(%rsp)
	cmpl	56(%rsp), %ebx
	je	.LBB14_50	# bb102
.LBB14_48:	# bb99.preheader
	movl	%r14d, %ebx
	cmpl	$0, 68(%rsp)
	jg	.LBB14_45	# bb.nph208
.LBB14_49:	# bb99.preheader.bb100_crit_edge
	pxor	%xmm0, %xmm0
	movaps	%xmm0, %xmm3
	jmp	.LBB14_47	# bb100
.LBB14_50:	# bb102
	movl	44(%rsp), %esi
	addl	28(%rsp), %esi
	movl	%esi, 44(%rsp)
	addl	36(%rsp), %ecx
	addl	32(%rsp), %edx
	decl	56(%rsp)
	movl	40(%rsp), %esi
	incl	%esi
	movl	%esi, 40(%rsp)
	cmpl	64(%rsp), %esi
	je	.LBB14_93	# return
.LBB14_51:	# bb95.preheader
	cmpl	$0, 68(%rsp)
	jg	.LBB14_40	# bb.nph203
.LBB14_52:	# bb95.preheader.bb96_crit_edge
	pxor	%xmm0, %xmm0
	jmp	.LBB14_43	# bb96
.LBB14_53:	# bb104
	cmpl	$121, 60(%rsp)
	jne	.LBB14_64	# bb117
.LBB14_54:	# bb104
	cmpl	$113, 56(%rsp)
	jne	.LBB14_64	# bb117
.LBB14_55:	# bb116.preheader
	cmpl	$0, 68(%rsp)
	jle	.LBB14_93	# return
.LBB14_56:	# bb.nph200
	cmpl	$0, 64(%rsp)
	jle	.LBB14_93	# return
.LBB14_57:	# bb114.preheader.preheader
	movl	136(%rsp), %ecx
	leal	(%rcx,%rcx), %ecx
	movl	%ecx, 12(%rsp)
	movl	152(%rsp), %ecx
	addl	%ecx, %ecx
	movl	%ecx, 16(%rsp)
	xorl	%ecx, %ecx
	movl	%ecx, 52(%rsp)
	movl	%ecx, 56(%rsp)
	movl	%ecx, 20(%rsp)
	jmp	.LBB14_63	# bb114.preheader
	.align	16
.LBB14_58:	# bb110
	movl	52(%rsp), %edi
	leal	(%rdi,%rcx), %edi
	movslq	%edi, %rdi
	movss	(%r10,%rdi,4), %xmm0
	movl	24(%rsp), %edi
	leal	(%rdi,%rcx), %edi
	movslq	%edi, %rdi
	movss	(%r10,%rdi,4), %xmm3
	movaps	%xmm2, %xmm4
	mulss	%xmm3, %xmm4
	mulss	%xmm1, %xmm3
	movaps	%xmm1, %xmm5
	mulss	%xmm0, %xmm5
	subss	%xmm4, %xmm5
	movl	28(%rsp), %edi
	leal	(%rdi,%rcx), %edi
	movslq	%edi, %rdi
	movss	(%r9,%rdi,4), %xmm4
	movaps	%xmm5, %xmm6
	mulss	%xmm4, %xmm6
	mulss	%xmm2, %xmm0
	addss	%xmm3, %xmm0
	movl	56(%rsp), %edi
	leal	(%rdi,%rcx), %edi
	movslq	%edi, %rdi
	movss	(%r9,%rdi,4), %xmm3
	movaps	%xmm0, %xmm7
	mulss	%xmm3, %xmm7
	subss	%xmm6, %xmm7
	addss	%xmm7, %xmm7
	movslq	%edx, %rdi
	addss	(%rax,%rdi,4), %xmm7
	movss	%xmm7, (%rax,%rdi,4)
	leal	1(%rdx), %edi
	movslq	%edi, %rdi
	movl	$0, (%rax,%rdi,4)
	movaps	%xmm1, %xmm6
	mulss	%xmm4, %xmm6
	movaps	%xmm2, %xmm7
	mulss	%xmm3, %xmm7
	subss	%xmm6, %xmm7
	mulss	%xmm1, %xmm3
	mulss	%xmm2, %xmm4
	addss	%xmm3, %xmm4
	xorps	.LCPI14_1(%rip), %xmm4
	movl	60(%rsp), %edi
	leal	1(%rdi), %edi
	cmpl	64(%rsp), %edi
	jge	.LBB14_61	# bb113
.LBB14_59:	# bb.nph196
	movl	44(%rsp), %edi
	leal	(%rdi,%rcx), %edi
	movl	40(%rsp), %r8d
	leal	(%r8,%rcx), %r8d
	movl	36(%rsp), %r11d
	leal	(%r11,%rcx), %r11d
	movl	32(%rsp), %ebx
	leal	(%rbx,%rcx), %ebx
	leal	3(%rdx), %r14d
	leal	2(%rdx), %r15d
	xorl	%r12d, %r12d
	movl	%r12d, %r13d
	.align	16
.LBB14_60:	# bb111
	leal	(%r11,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r10,%rbp,4), %xmm3
	movaps	%xmm3, %xmm6
	mulss	%xmm4, %xmm6
	leal	(%rbx,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r10,%rbp,4), %xmm8
	movaps	%xmm7, %xmm9
	mulss	%xmm8, %xmm9
	subss	%xmm6, %xmm9
	leal	(%rdi,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r9,%rbp,4), %xmm6
	movaps	%xmm5, %xmm10
	mulss	%xmm6, %xmm10
	leal	(%r8,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r9,%rbp,4), %xmm11
	movaps	%xmm0, %xmm12
	mulss	%xmm11, %xmm12
	subss	%xmm10, %xmm12
	addss	%xmm9, %xmm12
	leal	(%r15,%r12), %ebp
	movslq	%ebp, %rbp
	addss	(%rax,%rbp,4), %xmm12
	movss	%xmm12, (%rax,%rbp,4)
	mulss	%xmm4, %xmm8
	mulss	%xmm7, %xmm3
	addss	%xmm8, %xmm3
	mulss	%xmm5, %xmm11
	mulss	%xmm0, %xmm6
	addss	%xmm11, %xmm6
	addss	%xmm3, %xmm6
	leal	(%r14,%r12), %ebp
	movslq	%ebp, %rbp
	addss	(%rax,%rbp,4), %xmm6
	movss	%xmm6, (%rax,%rbp,4)
	addl	$2, %r12d
	incl	%r13d
	cmpl	%esi, %r13d
	jne	.LBB14_60	# bb111
.LBB14_61:	# bb113
	addl	48(%rsp), %edx
	addl	$2, %ecx
	decl	%esi
	movl	60(%rsp), %edi
	incl	%edi
	movl	%edi, 60(%rsp)
	cmpl	64(%rsp), %edi
	jne	.LBB14_58	# bb110
.LBB14_62:	# bb115
	movl	52(%rsp), %ecx
	addl	12(%rsp), %ecx
	movl	%ecx, 52(%rsp)
	movl	56(%rsp), %ecx
	addl	16(%rsp), %ecx
	movl	%ecx, 56(%rsp)
	movl	20(%rsp), %ecx
	incl	%ecx
	movl	%ecx, 20(%rsp)
	cmpl	68(%rsp), %ecx
	je	.LBB14_93	# return
.LBB14_63:	# bb114.preheader
	movl	136(%rsp), %ecx
	leal	2(,%rcx,2), %ecx
	movl	%ecx, 48(%rsp)
	movl	56(%rsp), %ecx
	leal	3(%rcx), %edx
	movl	%edx, 44(%rsp)
	leal	2(%rcx), %edx
	movl	%edx, 40(%rsp)
	movl	52(%rsp), %edx
	leal	3(%rdx), %esi
	movl	%esi, 36(%rsp)
	leal	2(%rdx), %esi
	movl	%esi, 32(%rsp)
	leal	1(%rcx), %ecx
	movl	%ecx, 28(%rsp)
	leal	1(%rdx), %ecx
	movl	%ecx, 24(%rsp)
	movl	64(%rsp), %ecx
	leal	-1(%rcx), %esi
	xorl	%edx, %edx
	movl	%edx, %ecx
	movl	%edx, 60(%rsp)
	jmp	.LBB14_58	# bb110
.LBB14_64:	# bb117
	cmpl	$122, 60(%rsp)
	jne	.LBB14_81	# bb133
.LBB14_65:	# bb117
	cmpl	$111, 56(%rsp)
	jne	.LBB14_81	# bb133
.LBB14_66:	# bb132.preheader
	cmpl	$0, 64(%rsp)
	jle	.LBB14_93	# return
.LBB14_67:	# bb.nph192
	movl	136(%rsp), %edx
	leal	(%rdx,%rdx), %edx
	movl	%edx, 36(%rsp)
	movl	152(%rsp), %edx
	leal	(%rdx,%rdx), %edx
	movl	%edx, 32(%rsp)
	movl	168(%rsp), %edx
	leal	2(,%rdx,2), %edi
	movl	%edi, 28(%rsp)
	addl	%edx, %edx
	movl	%edx, 24(%rsp)
	xorl	%edx, %edx
	movl	%edx, %edi
	movl	%edx, %ebx
	movl	%edx, 44(%rsp)
	movl	%edx, 56(%rsp)
	jmp	.LBB14_80	# bb127.preheader
.LBB14_68:	# bb.nph182
	leal	1(%rdx), %r14d
	leal	1(%r8), %r15d
	pxor	%xmm0, %xmm0
	xorl	%r12d, %r12d
	movl	%r12d, %r13d
	movaps	%xmm0, %xmm3
	.align	16
.LBB14_69:	# bb124
	leal	(%r8,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r10,%rbp,4), %xmm4
	movaps	%xmm1, %xmm5
	mulss	%xmm4, %xmm5
	leal	(%r15,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r10,%rbp,4), %xmm6
	movaps	%xmm2, %xmm7
	mulss	%xmm6, %xmm7
	addss	%xmm5, %xmm7
	leal	(%rbx,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r9,%rbp,4), %xmm5
	movaps	%xmm5, %xmm8
	mulss	%xmm7, %xmm8
	mulss	%xmm1, %xmm6
	mulss	%xmm2, %xmm4
	subss	%xmm6, %xmm4
	leal	(%rcx,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r9,%rbp,4), %xmm6
	movaps	%xmm6, %xmm9
	mulss	%xmm4, %xmm9
	subss	%xmm8, %xmm9
	leal	(%r11,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r10,%rbp,4), %xmm8
	movaps	%xmm1, %xmm10
	mulss	%xmm8, %xmm10
	leal	(%rdi,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r10,%rbp,4), %xmm11
	movaps	%xmm2, %xmm12
	mulss	%xmm11, %xmm12
	subss	%xmm10, %xmm12
	leal	(%r14,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r9,%rbp,4), %xmm10
	movaps	%xmm12, %xmm13
	mulss	%xmm10, %xmm13
	mulss	%xmm1, %xmm11
	mulss	%xmm2, %xmm8
	addss	%xmm11, %xmm8
	leal	(%rdx,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r9,%rbp,4), %xmm11
	movaps	%xmm8, %xmm14
	mulss	%xmm11, %xmm14
	subss	%xmm13, %xmm14
	addss	%xmm9, %xmm14
	addss	%xmm14, %xmm3
	mulss	%xmm7, %xmm6
	mulss	%xmm5, %xmm4
	addss	%xmm6, %xmm4
	mulss	%xmm10, %xmm8
	mulss	%xmm11, %xmm12
	addss	%xmm8, %xmm12
	addss	%xmm4, %xmm12
	addss	%xmm12, %xmm0
	addl	$2, %r12d
	incl	%r13d
	cmpl	68(%rsp), %r13d
	jne	.LBB14_69	# bb124
.LBB14_70:	# bb126
	movslq	%esi, %r14
	addss	(%rax,%r14,4), %xmm0
	movss	%xmm0, (%rax,%r14,4)
	leal	1(%rsi), %r14d
	movslq	%r14d, %r14
	addss	(%rax,%r14,4), %xmm3
	movss	%xmm3, (%rax,%r14,4)
	addl	52(%rsp), %r8d
	addl	48(%rsp), %edx
	addl	$2, %esi
	movl	60(%rsp), %r14d
	incl	%r14d
	movl	%r14d, 60(%rsp)
	cmpl	56(%rsp), %r14d
	jne	.LBB14_74	# bb125.preheader
.LBB14_71:	# bb130.preheader
	cmpl	$0, 68(%rsp)
	jg	.LBB14_76	# bb.nph189
.LBB14_72:	# bb130.preheader.bb131_crit_edge
	pxor	%xmm0, %xmm0
	jmp	.LBB14_79	# bb131
.LBB14_73:	# bb.nph186
	leal	1(%rdi), %r11d
	leal	1(%rbx), %ecx
	movl	136(%rsp), %edx
	leal	(%rdx,%rdx), %edx
	movl	%edx, 52(%rsp)
	movl	152(%rsp), %edx
	leal	(%rdx,%rdx), %edx
	movl	%edx, 48(%rsp)
	xorl	%r8d, %r8d
	movl	%r8d, %edx
	movl	44(%rsp), %esi
	movl	%r8d, 60(%rsp)
	.align	16
.LBB14_74:	# bb125.preheader
	cmpl	$0, 68(%rsp)
	jg	.LBB14_68	# bb.nph182
.LBB14_75:	# bb125.preheader.bb126_crit_edge
	pxor	%xmm0, %xmm0
	movaps	%xmm0, %xmm3
	jmp	.LBB14_70	# bb126
.LBB14_76:	# bb.nph189
	leal	1(%rbx), %ecx
	leal	1(%rdi), %edx
	pxor	%xmm0, %xmm0
	xorl	%esi, %esi
	movl	%esi, %r8d
	.align	16
.LBB14_77:	# bb129
	leal	(%rdi,%rsi), %r11d
	movslq	%r11d, %r11
	movss	(%r10,%r11,4), %xmm3
	movaps	%xmm1, %xmm4
	mulss	%xmm3, %xmm4
	leal	(%rdx,%rsi), %r11d
	movslq	%r11d, %r11
	movss	(%r10,%r11,4), %xmm5
	movaps	%xmm2, %xmm6
	mulss	%xmm5, %xmm6
	addss	%xmm4, %xmm6
	leal	(%rcx,%rsi), %r11d
	movslq	%r11d, %r11
	mulss	(%r9,%r11,4), %xmm6
	mulss	%xmm1, %xmm5
	mulss	%xmm2, %xmm3
	subss	%xmm5, %xmm3
	leal	(%rbx,%rsi), %r11d
	movslq	%r11d, %r11
	mulss	(%r9,%r11,4), %xmm3
	addss	%xmm6, %xmm3
	addss	%xmm3, %xmm0
	addl	$2, %esi
	incl	%r8d
	cmpl	68(%rsp), %r8d
	jne	.LBB14_77	# bb129
.LBB14_78:	# bb130.bb131_crit_edge
	addss	%xmm0, %xmm0
.LBB14_79:	# bb131
	movl	40(%rsp), %ecx
	movslq	%ecx, %rdx
	addss	(%rax,%rdx,4), %xmm0
	movss	%xmm0, (%rax,%rdx,4)
	movl	28(%rsp), %edx
	leal	(%rdx,%rcx), %edx
	incl	%ecx
	movslq	%ecx, %rcx
	movl	$0, (%rax,%rcx,4)
	addl	36(%rsp), %edi
	addl	32(%rsp), %ebx
	movl	44(%rsp), %ecx
	addl	24(%rsp), %ecx
	movl	%ecx, 44(%rsp)
	movl	56(%rsp), %ecx
	incl	%ecx
	movl	%ecx, 56(%rsp)
	cmpl	64(%rsp), %ecx
	je	.LBB14_93	# return
.LBB14_80:	# bb127.preheader
	movl	%edx, 40(%rsp)
	cmpl	$0, 56(%rsp)
	jg	.LBB14_73	# bb.nph186
	jmp	.LBB14_71	# bb130.preheader
.LBB14_81:	# bb133
	cmpl	$122, 60(%rsp)
	jne	.LBB14_92	# bb146
.LBB14_82:	# bb133
	cmpl	$113, 56(%rsp)
	jne	.LBB14_92	# bb146
.LBB14_83:	# bb145.preheader
	cmpl	$0, 68(%rsp)
	jle	.LBB14_93	# return
.LBB14_84:	# bb.nph178
	cmpl	$0, 64(%rsp)
	jle	.LBB14_93	# return
.LBB14_85:	# bb143.preheader.preheader
	movl	136(%rsp), %ecx
	leal	(%rcx,%rcx), %ecx
	movl	%ecx, 48(%rsp)
	movl	152(%rsp), %ecx
	addl	%ecx, %ecx
	movl	%ecx, 16(%rsp)
	xorl	%ebx, %ebx
	movl	%ebx, %r8d
	movl	%ebx, 52(%rsp)
	jmp	.LBB14_91	# bb143.preheader
	.align	16
.LBB14_86:	# bb139
	leal	(%r14,%r11), %r15d
	movslq	%r15d, %r15
	movss	(%r9,%r15,4), %xmm0
	leal	(%rdx,%r11), %r15d
	movslq	%r15d, %r15
	movss	(%r10,%r15,4), %xmm3
	movaps	%xmm2, %xmm4
	mulss	%xmm3, %xmm4
	mulss	%xmm1, %xmm3
	movaps	%xmm1, %xmm5
	mulss	%xmm0, %xmm5
	leal	(%r8,%r11), %r15d
	movslq	%r15d, %r15
	movss	(%r9,%r15,4), %xmm6
	movaps	%xmm2, %xmm7
	mulss	%xmm6, %xmm7
	subss	%xmm5, %xmm7
	leal	(%rbx,%r11), %r15d
	movslq	%r15d, %r15
	movss	(%r10,%r15,4), %xmm5
	movaps	%xmm1, %xmm8
	mulss	%xmm5, %xmm8
	subss	%xmm4, %xmm8
	mulss	%xmm2, %xmm5
	addss	%xmm3, %xmm5
	movaps	%xmm1, %xmm3
	mulss	%xmm6, %xmm3
	movaps	%xmm2, %xmm4
	mulss	%xmm0, %xmm4
	addss	%xmm3, %xmm4
	xorps	.LCPI14_1(%rip), %xmm4
	testl	%esi, %esi
	jle	.LBB14_89	# bb142
.LBB14_87:	# bb.nph174
	leal	1(%rdi), %r15d
	xorl	%r12d, %r12d
	movl	%r12d, %r13d
	.align	16
.LBB14_88:	# bb140
	leal	(%rdx,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r10,%rbp,4), %xmm3
	movaps	%xmm3, %xmm9
	mulss	%xmm4, %xmm9
	leal	(%rbx,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r10,%rbp,4), %xmm10
	movaps	%xmm7, %xmm11
	mulss	%xmm10, %xmm11
	subss	%xmm9, %xmm11
	leal	(%r14,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r9,%rbp,4), %xmm9
	movaps	%xmm8, %xmm12
	mulss	%xmm9, %xmm12
	leal	(%r8,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r9,%rbp,4), %xmm13
	movaps	%xmm5, %xmm14
	mulss	%xmm13, %xmm14
	subss	%xmm12, %xmm14
	addss	%xmm11, %xmm14
	leal	(%rdi,%r12), %ebp
	movslq	%ebp, %rbp
	addss	(%rax,%rbp,4), %xmm14
	movss	%xmm14, (%rax,%rbp,4)
	mulss	%xmm4, %xmm10
	mulss	%xmm7, %xmm3
	addss	%xmm10, %xmm3
	mulss	%xmm8, %xmm13
	mulss	%xmm5, %xmm9
	addss	%xmm13, %xmm9
	addss	%xmm3, %xmm9
	leal	(%r15,%r12), %ebp
	movslq	%ebp, %rbp
	addss	(%rax,%rbp,4), %xmm9
	movss	%xmm9, (%rax,%rbp,4)
	addl	$2, %r12d
	incl	%r13d
	cmpl	%esi, %r13d
	jne	.LBB14_88	# bb140
.LBB14_89:	# bb142
	mulss	%xmm0, %xmm8
	mulss	%xmm6, %xmm5
	subss	%xmm8, %xmm5
	addss	%xmm5, %xmm5
	movslq	%ecx, %r15
	addss	(%rax,%r15,4), %xmm5
	movss	%xmm5, (%rax,%r15,4)
	movl	60(%rsp), %r15d
	leal	(%r15,%rcx), %r15d
	incl	%ecx
	movslq	%ecx, %rcx
	movl	$0, (%rax,%rcx,4)
	addl	56(%rsp), %edi
	addl	$2, %r11d
	incl	%esi
	cmpl	64(%rsp), %esi
	movl	%r15d, %ecx
	jne	.LBB14_86	# bb139
.LBB14_90:	# bb144
	addl	48(%rsp), %ebx
	addl	16(%rsp), %r8d
	movl	52(%rsp), %ecx
	incl	%ecx
	movl	%ecx, 52(%rsp)
	cmpl	68(%rsp), %ecx
	je	.LBB14_93	# return
.LBB14_91:	# bb143.preheader
	leal	1(%rbx), %edx
	leal	1(%r8), %r14d
	movl	136(%rsp), %ecx
	leal	2(,%rcx,2), %esi
	movl	%esi, 60(%rsp)
	leal	(%rcx,%rcx), %ecx
	movl	%ecx, 56(%rsp)
	xorl	%ecx, %ecx
	movl	%ecx, %edi
	movl	%ecx, %r11d
	movl	%ecx, %esi
	jmp	.LBB14_86	# bb139
.LBB14_92:	# bb146
	xorl	%edi, %edi
	leaq	.str18, %rsi
	leaq	.str119, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB14_93:	# return
	addq	$72, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB14_94:	# bb83.preheader
	cmpl	$0, 64(%rsp)
	jle	.LBB14_11	# bb87
.LBB14_95:	# bb.nph162
	movl	168(%rsp), %edi
	leal	2(,%rdi,2), %r11d
	leal	(%rdi,%rdi), %ecx
	xorl	%edi, %edi
	movl	%edi, %esi
	movl	%edi, %edx
	jmp	.LBB14_38	# bb81.preheader
	.size	cblas_cher2k, .-cblas_cher2k
.Leh_func_end10:


	.align	16
	.globl	cblas_cher
	.type	cblas_cher,@function
cblas_cher:
.Leh_func_begin11:
.Llabel11:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$24, %rsp
	cmpl	$102, %edi
	movl	$4294967295, %eax
	movl	$1, %r10d
	cmove	%eax, %r10d
	pxor	%xmm1, %xmm1
	ucomiss	%xmm1, %xmm0
	setnp	%al
	sete	%r11b
	testb	%al, %r11b
	jne	.LBB15_24	# return
.LBB15_1:	# bb13
	cmpl	$121, %esi
	jne	.LBB15_3	# bb16
.LBB15_2:	# bb13
	cmpl	$101, %edi
	je	.LBB15_5	# bb20
.LBB15_3:	# bb16
	cmpl	$122, %esi
	jne	.LBB15_13	# bb29
.LBB15_4:	# bb16
	cmpl	$102, %edi
	jne	.LBB15_13	# bb29
.LBB15_5:	# bb20
	testl	%r8d, %r8d
	jg	.LBB15_25	# bb20.bb28.preheader_crit_edge
.LBB15_6:	# bb21
	movl	$1, %eax
	subl	%edx, %eax
	imull	%r8d, %eax
.LBB15_7:	# bb28.preheader
	testl	%edx, %edx
	jle	.LBB15_24	# return
.LBB15_8:	# bb.nph65
	leal	(%r8,%rax), %esi
	addl	%esi, %esi
	movl	%esi, 16(%rsp)
	cvtsi2ss	%r10d, %xmm1
	mulss	%xmm0, %xmm1
	negl	%r10d
	cvtsi2ss	%r10d, %xmm2
	movl	80(%rsp), %r10d
	leal	2(,%r10,2), %r10d
	movl	%r10d, 20(%rsp)
	leal	1(,%rax,2), %r10d
	movl	%r10d, 12(%rsp)
	addl	%eax, %eax
	leal	(%r8,%r8), %r10d
	movl	%r10d, 8(%rsp)
	leal	-1(%rdx), %r10d
	xorl	%esi, %esi
	movl	%esi, %edi
	movl	%esi, %r11d
	.align	16
.LBB15_9:	# bb24
	movl	12(%rsp), %ebx
	leal	(%rbx,%rdi), %ebx
	movslq	%ebx, %rbx
	movss	(%rcx,%rbx,4), %xmm3
	movaps	%xmm1, %xmm4
	mulss	%xmm3, %xmm4
	mulss	%xmm2, %xmm3
	mulss	%xmm4, %xmm3
	leal	(%rax,%rdi), %ebx
	movslq	%ebx, %rbx
	movss	(%rcx,%rbx,4), %xmm5
	movaps	%xmm5, %xmm6
	mulss	%xmm0, %xmm6
	mulss	%xmm6, %xmm5
	subss	%xmm3, %xmm5
	movslq	%esi, %rbx
	addss	(%r9,%rbx,4), %xmm5
	movss	%xmm5, (%r9,%rbx,4)
	leal	1(%rsi), %ebx
	movslq	%ebx, %rbx
	movl	$0, (%r9,%rbx,4)
	leal	1(%r11), %ebx
	cmpl	%edx, %ebx
	jge	.LBB15_12	# bb27
.LBB15_10:	# bb.nph62
	movl	16(%rsp), %ebx
	leal	(%rbx,%rdi), %ebx
	leal	(%r8,%r8), %r14d
	xorl	%r15d, %r15d
	movl	%esi, %r12d
	.align	16
.LBB15_11:	# bb25
	movslq	%ebx, %r13
	leal	1(%rbx), %ebp
	movslq	%ebp, %rbp
	movaps	%xmm2, %xmm3
	mulss	(%rcx,%rbp,4), %xmm3
	movaps	%xmm3, %xmm5
	mulss	%xmm4, %xmm5
	movss	(%rcx,%r13,4), %xmm7
	movaps	%xmm7, %xmm8
	mulss	%xmm6, %xmm8
	subss	%xmm5, %xmm8
	leal	2(%r12), %r13d
	movslq	%r13d, %rbp
	addss	(%r9,%rbp,4), %xmm8
	movss	%xmm8, (%r9,%rbp,4)
	mulss	%xmm6, %xmm3
	mulss	%xmm4, %xmm7
	addss	%xmm3, %xmm7
	addl	$3, %r12d
	movslq	%r12d, %r12
	addss	(%r9,%r12,4), %xmm7
	movss	%xmm7, (%r9,%r12,4)
	addl	%r14d, %ebx
	incl	%r15d
	cmpl	%r10d, %r15d
	movl	%r13d, %r12d
	jne	.LBB15_11	# bb25
.LBB15_12:	# bb27
	addl	20(%rsp), %esi
	addl	8(%rsp), %edi
	decl	%r10d
	incl	%r11d
	cmpl	%edx, %r11d
	jne	.LBB15_9	# bb24
	jmp	.LBB15_24	# return
.LBB15_13:	# bb29
	cmpl	$102, %edi
	sete	%al
	cmpl	$121, %esi
	sete	%r11b
	andb	%al, %r11b
	cmpl	$101, %edi
	sete	%al
	cmpl	$122, %esi
	sete	%sil
	testb	%al, %sil
	jne	.LBB15_15	# bb37
.LBB15_14:	# bb29
	notb	%r11b
	testb	$1, %r11b
	jne	.LBB15_23	# bb49
.LBB15_15:	# bb37
	testl	%r8d, %r8d
	jg	.LBB15_26	# bb37.bb48.preheader_crit_edge
.LBB15_16:	# bb38
	movl	$1, %eax
	subl	%edx, %eax
	imull	%r8d, %eax
.LBB15_17:	# bb48.preheader
	testl	%edx, %edx
	jle	.LBB15_24	# return
.LBB15_18:	# bb.nph
	cvtsi2ss	%r10d, %xmm1
	mulss	%xmm0, %xmm1
	movl	$1, %esi
	subl	%edx, %esi
	imull	%r8d, %esi
	movl	%esi, 8(%rsp)
	negl	%r10d
	cvtsi2ss	%r10d, %xmm2
	addl	%eax, %eax
	movl	80(%rsp), %esi
	leal	2(,%rsi,2), %edi
	movl	%edi, 20(%rsp)
	addl	%esi, %esi
	movl	%esi, 12(%rsp)
	leal	(%r8,%r8), %esi
	movl	%esi, 16(%rsp)
	xorl	%esi, %esi
	movl	%esi, %edi
	movl	%esi, %r10d
	.align	16
.LBB15_19:	# bb41
	xorl	%r11d, %r11d
	testl	%r10d, %r10d
	movl	%r10d, %ebx
	cmovs	%r11d, %ebx
	movslq	%eax, %r14
	movaps	%xmm0, %xmm3
	mulss	(%rcx,%r14,4), %xmm3
	testl	%r8d, %r8d
	movl	8(%rsp), %r14d
	cmovg	%r11d, %r14d
	addl	%r14d, %r14d
	leal	1(%rax), %r15d
	movslq	%r15d, %r15
	movaps	%xmm1, %xmm4
	mulss	(%rcx,%r15,4), %xmm4
	leal	(%r8,%r8), %r15d
	movl	%edi, %r12d
	jmp	.LBB15_21	# bb46
.LBB15_20:	# bb45
	movaps	%xmm5, %xmm7
	mulss	%xmm4, %xmm7
	movaps	%xmm6, %xmm8
	mulss	%xmm3, %xmm8
	subss	%xmm7, %xmm8
	movslq	%r12d, %r13
	addss	(%r9,%r13,4), %xmm8
	movss	%xmm8, (%r9,%r13,4)
	mulss	%xmm4, %xmm6
	mulss	%xmm3, %xmm5
	addss	%xmm6, %xmm5
	leal	1(%r12), %r13d
	movslq	%r13d, %r13
	addss	(%r9,%r13,4), %xmm5
	movss	%xmm5, (%r9,%r13,4)
	addl	%r15d, %r14d
	addl	$2, %r12d
	incl	%r11d
.LBB15_21:	# bb46
	movslq	%r14d, %r13
	leal	1(%r14), %ebp
	movslq	%ebp, %rbp
	movaps	%xmm2, %xmm5
	mulss	(%rcx,%rbp,4), %xmm5
	cmpl	%ebx, %r11d
	movss	(%rcx,%r13,4), %xmm6
	jne	.LBB15_20	# bb45
.LBB15_22:	# bb47
	mulss	%xmm4, %xmm5
	mulss	%xmm3, %xmm6
	subss	%xmm5, %xmm6
	movslq	%esi, %r11
	addss	(%r9,%r11,4), %xmm6
	movss	%xmm6, (%r9,%r11,4)
	movl	20(%rsp), %r11d
	leal	(%r11,%rsi), %r11d
	incl	%esi
	movslq	%esi, %rsi
	movl	$0, (%r9,%rsi,4)
	addl	16(%rsp), %eax
	addl	12(%rsp), %edi
	incl	%r10d
	cmpl	%edx, %r10d
	movl	%r11d, %esi
	jne	.LBB15_19	# bb41
	jmp	.LBB15_24	# return
.LBB15_23:	# bb49
	xorl	%edi, %edi
	leaq	.str20, %rsi
	leaq	.str121, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB15_24:	# return
	addq	$24, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB15_25:	# bb20.bb28.preheader_crit_edge
	xorl	%eax, %eax
	jmp	.LBB15_7	# bb28.preheader
.LBB15_26:	# bb37.bb48.preheader_crit_edge
	xorl	%eax, %eax
	jmp	.LBB15_17	# bb48.preheader
	.size	cblas_cher, .-cblas_cher
.Leh_func_end11:


	.section	.rodata.cst4,"aM",@progbits,4
	.align	4
.LCPI16_0:					
	.long	1065353216	# float 1.000000e+00
	.text
	.align	16
	.globl	cblas_cherk
	.type	cblas_cherk,@function
cblas_cherk:
.Leh_func_begin12:
.Llabel12:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$24, %rsp
	ucomiss	.LCPI16_0(%rip), %xmm1
	movq	88(%rsp), %rax
	jne	.LBB16_3	# bb17
	jp	.LBB16_3	# bb17
.LBB16_1:	# bb
	pxor	%xmm2, %xmm2
	ucomiss	%xmm2, %xmm0
	setnp	%r10b
	sete	%r11b
	testb	%r10b, %r11b
	jne	.LBB16_84	# return
.LBB16_2:	# bb
	testl	%r8d, %r8d
	je	.LBB16_84	# return
.LBB16_3:	# bb17
	cmpl	$101, %edi
	je	.LBB16_5	# bb26
.LBB16_4:	# bb19
	cmpl	$111, %edx
	movl	$113, %edi
	movl	$111, %edx
	cmove	%edi, %edx
	cmpl	$121, %esi
	movl	$122, %edi
	movl	$121, %esi
	cmove	%edi, %esi
.LBB16_5:	# bb26
	movl	%edx, 20(%rsp)
	pxor	%xmm2, %xmm2
	ucomiss	%xmm2, %xmm1
	jne	.LBB16_24	# bb40
	jp	.LBB16_24	# bb40
.LBB16_6:	# bb27
	cmpl	$121, %esi
	je	.LBB16_16	# bb33.preheader
.LBB16_7:	# bb39.preheader
	testl	%ecx, %ecx
	jle	.LBB16_11	# bb57
.LBB16_8:	# bb.nph134
	movl	96(%rsp), %edx
	leal	(%rdx,%rdx), %edx
	xorl	%r10d, %r10d
	movl	%r10d, %r11d
	jmp	.LBB16_22	# bb37.preheader
	.align	16
.LBB16_9:	# bb30
	movslq	%edx, %r15
	movl	$0, (%rax,%r15,4)
	leal	1(%rdx), %r15d
	movslq	%r15d, %r15
	movl	$0, (%rax,%r15,4)
	addl	$2, %edx
	incl	%edi
	cmpl	%r14d, %edi
	jne	.LBB16_9	# bb30
.LBB16_10:	# bb32
	addl	%r10d, %ebx
	decl	%r14d
	incl	%r11d
	cmpl	%ecx, %r11d
	jne	.LBB16_18	# bb31.preheader
.LBB16_11:	# bb57
	pxor	%xmm1, %xmm1
	ucomiss	%xmm1, %xmm0
	setnp	%dl
	sete	%dil
	testb	%dl, %dil
	jne	.LBB16_84	# return
.LBB16_12:	# bb58
	cmpl	$121, %esi
	jne	.LBB16_47	# bb71
.LBB16_13:	# bb58
	cmpl	$111, 20(%rsp)
	jne	.LBB16_47	# bb71
.LBB16_14:	# bb70.preheader
	testl	%ecx, %ecx
	jle	.LBB16_84	# return
.LBB16_15:	# bb.nph174
	movl	80(%rsp), %esi
	leal	(%rsi,%rsi), %esi
	movl	%esi, 12(%rsp)
	movl	96(%rsp), %esi
	leal	2(,%rsi,2), %esi
	movl	%esi, 16(%rsp)
	xorl	%r14d, %r14d
	movl	%r14d, 20(%rsp)
	movl	%ecx, %esi
	movl	%r14d, %edx
	jmp	.LBB16_43	# bb68.preheader
.LBB16_16:	# bb33.preheader
	testl	%ecx, %ecx
	jle	.LBB16_11	# bb57
.LBB16_17:	# bb.nph178
	movl	96(%rsp), %edx
	leal	2(,%rdx,2), %r10d
	xorl	%ebx, %ebx
	movl	%ecx, %r14d
	movl	%ebx, %r11d
	.align	16
.LBB16_18:	# bb31.preheader
	cmpl	%ecx, %r11d
	jge	.LBB16_10	# bb32
.LBB16_19:	# bb31.preheader.bb30_crit_edge
	xorl	%edi, %edi
	movl	%ebx, %edx
	jmp	.LBB16_9	# bb30
	.align	16
.LBB16_20:	# bb36
	movslq	%edi, %r14
	movl	$0, (%rax,%r14,4)
	leal	1(%rdi), %r14d
	movslq	%r14d, %r14
	movl	$0, (%rax,%r14,4)
	addl	$2, %edi
	incl	%ebx
	cmpl	%r11d, %ebx
	jle	.LBB16_20	# bb36
.LBB16_21:	# bb38
	addl	%edx, %r10d
	incl	%r11d
	cmpl	%ecx, %r11d
	je	.LBB16_11	# bb57
.LBB16_22:	# bb37.preheader
	testl	%r11d, %r11d
	js	.LBB16_21	# bb38
.LBB16_23:	# bb37.preheader.bb36_crit_edge
	xorl	%ebx, %ebx
	movl	%r10d, %edi
	jmp	.LBB16_20	# bb36
.LBB16_24:	# bb40
	ucomiss	.LCPI16_0(%rip), %xmm1
	jne	.LBB16_28	# bb41
	jp	.LBB16_28	# bb41
.LBB16_25:	# bb56.preheader
	testl	%ecx, %ecx
	jle	.LBB16_11	# bb57
.LBB16_26:	# bb.nph
	movl	96(%rsp), %edx
	leal	2(,%rdx,2), %edx
	xorl	%edi, %edi
	movl	$1, %r10d
	.align	16
.LBB16_27:	# bb55
	movslq	%r10d, %r11
	movl	$0, (%rax,%r11,4)
	addl	%edx, %r10d
	incl	%edi
	cmpl	%ecx, %edi
	jne	.LBB16_27	# bb55
	jmp	.LBB16_11	# bb57
.LBB16_28:	# bb41
	cmpl	$121, %esi
	jne	.LBB16_85	# bb53.preheader
.LBB16_29:	# bb47.preheader
	testl	%ecx, %ecx
	jle	.LBB16_11	# bb57
.LBB16_30:	# bb.nph130
	movl	96(%rsp), %edx
	leal	2(,%rdx,2), %edx
	leal	-1(%rcx), %edi
	xorl	%r10d, %r10d
	movl	%r10d, %r11d
	.align	16
.LBB16_31:	# bb43
	movslq	%r10d, %rbx
	movaps	%xmm1, %xmm2
	mulss	(%rax,%rbx,4), %xmm2
	movss	%xmm2, (%rax,%rbx,4)
	leal	1(%r10), %ebx
	movslq	%ebx, %rbx
	movl	$0, (%rax,%rbx,4)
	leal	1(%r11), %ebx
	cmpl	%ecx, %ebx
	jge	.LBB16_34	# bb46
.LBB16_32:	# bb43.bb44_crit_edge
	xorl	%ebx, %ebx
	movl	%r10d, %r14d
	.align	16
.LBB16_33:	# bb44
	leal	2(%r14), %r15d
	movslq	%r15d, %r12
	movaps	%xmm1, %xmm2
	mulss	(%rax,%r12,4), %xmm2
	movss	%xmm2, (%rax,%r12,4)
	addl	$3, %r14d
	movslq	%r14d, %r14
	movaps	%xmm1, %xmm2
	mulss	(%rax,%r14,4), %xmm2
	movss	%xmm2, (%rax,%r14,4)
	incl	%ebx
	cmpl	%edi, %ebx
	movl	%r15d, %r14d
	jne	.LBB16_33	# bb44
.LBB16_34:	# bb46
	addl	%edx, %r10d
	decl	%edi
	incl	%r11d
	cmpl	%ecx, %r11d
	je	.LBB16_11	# bb57
	jmp	.LBB16_31	# bb43
	.align	16
.LBB16_35:	# bb50
	movslq	%r15d, %r12
	movaps	%xmm1, %xmm2
	mulss	(%rax,%r12,4), %xmm2
	movss	%xmm2, (%rax,%r12,4)
	leal	1(%r15), %r12d
	movslq	%r12d, %r12
	movaps	%xmm1, %xmm2
	mulss	(%rax,%r12,4), %xmm2
	movss	%xmm2, (%rax,%r12,4)
	addl	$2, %r15d
	incl	%ebx
	cmpl	%r10d, %ebx
	jne	.LBB16_35	# bb50
.LBB16_36:	# bb52
	movslq	%r14d, %rbx
	movaps	%xmm1, %xmm2
	mulss	(%rax,%rbx,4), %xmm2
	movss	%xmm2, (%rax,%rbx,4)
	leal	(%rdi,%r14), %ebx
	incl	%r14d
	movslq	%r14d, %r14
	movl	$0, (%rax,%r14,4)
	addl	%edx, %r11d
	incl	%r10d
	cmpl	%ecx, %r10d
	je	.LBB16_11	# bb57
.LBB16_37:	# bb51.preheader
	movl	%ebx, %r14d
	testl	%r10d, %r10d
	jle	.LBB16_36	# bb52
.LBB16_38:	# bb51.preheader.bb50_crit_edge
	xorl	%ebx, %ebx
	movl	%r11d, %r15d
	jmp	.LBB16_35	# bb50
.LBB16_39:	# bb.nph168
	leal	1(%rbx), %r15d
	pxor	%xmm1, %xmm1
	xorl	%r12d, %r12d
	movl	%r14d, %r13d
	movaps	%xmm1, %xmm2
	.align	16
.LBB16_40:	# bb65
	movslq	%r13d, %rbp
	movss	(%r9,%rbp,4), %xmm3
	leal	(%r15,%r13), %ebp
	movslq	%ebp, %rbp
	movss	(%r9,%rbp,4), %xmm4
	movaps	%xmm3, %xmm5
	mulss	%xmm4, %xmm5
	leal	(%rbx,%r13), %ebp
	movslq	%ebp, %rbp
	movss	(%r9,%rbp,4), %xmm6
	leal	1(%r13), %ebp
	movslq	%ebp, %rbp
	movss	(%r9,%rbp,4), %xmm7
	movaps	%xmm7, %xmm8
	mulss	%xmm6, %xmm8
	subss	%xmm5, %xmm8
	addss	%xmm8, %xmm2
	mulss	%xmm4, %xmm7
	mulss	%xmm6, %xmm3
	addss	%xmm7, %xmm3
	addss	%xmm3, %xmm1
	addl	$2, %r13d
	incl	%r12d
	cmpl	%r8d, %r12d
	jne	.LBB16_40	# bb65
.LBB16_41:	# bb67
	mulss	%xmm0, %xmm1
	movslq	%r11d, %r15
	addss	(%rax,%r15,4), %xmm1
	movss	%xmm1, (%rax,%r15,4)
	mulss	%xmm0, %xmm2
	leal	1(%r11), %r15d
	movslq	%r15d, %r15
	addss	(%rax,%r15,4), %xmm2
	movss	%xmm2, (%rax,%r15,4)
	addl	%edi, %ebx
	addl	$2, %r11d
	incl	%r10d
	cmpl	%esi, %r10d
	jne	.LBB16_45	# bb66.preheader
.LBB16_42:	# bb69
	addl	12(%rsp), %r14d
	movl	20(%rsp), %edi
	addl	16(%rsp), %edi
	movl	%edi, 20(%rsp)
	decl	%esi
	incl	%edx
	cmpl	%ecx, %edx
	je	.LBB16_84	# return
.LBB16_43:	# bb68.preheader
	cmpl	%ecx, %edx
	jge	.LBB16_42	# bb69
.LBB16_44:	# bb.nph172
	movl	80(%rsp), %edi
	leal	(%rdi,%rdi), %edi
	xorl	%ebx, %ebx
	movl	20(%rsp), %r11d
	movl	%ebx, %r10d
	.align	16
.LBB16_45:	# bb66.preheader
	testl	%r8d, %r8d
	jg	.LBB16_39	# bb.nph168
.LBB16_46:	# bb66.preheader.bb67_crit_edge
	pxor	%xmm1, %xmm1
	movaps	%xmm1, %xmm2
	jmp	.LBB16_41	# bb67
.LBB16_47:	# bb71
	cmpl	$121, %esi
	jne	.LBB16_59	# bb84
.LBB16_48:	# bb71
	cmpl	$113, 20(%rsp)
	jne	.LBB16_59	# bb84
.LBB16_49:	# bb83.preheader
	testl	%ecx, %ecx
	jle	.LBB16_84	# return
.LBB16_50:	# bb.nph164
	movl	96(%rsp), %esi
	leal	2(,%rsi,2), %esi
	movl	%esi, 12(%rsp)
	xorl	%r10d, %r10d
	movl	%ecx, 20(%rsp)
	movl	%r10d, 16(%rsp)
	jmp	.LBB16_55	# bb81.preheader
.LBB16_51:	# bb78.preheader
	leal	1(%rdx), %ebx
	movl	80(%rsp), %r11d
	leal	(%r11,%r11), %r11d
	pxor	%xmm2, %xmm2
	xorl	%r12d, %r12d
	movl	%esi, %r14d
	movaps	%xmm2, %xmm1
	.align	16
.LBB16_52:	# bb78
	movslq	%r14d, %r13
	movss	(%r9,%r13,4), %xmm3
	leal	(%rdx,%r14), %r13d
	movslq	%r13d, %r13
	movss	(%r9,%r13,4), %xmm4
	movaps	%xmm3, %xmm5
	mulss	%xmm4, %xmm5
	leal	(%rbx,%r14), %r13d
	leal	1(%r14), %ebp
	movslq	%ebp, %rbp
	movss	(%r9,%rbp,4), %xmm6
	mulss	%xmm6, %xmm4
	movslq	%r13d, %r13
	movss	(%r9,%r13,4), %xmm7
	mulss	%xmm7, %xmm3
	subss	%xmm4, %xmm3
	addss	%xmm3, %xmm1
	mulss	%xmm6, %xmm7
	addss	%xmm5, %xmm7
	addss	%xmm7, %xmm2
	addl	%r11d, %r14d
	incl	%r12d
	cmpl	%r8d, %r12d
	jne	.LBB16_52	# bb78
.LBB16_53:	# bb80
	leal	(%r10,%rdx), %r11d
	movslq	%r11d, %r11
	mulss	%xmm0, %xmm2
	addss	(%rax,%r11,4), %xmm2
	movss	%xmm2, (%rax,%r11,4)
	leal	(%r15,%rdx), %r11d
	movslq	%r11d, %r11
	mulss	%xmm0, %xmm1
	addss	(%rax,%r11,4), %xmm1
	movss	%xmm1, (%rax,%r11,4)
	addl	$2, %edx
	incl	%edi
	cmpl	20(%rsp), %edi
	jne	.LBB16_57	# bb79.preheader
.LBB16_54:	# bb82
	addl	12(%rsp), %r10d
	decl	20(%rsp)
	movl	16(%rsp), %esi
	incl	%esi
	movl	%esi, 16(%rsp)
	cmpl	%ecx, %esi
	je	.LBB16_84	# return
.LBB16_55:	# bb81.preheader
	cmpl	%ecx, 16(%rsp)
	jge	.LBB16_54	# bb82
.LBB16_56:	# bb.nph162
	movl	16(%rsp), %esi
	leal	(%rsi,%rsi), %esi
	leal	1(%r10), %r15d
	xorl	%edx, %edx
	movl	%edx, %edi
	.align	16
.LBB16_57:	# bb79.preheader
	testl	%r8d, %r8d
	jg	.LBB16_51	# bb78.preheader
.LBB16_58:	# bb79.preheader.bb80_crit_edge
	pxor	%xmm2, %xmm2
	movaps	%xmm2, %xmm1
	jmp	.LBB16_53	# bb80
.LBB16_59:	# bb84
	cmpl	$122, %esi
	jne	.LBB16_71	# bb97
.LBB16_60:	# bb84
	cmpl	$111, 20(%rsp)
	jne	.LBB16_71	# bb97
.LBB16_61:	# bb96.preheader
	testl	%ecx, %ecx
	jle	.LBB16_84	# return
.LBB16_62:	# bb.nph154
	movl	80(%rsp), %esi
	leal	(%rsi,%rsi), %esi
	movl	%esi, 12(%rsp)
	movl	96(%rsp), %esi
	addl	%esi, %esi
	movl	%esi, 16(%rsp)
	xorl	%edi, %edi
	movl	%edi, 20(%rsp)
	movl	%edi, %esi
	jmp	.LBB16_67	# bb94.preheader
.LBB16_63:	# bb.nph148
	leal	1(%r10), %r15d
	pxor	%xmm1, %xmm1
	xorl	%r12d, %r12d
	movl	%r12d, %r13d
	movaps	%xmm1, %xmm2
	.align	16
.LBB16_64:	# bb91
	leal	(%r15,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r9,%rbp,4), %xmm3
	leal	(%rdi,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r9,%rbp,4), %xmm4
	movaps	%xmm4, %xmm5
	mulss	%xmm3, %xmm5
	leal	(%r10,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r9,%rbp,4), %xmm6
	leal	(%r14,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r9,%rbp,4), %xmm7
	movaps	%xmm7, %xmm8
	mulss	%xmm6, %xmm8
	subss	%xmm5, %xmm8
	addss	%xmm8, %xmm2
	mulss	%xmm3, %xmm7
	mulss	%xmm6, %xmm4
	addss	%xmm7, %xmm4
	addss	%xmm4, %xmm1
	addl	$2, %r12d
	incl	%r13d
	cmpl	%r8d, %r13d
	jne	.LBB16_64	# bb91
.LBB16_65:	# bb93
	mulss	%xmm0, %xmm1
	movslq	%ebx, %r15
	addss	(%rax,%r15,4), %xmm1
	movss	%xmm1, (%rax,%r15,4)
	mulss	%xmm0, %xmm2
	leal	1(%rbx), %r15d
	movslq	%r15d, %r15
	addss	(%rax,%r15,4), %xmm2
	movss	%xmm2, (%rax,%r15,4)
	addl	%edx, %r10d
	addl	$2, %ebx
	incl	%r11d
	cmpl	%esi, %r11d
	jle	.LBB16_69	# bb92.preheader
.LBB16_66:	# bb95
	addl	12(%rsp), %edi
	movl	20(%rsp), %edx
	addl	16(%rsp), %edx
	movl	%edx, 20(%rsp)
	incl	%esi
	cmpl	%ecx, %esi
	je	.LBB16_84	# return
.LBB16_67:	# bb94.preheader
	testl	%esi, %esi
	js	.LBB16_66	# bb95
.LBB16_68:	# bb.nph152
	leal	1(%rdi), %r14d
	movl	80(%rsp), %edx
	leal	(%rdx,%rdx), %edx
	xorl	%r10d, %r10d
	movl	20(%rsp), %ebx
	movl	%r10d, %r11d
	.align	16
.LBB16_69:	# bb92.preheader
	testl	%r8d, %r8d
	jg	.LBB16_63	# bb.nph148
.LBB16_70:	# bb92.preheader.bb93_crit_edge
	pxor	%xmm1, %xmm1
	movaps	%xmm1, %xmm2
	jmp	.LBB16_65	# bb93
.LBB16_71:	# bb97
	cmpl	$122, %esi
	jne	.LBB16_83	# bb110
.LBB16_72:	# bb97
	cmpl	$113, 20(%rsp)
	jne	.LBB16_83	# bb110
.LBB16_73:	# bb109.preheader
	testl	%ecx, %ecx
	jle	.LBB16_84	# return
.LBB16_74:	# bb.nph144
	movl	96(%rsp), %r11d
	addl	%r11d, %r11d
	movl	%r11d, 16(%rsp)
	xorl	%r11d, %r11d
	movl	%r11d, 20(%rsp)
	movl	%r11d, %r12d
	jmp	.LBB16_79	# bb107.preheader
.LBB16_75:	# bb104.preheader
	leal	1(%r15), %ebx
	movl	80(%rsp), %edx
	leal	(%rdx,%rdx), %esi
	pxor	%xmm1, %xmm1
	xorl	%r13d, %r13d
	movl	%r13d, %edx
	movaps	%xmm1, %xmm2
	.align	16
.LBB16_76:	# bb104
	leal	(%r15,%r13), %ebp
	movslq	%ebp, %rbp
	movss	(%r9,%rbp,4), %xmm3
	leal	(%r12,%r13), %ebp
	movslq	%ebp, %rbp
	movss	(%r9,%rbp,4), %xmm4
	movaps	%xmm4, %xmm5
	mulss	%xmm3, %xmm5
	leal	(%rdi,%r13), %ebp
	movslq	%ebp, %rbp
	movss	(%r9,%rbp,4), %xmm6
	mulss	%xmm6, %xmm3
	leal	(%rbx,%r13), %ebp
	movslq	%ebp, %rbp
	movss	(%r9,%rbp,4), %xmm7
	mulss	%xmm7, %xmm4
	subss	%xmm3, %xmm4
	addss	%xmm4, %xmm2
	mulss	%xmm6, %xmm7
	addss	%xmm5, %xmm7
	addss	%xmm7, %xmm1
	addl	%esi, %r13d
	incl	%edx
	cmpl	%r8d, %edx
	jne	.LBB16_76	# bb104
.LBB16_77:	# bb106
	movl	20(%rsp), %edx
	leal	(%rdx,%r15), %edx
	movslq	%edx, %rdx
	mulss	%xmm0, %xmm1
	addss	(%rax,%rdx,4), %xmm1
	movss	%xmm1, (%rax,%rdx,4)
	leal	(%r10,%r15), %edx
	movslq	%edx, %rdx
	mulss	%xmm0, %xmm2
	addss	(%rax,%rdx,4), %xmm2
	movss	%xmm2, (%rax,%rdx,4)
	addl	$2, %r15d
	incl	%r14d
	cmpl	%r11d, %r14d
	jle	.LBB16_81	# bb105.preheader
.LBB16_78:	# bb108
	movl	20(%rsp), %edx
	addl	16(%rsp), %edx
	movl	%edx, 20(%rsp)
	addl	$2, %r12d
	incl	%r11d
	cmpl	%ecx, %r11d
	je	.LBB16_84	# return
.LBB16_79:	# bb107.preheader
	testl	%r11d, %r11d
	js	.LBB16_78	# bb108
.LBB16_80:	# bb.nph142
	leal	1(%r12), %edi
	movl	20(%rsp), %edx
	leal	1(%rdx), %r10d
	xorl	%r15d, %r15d
	movl	%r15d, %r14d
	.align	16
.LBB16_81:	# bb105.preheader
	testl	%r8d, %r8d
	jg	.LBB16_75	# bb104.preheader
.LBB16_82:	# bb105.preheader.bb106_crit_edge
	pxor	%xmm1, %xmm1
	movaps	%xmm1, %xmm2
	jmp	.LBB16_77	# bb106
.LBB16_83:	# bb110
	xorl	%edi, %edi
	leaq	.str22, %rsi
	leaq	.str123, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB16_84:	# return
	addq	$24, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB16_85:	# bb53.preheader
	testl	%ecx, %ecx
	jle	.LBB16_11	# bb57
.LBB16_86:	# bb.nph124
	movl	96(%rsp), %ebx
	leal	2(,%rbx,2), %edi
	leal	(%rbx,%rbx), %edx
	xorl	%ebx, %ebx
	movl	%ebx, %r11d
	movl	%ebx, %r10d
	jmp	.LBB16_37	# bb51.preheader
	.size	cblas_cherk, .-cblas_cherk
.Leh_func_end12:


	.section	.rodata.cst4,"aM",@progbits,4
	.align	4
.LCPI17_0:					
	.long	1065353216	# float 1.000000e+00
	.text
	.align	16
	.globl	cblas_chpmv
	.type	cblas_chpmv,@function
cblas_chpmv:
.Leh_func_begin13:
.Llabel13:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	movss	(%rcx), %xmm0
	pxor	%xmm1, %xmm1
	ucomiss	%xmm1, %xmm0
	setp	%al
	setne	%r10b
	setnp	%r11b
	sete	%bl
	andb	%r11b, %bl
	movss	4(%rcx), %xmm2
	ucomiss	%xmm1, %xmm2
	setp	%cl
	setnp	%r11b
	sete	%r14b
	setne	%r15b
	andb	%r11b, %r14b
	andb	%bl, %r14b
	orb	%al, %r10b
	orb	%cl, %r15b
	orb	%r10b, %r15b
	cmpl	$102, %edi
	movl	$4294967295, %eax
	movl	$1, %ecx
	cmove	%eax, %ecx
	testb	%r15b, %r15b
	movq	104(%rsp), %rax
	movss	4(%rax), %xmm1
	movss	(%rax), %xmm3
	movq	112(%rsp), %rax
	jne	.LBB17_3	# bb23
.LBB17_1:	# entry
	ucomiss	.LCPI17_0(%rip), %xmm3
	jne	.LBB17_3	# bb23
	jp	.LBB17_3	# bb23
.LBB17_2:	# entry
	pxor	%xmm4, %xmm4
	ucomiss	%xmm4, %xmm1
	setnp	%r10b
	sete	%r11b
	testb	%r10b, %r11b
	jne	.LBB17_45	# return
.LBB17_3:	# bb23
	pxor	%xmm4, %xmm4
	ucomiss	%xmm4, %xmm1
	jne	.LBB17_10	# bb31
	jp	.LBB17_10	# bb31
.LBB17_4:	# bb23
	pxor	%xmm4, %xmm4
	ucomiss	%xmm4, %xmm3
	jne	.LBB17_10	# bb31
	jp	.LBB17_10	# bb31
.LBB17_5:	# bb25
	cmpl	$0, 120(%rsp)
	jg	.LBB17_46	# bb25.bb30.preheader_crit_edge
.LBB17_6:	# bb26
	movl	$1, %r10d
	subl	%edx, %r10d
	imull	120(%rsp), %r10d
.LBB17_7:	# bb30.preheader
	testl	%edx, %edx
	jle	.LBB17_17	# bb39
.LBB17_8:	# bb.nph
	movl	120(%rsp), %r11d
	leal	(%r11,%r11), %r11d
	addl	%r10d, %r10d
	xorl	%ebx, %ebx
	.align	16
.LBB17_9:	# bb29
	movslq	%r10d, %r15
	movl	$0, (%rax,%r15,4)
	leal	(%r11,%r10), %r15d
	incl	%r10d
	movslq	%r10d, %r10
	movl	$0, (%rax,%r10,4)
	incl	%ebx
	cmpl	%edx, %ebx
	movl	%r15d, %r10d
	jne	.LBB17_9	# bb29
	jmp	.LBB17_17	# bb39
.LBB17_10:	# bb31
	pxor	%xmm4, %xmm4
	ucomiss	%xmm4, %xmm1
	jne	.LBB17_12	# bb33
	jp	.LBB17_12	# bb33
.LBB17_11:	# bb31
	ucomiss	.LCPI17_0(%rip), %xmm3
	setnp	%r10b
	sete	%r11b
	testb	%r10b, %r11b
	jne	.LBB17_17	# bb39
.LBB17_12:	# bb33
	cmpl	$0, 120(%rsp)
	jg	.LBB17_47	# bb33.bb38.preheader_crit_edge
.LBB17_13:	# bb34
	movl	$1, %r10d
	subl	%edx, %r10d
	imull	120(%rsp), %r10d
.LBB17_14:	# bb38.preheader
	testl	%edx, %edx
	jle	.LBB17_17	# bb39
.LBB17_15:	# bb.nph140
	movl	120(%rsp), %r11d
	leal	(%r11,%r11), %r11d
	addl	%r10d, %r10d
	xorl	%ebx, %ebx
	.align	16
.LBB17_16:	# bb37
	movslq	%r10d, %r15
	movss	(%rax,%r15,4), %xmm4
	movaps	%xmm4, %xmm5
	mulss	%xmm3, %xmm5
	leal	1(%r10), %r12d
	movslq	%r12d, %r12
	movss	(%rax,%r12,4), %xmm6
	movaps	%xmm6, %xmm7
	mulss	%xmm1, %xmm7
	subss	%xmm7, %xmm5
	movss	%xmm5, (%rax,%r15,4)
	mulss	%xmm1, %xmm4
	mulss	%xmm3, %xmm6
	addss	%xmm4, %xmm6
	movss	%xmm6, (%rax,%r12,4)
	addl	%r11d, %r10d
	incl	%ebx
	cmpl	%edx, %ebx
	jne	.LBB17_16	# bb37
.LBB17_17:	# bb39
	testb	$1, %r14b
	jne	.LBB17_45	# return
.LBB17_18:	# bb41
	cmpl	$121, %esi
	jne	.LBB17_20	# bb44
.LBB17_19:	# bb41
	cmpl	$101, %edi
	je	.LBB17_22	# bb48
.LBB17_20:	# bb44
	cmpl	$122, %esi
	jne	.LBB17_32	# bb66
.LBB17_21:	# bb44
	cmpl	$102, %edi
	jne	.LBB17_32	# bb66
.LBB17_22:	# bb48
	cmpl	$0, 96(%rsp)
	jg	.LBB17_48	# bb48.bb51_crit_edge
.LBB17_23:	# bb49
	movl	$1, %esi
	subl	%edx, %esi
	imull	96(%rsp), %esi
	movl	%esi, 36(%rsp)
.LBB17_24:	# bb51
	cmpl	$0, 120(%rsp)
	jg	.LBB17_49	# bb51.bb65.preheader_crit_edge
.LBB17_25:	# bb52
	movl	$1, %esi
	subl	%edx, %esi
	imull	120(%rsp), %esi
.LBB17_26:	# bb65.preheader
	testl	%edx, %edx
	jle	.LBB17_45	# return
.LBB17_27:	# bb.nph137
	movl	$1, %edi
	subl	%edx, %edi
	movl	%edi, %r10d
	movl	96(%rsp), %r11d
	imull	%r11d, %r10d
	movl	%r10d, 4(%rsp)
	movl	120(%rsp), %r10d
	imull	%r10d, %edi
	movl	%edi, (%rsp)
	addl	%esi, %esi
	movl	36(%rsp), %edi
	addl	%edi, %edi
	movl	%edi, 36(%rsp)
	leal	1(,%rdx,2), %edi
	movl	%edi, 20(%rsp)
	leal	-1(%rdx), %edi
	movl	%edi, 16(%rsp)
	cvtsi2ss	%ecx, %xmm1
	leal	(%r10,%r10), %ecx
	movl	%ecx, 12(%rsp)
	leal	(%r11,%r11), %ecx
	movl	%ecx, 8(%rsp)
	xorl	%ecx, %ecx
	movl	%ecx, 32(%rsp)
	movl	%r11d, 28(%rsp)
	movl	%r10d, 24(%rsp)
	.align	16
.LBB17_28:	# bb55
	movl	32(%rsp), %r10d
	movl	20(%rsp), %edi
	leal	(%rdi,%r10), %edi
	imull	%ecx, %edi
	movl	%edi, %r10d
	shrl	$31, %r10d
	addl	%edi, %r10d
	movl	%r10d, %edi
	andl	$4294967294, %edi
	movslq	%edi, %rdi
	movss	(%r8,%rdi,4), %xmm3
	movl	36(%rsp), %edi
	movslq	%edi, %r11
	movss	(%r9,%r11,4), %xmm4
	movaps	%xmm0, %xmm5
	mulss	%xmm4, %xmm5
	leal	1(%rdi), %edi
	movslq	%edi, %rdi
	movss	(%r9,%rdi,4), %xmm6
	movaps	%xmm2, %xmm7
	mulss	%xmm6, %xmm7
	subss	%xmm7, %xmm5
	movaps	%xmm5, %xmm7
	mulss	%xmm3, %xmm7
	movslq	%esi, %rdi
	addss	(%rax,%rdi,4), %xmm7
	movss	%xmm7, (%rax,%rdi,4)
	mulss	%xmm2, %xmm4
	mulss	%xmm0, %xmm6
	addss	%xmm4, %xmm6
	mulss	%xmm6, %xmm3
	incl	%esi
	movslq	%esi, %rsi
	addss	(%rax,%rsi,4), %xmm3
	movss	%xmm3, (%rax,%rsi,4)
	xorl	%r11d, %r11d
	cmpl	$0, 120(%rsp)
	movl	(%rsp), %ebx
	cmovg	%r11d, %ebx
	cmpl	$0, 96(%rsp)
	cmovle	4(%rsp), %r11d
	sarl	%r10d
	leal	1(%rcx), %r14d
	cmpl	%edx, %r14d
	jge	.LBB17_50	# bb55.bb64_crit_edge
.LBB17_29:	# bb.nph131
	movl	32(%rsp), %r14d
	movl	16(%rsp), %r15d
	leal	(%r15,%r14), %r14d
	addl	%r10d, %r10d
	pxor	%xmm3, %xmm3
	xorl	%r15d, %r15d
	movl	24(%rsp), %r12d
	movl	28(%rsp), %r13d
	movaps	%xmm3, %xmm4
	.align	16
.LBB17_30:	# bb62
	leal	3(%r10), %ebp
	movslq	%ebp, %rbp
	movaps	%xmm1, %xmm7
	mulss	(%r8,%rbp,4), %xmm7
	movaps	%xmm7, %xmm8
	mulss	%xmm6, %xmm8
	addl	$2, %r10d
	movslq	%r10d, %rbp
	movss	(%r8,%rbp,4), %xmm9
	movaps	%xmm5, %xmm10
	mulss	%xmm9, %xmm10
	addss	%xmm8, %xmm10
	addl	%ebx, %r12d
	leal	(%r12,%r12), %ebx
	movslq	%ebx, %rbx
	addss	(%rax,%rbx,4), %xmm10
	movss	%xmm10, (%rax,%rbx,4)
	movaps	%xmm5, %xmm8
	mulss	%xmm7, %xmm8
	movaps	%xmm6, %xmm10
	mulss	%xmm9, %xmm10
	subss	%xmm8, %xmm10
	leal	1(,%r12,2), %ebx
	movslq	%ebx, %rbx
	addss	(%rax,%rbx,4), %xmm10
	movss	%xmm10, (%rax,%rbx,4)
	addl	%r11d, %r13d
	leal	1(,%r13,2), %r11d
	movslq	%r11d, %r11
	movss	(%r9,%r11,4), %xmm8
	movaps	%xmm8, %xmm10
	mulss	%xmm9, %xmm10
	leal	(%r13,%r13), %r11d
	movslq	%r11d, %r11
	movss	(%r9,%r11,4), %xmm11
	movaps	%xmm11, %xmm12
	mulss	%xmm7, %xmm12
	addss	%xmm10, %xmm12
	addss	%xmm12, %xmm4
	mulss	%xmm8, %xmm7
	mulss	%xmm9, %xmm11
	subss	%xmm7, %xmm11
	addss	%xmm11, %xmm3
	incl	%r15d
	cmpl	%r14d, %r15d
	movl	120(%rsp), %ebx
	movl	96(%rsp), %r11d
	jne	.LBB17_30	# bb62
.LBB17_31:	# bb64
	movaps	%xmm2, %xmm5
	mulss	%xmm4, %xmm5
	movaps	%xmm0, %xmm6
	mulss	%xmm3, %xmm6
	subss	%xmm5, %xmm6
	addss	(%rax,%rdi,4), %xmm6
	movss	%xmm6, (%rax,%rdi,4)
	mulss	%xmm2, %xmm3
	mulss	%xmm0, %xmm4
	addss	%xmm3, %xmm4
	addss	(%rax,%rsi,4), %xmm4
	movss	%xmm4, (%rax,%rsi,4)
	movl	%edi, %esi
	addl	12(%rsp), %esi
	movl	36(%rsp), %edi
	addl	8(%rsp), %edi
	movl	%edi, 36(%rsp)
	movl	96(%rsp), %edi
	addl	%edi, 28(%rsp)
	movl	120(%rsp), %edi
	addl	%edi, 24(%rsp)
	decl	32(%rsp)
	incl	%ecx
	cmpl	%edx, %ecx
	jne	.LBB17_28	# bb55
	jmp	.LBB17_45	# return
.LBB17_32:	# bb66
	cmpl	$102, %edi
	sete	%r10b
	cmpl	$121, %esi
	sete	%r11b
	andb	%r10b, %r11b
	cmpl	$101, %edi
	sete	%dil
	cmpl	$122, %esi
	sete	%sil
	testb	%dil, %sil
	jne	.LBB17_34	# bb74
.LBB17_33:	# bb66
	notb	%r11b
	testb	$1, %r11b
	jne	.LBB17_44	# bb92
.LBB17_34:	# bb74
	cmpl	$0, 96(%rsp)
	jg	.LBB17_51	# bb74.bb77_crit_edge
.LBB17_35:	# bb75
	movl	$1, %esi
	subl	%edx, %esi
	imull	96(%rsp), %esi
	movl	%esi, 36(%rsp)
.LBB17_36:	# bb77
	cmpl	$0, 120(%rsp)
	jg	.LBB17_52	# bb77.bb91.preheader_crit_edge
.LBB17_37:	# bb78
	movl	$1, %esi
	subl	%edx, %esi
	imull	120(%rsp), %esi
.LBB17_38:	# bb91.preheader
	testl	%edx, %edx
	jle	.LBB17_45	# return
.LBB17_39:	# bb.nph119
	movl	$1, %edi
	subl	%edx, %edi
	movl	%edi, %r10d
	movl	120(%rsp), %r11d
	imull	%r11d, %r10d
	movl	%r10d, 24(%rsp)
	movl	96(%rsp), %r10d
	imull	%r10d, %edi
	movl	%edi, 20(%rsp)
	movl	36(%rsp), %edi
	addl	%edi, %edi
	movl	%edi, 36(%rsp)
	addl	%esi, %esi
	cvtsi2ss	%ecx, %xmm1
	leal	(%r10,%r10), %ecx
	movl	%ecx, 32(%rsp)
	leal	(%r11,%r11), %ecx
	movl	%ecx, 28(%rsp)
	xorl	%ecx, %ecx
	.align	16
.LBB17_40:	# bb81
	leal	1(%rcx), %edi
	imull	%ecx, %edi
	movl	%edi, %r10d
	shrl	$31, %r10d
	addl	%edi, %r10d
	sarl	%r10d
	leal	(%r10,%rcx), %edi
	addl	%edi, %edi
	movslq	%edi, %rdi
	movss	(%r8,%rdi,4), %xmm3
	movl	36(%rsp), %edi
	movslq	%edi, %r11
	movss	(%r9,%r11,4), %xmm4
	movaps	%xmm0, %xmm5
	mulss	%xmm4, %xmm5
	leal	1(%rdi), %edi
	movslq	%edi, %rdi
	movss	(%r9,%rdi,4), %xmm6
	movaps	%xmm2, %xmm7
	mulss	%xmm6, %xmm7
	subss	%xmm7, %xmm5
	movaps	%xmm5, %xmm7
	mulss	%xmm3, %xmm7
	movslq	%esi, %rdi
	addss	(%rax,%rdi,4), %xmm7
	movss	%xmm7, (%rax,%rdi,4)
	mulss	%xmm2, %xmm4
	mulss	%xmm0, %xmm6
	addss	%xmm4, %xmm6
	mulss	%xmm6, %xmm3
	incl	%esi
	movslq	%esi, %rsi
	addss	(%rax,%rsi,4), %xmm3
	movss	%xmm3, (%rax,%rsi,4)
	xorl	%r11d, %r11d
	cmpl	$0, 120(%rsp)
	movl	24(%rsp), %ebx
	cmovg	%r11d, %ebx
	cmpl	$0, 96(%rsp)
	cmovle	20(%rsp), %r11d
	testl	%ecx, %ecx
	jle	.LBB17_53	# bb81.bb90_crit_edge
.LBB17_41:	# bb88.preheader
	movl	96(%rsp), %r14d
	leal	(%r14,%r14), %r14d
	addl	%r11d, %r11d
	movl	120(%rsp), %r15d
	leal	(%r15,%r15), %r15d
	addl	%ebx, %ebx
	addl	%r10d, %r10d
	pxor	%xmm3, %xmm3
	xorl	%r12d, %r12d
	movaps	%xmm3, %xmm4
	.align	16
.LBB17_42:	# bb88
	movslq	%r10d, %r13
	leal	1(%r10), %ebp
	movslq	%ebp, %rbp
	movaps	%xmm1, %xmm7
	mulss	(%r8,%rbp,4), %xmm7
	movaps	%xmm7, %xmm8
	mulss	%xmm6, %xmm8
	movss	(%r8,%r13,4), %xmm9
	movaps	%xmm5, %xmm10
	mulss	%xmm9, %xmm10
	addss	%xmm8, %xmm10
	movslq	%ebx, %r13
	addss	(%rax,%r13,4), %xmm10
	movss	%xmm10, (%rax,%r13,4)
	movaps	%xmm5, %xmm8
	mulss	%xmm7, %xmm8
	movaps	%xmm6, %xmm10
	mulss	%xmm9, %xmm10
	subss	%xmm8, %xmm10
	leal	1(%rbx), %r13d
	movslq	%r13d, %r13
	addss	(%rax,%r13,4), %xmm10
	movss	%xmm10, (%rax,%r13,4)
	movslq	%r11d, %r13
	leal	1(%r11), %ebp
	movslq	%ebp, %rbp
	movss	(%r9,%rbp,4), %xmm8
	movaps	%xmm8, %xmm10
	mulss	%xmm9, %xmm10
	movss	(%r9,%r13,4), %xmm11
	movaps	%xmm11, %xmm12
	mulss	%xmm7, %xmm12
	addss	%xmm10, %xmm12
	addss	%xmm12, %xmm3
	mulss	%xmm7, %xmm8
	mulss	%xmm9, %xmm11
	subss	%xmm8, %xmm11
	addss	%xmm11, %xmm4
	addl	%r14d, %r11d
	addl	%r15d, %ebx
	addl	$2, %r10d
	incl	%r12d
	cmpl	%ecx, %r12d
	jne	.LBB17_42	# bb88
.LBB17_43:	# bb90
	movaps	%xmm2, %xmm5
	mulss	%xmm3, %xmm5
	movaps	%xmm0, %xmm6
	mulss	%xmm4, %xmm6
	subss	%xmm5, %xmm6
	addss	(%rax,%rdi,4), %xmm6
	movss	%xmm6, (%rax,%rdi,4)
	mulss	%xmm2, %xmm4
	mulss	%xmm0, %xmm3
	addss	%xmm4, %xmm3
	addss	(%rax,%rsi,4), %xmm3
	movss	%xmm3, (%rax,%rsi,4)
	movl	%edi, %esi
	addl	28(%rsp), %esi
	movl	36(%rsp), %edi
	addl	32(%rsp), %edi
	movl	%edi, 36(%rsp)
	incl	%ecx
	cmpl	%edx, %ecx
	jne	.LBB17_40	# bb81
	jmp	.LBB17_45	# return
.LBB17_44:	# bb92
	xorl	%edi, %edi
	leaq	.str24, %rsi
	leaq	.str125, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB17_45:	# return
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB17_46:	# bb25.bb30.preheader_crit_edge
	xorl	%r10d, %r10d
	jmp	.LBB17_7	# bb30.preheader
.LBB17_47:	# bb33.bb38.preheader_crit_edge
	xorl	%r10d, %r10d
	jmp	.LBB17_14	# bb38.preheader
.LBB17_48:	# bb48.bb51_crit_edge
	movl	$0, 36(%rsp)
	jmp	.LBB17_24	# bb51
.LBB17_49:	# bb51.bb65.preheader_crit_edge
	xorl	%esi, %esi
	jmp	.LBB17_26	# bb65.preheader
.LBB17_50:	# bb55.bb64_crit_edge
	pxor	%xmm3, %xmm3
	movaps	%xmm3, %xmm4
	jmp	.LBB17_31	# bb64
.LBB17_51:	# bb74.bb77_crit_edge
	movl	$0, 36(%rsp)
	jmp	.LBB17_36	# bb77
.LBB17_52:	# bb77.bb91.preheader_crit_edge
	xorl	%esi, %esi
	jmp	.LBB17_38	# bb91.preheader
.LBB17_53:	# bb81.bb90_crit_edge
	pxor	%xmm3, %xmm3
	movaps	%xmm3, %xmm4
	jmp	.LBB17_43	# bb90
	.size	cblas_chpmv, .-cblas_chpmv
.Leh_func_end13:


	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI18_0:					
	.long	2147483648	# float -0.000000e+00
	.long	2147483648	# float -0.000000e+00
	.long	2147483648	# float -0.000000e+00
	.long	2147483648	# float -0.000000e+00
	.text
	.align	16
	.globl	cblas_chpr2
	.type	cblas_chpr2,@function
cblas_chpr2:
.Leh_func_begin14:
.Llabel14:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$56, %rsp
	cmpl	$102, %edi
	movl	$4294967295, %eax
	movl	$1, %r10d
	cmove	%eax, %r10d
	movl	%r10d, 52(%rsp)
	movss	4(%rcx), %xmm0
	pxor	%xmm1, %xmm1
	ucomiss	%xmm1, %xmm0
	movss	(%rcx), %xmm1
	movq	128(%rsp), %rax
	movq	112(%rsp), %rcx
	movl	%edx, 48(%rsp)
	jne	.LBB18_2	# bb20
	jp	.LBB18_2	# bb20
.LBB18_1:	# entry
	pxor	%xmm2, %xmm2
	ucomiss	%xmm2, %xmm1
	setnp	%dl
	sete	%r10b
	testb	%dl, %r10b
	jne	.LBB18_29	# return
.LBB18_2:	# bb20
	cmpl	$121, %esi
	jne	.LBB18_4	# bb23
.LBB18_3:	# bb20
	cmpl	$101, %edi
	je	.LBB18_6	# bb27
.LBB18_4:	# bb23
	cmpl	$122, %esi
	jne	.LBB18_16	# bb39
.LBB18_5:	# bb23
	cmpl	$102, %edi
	jne	.LBB18_16	# bb39
.LBB18_6:	# bb27
	testl	%r9d, %r9d
	jg	.LBB18_30	# bb27.bb30_crit_edge
.LBB18_7:	# bb28
	movl	$1, %edx
	subl	48(%rsp), %edx
	imull	%r9d, %edx
	movl	%edx, 40(%rsp)
.LBB18_8:	# bb30
	cmpl	$0, 120(%rsp)
	jg	.LBB18_31	# bb30.bb38.preheader_crit_edge
.LBB18_9:	# bb31
	movl	$1, %edx
	subl	48(%rsp), %edx
	imull	120(%rsp), %edx
	movl	%edx, 36(%rsp)
.LBB18_10:	# bb38.preheader
	cmpl	$0, 48(%rsp)
	jle	.LBB18_29	# return
.LBB18_11:	# bb.nph95
	movl	40(%rsp), %edx
	leal	(%r9,%rdx), %esi
	addl	%esi, %esi
	movl	%esi, 8(%rsp)
	movl	36(%rsp), %esi
	movl	120(%rsp), %edi
	leal	(%rdi,%rsi), %r10d
	addl	%r10d, %r10d
	movl	%r10d, 4(%rsp)
	leal	1(,%rdx,2), %r10d
	movl	%r10d, 32(%rsp)
	addl	%edx, %edx
	movl	%edx, 40(%rsp)
	leal	(%r9,%r9), %edx
	movl	%edx, 28(%rsp)
	leal	1(,%rsi,2), %edx
	movl	%edx, 24(%rsp)
	addl	%esi, %esi
	movl	%esi, 36(%rsp)
	leal	(%rdi,%rdi), %edx
	movl	%edx, 20(%rsp)
	movl	48(%rsp), %edx
	leal	1(,%rdx,2), %esi
	movl	%esi, 16(%rsp)
	leal	-1(%rdx), %edx
	movl	%edx, 12(%rsp)
	cvtsi2ss	52(%rsp), %xmm2
	movaps	%xmm0, %xmm3
	xorps	.LCPI18_0(%rip), %xmm3
	xorl	%edx, %edx
	movl	%edx, 52(%rsp)
	movl	%edx, 44(%rsp)
	movl	%edx, %esi
	.align	16
.LBB18_12:	# bb34
	movl	32(%rsp), %edi
	leal	(%rdi,%rdx), %edi
	movslq	%edi, %rdi
	movss	(%r8,%rdi,4), %xmm4
	movaps	%xmm1, %xmm5
	mulss	%xmm4, %xmm5
	movl	40(%rsp), %edi
	leal	(%rdi,%rdx), %edi
	movslq	%edi, %rdi
	movss	(%r8,%rdi,4), %xmm6
	movaps	%xmm0, %xmm7
	mulss	%xmm6, %xmm7
	addss	%xmm5, %xmm7
	movl	52(%rsp), %edi
	movl	24(%rsp), %r10d
	leal	(%r10,%rdi), %r10d
	movslq	%r10d, %r10
	movss	(%rcx,%r10,4), %xmm5
	movaps	%xmm7, %xmm8
	mulss	%xmm5, %xmm8
	mulss	%xmm0, %xmm4
	mulss	%xmm1, %xmm6
	subss	%xmm4, %xmm6
	movl	36(%rsp), %r10d
	leal	(%r10,%rdi), %edi
	movslq	%edi, %rdi
	movss	(%rcx,%rdi,4), %xmm4
	movaps	%xmm6, %xmm9
	mulss	%xmm4, %xmm9
	addss	%xmm8, %xmm9
	addss	%xmm9, %xmm9
	movl	44(%rsp), %r10d
	movl	16(%rsp), %edi
	leal	(%rdi,%r10), %edi
	imull	%esi, %edi
	movl	%edi, %r10d
	shrl	$31, %r10d
	addl	%edi, %r10d
	movl	%r10d, %edi
	andl	$4294967294, %edi
	movslq	%edi, %r11
	addss	(%rax,%r11,4), %xmm9
	movss	%xmm9, (%rax,%r11,4)
	orl	$1, %edi
	movslq	%edi, %rdi
	movl	$0, (%rax,%rdi,4)
	movaps	%xmm0, %xmm8
	mulss	%xmm5, %xmm8
	movaps	%xmm1, %xmm9
	mulss	%xmm4, %xmm9
	addss	%xmm8, %xmm9
	mulss	%xmm1, %xmm5
	mulss	%xmm3, %xmm4
	addss	%xmm5, %xmm4
	sarl	%r10d
	leal	1(%rsi), %edi
	cmpl	48(%rsp), %edi
	jge	.LBB18_15	# bb37
.LBB18_13:	# bb.nph91
	movl	8(%rsp), %edi
	leal	(%rdi,%rdx), %edi
	movl	52(%rsp), %r11d
	movl	4(%rsp), %ebx
	leal	(%rbx,%r11), %r11d
	movl	44(%rsp), %r14d
	movl	12(%rsp), %ebx
	leal	(%rbx,%r14), %ebx
	movl	120(%rsp), %r14d
	leal	(%r14,%r14), %r14d
	leal	(%r9,%r9), %r15d
	addl	%r10d, %r10d
	xorl	%r12d, %r12d
	.align	16
.LBB18_14:	# bb35
	movslq	%edi, %r13
	movss	(%r8,%r13,4), %xmm5
	movaps	%xmm9, %xmm8
	mulss	%xmm5, %xmm8
	leal	1(%rdi), %r13d
	movslq	%r13d, %r13
	movss	(%r8,%r13,4), %xmm10
	movaps	%xmm4, %xmm11
	mulss	%xmm10, %xmm11
	addss	%xmm8, %xmm11
	movslq	%r11d, %r13
	movss	(%rcx,%r13,4), %xmm8
	movaps	%xmm6, %xmm12
	mulss	%xmm8, %xmm12
	leal	1(%r11), %r13d
	movslq	%r13d, %r13
	movss	(%rcx,%r13,4), %xmm13
	movaps	%xmm7, %xmm14
	mulss	%xmm13, %xmm14
	addss	%xmm12, %xmm14
	addss	%xmm11, %xmm14
	leal	2(%r10), %r13d
	movslq	%r13d, %rbp
	addss	(%rax,%rbp,4), %xmm14
	movss	%xmm14, (%rax,%rbp,4)
	mulss	%xmm4, %xmm5
	mulss	%xmm9, %xmm10
	subss	%xmm10, %xmm5
	mulss	%xmm7, %xmm8
	mulss	%xmm6, %xmm13
	subss	%xmm13, %xmm8
	addss	%xmm5, %xmm8
	mulss	%xmm2, %xmm8
	addl	$3, %r10d
	movslq	%r10d, %r10
	addss	(%rax,%r10,4), %xmm8
	movss	%xmm8, (%rax,%r10,4)
	addl	%r14d, %r11d
	addl	%r15d, %edi
	incl	%r12d
	cmpl	%ebx, %r12d
	movl	%r13d, %r10d
	jne	.LBB18_14	# bb35
.LBB18_15:	# bb37
	addl	28(%rsp), %edx
	movl	52(%rsp), %edi
	addl	20(%rsp), %edi
	movl	%edi, 52(%rsp)
	decl	44(%rsp)
	incl	%esi
	cmpl	48(%rsp), %esi
	jne	.LBB18_12	# bb34
	jmp	.LBB18_29	# return
.LBB18_16:	# bb39
	cmpl	$102, %edi
	sete	%dl
	cmpl	$121, %esi
	sete	%r10b
	andb	%dl, %r10b
	cmpl	$101, %edi
	sete	%dl
	cmpl	$122, %esi
	sete	%sil
	testb	%dl, %sil
	jne	.LBB18_18	# bb47
.LBB18_17:	# bb39
	notb	%r10b
	testb	$1, %r10b
	jne	.LBB18_28	# bb65
.LBB18_18:	# bb47
	testl	%r9d, %r9d
	jg	.LBB18_32	# bb47.bb50_crit_edge
.LBB18_19:	# bb48
	movl	$1, %edx
	subl	48(%rsp), %edx
	imull	%r9d, %edx
.LBB18_20:	# bb50
	cmpl	$0, 120(%rsp)
	jg	.LBB18_33	# bb50.bb64.preheader_crit_edge
.LBB18_21:	# bb51
	movl	$1, %esi
	subl	48(%rsp), %esi
	imull	120(%rsp), %esi
.LBB18_22:	# bb64.preheader
	cmpl	$0, 48(%rsp)
	jle	.LBB18_29	# return
.LBB18_23:	# bb.nph81
	movl	$1, %edi
	subl	48(%rsp), %edi
	movl	%edi, %r10d
	imull	%r9d, %r10d
	movl	%r10d, 40(%rsp)
	movl	120(%rsp), %r10d
	imull	%r10d, %edi
	addl	%esi, %esi
	addl	%edx, %edx
	cvtsi2ss	52(%rsp), %xmm2
	movaps	%xmm0, %xmm3
	xorps	.LCPI18_0(%rip), %xmm3
	movss	%xmm3, 36(%rsp)
	leal	(%r10,%r10), %r10d
	movl	%r10d, 52(%rsp)
	leal	(%r9,%r9), %r10d
	movl	%r10d, 44(%rsp)
	xorl	%r10d, %r10d
	.align	16
.LBB18_24:	# bb54
	xorl	%r11d, %r11d
	cmpl	$0, 120(%rsp)
	movl	%edi, %ebx
	cmovg	%r11d, %ebx
	testl	%r9d, %r9d
	cmovle	40(%rsp), %r11d
	movslq	%esi, %r14
	movss	(%rcx,%r14,4), %xmm3
	movaps	%xmm1, %xmm4
	mulss	%xmm3, %xmm4
	movaps	%xmm3, %xmm5
	mulss	36(%rsp), %xmm5
	leal	1(%rsi), %r14d
	movslq	%r14d, %r14
	movss	(%rcx,%r14,4), %xmm6
	movaps	%xmm1, %xmm7
	mulss	%xmm6, %xmm7
	addss	%xmm5, %xmm7
	movaps	%xmm0, %xmm5
	mulss	%xmm6, %xmm5
	addss	%xmm4, %xmm5
	movslq	%edx, %r14
	movss	(%r8,%r14,4), %xmm4
	movaps	%xmm0, %xmm8
	mulss	%xmm4, %xmm8
	leal	1(%rdx), %r14d
	movslq	%r14d, %r14
	movss	(%r8,%r14,4), %xmm9
	movaps	%xmm1, %xmm10
	mulss	%xmm9, %xmm10
	addss	%xmm8, %xmm10
	mulss	%xmm1, %xmm4
	mulss	%xmm0, %xmm9
	subss	%xmm9, %xmm4
	testl	%r10d, %r10d
	jle	.LBB18_27	# bb63
.LBB18_25:	# bb.nph
	leal	1(%r10), %r14d
	imull	%r10d, %r14d
	movl	%r14d, %r15d
	shrl	$31, %r15d
	addl	%r14d, %r15d
	andl	$4294967294, %r15d
	leal	(%r9,%r9), %r14d
	addl	%r11d, %r11d
	movl	120(%rsp), %r12d
	leal	(%r12,%r12), %r12d
	addl	%ebx, %ebx
	xorl	%r13d, %r13d
	.align	16
.LBB18_26:	# bb61
	movslq	%r11d, %rbp
	movss	(%r8,%rbp,4), %xmm8
	movaps	%xmm5, %xmm9
	mulss	%xmm8, %xmm9
	leal	1(%r11), %ebp
	movslq	%ebp, %rbp
	movss	(%r8,%rbp,4), %xmm11
	movaps	%xmm7, %xmm12
	mulss	%xmm11, %xmm12
	addss	%xmm9, %xmm12
	movslq	%ebx, %rbp
	movss	(%rcx,%rbp,4), %xmm9
	movaps	%xmm4, %xmm13
	mulss	%xmm9, %xmm13
	leal	1(%rbx), %ebp
	movslq	%ebp, %rbp
	movss	(%rcx,%rbp,4), %xmm14
	movaps	%xmm10, %xmm15
	mulss	%xmm14, %xmm15
	addss	%xmm13, %xmm15
	addss	%xmm12, %xmm15
	movslq	%r15d, %rbp
	addss	(%rax,%rbp,4), %xmm15
	movss	%xmm15, (%rax,%rbp,4)
	mulss	%xmm7, %xmm8
	mulss	%xmm5, %xmm11
	subss	%xmm11, %xmm8
	mulss	%xmm10, %xmm9
	mulss	%xmm4, %xmm14
	subss	%xmm14, %xmm9
	addss	%xmm8, %xmm9
	mulss	%xmm2, %xmm9
	leal	1(%r15), %ebp
	movslq	%ebp, %rbp
	addss	(%rax,%rbp,4), %xmm9
	movss	%xmm9, (%rax,%rbp,4)
	addl	%r14d, %r11d
	addl	%r12d, %ebx
	addl	$2, %r15d
	incl	%r13d
	cmpl	%r10d, %r13d
	jne	.LBB18_26	# bb61
.LBB18_27:	# bb63
	leal	1(%r10), %r11d
	movl	%r11d, %ebx
	imull	%r10d, %ebx
	movl	%ebx, %r14d
	shrl	$31, %r14d
	addl	%ebx, %r14d
	sarl	%r14d
	addl	%r10d, %r14d
	leal	(%r14,%r14), %r10d
	movslq	%r10d, %r10
	mulss	%xmm6, %xmm10
	mulss	%xmm3, %xmm4
	addss	%xmm10, %xmm4
	addss	%xmm4, %xmm4
	addss	(%rax,%r10,4), %xmm4
	movss	%xmm4, (%rax,%r10,4)
	leal	1(,%r14,2), %r10d
	movslq	%r10d, %r10
	movl	$0, (%rax,%r10,4)
	addl	52(%rsp), %esi
	addl	44(%rsp), %edx
	cmpl	48(%rsp), %r11d
	movl	%r11d, %r10d
	jne	.LBB18_24	# bb54
	jmp	.LBB18_29	# return
.LBB18_28:	# bb65
	xorl	%edi, %edi
	leaq	.str26, %rsi
	leaq	.str127, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB18_29:	# return
	addq	$56, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB18_30:	# bb27.bb30_crit_edge
	movl	$0, 40(%rsp)
	jmp	.LBB18_8	# bb30
.LBB18_31:	# bb30.bb38.preheader_crit_edge
	movl	$0, 36(%rsp)
	jmp	.LBB18_10	# bb38.preheader
.LBB18_32:	# bb47.bb50_crit_edge
	xorl	%edx, %edx
	jmp	.LBB18_20	# bb50
.LBB18_33:	# bb50.bb64.preheader_crit_edge
	xorl	%esi, %esi
	jmp	.LBB18_22	# bb64.preheader
	.size	cblas_chpr2, .-cblas_chpr2
.Leh_func_end14:


	.align	16
	.globl	cblas_chpr
	.type	cblas_chpr,@function
cblas_chpr:
.Leh_func_begin15:
.Llabel15:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$24, %rsp
	cmpl	$102, %edi
	movl	$4294967295, %eax
	movl	$1, %r10d
	cmove	%eax, %r10d
	pxor	%xmm1, %xmm1
	ucomiss	%xmm1, %xmm0
	setnp	%al
	sete	%r11b
	testb	%al, %r11b
	jne	.LBB19_24	# return
.LBB19_1:	# bb13
	cmpl	$121, %esi
	jne	.LBB19_3	# bb16
.LBB19_2:	# bb13
	cmpl	$101, %edi
	je	.LBB19_5	# bb20
.LBB19_3:	# bb16
	cmpl	$122, %esi
	jne	.LBB19_13	# bb29
.LBB19_4:	# bb16
	cmpl	$102, %edi
	jne	.LBB19_13	# bb29
.LBB19_5:	# bb20
	testl	%r8d, %r8d
	jg	.LBB19_25	# bb20.bb28.preheader_crit_edge
.LBB19_6:	# bb21
	movl	$1, %eax
	subl	%edx, %eax
	imull	%r8d, %eax
.LBB19_7:	# bb28.preheader
	testl	%edx, %edx
	jle	.LBB19_24	# return
.LBB19_8:	# bb.nph73
	leal	(%r8,%rax), %esi
	addl	%esi, %esi
	movl	%esi, 16(%rsp)
	cvtsi2ss	%r10d, %xmm1
	mulss	%xmm0, %xmm1
	negl	%r10d
	cvtsi2ss	%r10d, %xmm2
	leal	1(,%rax,2), %r10d
	movl	%r10d, 20(%rsp)
	addl	%eax, %eax
	leal	(%r8,%r8), %r10d
	movl	%r10d, 12(%rsp)
	leal	1(,%rdx,2), %r10d
	movl	%r10d, 8(%rsp)
	leal	-1(%rdx), %r10d
	movl	%r10d, 4(%rsp)
	xorl	%r10d, %r10d
	movl	%r10d, %esi
	movl	%r10d, %edi
	.align	16
.LBB19_9:	# bb24
	movl	20(%rsp), %r11d
	leal	(%r11,%r10), %r11d
	movslq	%r11d, %r11
	movss	(%rcx,%r11,4), %xmm3
	movaps	%xmm1, %xmm4
	mulss	%xmm3, %xmm4
	mulss	%xmm2, %xmm3
	mulss	%xmm4, %xmm3
	leal	(%rax,%r10), %r11d
	movslq	%r11d, %r11
	movss	(%rcx,%r11,4), %xmm5
	movaps	%xmm5, %xmm6
	mulss	%xmm0, %xmm6
	mulss	%xmm6, %xmm5
	subss	%xmm3, %xmm5
	movl	8(%rsp), %r11d
	leal	(%r11,%rsi), %r11d
	imull	%edi, %r11d
	movl	%r11d, %ebx
	shrl	$31, %ebx
	addl	%r11d, %ebx
	movl	%ebx, %r11d
	andl	$4294967294, %r11d
	movslq	%r11d, %r14
	addss	(%r9,%r14,4), %xmm5
	movss	%xmm5, (%r9,%r14,4)
	orl	$1, %r11d
	movslq	%r11d, %r11
	movl	$0, (%r9,%r11,4)
	sarl	%ebx
	leal	1(%rdi), %r11d
	cmpl	%edx, %r11d
	jge	.LBB19_12	# bb27
.LBB19_10:	# bb.nph70
	movl	16(%rsp), %r11d
	leal	(%r11,%r10), %r11d
	movl	4(%rsp), %r14d
	leal	(%r14,%rsi), %r14d
	leal	(%r8,%r8), %r15d
	addl	%ebx, %ebx
	xorl	%r12d, %r12d
	.align	16
.LBB19_11:	# bb25
	movslq	%r11d, %r13
	leal	1(%r11), %ebp
	movslq	%ebp, %rbp
	movaps	%xmm2, %xmm3
	mulss	(%rcx,%rbp,4), %xmm3
	movaps	%xmm3, %xmm5
	mulss	%xmm4, %xmm5
	movss	(%rcx,%r13,4), %xmm7
	movaps	%xmm7, %xmm8
	mulss	%xmm6, %xmm8
	subss	%xmm5, %xmm8
	leal	2(%rbx), %r13d
	movslq	%r13d, %rbp
	addss	(%r9,%rbp,4), %xmm8
	movss	%xmm8, (%r9,%rbp,4)
	mulss	%xmm6, %xmm3
	mulss	%xmm4, %xmm7
	addss	%xmm3, %xmm7
	addl	$3, %ebx
	movslq	%ebx, %rbx
	addss	(%r9,%rbx,4), %xmm7
	movss	%xmm7, (%r9,%rbx,4)
	addl	%r15d, %r11d
	incl	%r12d
	cmpl	%r14d, %r12d
	movl	%r13d, %ebx
	jne	.LBB19_11	# bb25
.LBB19_12:	# bb27
	addl	12(%rsp), %r10d
	decl	%esi
	incl	%edi
	cmpl	%edx, %edi
	jne	.LBB19_9	# bb24
	jmp	.LBB19_24	# return
.LBB19_13:	# bb29
	cmpl	$102, %edi
	sete	%al
	cmpl	$121, %esi
	sete	%r11b
	andb	%al, %r11b
	cmpl	$101, %edi
	sete	%al
	cmpl	$122, %esi
	sete	%sil
	testb	%al, %sil
	jne	.LBB19_15	# bb37
.LBB19_14:	# bb29
	notb	%r11b
	testb	$1, %r11b
	jne	.LBB19_23	# bb49
.LBB19_15:	# bb37
	testl	%r8d, %r8d
	jg	.LBB19_26	# bb37.bb48.preheader_crit_edge
.LBB19_16:	# bb38
	movl	$1, %eax
	subl	%edx, %eax
	imull	%r8d, %eax
.LBB19_17:	# bb48.preheader
	testl	%edx, %edx
	jle	.LBB19_24	# return
.LBB19_18:	# bb.nph
	cvtsi2ss	%r10d, %xmm1
	mulss	%xmm0, %xmm1
	movl	$1, %esi
	subl	%edx, %esi
	imull	%r8d, %esi
	negl	%r10d
	cvtsi2ss	%r10d, %xmm2
	addl	%eax, %eax
	leal	(%r8,%r8), %edi
	movl	%edi, 20(%rsp)
	xorl	%edi, %edi
	.align	16
.LBB19_19:	# bb41
	xorl	%r10d, %r10d
	testl	%edi, %edi
	movl	%edi, %r11d
	cmovs	%r10d, %r11d
	movslq	%eax, %rbx
	movaps	%xmm0, %xmm3
	mulss	(%rcx,%rbx,4), %xmm3
	testl	%r8d, %r8d
	movl	%esi, %ebx
	cmovg	%r10d, %ebx
	addl	%ebx, %ebx
	leal	1(%rax), %r14d
	movslq	%r14d, %r14
	movaps	%xmm1, %xmm4
	mulss	(%rcx,%r14,4), %xmm4
	leal	1(%rdi), %r14d
	imull	%edi, %r14d
	movl	%r14d, %r15d
	shrl	$31, %r15d
	addl	%r14d, %r15d
	movl	%r15d, %r14d
	andl	$4294967294, %r14d
	sarl	%r15d
	leal	(%r8,%r8), %r12d
	jmp	.LBB19_21	# bb46
.LBB19_20:	# bb45
	movaps	%xmm6, %xmm7
	mulss	%xmm4, %xmm7
	movaps	%xmm5, %xmm8
	mulss	%xmm3, %xmm8
	subss	%xmm7, %xmm8
	movslq	%r14d, %r13
	addss	(%r9,%r13,4), %xmm8
	movss	%xmm8, (%r9,%r13,4)
	mulss	%xmm4, %xmm5
	mulss	%xmm3, %xmm6
	addss	%xmm5, %xmm6
	leal	1(%r14), %r13d
	movslq	%r13d, %r13
	addss	(%r9,%r13,4), %xmm6
	movss	%xmm6, (%r9,%r13,4)
	addl	%r12d, %ebx
	addl	$2, %r14d
	incl	%r10d
.LBB19_21:	# bb46
	movslq	%ebx, %r13
	leal	1(%rbx), %ebp
	movslq	%ebp, %rbp
	movaps	%xmm2, %xmm6
	mulss	(%rcx,%rbp,4), %xmm6
	cmpl	%r11d, %r10d
	movss	(%rcx,%r13,4), %xmm5
	jne	.LBB19_20	# bb45
.LBB19_22:	# bb47
	mulss	%xmm4, %xmm6
	mulss	%xmm3, %xmm5
	subss	%xmm6, %xmm5
	addl	%edi, %r15d
	leal	(%r15,%r15), %r10d
	movslq	%r10d, %r10
	addss	(%r9,%r10,4), %xmm5
	movss	%xmm5, (%r9,%r10,4)
	leal	1(,%r15,2), %r10d
	movslq	%r10d, %r10
	movl	$0, (%r9,%r10,4)
	addl	20(%rsp), %eax
	incl	%edi
	cmpl	%edx, %edi
	jne	.LBB19_19	# bb41
	jmp	.LBB19_24	# return
.LBB19_23:	# bb49
	xorl	%edi, %edi
	leaq	.str28, %rsi
	leaq	.str129, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB19_24:	# return
	addq	$24, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB19_25:	# bb20.bb28.preheader_crit_edge
	xorl	%eax, %eax
	jmp	.LBB19_7	# bb28.preheader
.LBB19_26:	# bb37.bb48.preheader_crit_edge
	xorl	%eax, %eax
	jmp	.LBB19_17	# bb48.preheader
	.size	cblas_chpr, .-cblas_chpr
.Leh_func_end15:


	.align	16
	.globl	cblas_cscal
	.type	cblas_cscal,@function
cblas_cscal:
	testl	%ecx, %ecx
	movss	4(%rsi), %xmm0
	movss	(%rsi), %xmm1
	jle	.LBB20_4	# return
.LBB20_1:	# entry
	testl	%edi, %edi
	jle	.LBB20_4	# return
.LBB20_2:	# bb.nph
	addl	%ecx, %ecx
	xorl	%eax, %eax
	movl	%eax, %esi
	.align	16
.LBB20_3:	# bb1
	movslq	%eax, %r8
	movss	(%rdx,%r8,4), %xmm2
	movaps	%xmm2, %xmm3
	mulss	%xmm1, %xmm3
	leal	1(%rax), %r9d
	movslq	%r9d, %r9
	movss	(%rdx,%r9,4), %xmm4
	movaps	%xmm4, %xmm5
	mulss	%xmm0, %xmm5
	subss	%xmm5, %xmm3
	movss	%xmm3, (%rdx,%r8,4)
	mulss	%xmm0, %xmm2
	mulss	%xmm1, %xmm4
	addss	%xmm2, %xmm4
	movss	%xmm4, (%rdx,%r9,4)
	addl	%ecx, %eax
	incl	%esi
	cmpl	%edi, %esi
	jne	.LBB20_3	# bb1
.LBB20_4:	# return
	ret
	.size	cblas_cscal, .-cblas_cscal


	.align	16
	.globl	cblas_csscal
	.type	cblas_csscal,@function
cblas_csscal:
	testl	%edx, %edx
	jle	.LBB21_4	# return
.LBB21_1:	# entry
	testl	%edi, %edi
	jle	.LBB21_4	# return
.LBB21_2:	# bb.nph
	addl	%edx, %edx
	xorl	%eax, %eax
	movl	%eax, %ecx
	.align	16
.LBB21_3:	# bb1
	movslq	%eax, %r8
	movaps	%xmm0, %xmm1
	mulss	(%rsi,%r8,4), %xmm1
	movss	%xmm1, (%rsi,%r8,4)
	leal	1(%rax), %r8d
	movslq	%r8d, %r8
	movaps	%xmm0, %xmm1
	mulss	(%rsi,%r8,4), %xmm1
	movss	%xmm1, (%rsi,%r8,4)
	addl	%edx, %eax
	incl	%ecx
	cmpl	%edi, %ecx
	jne	.LBB21_3	# bb1
.LBB21_4:	# return
	ret
	.size	cblas_csscal, .-cblas_csscal


	.align	16
	.globl	cblas_cswap
	.type	cblas_cswap,@function
cblas_cswap:
	pushq	%r14
	pushq	%rbx
	testl	%edx, %edx
	jg	.LBB22_8	# entry.bb2_crit_edge
.LBB22_1:	# bb
	movl	$1, %eax
	subl	%edi, %eax
	imull	%edx, %eax
.LBB22_2:	# bb2
	testl	%r8d, %r8d
	jg	.LBB22_9	# bb2.bb7.preheader_crit_edge
.LBB22_3:	# bb3
	movl	$1, %r9d
	subl	%edi, %r9d
	imull	%r8d, %r9d
.LBB22_4:	# bb7.preheader
	testl	%edi, %edi
	jle	.LBB22_7	# return
.LBB22_5:	# bb.nph
	addl	%r8d, %r8d
	addl	%r9d, %r9d
	addl	%edx, %edx
	addl	%eax, %eax
	xorl	%r10d, %r10d
	.align	16
.LBB22_6:	# bb6
	movslq	%r9d, %r11
	movss	(%rcx,%r11,4), %xmm0
	movslq	%eax, %rbx
	movss	(%rsi,%rbx,4), %xmm1
	leal	1(%rax), %r14d
	movslq	%r14d, %r14
	movss	(%rsi,%r14,4), %xmm2
	movss	%xmm0, (%rsi,%rbx,4)
	leal	1(%r9), %ebx
	movslq	%ebx, %rbx
	movss	(%rcx,%rbx,4), %xmm0
	movss	%xmm0, (%rsi,%r14,4)
	movss	%xmm1, (%rcx,%r11,4)
	movss	%xmm2, (%rcx,%rbx,4)
	addl	%r8d, %r9d
	addl	%edx, %eax
	incl	%r10d
	cmpl	%edi, %r10d
	jne	.LBB22_6	# bb6
.LBB22_7:	# return
	popq	%rbx
	popq	%r14
	ret
.LBB22_8:	# entry.bb2_crit_edge
	xorl	%eax, %eax
	jmp	.LBB22_2	# bb2
.LBB22_9:	# bb2.bb7.preheader_crit_edge
	xorl	%r9d, %r9d
	jmp	.LBB22_4	# bb7.preheader
	.size	cblas_cswap, .-cblas_cswap


	.section	.rodata.cst4,"aM",@progbits,4
	.align	4
.LCPI23_0:					
	.long	1065353216	# float 1.000000e+00
	.text
	.align	16
	.globl	cblas_csymm
	.type	cblas_csymm,@function
cblas_csymm:
.Leh_func_begin16:
.Llabel16:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$104, %rsp
	movl	%ecx, 76(%rsp)
	movss	(%r9), %xmm0
	pxor	%xmm1, %xmm1
	ucomiss	%xmm1, %xmm0
	setp	%al
	setne	%cl
	setnp	%r10b
	sete	%r11b
	andb	%r10b, %r11b
	movss	4(%r9), %xmm2
	ucomiss	%xmm1, %xmm2
	setp	%r9b
	setnp	%r10b
	sete	%bl
	setne	%r14b
	andb	%r10b, %bl
	andb	%r11b, %bl
	movb	%bl, 96(%rsp)
	orb	%al, %cl
	orb	%r9b, %r14b
	orb	%cl, %r14b
	testb	%r14b, %r14b
	movq	192(%rsp), %rax
	movss	4(%rax), %xmm1
	movss	(%rax), %xmm3
	movq	200(%rsp), %rax
	jne	.LBB23_3	# bb33
.LBB23_1:	# entry
	ucomiss	.LCPI23_0(%rip), %xmm3
	jne	.LBB23_3	# bb33
	jp	.LBB23_3	# bb33
.LBB23_2:	# entry
	pxor	%xmm4, %xmm4
	ucomiss	%xmm4, %xmm1
	setnp	%cl
	sete	%r9b
	testb	%cl, %r9b
	jne	.LBB23_66	# return
.LBB23_3:	# bb33
	cmpl	$101, %edi
	je	.LBB23_67	# bb33.bb42_crit_edge
.LBB23_4:	# bb35
	cmpl	$141, %esi
	movl	$142, %ecx
	movl	$141, %esi
	cmove	%ecx, %esi
	cmpl	$121, %edx
	movl	$122, %ecx
	movl	$121, %edx
	cmove	%ecx, %edx
	movl	76(%rsp), %ecx
	movl	%ecx, 100(%rsp)
	movl	%r8d, 76(%rsp)
.LBB23_5:	# bb42
	pxor	%xmm4, %xmm4
	ucomiss	%xmm4, %xmm1
	jne	.LBB23_13	# bb50
	jp	.LBB23_13	# bb50
.LBB23_6:	# bb42
	pxor	%xmm4, %xmm4
	ucomiss	%xmm4, %xmm3
	jne	.LBB23_13	# bb50
	jp	.LBB23_13	# bb50
.LBB23_7:	# bb49.preheader
	cmpl	$0, 76(%rsp)
	jle	.LBB23_20	# bb58
.LBB23_8:	# bb.nph124
	cmpl	$0, 100(%rsp)
	jle	.LBB23_20	# bb58
.LBB23_9:	# bb47.preheader.preheader
	movl	208(%rsp), %ecx
	leal	(%rcx,%rcx), %edi
	xorl	%ecx, %ecx
	movl	%ecx, %r8d
	jmp	.LBB23_12	# bb47.preheader
	.align	16
.LBB23_10:	# bb46
	movslq	%r9d, %r11
	movl	$0, (%rax,%r11,4)
	leal	1(%r9), %r11d
	movslq	%r11d, %r11
	movl	$0, (%rax,%r11,4)
	addl	$2, %r9d
	incl	%r10d
	cmpl	100(%rsp), %r10d
	jne	.LBB23_10	# bb46
.LBB23_11:	# bb48
	addl	%edi, %ecx
	incl	%r8d
	cmpl	76(%rsp), %r8d
	je	.LBB23_20	# bb58
.LBB23_12:	# bb47.preheader
	xorl	%r10d, %r10d
	movl	%ecx, %r9d
	jmp	.LBB23_10	# bb46
.LBB23_13:	# bb50
	pxor	%xmm4, %xmm4
	ucomiss	%xmm4, %xmm1
	setnp	%cl
	sete	%dil
	andb	%cl, %dil
	ucomiss	.LCPI23_0(%rip), %xmm3
	setnp	%cl
	sete	%r8b
	andb	%cl, %r8b
	testb	%r8b, %dil
	jne	.LBB23_20	# bb58
.LBB23_14:	# bb50
	cmpl	$0, 76(%rsp)
	jle	.LBB23_20	# bb58
.LBB23_15:	# bb.nph175
	cmpl	$0, 100(%rsp)
	jle	.LBB23_20	# bb58
.LBB23_16:	# bb55.preheader.preheader
	movl	208(%rsp), %ecx
	leal	(%rcx,%rcx), %ecx
	xorl	%r8d, %r8d
	movl	%r8d, %edi
	.align	16
.LBB23_17:	# bb55.preheader
	xorl	%r9d, %r9d
	movl	%r8d, %r10d
	.align	16
.LBB23_18:	# bb54
	movslq	%r10d, %r11
	movss	(%rax,%r11,4), %xmm4
	movaps	%xmm3, %xmm5
	mulss	%xmm4, %xmm5
	leal	1(%r10), %ebx
	movslq	%ebx, %rbx
	movss	(%rax,%rbx,4), %xmm6
	movaps	%xmm1, %xmm7
	mulss	%xmm6, %xmm7
	subss	%xmm7, %xmm5
	movss	%xmm5, (%rax,%r11,4)
	mulss	%xmm1, %xmm4
	mulss	%xmm3, %xmm6
	addss	%xmm4, %xmm6
	movss	%xmm6, (%rax,%rbx,4)
	addl	$2, %r10d
	incl	%r9d
	cmpl	100(%rsp), %r9d
	jne	.LBB23_18	# bb54
.LBB23_19:	# bb56
	addl	%ecx, %r8d
	incl	%edi
	cmpl	76(%rsp), %edi
	jne	.LBB23_17	# bb55.preheader
.LBB23_20:	# bb58
	testb	$1, 96(%rsp)
	jne	.LBB23_66	# return
.LBB23_21:	# bb60
	cmpl	$121, %edx
	jne	.LBB23_32	# bb72
.LBB23_22:	# bb60
	cmpl	$141, %esi
	jne	.LBB23_32	# bb72
.LBB23_23:	# bb71.preheader
	cmpl	$0, 76(%rsp)
	jle	.LBB23_66	# return
.LBB23_24:	# bb.nph171
	cmpl	$0, 100(%rsp)
	jle	.LBB23_66	# return
.LBB23_25:	# bb69.preheader.preheader
	movl	168(%rsp), %ecx
	leal	2(,%rcx,2), %ecx
	movl	%ecx, 4(%rsp)
	movl	184(%rsp), %ecx
	leal	(%rcx,%rcx), %ecx
	movl	%ecx, 20(%rsp)
	movl	208(%rsp), %ecx
	leal	(%rcx,%rcx), %ecx
	movl	%ecx, 16(%rsp)
	movl	76(%rsp), %ecx
	leal	-1(%rcx), %ecx
	xorl	%edx, %edx
	movl	%edx, 64(%rsp)
	movl	%edx, 92(%rsp)
	movl	%edx, 80(%rsp)
	movl	%edx, 24(%rsp)
	jmp	.LBB23_31	# bb69.preheader
	.align	16
.LBB23_26:	# bb65
	movl	92(%rsp), %esi
	leal	(%rsi,%rdx), %esi
	movslq	%esi, %rsi
	movq	176(%rsp), %rdi
	movss	(%rdi,%rsi,4), %xmm1
	movaps	%xmm2, %xmm3
	mulss	%xmm1, %xmm3
	movl	32(%rsp), %esi
	leal	(%rsi,%rdx), %esi
	movslq	%esi, %rsi
	movss	(%rdi,%rsi,4), %xmm4
	movaps	%xmm0, %xmm5
	mulss	%xmm4, %xmm5
	addss	%xmm3, %xmm5
	movq	160(%rsp), %rsi
	movq	40(%rsp), %rdi
	movss	(%rsi,%rdi,4), %xmm3
	movaps	%xmm5, %xmm6
	mulss	%xmm3, %xmm6
	mulss	%xmm2, %xmm4
	mulss	%xmm0, %xmm1
	subss	%xmm4, %xmm1
	movq	48(%rsp), %rdi
	movss	(%rsi,%rdi,4), %xmm4
	movaps	%xmm1, %xmm7
	mulss	%xmm4, %xmm7
	subss	%xmm6, %xmm7
	movl	80(%rsp), %esi
	leal	(%rsi,%rdx), %esi
	movslq	%esi, %rsi
	addss	(%rax,%rsi,4), %xmm7
	movss	%xmm7, (%rax,%rsi,4)
	mulss	%xmm5, %xmm4
	mulss	%xmm1, %xmm3
	addss	%xmm4, %xmm3
	movl	36(%rsp), %edi
	leal	(%rdi,%rdx), %edi
	movslq	%edi, %rdi
	addss	(%rax,%rdi,4), %xmm3
	movss	%xmm3, (%rax,%rdi,4)
	movl	28(%rsp), %r8d
	cmpl	76(%rsp), %r8d
	jge	.LBB23_68	# bb65.bb68_crit_edge
.LBB23_27:	# bb.nph165
	movl	60(%rsp), %r8d
	leal	(%r8,%rdx), %r8d
	movl	56(%rsp), %r9d
	leal	(%r9,%rdx), %r9d
	movl	184(%rsp), %r10d
	leal	(%r10,%r10), %r10d
	movl	208(%rsp), %r11d
	leal	(%r11,%r11), %r11d
	pxor	%xmm3, %xmm3
	xorl	%ebx, %ebx
	movl	64(%rsp), %r14d
	movaps	%xmm3, %xmm4
	.align	16
.LBB23_28:	# bb66
	leal	3(%r14), %r15d
	movslq	%r15d, %r15
	movq	160(%rsp), %r12
	movss	(%r12,%r15,4), %xmm6
	movaps	%xmm6, %xmm7
	mulss	%xmm5, %xmm7
	addl	$2, %r14d
	movslq	%r14d, %r15
	movss	(%r12,%r15,4), %xmm8
	movaps	%xmm8, %xmm9
	mulss	%xmm1, %xmm9
	subss	%xmm7, %xmm9
	movslq	%r9d, %r15
	addss	(%rax,%r15,4), %xmm9
	movslq	%r8d, %r12
	movq	176(%rsp), %r13
	movss	(%r13,%r12,4), %xmm7
	leal	1(%r8), %r12d
	movslq	%r12d, %r12
	movss	(%r13,%r12,4), %xmm10
	movss	%xmm9, (%rax,%r15,4)
	movaps	%xmm6, %xmm9
	mulss	%xmm1, %xmm9
	movaps	%xmm8, %xmm11
	mulss	%xmm5, %xmm11
	addss	%xmm9, %xmm11
	leal	1(%r9), %r15d
	movslq	%r15d, %r15
	addss	(%rax,%r15,4), %xmm11
	movss	%xmm11, (%rax,%r15,4)
	movaps	%xmm6, %xmm9
	mulss	%xmm7, %xmm9
	movaps	%xmm8, %xmm11
	mulss	%xmm10, %xmm11
	addss	%xmm9, %xmm11
	addss	%xmm11, %xmm4
	mulss	%xmm10, %xmm6
	mulss	%xmm7, %xmm8
	subss	%xmm6, %xmm8
	addss	%xmm8, %xmm3
	addl	%r10d, %r8d
	addl	%r11d, %r9d
	incl	%ebx
	cmpl	%ecx, %ebx
	jne	.LBB23_28	# bb66
.LBB23_29:	# bb68
	movaps	%xmm2, %xmm1
	mulss	%xmm4, %xmm1
	movaps	%xmm0, %xmm5
	mulss	%xmm3, %xmm5
	subss	%xmm1, %xmm5
	addss	(%rax,%rsi,4), %xmm5
	movss	%xmm5, (%rax,%rsi,4)
	mulss	%xmm2, %xmm3
	mulss	%xmm0, %xmm4
	addss	%xmm3, %xmm4
	addss	(%rax,%rdi,4), %xmm4
	movss	%xmm4, (%rax,%rdi,4)
	addl	$2, %edx
	movl	96(%rsp), %esi
	incl	%esi
	movl	%esi, 96(%rsp)
	cmpl	100(%rsp), %esi
	jne	.LBB23_26	# bb65
.LBB23_30:	# bb70
	movl	64(%rsp), %edx
	addl	4(%rsp), %edx
	movl	%edx, 64(%rsp)
	movl	92(%rsp), %edx
	addl	20(%rsp), %edx
	movl	%edx, 92(%rsp)
	movl	80(%rsp), %edx
	addl	16(%rsp), %edx
	movl	%edx, 80(%rsp)
	decl	%ecx
	movl	24(%rsp), %edx
	incl	%edx
	movl	%edx, 24(%rsp)
	cmpl	76(%rsp), %edx
	je	.LBB23_66	# return
.LBB23_31:	# bb69.preheader
	movl	92(%rsp), %esi
	movl	20(%rsp), %edx
	leal	(%rdx,%rsi), %edx
	movl	%edx, 60(%rsp)
	movl	80(%rsp), %edi
	movl	16(%rsp), %edx
	leal	(%rdx,%rdi), %edx
	movl	%edx, 56(%rsp)
	movl	64(%rsp), %edx
	movslq	%edx, %r8
	movq	%r8, 48(%rsp)
	leal	1(%rdx), %edx
	movslq	%edx, %rdx
	movq	%rdx, 40(%rsp)
	leal	1(%rdi), %edx
	movl	%edx, 36(%rsp)
	leal	1(%rsi), %edx
	movl	%edx, 32(%rsp)
	movl	24(%rsp), %edx
	leal	1(%rdx), %edx
	movl	%edx, 28(%rsp)
	xorl	%edx, %edx
	movl	%edx, 96(%rsp)
	jmp	.LBB23_26	# bb65
.LBB23_32:	# bb72
	cmpl	$122, %edx
	jne	.LBB23_43	# bb85
.LBB23_33:	# bb72
	cmpl	$141, %esi
	jne	.LBB23_43	# bb85
.LBB23_34:	# bb84.preheader
	cmpl	$0, 76(%rsp)
	jle	.LBB23_66	# return
.LBB23_35:	# bb.nph159
	cmpl	$0, 100(%rsp)
	jle	.LBB23_66	# return
.LBB23_36:	# bb82.preheader.preheader
	movl	168(%rsp), %ecx
	leal	2(,%rcx,2), %esi
	movl	%esi, 48(%rsp)
	addl	%ecx, %ecx
	movl	%ecx, 16(%rsp)
	movl	184(%rsp), %ecx
	leal	(%rcx,%rcx), %ecx
	movl	%ecx, 40(%rsp)
	movl	208(%rsp), %ecx
	leal	(%rcx,%rcx), %ecx
	movl	%ecx, 36(%rsp)
	xorl	%ecx, %ecx
	movl	%ecx, 80(%rsp)
	movl	%ecx, 96(%rsp)
	movl	%ecx, 92(%rsp)
	movl	%ecx, %esi
	jmp	.LBB23_42	# bb82.preheader
	.align	16
.LBB23_37:	# bb78
	movl	96(%rsp), %r8d
	leal	(%r8,%rcx), %r8d
	movslq	%r8d, %r8
	movq	176(%rsp), %r9
	movss	(%r9,%r8,4), %xmm1
	movaps	%xmm2, %xmm3
	mulss	%xmm1, %xmm3
	movl	56(%rsp), %r8d
	leal	(%r8,%rcx), %r8d
	movslq	%r8d, %r8
	movss	(%r9,%r8,4), %xmm4
	movaps	%xmm0, %xmm5
	mulss	%xmm4, %xmm5
	addss	%xmm3, %xmm5
	mulss	%xmm2, %xmm4
	mulss	%xmm0, %xmm1
	subss	%xmm4, %xmm1
	testl	%esi, %esi
	jle	.LBB23_69	# bb78.bb81_crit_edge
.LBB23_38:	# bb79.preheader
	movl	184(%rsp), %r8d
	leal	(%r8,%r8), %r8d
	movl	208(%rsp), %r9d
	leal	(%r9,%r9), %r9d
	pxor	%xmm3, %xmm3
	xorl	%r10d, %r10d
	movl	%ecx, %r11d
	movl	%ecx, %ebx
	movl	80(%rsp), %r14d
	movaps	%xmm3, %xmm4
	.align	16
.LBB23_39:	# bb79
	movslq	%r14d, %r15
	movq	160(%rsp), %r12
	movss	(%r12,%r15,4), %xmm6
	movaps	%xmm6, %xmm7
	mulss	%xmm1, %xmm7
	leal	1(%r14), %r15d
	movslq	%r15d, %r15
	movss	(%r12,%r15,4), %xmm8
	movaps	%xmm8, %xmm9
	mulss	%xmm5, %xmm9
	subss	%xmm9, %xmm7
	movslq	%ebx, %r15
	addss	(%rax,%r15,4), %xmm7
	movslq	%r11d, %r12
	movq	176(%rsp), %r13
	movss	(%r13,%r12,4), %xmm9
	leal	1(%r11), %r12d
	movslq	%r12d, %r12
	movss	(%r13,%r12,4), %xmm10
	movss	%xmm7, (%rax,%r15,4)
	movaps	%xmm6, %xmm7
	mulss	%xmm5, %xmm7
	movaps	%xmm8, %xmm11
	mulss	%xmm1, %xmm11
	addss	%xmm7, %xmm11
	leal	1(%rbx), %r15d
	movslq	%r15d, %r15
	addss	(%rax,%r15,4), %xmm11
	movss	%xmm11, (%rax,%r15,4)
	movaps	%xmm8, %xmm7
	mulss	%xmm9, %xmm7
	movaps	%xmm6, %xmm11
	mulss	%xmm10, %xmm11
	addss	%xmm7, %xmm11
	addss	%xmm11, %xmm4
	mulss	%xmm9, %xmm6
	mulss	%xmm10, %xmm8
	subss	%xmm8, %xmm6
	addss	%xmm6, %xmm3
	addl	%r8d, %r11d
	addl	%r9d, %ebx
	addl	$2, %r14d
	incl	%r10d
	cmpl	%esi, %r10d
	jne	.LBB23_39	# bb79
.LBB23_40:	# bb81
	movq	160(%rsp), %r9
	movq	64(%rsp), %r8
	movss	(%r9,%r8,4), %xmm6
	movaps	%xmm5, %xmm7
	mulss	%xmm6, %xmm7
	movss	(%r9,%rdx,4), %xmm8
	movaps	%xmm1, %xmm9
	mulss	%xmm8, %xmm9
	subss	%xmm7, %xmm9
	movl	92(%rsp), %r8d
	leal	(%r8,%rcx), %r8d
	movslq	%r8d, %r8
	addss	(%rax,%r8,4), %xmm9
	movss	%xmm9, (%rax,%r8,4)
	mulss	%xmm8, %xmm5
	mulss	%xmm6, %xmm1
	addss	%xmm5, %xmm1
	movl	60(%rsp), %r9d
	leal	(%r9,%rcx), %r9d
	movslq	%r9d, %r9
	addss	(%rax,%r9,4), %xmm1
	movss	%xmm1, (%rax,%r9,4)
	movaps	%xmm2, %xmm1
	mulss	%xmm4, %xmm1
	movaps	%xmm0, %xmm5
	mulss	%xmm3, %xmm5
	subss	%xmm1, %xmm5
	addss	(%rax,%r8,4), %xmm5
	movss	%xmm5, (%rax,%r8,4)
	mulss	%xmm2, %xmm3
	mulss	%xmm0, %xmm4
	addss	%xmm3, %xmm4
	addss	(%rax,%r9,4), %xmm4
	movss	%xmm4, (%rax,%r9,4)
	addl	$2, %ecx
	incl	%edi
	cmpl	100(%rsp), %edi
	jne	.LBB23_37	# bb78
.LBB23_41:	# bb83
	movl	%edx, %ecx
	addl	48(%rsp), %ecx
	movl	80(%rsp), %edi
	addl	16(%rsp), %edi
	movl	%edi, 80(%rsp)
	movl	96(%rsp), %edi
	addl	40(%rsp), %edi
	movl	%edi, 96(%rsp)
	movl	92(%rsp), %edi
	addl	36(%rsp), %edi
	movl	%edi, 92(%rsp)
	incl	%esi
	cmpl	76(%rsp), %esi
	je	.LBB23_66	# return
.LBB23_42:	# bb82.preheader
	movslq	%ecx, %rdx
	incl	%ecx
	movslq	%ecx, %rcx
	movq	%rcx, 64(%rsp)
	movl	92(%rsp), %ecx
	leal	1(%rcx), %ecx
	movl	%ecx, 60(%rsp)
	movl	96(%rsp), %ecx
	leal	1(%rcx), %ecx
	movl	%ecx, 56(%rsp)
	xorl	%ecx, %ecx
	movl	%ecx, %edi
	jmp	.LBB23_37	# bb78
.LBB23_43:	# bb85
	cmpl	$121, %edx
	jne	.LBB23_54	# bb98
.LBB23_44:	# bb85
	cmpl	$142, %esi
	jne	.LBB23_54	# bb98
.LBB23_45:	# bb97.preheader
	cmpl	$0, 76(%rsp)
	jle	.LBB23_66	# return
.LBB23_46:	# bb.nph147
	cmpl	$0, 100(%rsp)
	jle	.LBB23_66	# return
.LBB23_47:	# bb95.preheader.preheader
	movl	184(%rsp), %ecx
	addl	%ecx, %ecx
	movl	%ecx, 12(%rsp)
	movl	208(%rsp), %ecx
	addl	%ecx, %ecx
	movl	%ecx, 8(%rsp)
	xorl	%ecx, %ecx
	movl	%ecx, 60(%rsp)
	movl	%ecx, 64(%rsp)
	movl	%ecx, 20(%rsp)
	jmp	.LBB23_53	# bb95.preheader
	.align	16
.LBB23_48:	# bb91
	movl	60(%rsp), %esi
	leal	(%rsi,%rcx), %esi
	movslq	%esi, %rsi
	movq	176(%rsp), %rdi
	movss	(%rdi,%rsi,4), %xmm1
	movaps	%xmm2, %xmm3
	mulss	%xmm1, %xmm3
	movl	24(%rsp), %esi
	leal	(%rsi,%rcx), %esi
	movslq	%esi, %rsi
	movss	(%rdi,%rsi,4), %xmm4
	movaps	%xmm0, %xmm5
	mulss	%xmm4, %xmm5
	addss	%xmm3, %xmm5
	movl	96(%rsp), %esi
	movslq	%esi, %rdi
	leal	1(%rsi), %esi
	movslq	%esi, %rsi
	movq	160(%rsp), %r8
	movss	(%r8,%rsi,4), %xmm3
	movaps	%xmm5, %xmm6
	mulss	%xmm3, %xmm6
	mulss	%xmm2, %xmm4
	mulss	%xmm0, %xmm1
	subss	%xmm4, %xmm1
	movss	(%r8,%rdi,4), %xmm4
	movaps	%xmm1, %xmm7
	mulss	%xmm4, %xmm7
	subss	%xmm6, %xmm7
	movl	64(%rsp), %esi
	leal	(%rsi,%rcx), %esi
	movslq	%esi, %rsi
	movq	%rsi, 80(%rsp)
	addss	(%rax,%rsi,4), %xmm7
	movss	%xmm7, (%rax,%rsi,4)
	mulss	%xmm5, %xmm4
	mulss	%xmm1, %xmm3
	addss	%xmm4, %xmm3
	movl	28(%rsp), %esi
	leal	(%rsi,%rcx), %esi
	movslq	%esi, %rsi
	addss	(%rax,%rsi,4), %xmm3
	movss	%xmm3, (%rax,%rsi,4)
	movl	92(%rsp), %edi
	leal	1(%rdi), %edi
	cmpl	100(%rsp), %edi
	jge	.LBB23_70	# bb91.bb94_crit_edge
.LBB23_49:	# bb.nph141
	movl	48(%rsp), %edi
	leal	(%rdi,%rcx), %edi
	movl	40(%rsp), %r8d
	leal	(%r8,%rcx), %r8d
	movl	36(%rsp), %r9d
	leal	(%r9,%rcx), %r9d
	movl	32(%rsp), %r10d
	leal	(%r10,%rcx), %r10d
	movl	96(%rsp), %r11d
	leal	3(%r11), %ebx
	leal	2(%r11), %r11d
	pxor	%xmm3, %xmm3
	xorl	%r14d, %r14d
	movl	%r14d, %r15d
	movaps	%xmm3, %xmm4
	.align	16
.LBB23_50:	# bb92
	leal	(%rbx,%r14), %r12d
	movslq	%r12d, %r12
	movq	160(%rsp), %r13
	movss	(%r13,%r12,4), %xmm6
	movaps	%xmm5, %xmm7
	mulss	%xmm6, %xmm7
	leal	(%r11,%r14), %r12d
	movslq	%r12d, %r12
	movss	(%r13,%r12,4), %xmm8
	movaps	%xmm1, %xmm9
	mulss	%xmm8, %xmm9
	subss	%xmm7, %xmm9
	leal	(%r8,%r14), %r12d
	movslq	%r12d, %r12
	addss	(%rax,%r12,4), %xmm9
	leal	(%r9,%r14), %r13d
	movslq	%r13d, %r13
	movq	176(%rsp), %rbp
	movss	(%rbp,%r13,4), %xmm7
	leal	(%r10,%r14), %r13d
	movslq	%r13d, %r13
	movss	(%rbp,%r13,4), %xmm10
	movss	%xmm9, (%rax,%r12,4)
	movaps	%xmm5, %xmm9
	mulss	%xmm8, %xmm9
	movaps	%xmm1, %xmm11
	mulss	%xmm6, %xmm11
	addss	%xmm9, %xmm11
	leal	(%rdi,%r14), %r12d
	movslq	%r12d, %r12
	addss	(%rax,%r12,4), %xmm11
	movss	%xmm11, (%rax,%r12,4)
	movaps	%xmm7, %xmm9
	mulss	%xmm8, %xmm9
	movaps	%xmm10, %xmm11
	mulss	%xmm6, %xmm11
	addss	%xmm9, %xmm11
	addss	%xmm11, %xmm4
	mulss	%xmm6, %xmm7
	mulss	%xmm8, %xmm10
	subss	%xmm7, %xmm10
	addss	%xmm10, %xmm3
	addl	$2, %r14d
	incl	%r15d
	cmpl	%edx, %r15d
	jne	.LBB23_50	# bb92
.LBB23_51:	# bb94
	movaps	%xmm2, %xmm1
	mulss	%xmm4, %xmm1
	movaps	%xmm0, %xmm5
	mulss	%xmm3, %xmm5
	subss	%xmm1, %xmm5
	movq	80(%rsp), %rdi
	addss	(%rax,%rdi,4), %xmm5
	movss	%xmm5, (%rax,%rdi,4)
	mulss	%xmm2, %xmm3
	mulss	%xmm0, %xmm4
	addss	%xmm3, %xmm4
	addss	(%rax,%rsi,4), %xmm4
	movss	%xmm4, (%rax,%rsi,4)
	movl	96(%rsp), %esi
	addl	56(%rsp), %esi
	movl	%esi, 96(%rsp)
	addl	$2, %ecx
	decl	%edx
	movl	92(%rsp), %esi
	incl	%esi
	movl	%esi, 92(%rsp)
	cmpl	100(%rsp), %esi
	jne	.LBB23_48	# bb91
.LBB23_52:	# bb96
	movl	60(%rsp), %ecx
	addl	12(%rsp), %ecx
	movl	%ecx, 60(%rsp)
	movl	64(%rsp), %ecx
	addl	8(%rsp), %ecx
	movl	%ecx, 64(%rsp)
	movl	20(%rsp), %ecx
	incl	%ecx
	movl	%ecx, 20(%rsp)
	cmpl	76(%rsp), %ecx
	je	.LBB23_66	# return
.LBB23_53:	# bb95.preheader
	movl	168(%rsp), %ecx
	leal	2(,%rcx,2), %ecx
	movl	%ecx, 56(%rsp)
	movl	64(%rsp), %ecx
	leal	3(%rcx), %edx
	movl	%edx, 48(%rsp)
	leal	2(%rcx), %edx
	movl	%edx, 40(%rsp)
	movl	60(%rsp), %edx
	leal	3(%rdx), %esi
	movl	%esi, 36(%rsp)
	leal	2(%rdx), %esi
	movl	%esi, 32(%rsp)
	leal	1(%rcx), %ecx
	movl	%ecx, 28(%rsp)
	leal	1(%rdx), %ecx
	movl	%ecx, 24(%rsp)
	movl	100(%rsp), %ecx
	leal	-1(%rcx), %edx
	xorl	%esi, %esi
	movl	%esi, 96(%rsp)
	movl	%esi, %ecx
	movl	%esi, 92(%rsp)
	jmp	.LBB23_48	# bb91
.LBB23_54:	# bb98
	cmpl	$122, %edx
	jne	.LBB23_65	# bb111
.LBB23_55:	# bb98
	cmpl	$142, %esi
	jne	.LBB23_65	# bb111
.LBB23_56:	# bb110.preheader
	cmpl	$0, 76(%rsp)
	jle	.LBB23_66	# return
.LBB23_57:	# bb.nph135
	cmpl	$0, 100(%rsp)
	jle	.LBB23_66	# return
.LBB23_58:	# bb108.preheader.preheader
	movl	184(%rsp), %ecx
	addl	%ecx, %ecx
	movl	%ecx, 12(%rsp)
	movl	208(%rsp), %ecx
	addl	%ecx, %ecx
	movl	%ecx, 8(%rsp)
	xorl	%edx, %edx
	movl	%edx, %esi
	movl	%edx, 80(%rsp)
	jmp	.LBB23_64	# bb108.preheader
	.align	16
.LBB23_59:	# bb104
	leal	(%rdx,%r10), %ebx
	movslq	%ebx, %rbx
	movq	176(%rsp), %r14
	movss	(%r14,%rbx,4), %xmm1
	movaps	%xmm2, %xmm3
	mulss	%xmm1, %xmm3
	leal	(%r8,%r10), %ebx
	movslq	%ebx, %rbx
	movss	(%r14,%rbx,4), %xmm4
	movaps	%xmm0, %xmm5
	mulss	%xmm4, %xmm5
	addss	%xmm3, %xmm5
	mulss	%xmm2, %xmm4
	mulss	%xmm0, %xmm1
	subss	%xmm4, %xmm1
	testl	%r11d, %r11d
	jle	.LBB23_71	# bb104.bb107_crit_edge
.LBB23_60:	# bb.nph128
	leal	1(%rdi), %ebx
	pxor	%xmm3, %xmm3
	xorl	%r14d, %r14d
	movl	%r14d, %r15d
	movaps	%xmm3, %xmm4
	.align	16
.LBB23_61:	# bb105
	leal	(%rbx,%r14), %r12d
	movslq	%r12d, %r12
	movq	160(%rsp), %r13
	movss	(%r13,%r12,4), %xmm6
	movaps	%xmm5, %xmm7
	mulss	%xmm6, %xmm7
	leal	(%rdi,%r14), %r12d
	movslq	%r12d, %r12
	movss	(%r13,%r12,4), %xmm8
	movaps	%xmm1, %xmm9
	mulss	%xmm8, %xmm9
	subss	%xmm7, %xmm9
	leal	(%rsi,%r14), %r12d
	movslq	%r12d, %r12
	addss	(%rax,%r12,4), %xmm9
	leal	(%r8,%r14), %r13d
	movslq	%r13d, %r13
	movq	176(%rsp), %rbp
	movss	(%rbp,%r13,4), %xmm7
	leal	(%rdx,%r14), %r13d
	movslq	%r13d, %r13
	movss	(%rbp,%r13,4), %xmm10
	movss	%xmm9, (%rax,%r12,4)
	movaps	%xmm5, %xmm9
	mulss	%xmm8, %xmm9
	movaps	%xmm1, %xmm11
	mulss	%xmm6, %xmm11
	addss	%xmm9, %xmm11
	leal	(%rcx,%r14), %r12d
	movslq	%r12d, %r12
	addss	(%rax,%r12,4), %xmm11
	movss	%xmm11, (%rax,%r12,4)
	movaps	%xmm7, %xmm9
	mulss	%xmm8, %xmm9
	movaps	%xmm10, %xmm11
	mulss	%xmm6, %xmm11
	addss	%xmm9, %xmm11
	addss	%xmm11, %xmm4
	mulss	%xmm6, %xmm7
	mulss	%xmm8, %xmm10
	subss	%xmm7, %xmm10
	addss	%xmm10, %xmm3
	addl	$2, %r14d
	incl	%r15d
	cmpl	%r11d, %r15d
	jne	.LBB23_61	# bb105
.LBB23_62:	# bb107
	movslq	%r9d, %rbx
	movq	160(%rsp), %r14
	movss	(%r14,%rbx,4), %xmm6
	movaps	%xmm1, %xmm7
	mulss	%xmm6, %xmm7
	leal	1(%r9), %ebx
	movslq	%ebx, %rbx
	movss	(%r14,%rbx,4), %xmm8
	movaps	%xmm5, %xmm9
	mulss	%xmm8, %xmm9
	subss	%xmm9, %xmm7
	leal	(%rsi,%r10), %ebx
	movslq	%ebx, %rbx
	addss	(%rax,%rbx,4), %xmm7
	movss	%xmm7, (%rax,%rbx,4)
	mulss	%xmm6, %xmm5
	mulss	%xmm8, %xmm1
	addss	%xmm5, %xmm1
	leal	(%rcx,%r10), %r14d
	movslq	%r14d, %r14
	addss	(%rax,%r14,4), %xmm1
	movss	%xmm1, (%rax,%r14,4)
	movaps	%xmm2, %xmm1
	mulss	%xmm4, %xmm1
	movaps	%xmm0, %xmm5
	mulss	%xmm3, %xmm5
	subss	%xmm1, %xmm5
	addss	(%rax,%rbx,4), %xmm5
	movss	%xmm5, (%rax,%rbx,4)
	mulss	%xmm2, %xmm3
	mulss	%xmm0, %xmm4
	addss	%xmm3, %xmm4
	addss	(%rax,%r14,4), %xmm4
	movss	%xmm4, (%rax,%r14,4)
	addl	96(%rsp), %r9d
	addl	92(%rsp), %edi
	addl	$2, %r10d
	incl	%r11d
	cmpl	100(%rsp), %r11d
	jne	.LBB23_59	# bb104
.LBB23_63:	# bb109
	addl	12(%rsp), %edx
	addl	8(%rsp), %esi
	movl	80(%rsp), %ecx
	incl	%ecx
	movl	%ecx, 80(%rsp)
	cmpl	76(%rsp), %ecx
	je	.LBB23_66	# return
.LBB23_64:	# bb108.preheader
	leal	1(%rdx), %r8d
	leal	1(%rsi), %ecx
	movl	168(%rsp), %edi
	leal	2(,%rdi,2), %r9d
	movl	%r9d, 96(%rsp)
	leal	(%rdi,%rdi), %edi
	movl	%edi, 92(%rsp)
	xorl	%r9d, %r9d
	movl	%r9d, %edi
	movl	%r9d, %r10d
	movl	%r9d, %r11d
	jmp	.LBB23_59	# bb104
.LBB23_65:	# bb111
	xorl	%edi, %edi
	leaq	.str30, %rsi
	leaq	.str131, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB23_66:	# return
	addq	$104, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB23_67:	# bb33.bb42_crit_edge
	movl	%r8d, 100(%rsp)
	jmp	.LBB23_5	# bb42
.LBB23_68:	# bb65.bb68_crit_edge
	pxor	%xmm3, %xmm3
	movaps	%xmm3, %xmm4
	jmp	.LBB23_29	# bb68
.LBB23_69:	# bb78.bb81_crit_edge
	pxor	%xmm3, %xmm3
	movaps	%xmm3, %xmm4
	jmp	.LBB23_40	# bb81
.LBB23_70:	# bb91.bb94_crit_edge
	pxor	%xmm3, %xmm3
	movaps	%xmm3, %xmm4
	jmp	.LBB23_51	# bb94
.LBB23_71:	# bb104.bb107_crit_edge
	pxor	%xmm3, %xmm3
	movaps	%xmm3, %xmm4
	jmp	.LBB23_62	# bb107
	.size	cblas_csymm, .-cblas_csymm
.Leh_func_end16:


	.section	.rodata.cst4,"aM",@progbits,4
	.align	4
.LCPI24_0:					
	.long	1065353216	# float 1.000000e+00
	.text
	.align	16
	.globl	cblas_csyr2k
	.type	cblas_csyr2k,@function
cblas_csyr2k:
.Leh_func_begin17:
.Llabel17:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$56, %rsp
	movl	%r8d, 52(%rsp)
	movl	%edx, 48(%rsp)
	movss	(%r9), %xmm0
	pxor	%xmm1, %xmm1
	ucomiss	%xmm1, %xmm0
	setp	%al
	setne	%dl
	setnp	%r8b
	sete	%r10b
	andb	%r8b, %r10b
	movss	4(%r9), %xmm2
	ucomiss	%xmm1, %xmm2
	setp	%r8b
	setnp	%r9b
	sete	%r11b
	setne	%bl
	andb	%r9b, %r11b
	andb	%r10b, %r11b
	movb	%r11b, 44(%rsp)
	orb	%al, %dl
	orb	%r8b, %bl
	orb	%dl, %bl
	testb	%bl, %bl
	movq	144(%rsp), %rax
	movss	4(%rax), %xmm1
	movss	(%rax), %xmm3
	movq	152(%rsp), %rax
	movq	128(%rsp), %rdx
	movq	112(%rsp), %r8
	jne	.LBB24_3	# bb27
.LBB24_1:	# entry
	ucomiss	.LCPI24_0(%rip), %xmm3
	jne	.LBB24_3	# bb27
	jp	.LBB24_3	# bb27
.LBB24_2:	# entry
	pxor	%xmm4, %xmm4
	ucomiss	%xmm4, %xmm1
	setnp	%r9b
	sete	%r10b
	testb	%r9b, %r10b
	jne	.LBB24_83	# return
.LBB24_3:	# bb27
	cmpl	$101, %edi
	je	.LBB24_5	# bb36
.LBB24_4:	# bb29
	cmpl	$111, 48(%rsp)
	movl	$112, %edi
	movl	$111, %r9d
	cmove	%edi, %r9d
	movl	%r9d, 48(%rsp)
	cmpl	$121, %esi
	movl	$122, %edi
	movl	$121, %esi
	cmove	%edi, %esi
.LBB24_5:	# bb36
	pxor	%xmm4, %xmm4
	ucomiss	%xmm4, %xmm1
	jne	.LBB24_25	# bb51
	jp	.LBB24_25	# bb51
.LBB24_6:	# bb36
	pxor	%xmm4, %xmm4
	ucomiss	%xmm4, %xmm3
	jne	.LBB24_25	# bb51
	jp	.LBB24_25	# bb51
.LBB24_7:	# bb38
	cmpl	$121, %esi
	je	.LBB24_17	# bb44.preheader
.LBB24_8:	# bb50.preheader
	testl	%ecx, %ecx
	jle	.LBB24_12	# bb66
.LBB24_9:	# bb.nph134
	movl	160(%rsp), %edi
	leal	(%rdi,%rdi), %ebx
	xorl	%r10d, %r10d
	movl	%r10d, %r11d
	jmp	.LBB24_23	# bb48.preheader
	.align	16
.LBB24_10:	# bb41
	movslq	%ebx, %r15
	movl	$0, (%rax,%r15,4)
	leal	1(%rbx), %r15d
	movslq	%r15d, %r15
	movl	$0, (%rax,%r15,4)
	addl	$2, %ebx
	incl	%edi
	cmpl	%r10d, %edi
	jne	.LBB24_10	# bb41
.LBB24_11:	# bb43
	addl	%r11d, %r14d
	decl	%r10d
	incl	%r9d
	cmpl	%ecx, %r9d
	jne	.LBB24_19	# bb42.preheader
.LBB24_12:	# bb66
	testb	$1, 44(%rsp)
	jne	.LBB24_83	# return
.LBB24_13:	# bb68
	cmpl	$121, %esi
	jne	.LBB24_48	# bb80
.LBB24_14:	# bb68
	cmpl	$111, 48(%rsp)
	jne	.LBB24_48	# bb80
.LBB24_15:	# bb79.preheader
	testl	%ecx, %ecx
	jle	.LBB24_83	# return
.LBB24_16:	# bb.nph174
	movl	160(%rsp), %esi
	leal	2(,%rsi,2), %esi
	movl	%esi, 16(%rsp)
	movl	136(%rsp), %esi
	leal	(%rsi,%rsi), %esi
	movl	%esi, 12(%rsp)
	movl	120(%rsp), %esi
	leal	(%rsi,%rsi), %esi
	movl	%esi, 8(%rsp)
	xorl	%r9d, %r9d
	movl	%r9d, 24(%rsp)
	movl	%r9d, %esi
	movl	%r9d, %edi
	movl	%ecx, 40(%rsp)
	movl	%r9d, 28(%rsp)
	jmp	.LBB24_44	# bb77.preheader
.LBB24_17:	# bb44.preheader
	testl	%ecx, %ecx
	jle	.LBB24_12	# bb66
.LBB24_18:	# bb.nph138
	movl	160(%rsp), %edi
	leal	2(,%rdi,2), %r11d
	xorl	%r14d, %r14d
	movl	%ecx, %r10d
	movl	%r14d, %r9d
	.align	16
.LBB24_19:	# bb42.preheader
	cmpl	%ecx, %r9d
	jge	.LBB24_11	# bb43
.LBB24_20:	# bb42.preheader.bb41_crit_edge
	xorl	%edi, %edi
	movl	%r14d, %ebx
	jmp	.LBB24_10	# bb41
	.align	16
.LBB24_21:	# bb47
	movslq	%edi, %r14
	movl	$0, (%rax,%r14,4)
	leal	1(%rdi), %r14d
	movslq	%r14d, %r14
	movl	$0, (%rax,%r14,4)
	addl	$2, %edi
	incl	%r9d
	cmpl	%r11d, %r9d
	jle	.LBB24_21	# bb47
.LBB24_22:	# bb49
	addl	%ebx, %r10d
	incl	%r11d
	cmpl	%ecx, %r11d
	je	.LBB24_12	# bb66
.LBB24_23:	# bb48.preheader
	testl	%r11d, %r11d
	js	.LBB24_22	# bb49
.LBB24_24:	# bb48.preheader.bb47_crit_edge
	xorl	%r9d, %r9d
	movl	%r10d, %edi
	jmp	.LBB24_21	# bb47
.LBB24_25:	# bb51
	pxor	%xmm4, %xmm4
	ucomiss	%xmm4, %xmm1
	jne	.LBB24_27	# bb53
	jp	.LBB24_27	# bb53
.LBB24_26:	# bb51
	ucomiss	.LCPI24_0(%rip), %xmm3
	setnp	%dil
	sete	%r9b
	testb	%dil, %r9b
	jne	.LBB24_12	# bb66
.LBB24_27:	# bb53
	cmpl	$121, %esi
	je	.LBB24_32	# bb59.preheader
.LBB24_28:	# bb65.preheader
	testl	%ecx, %ecx
	jle	.LBB24_12	# bb66
.LBB24_29:	# bb.nph142
	movl	160(%rsp), %edi
	leal	(%rdi,%rdi), %r10d
	xorl	%ebx, %ebx
	movl	%ebx, %edi
	jmp	.LBB24_38	# bb63.preheader
	.align	16
.LBB24_30:	# bb56
	movslq	%ebx, %r15
	movss	(%rax,%r15,4), %xmm4
	movaps	%xmm3, %xmm5
	mulss	%xmm4, %xmm5
	leal	1(%rbx), %r12d
	movslq	%r12d, %r12
	movss	(%rax,%r12,4), %xmm6
	movaps	%xmm1, %xmm7
	mulss	%xmm6, %xmm7
	subss	%xmm7, %xmm5
	movss	%xmm5, (%rax,%r15,4)
	mulss	%xmm1, %xmm4
	mulss	%xmm3, %xmm6
	addss	%xmm4, %xmm6
	movss	%xmm6, (%rax,%r12,4)
	addl	$2, %ebx
	incl	%r14d
	cmpl	%r10d, %r14d
	jne	.LBB24_30	# bb56
.LBB24_31:	# bb58
	addl	%r11d, %r9d
	decl	%r10d
	incl	%edi
	cmpl	%ecx, %edi
	je	.LBB24_12	# bb66
	jmp	.LBB24_34	# bb57.preheader
.LBB24_32:	# bb59.preheader
	testl	%ecx, %ecx
	jle	.LBB24_12	# bb66
.LBB24_33:	# bb.nph178
	movl	160(%rsp), %edi
	leal	2(,%rdi,2), %r11d
	xorl	%r9d, %r9d
	movl	%ecx, %r10d
	movl	%r9d, %edi
	.align	16
.LBB24_34:	# bb57.preheader
	cmpl	%ecx, %edi
	jge	.LBB24_31	# bb58
.LBB24_35:	# bb57.preheader.bb56_crit_edge
	xorl	%r14d, %r14d
	movl	%r9d, %ebx
	jmp	.LBB24_30	# bb56
	.align	16
.LBB24_36:	# bb62
	movslq	%r11d, %r14
	movss	(%rax,%r14,4), %xmm4
	movaps	%xmm3, %xmm5
	mulss	%xmm4, %xmm5
	leal	1(%r11), %r15d
	movslq	%r15d, %r15
	movss	(%rax,%r15,4), %xmm6
	movaps	%xmm1, %xmm7
	mulss	%xmm6, %xmm7
	subss	%xmm7, %xmm5
	movss	%xmm5, (%rax,%r14,4)
	mulss	%xmm1, %xmm4
	mulss	%xmm3, %xmm6
	addss	%xmm4, %xmm6
	movss	%xmm6, (%rax,%r15,4)
	addl	$2, %r11d
	incl	%r9d
	cmpl	%edi, %r9d
	jle	.LBB24_36	# bb62
.LBB24_37:	# bb64
	addl	%r10d, %ebx
	incl	%edi
	cmpl	%ecx, %edi
	je	.LBB24_12	# bb66
.LBB24_38:	# bb63.preheader
	testl	%edi, %edi
	js	.LBB24_37	# bb64
.LBB24_39:	# bb63.preheader.bb62_crit_edge
	xorl	%r9d, %r9d
	movl	%ebx, %r11d
	jmp	.LBB24_36	# bb62
.LBB24_40:	# bb.nph168
	leal	1(%r9), %r14d
	leal	1(%rbx), %r15d
	pxor	%xmm1, %xmm1
	xorl	%r12d, %r12d
	movl	%r12d, %r13d
	movaps	%xmm1, %xmm3
	.align	16
.LBB24_41:	# bb74
	leal	(%rbx,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r8,%rbp,4), %xmm4
	leal	(%r11,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%rdx,%rbp,4), %xmm5
	movaps	%xmm5, %xmm6
	mulss	%xmm4, %xmm6
	leal	(%r15,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r8,%rbp,4), %xmm7
	leal	(%rsi,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%rdx,%rbp,4), %xmm8
	movaps	%xmm8, %xmm9
	mulss	%xmm7, %xmm9
	addss	%xmm6, %xmm9
	leal	(%r9,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%rdx,%rbp,4), %xmm6
	leal	(%r10,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r8,%rbp,4), %xmm10
	movaps	%xmm10, %xmm11
	mulss	%xmm6, %xmm11
	leal	(%r14,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%rdx,%rbp,4), %xmm12
	leal	(%rdi,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r8,%rbp,4), %xmm13
	movaps	%xmm13, %xmm14
	mulss	%xmm12, %xmm14
	addss	%xmm11, %xmm14
	addss	%xmm9, %xmm14
	addss	%xmm14, %xmm3
	mulss	%xmm7, %xmm5
	mulss	%xmm4, %xmm8
	subss	%xmm5, %xmm8
	mulss	%xmm12, %xmm10
	mulss	%xmm6, %xmm13
	subss	%xmm10, %xmm13
	addss	%xmm8, %xmm13
	addss	%xmm13, %xmm1
	addl	$2, %r12d
	incl	%r13d
	cmpl	52(%rsp), %r13d
	jne	.LBB24_41	# bb74
.LBB24_42:	# bb76
	movaps	%xmm2, %xmm4
	mulss	%xmm3, %xmm4
	movaps	%xmm0, %xmm5
	mulss	%xmm1, %xmm5
	subss	%xmm4, %xmm5
	movl	48(%rsp), %r14d
	movslq	%r14d, %r15
	addss	(%rax,%r15,4), %xmm5
	movss	%xmm5, (%rax,%r15,4)
	mulss	%xmm2, %xmm1
	mulss	%xmm0, %xmm3
	addss	%xmm1, %xmm3
	leal	1(%r14), %r15d
	movslq	%r15d, %r15
	addss	(%rax,%r15,4), %xmm3
	movss	%xmm3, (%rax,%r15,4)
	addl	36(%rsp), %ebx
	addl	32(%rsp), %r9d
	addl	$2, %r14d
	movl	%r14d, 48(%rsp)
	movl	44(%rsp), %r14d
	incl	%r14d
	movl	%r14d, 44(%rsp)
	cmpl	40(%rsp), %r14d
	jne	.LBB24_46	# bb75.preheader
.LBB24_43:	# bb78
	movl	24(%rsp), %r9d
	addl	16(%rsp), %r9d
	movl	%r9d, 24(%rsp)
	addl	12(%rsp), %esi
	addl	8(%rsp), %edi
	decl	40(%rsp)
	movl	28(%rsp), %r9d
	incl	%r9d
	movl	%r9d, 28(%rsp)
	cmpl	%ecx, %r9d
	je	.LBB24_83	# return
.LBB24_44:	# bb77.preheader
	cmpl	%ecx, 28(%rsp)
	jge	.LBB24_43	# bb78
.LBB24_45:	# bb.nph172
	leal	1(%rsi), %r11d
	leal	1(%rdi), %r10d
	movl	120(%rsp), %r9d
	leal	(%r9,%r9), %r9d
	movl	%r9d, 36(%rsp)
	movl	136(%rsp), %r9d
	leal	(%r9,%r9), %r9d
	movl	%r9d, 32(%rsp)
	movl	$0, 44(%rsp)
	movl	%edi, %ebx
	movl	%esi, %r9d
	movl	24(%rsp), %r14d
	movl	%r14d, 48(%rsp)
	.align	16
.LBB24_46:	# bb75.preheader
	cmpl	$0, 52(%rsp)
	jg	.LBB24_40	# bb.nph168
.LBB24_47:	# bb75.preheader.bb76_crit_edge
	pxor	%xmm1, %xmm1
	movaps	%xmm1, %xmm3
	jmp	.LBB24_42	# bb76
.LBB24_48:	# bb80
	cmpl	$121, %esi
	jne	.LBB24_59	# bb93
.LBB24_49:	# bb80
	cmpl	$112, 48(%rsp)
	jne	.LBB24_59	# bb93
.LBB24_50:	# bb92.preheader
	cmpl	$0, 52(%rsp)
	jle	.LBB24_83	# return
.LBB24_51:	# bb.nph164
	testl	%ecx, %ecx
	jle	.LBB24_83	# return
.LBB24_52:	# bb90.preheader.preheader
	movl	120(%rsp), %esi
	leal	(%rsi,%rsi), %esi
	movl	%esi, 16(%rsp)
	movl	136(%rsp), %esi
	addl	%esi, %esi
	movl	%esi, 20(%rsp)
	xorl	%esi, %esi
	movl	%esi, 40(%rsp)
	movl	%esi, 44(%rsp)
	movl	%esi, 24(%rsp)
	jmp	.LBB24_58	# bb90.preheader
	.align	16
.LBB24_53:	# bb86
	movl	44(%rsp), %r10d
	leal	(%r10,%r9), %r10d
	movslq	%r10d, %r10
	movss	(%rdx,%r10,4), %xmm1
	movaps	%xmm2, %xmm3
	mulss	%xmm1, %xmm3
	movl	36(%rsp), %r10d
	leal	(%r10,%r9), %r10d
	movslq	%r10d, %r10
	movss	(%rdx,%r10,4), %xmm4
	movaps	%xmm0, %xmm5
	mulss	%xmm4, %xmm5
	addss	%xmm3, %xmm5
	mulss	%xmm2, %xmm4
	mulss	%xmm0, %xmm1
	subss	%xmm4, %xmm1
	movl	40(%rsp), %r10d
	leal	(%r10,%r9), %r10d
	movslq	%r10d, %r10
	movss	(%r8,%r10,4), %xmm3
	movaps	%xmm2, %xmm4
	mulss	%xmm3, %xmm4
	movl	32(%rsp), %r10d
	leal	(%r10,%r9), %r10d
	movslq	%r10d, %r10
	movss	(%r8,%r10,4), %xmm6
	movaps	%xmm0, %xmm7
	mulss	%xmm6, %xmm7
	addss	%xmm4, %xmm7
	mulss	%xmm2, %xmm6
	mulss	%xmm0, %xmm3
	subss	%xmm6, %xmm3
	cmpl	%ecx, 48(%rsp)
	jge	.LBB24_56	# bb89
.LBB24_54:	# bb.nph160
	movl	36(%rsp), %r10d
	leal	(%r10,%r9), %r10d
	movl	44(%rsp), %r11d
	leal	(%r11,%r9), %r11d
	movl	32(%rsp), %ebx
	leal	(%rbx,%r9), %ebx
	movl	40(%rsp), %r14d
	leal	(%r14,%r9), %r14d
	leal	1(%rdi), %r15d
	xorl	%r12d, %r12d
	movl	%r12d, %r13d
	.align	16
.LBB24_55:	# bb87
	leal	(%rbx,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r8,%rbp,4), %xmm4
	movaps	%xmm5, %xmm6
	mulss	%xmm4, %xmm6
	leal	(%r14,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r8,%rbp,4), %xmm8
	movaps	%xmm1, %xmm9
	mulss	%xmm8, %xmm9
	subss	%xmm6, %xmm9
	leal	(%r10,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%rdx,%rbp,4), %xmm6
	movaps	%xmm7, %xmm10
	mulss	%xmm6, %xmm10
	leal	(%r11,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%rdx,%rbp,4), %xmm11
	movaps	%xmm3, %xmm12
	mulss	%xmm11, %xmm12
	subss	%xmm10, %xmm12
	addss	%xmm9, %xmm12
	leal	(%rdi,%r12), %ebp
	movslq	%ebp, %rbp
	addss	(%rax,%rbp,4), %xmm12
	movss	%xmm12, (%rax,%rbp,4)
	mulss	%xmm5, %xmm8
	mulss	%xmm1, %xmm4
	addss	%xmm8, %xmm4
	mulss	%xmm7, %xmm11
	mulss	%xmm3, %xmm6
	addss	%xmm11, %xmm6
	addss	%xmm4, %xmm6
	leal	(%r15,%r12), %ebp
	movslq	%ebp, %rbp
	addss	(%rax,%rbp,4), %xmm6
	movss	%xmm6, (%rax,%rbp,4)
	addl	$2, %r12d
	incl	%r13d
	cmpl	%esi, %r13d
	jne	.LBB24_55	# bb87
.LBB24_56:	# bb89
	addl	28(%rsp), %edi
	addl	$2, %r9d
	decl	%esi
	movl	48(%rsp), %r10d
	incl	%r10d
	movl	%r10d, 48(%rsp)
	cmpl	%ecx, %r10d
	jne	.LBB24_53	# bb86
.LBB24_57:	# bb91
	movl	40(%rsp), %esi
	addl	16(%rsp), %esi
	movl	%esi, 40(%rsp)
	movl	44(%rsp), %esi
	addl	20(%rsp), %esi
	movl	%esi, 44(%rsp)
	movl	24(%rsp), %esi
	incl	%esi
	movl	%esi, 24(%rsp)
	cmpl	52(%rsp), %esi
	je	.LBB24_83	# return
.LBB24_58:	# bb90.preheader
	movl	120(%rsp), %esi
	leal	2(,%rsi,2), %esi
	movl	%esi, 28(%rsp)
	movl	44(%rsp), %esi
	leal	1(%rsi), %esi
	movl	%esi, 36(%rsp)
	movl	40(%rsp), %esi
	leal	1(%rsi), %esi
	movl	%esi, 32(%rsp)
	xorl	%edi, %edi
	movl	%edi, %r9d
	movl	%ecx, %esi
	movl	%edi, 48(%rsp)
	jmp	.LBB24_53	# bb86
.LBB24_59:	# bb93
	cmpl	$122, %esi
	jne	.LBB24_71	# bb106
.LBB24_60:	# bb93
	cmpl	$111, 48(%rsp)
	jne	.LBB24_71	# bb106
.LBB24_61:	# bb105.preheader
	testl	%ecx, %ecx
	jle	.LBB24_83	# return
.LBB24_62:	# bb.nph158
	movl	120(%rsp), %esi
	leal	(%rsi,%rsi), %esi
	movl	%esi, 24(%rsp)
	movl	136(%rsp), %esi
	leal	(%rsi,%rsi), %esi
	movl	%esi, 16(%rsp)
	movl	160(%rsp), %esi
	addl	%esi, %esi
	movl	%esi, 12(%rsp)
	xorl	%r11d, %r11d
	movl	%r11d, %esi
	movl	%r11d, 28(%rsp)
	movl	%r11d, 40(%rsp)
	jmp	.LBB24_67	# bb103.preheader
.LBB24_63:	# bb.nph152
	leal	1(%r10), %r14d
	leal	1(%rbx), %r15d
	pxor	%xmm1, %xmm1
	xorl	%r12d, %r12d
	movl	%r12d, %r13d
	movaps	%xmm1, %xmm3
	.align	16
.LBB24_64:	# bb100
	leal	(%rbx,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r8,%rbp,4), %xmm4
	leal	(%rdi,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%rdx,%rbp,4), %xmm5
	movaps	%xmm5, %xmm6
	mulss	%xmm4, %xmm6
	leal	(%r15,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r8,%rbp,4), %xmm7
	leal	(%rsi,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%rdx,%rbp,4), %xmm8
	movaps	%xmm8, %xmm9
	mulss	%xmm7, %xmm9
	addss	%xmm6, %xmm9
	leal	(%r10,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%rdx,%rbp,4), %xmm6
	leal	(%r9,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r8,%rbp,4), %xmm10
	movaps	%xmm10, %xmm11
	mulss	%xmm6, %xmm11
	leal	(%r14,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%rdx,%rbp,4), %xmm12
	leal	(%r11,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r8,%rbp,4), %xmm13
	movaps	%xmm13, %xmm14
	mulss	%xmm12, %xmm14
	addss	%xmm11, %xmm14
	addss	%xmm9, %xmm14
	addss	%xmm14, %xmm3
	mulss	%xmm7, %xmm5
	mulss	%xmm4, %xmm8
	subss	%xmm5, %xmm8
	mulss	%xmm12, %xmm10
	mulss	%xmm6, %xmm13
	subss	%xmm10, %xmm13
	addss	%xmm8, %xmm13
	addss	%xmm13, %xmm1
	addl	$2, %r12d
	incl	%r13d
	cmpl	52(%rsp), %r13d
	jne	.LBB24_64	# bb100
.LBB24_65:	# bb102
	movaps	%xmm2, %xmm4
	mulss	%xmm3, %xmm4
	movaps	%xmm0, %xmm5
	mulss	%xmm1, %xmm5
	subss	%xmm4, %xmm5
	movl	44(%rsp), %r14d
	movslq	%r14d, %r15
	addss	(%rax,%r15,4), %xmm5
	movss	%xmm5, (%rax,%r15,4)
	mulss	%xmm2, %xmm1
	mulss	%xmm0, %xmm3
	addss	%xmm1, %xmm3
	leal	1(%r14), %r15d
	movslq	%r15d, %r15
	addss	(%rax,%r15,4), %xmm3
	movss	%xmm3, (%rax,%r15,4)
	addl	36(%rsp), %ebx
	addl	32(%rsp), %r10d
	addl	$2, %r14d
	movl	%r14d, 44(%rsp)
	movl	48(%rsp), %r14d
	incl	%r14d
	movl	%r14d, 48(%rsp)
	cmpl	40(%rsp), %r14d
	jle	.LBB24_69	# bb101.preheader
.LBB24_66:	# bb104
	addl	24(%rsp), %r11d
	addl	16(%rsp), %esi
	movl	28(%rsp), %edi
	addl	12(%rsp), %edi
	movl	%edi, 28(%rsp)
	movl	40(%rsp), %edi
	incl	%edi
	movl	%edi, 40(%rsp)
	cmpl	%ecx, %edi
	je	.LBB24_83	# return
.LBB24_67:	# bb103.preheader
	cmpl	$0, 40(%rsp)
	js	.LBB24_66	# bb104
.LBB24_68:	# bb.nph156
	leal	1(%r11), %r9d
	leal	1(%rsi), %edi
	movl	120(%rsp), %r10d
	leal	(%r10,%r10), %r10d
	movl	%r10d, 36(%rsp)
	movl	136(%rsp), %r10d
	leal	(%r10,%r10), %r10d
	movl	%r10d, 32(%rsp)
	xorl	%ebx, %ebx
	movl	%ebx, %r10d
	movl	28(%rsp), %r14d
	movl	%r14d, 44(%rsp)
	movl	%ebx, 48(%rsp)
	.align	16
.LBB24_69:	# bb101.preheader
	cmpl	$0, 52(%rsp)
	jg	.LBB24_63	# bb.nph152
.LBB24_70:	# bb101.preheader.bb102_crit_edge
	pxor	%xmm1, %xmm1
	movaps	%xmm1, %xmm3
	jmp	.LBB24_65	# bb102
.LBB24_71:	# bb106
	cmpl	$122, %esi
	jne	.LBB24_82	# bb119
.LBB24_72:	# bb106
	cmpl	$112, 48(%rsp)
	jne	.LBB24_82	# bb119
.LBB24_73:	# bb118.preheader
	cmpl	$0, 52(%rsp)
	jle	.LBB24_83	# return
.LBB24_74:	# bb.nph148
	testl	%ecx, %ecx
	jle	.LBB24_83	# return
.LBB24_75:	# bb116.preheader.preheader
	movl	120(%rsp), %esi
	leal	(%rsi,%rsi), %esi
	movl	%esi, 40(%rsp)
	movl	136(%rsp), %esi
	addl	%esi, %esi
	movl	%esi, 20(%rsp)
	xorl	%r10d, %r10d
	movl	%r10d, %edi
	movl	%r10d, 44(%rsp)
	jmp	.LBB24_81	# bb116.preheader
	.align	16
.LBB24_76:	# bb112
	leal	(%rdi,%r9), %r15d
	movslq	%r15d, %r15
	movss	(%rdx,%r15,4), %xmm1
	movaps	%xmm2, %xmm3
	mulss	%xmm1, %xmm3
	leal	(%r11,%r9), %r15d
	movslq	%r15d, %r15
	movss	(%rdx,%r15,4), %xmm4
	movaps	%xmm0, %xmm5
	mulss	%xmm4, %xmm5
	addss	%xmm3, %xmm5
	mulss	%xmm2, %xmm4
	mulss	%xmm0, %xmm1
	subss	%xmm4, %xmm1
	leal	(%r10,%r9), %r15d
	movslq	%r15d, %r15
	movss	(%r8,%r15,4), %xmm3
	movaps	%xmm2, %xmm4
	mulss	%xmm3, %xmm4
	leal	(%rbx,%r9), %r15d
	movslq	%r15d, %r15
	movss	(%r8,%r15,4), %xmm6
	movaps	%xmm0, %xmm7
	mulss	%xmm6, %xmm7
	addss	%xmm4, %xmm7
	mulss	%xmm2, %xmm6
	mulss	%xmm0, %xmm3
	subss	%xmm6, %xmm3
	testl	%esi, %esi
	js	.LBB24_79	# bb115
.LBB24_77:	# bb.nph144
	leal	1(%r14), %r15d
	xorl	%r12d, %r12d
	movl	%r12d, %r13d
	.align	16
.LBB24_78:	# bb113
	leal	(%rbx,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r8,%rbp,4), %xmm4
	movaps	%xmm5, %xmm6
	mulss	%xmm4, %xmm6
	leal	(%r10,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r8,%rbp,4), %xmm8
	movaps	%xmm1, %xmm9
	mulss	%xmm8, %xmm9
	subss	%xmm6, %xmm9
	leal	(%r11,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%rdx,%rbp,4), %xmm6
	movaps	%xmm7, %xmm10
	mulss	%xmm6, %xmm10
	leal	(%rdi,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%rdx,%rbp,4), %xmm11
	movaps	%xmm3, %xmm12
	mulss	%xmm11, %xmm12
	subss	%xmm10, %xmm12
	addss	%xmm9, %xmm12
	leal	(%r14,%r12), %ebp
	movslq	%ebp, %rbp
	addss	(%rax,%rbp,4), %xmm12
	movss	%xmm12, (%rax,%rbp,4)
	mulss	%xmm5, %xmm8
	mulss	%xmm1, %xmm4
	addss	%xmm8, %xmm4
	mulss	%xmm7, %xmm11
	mulss	%xmm3, %xmm6
	addss	%xmm11, %xmm6
	addss	%xmm4, %xmm6
	leal	(%r15,%r12), %ebp
	movslq	%ebp, %rbp
	addss	(%rax,%rbp,4), %xmm6
	movss	%xmm6, (%rax,%rbp,4)
	addl	$2, %r12d
	incl	%r13d
	cmpl	%esi, %r13d
	jle	.LBB24_78	# bb113
.LBB24_79:	# bb115
	addl	48(%rsp), %r14d
	addl	$2, %r9d
	incl	%esi
	cmpl	%ecx, %esi
	jne	.LBB24_76	# bb112
.LBB24_80:	# bb117
	addl	40(%rsp), %r10d
	addl	20(%rsp), %edi
	movl	44(%rsp), %esi
	incl	%esi
	movl	%esi, 44(%rsp)
	cmpl	52(%rsp), %esi
	je	.LBB24_83	# return
.LBB24_81:	# bb116.preheader
	leal	1(%r10), %ebx
	leal	1(%rdi), %r11d
	movl	120(%rsp), %esi
	leal	(%rsi,%rsi), %esi
	movl	%esi, 48(%rsp)
	xorl	%r14d, %r14d
	movl	%r14d, %r9d
	movl	%r14d, %esi
	jmp	.LBB24_76	# bb112
.LBB24_82:	# bb119
	xorl	%edi, %edi
	leaq	.str32, %rsi
	leaq	.str133, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB24_83:	# return
	addq	$56, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
	.size	cblas_csyr2k, .-cblas_csyr2k
.Leh_func_end17:


	.section	.rodata.cst4,"aM",@progbits,4
	.align	4
.LCPI25_0:					
	.long	1065353216	# float 1.000000e+00
	.text
	.align	16
	.globl	cblas_csyrk
	.type	cblas_csyrk,@function
cblas_csyrk:
.Leh_func_begin18:
.Llabel18:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$24, %rsp
	movss	(%r9), %xmm0
	pxor	%xmm1, %xmm1
	ucomiss	%xmm1, %xmm0
	setp	%al
	setne	%r10b
	setnp	%r11b
	sete	%bl
	andb	%r11b, %bl
	movss	4(%r9), %xmm2
	ucomiss	%xmm1, %xmm2
	setp	%r9b
	setnp	%r11b
	sete	%r14b
	setne	%r15b
	andb	%r11b, %r14b
	andb	%bl, %r14b
	movb	%r14b, 12(%rsp)
	orb	%al, %r10b
	orb	%r9b, %r15b
	orb	%r10b, %r15b
	testb	%r15b, %r15b
	movq	96(%rsp), %rax
	movss	4(%rax), %xmm1
	movss	(%rax), %xmm3
	movq	104(%rsp), %rax
	movq	80(%rsp), %r9
	jne	.LBB25_3	# bb19
.LBB25_1:	# entry
	ucomiss	.LCPI25_0(%rip), %xmm3
	jne	.LBB25_3	# bb19
	jp	.LBB25_3	# bb19
.LBB25_2:	# entry
	pxor	%xmm4, %xmm4
	ucomiss	%xmm4, %xmm1
	setnp	%r10b
	sete	%r11b
	testb	%r10b, %r11b
	jne	.LBB25_85	# return
.LBB25_3:	# bb19
	cmpl	$101, %edi
	je	.LBB25_86	# bb20
.LBB25_4:	# bb24
	cmpl	$111, %edx
	movl	$112, %edx
	movl	$111, %edi
	cmove	%edx, %edi
	movl	%edi, 20(%rsp)
	cmpl	$121, %esi
	movl	$122, %edx
	movl	$121, %esi
	cmove	%edx, %esi
.LBB25_5:	# bb31
	pxor	%xmm4, %xmm4
	ucomiss	%xmm4, %xmm1
	jne	.LBB25_25	# bb46
	jp	.LBB25_25	# bb46
.LBB25_6:	# bb31
	pxor	%xmm4, %xmm4
	ucomiss	%xmm4, %xmm3
	jne	.LBB25_25	# bb46
	jp	.LBB25_25	# bb46
.LBB25_7:	# bb33
	cmpl	$121, %esi
	je	.LBB25_17	# bb39.preheader
.LBB25_8:	# bb45.preheader
	testl	%ecx, %ecx
	jle	.LBB25_12	# bb61
.LBB25_9:	# bb.nph129
	movl	112(%rsp), %edx
	leal	(%rdx,%rdx), %ebx
	xorl	%r11d, %r11d
	movl	%r11d, %r10d
	jmp	.LBB25_23	# bb43.preheader
	.align	16
.LBB25_10:	# bb36
	movslq	%r10d, %r15
	movl	$0, (%rax,%r15,4)
	leal	1(%r10), %r15d
	movslq	%r15d, %r15
	movl	$0, (%rax,%r15,4)
	addl	$2, %r10d
	incl	%ebx
	cmpl	%r14d, %ebx
	jne	.LBB25_10	# bb36
.LBB25_11:	# bb38
	addl	%edx, %r11d
	decl	%r14d
	incl	%edi
	cmpl	%ecx, %edi
	jne	.LBB25_19	# bb37.preheader
.LBB25_12:	# bb61
	testb	$1, 12(%rsp)
	jne	.LBB25_85	# return
.LBB25_13:	# bb63
	cmpl	$121, %esi
	jne	.LBB25_48	# bb75
.LBB25_14:	# bb63
	cmpl	$111, 20(%rsp)
	jne	.LBB25_48	# bb75
.LBB25_15:	# bb74.preheader
	testl	%ecx, %ecx
	jle	.LBB25_85	# return
.LBB25_16:	# bb.nph177
	movl	88(%rsp), %esi
	leal	(%rsi,%rsi), %esi
	movl	%esi, 8(%rsp)
	movl	112(%rsp), %esi
	leal	2(,%rsi,2), %esi
	movl	%esi, 4(%rsp)
	xorl	%edx, %edx
	movl	%edx, 20(%rsp)
	movl	%edx, 16(%rsp)
	movl	%ecx, %esi
	movl	%edx, 12(%rsp)
	jmp	.LBB25_44	# bb72.preheader
.LBB25_17:	# bb39.preheader
	testl	%ecx, %ecx
	jle	.LBB25_12	# bb61
.LBB25_18:	# bb.nph133
	movl	112(%rsp), %edx
	leal	2(,%rdx,2), %edx
	xorl	%r11d, %r11d
	movl	%ecx, %r14d
	movl	%r11d, %edi
	.align	16
.LBB25_19:	# bb37.preheader
	cmpl	%ecx, %edi
	jge	.LBB25_11	# bb38
.LBB25_20:	# bb37.preheader.bb36_crit_edge
	xorl	%ebx, %ebx
	movl	%r11d, %r10d
	jmp	.LBB25_10	# bb36
	.align	16
.LBB25_21:	# bb42
	movslq	%edx, %r14
	movl	$0, (%rax,%r14,4)
	leal	1(%rdx), %r14d
	movslq	%r14d, %r14
	movl	$0, (%rax,%r14,4)
	addl	$2, %edx
	incl	%edi
	cmpl	%r10d, %edi
	jle	.LBB25_21	# bb42
.LBB25_22:	# bb44
	addl	%ebx, %r11d
	incl	%r10d
	cmpl	%ecx, %r10d
	je	.LBB25_12	# bb61
.LBB25_23:	# bb43.preheader
	testl	%r10d, %r10d
	js	.LBB25_22	# bb44
.LBB25_24:	# bb43.preheader.bb42_crit_edge
	xorl	%edi, %edi
	movl	%r11d, %edx
	jmp	.LBB25_21	# bb42
.LBB25_25:	# bb46
	pxor	%xmm4, %xmm4
	ucomiss	%xmm4, %xmm1
	jne	.LBB25_27	# bb48
	jp	.LBB25_27	# bb48
.LBB25_26:	# bb46
	ucomiss	.LCPI25_0(%rip), %xmm3
	setnp	%dl
	sete	%dil
	testb	%dl, %dil
	jne	.LBB25_12	# bb61
.LBB25_27:	# bb48
	cmpl	$121, %esi
	je	.LBB25_32	# bb54.preheader
.LBB25_28:	# bb60.preheader
	testl	%ecx, %ecx
	jle	.LBB25_12	# bb61
.LBB25_29:	# bb.nph137
	movl	112(%rsp), %edx
	leal	(%rdx,%rdx), %r10d
	xorl	%r11d, %r11d
	movl	%r11d, %edi
	jmp	.LBB25_38	# bb58.preheader
	.align	16
.LBB25_30:	# bb51
	movslq	%ebx, %r15
	movss	(%rax,%r15,4), %xmm4
	movaps	%xmm3, %xmm5
	mulss	%xmm4, %xmm5
	leal	1(%rbx), %r12d
	movslq	%r12d, %r12
	movss	(%rax,%r12,4), %xmm6
	movaps	%xmm1, %xmm7
	mulss	%xmm6, %xmm7
	subss	%xmm7, %xmm5
	movss	%xmm5, (%rax,%r15,4)
	mulss	%xmm1, %xmm4
	mulss	%xmm3, %xmm6
	addss	%xmm4, %xmm6
	movss	%xmm6, (%rax,%r12,4)
	addl	$2, %ebx
	incl	%r14d
	cmpl	%r11d, %r14d
	jne	.LBB25_30	# bb51
.LBB25_31:	# bb53
	addl	%edi, %edx
	decl	%r11d
	incl	%r10d
	cmpl	%ecx, %r10d
	je	.LBB25_12	# bb61
	jmp	.LBB25_34	# bb52.preheader
.LBB25_32:	# bb54.preheader
	testl	%ecx, %ecx
	jle	.LBB25_12	# bb61
.LBB25_33:	# bb.nph181
	movl	112(%rsp), %edx
	leal	2(,%rdx,2), %edi
	xorl	%edx, %edx
	movl	%ecx, %r11d
	movl	%edx, %r10d
	.align	16
.LBB25_34:	# bb52.preheader
	cmpl	%ecx, %r10d
	jge	.LBB25_31	# bb53
.LBB25_35:	# bb52.preheader.bb51_crit_edge
	xorl	%r14d, %r14d
	movl	%edx, %ebx
	jmp	.LBB25_30	# bb51
	.align	16
.LBB25_36:	# bb57
	movslq	%edx, %r14
	movss	(%rax,%r14,4), %xmm4
	movaps	%xmm3, %xmm5
	mulss	%xmm4, %xmm5
	leal	1(%rdx), %r15d
	movslq	%r15d, %r15
	movss	(%rax,%r15,4), %xmm6
	movaps	%xmm1, %xmm7
	mulss	%xmm6, %xmm7
	subss	%xmm7, %xmm5
	movss	%xmm5, (%rax,%r14,4)
	mulss	%xmm1, %xmm4
	mulss	%xmm3, %xmm6
	addss	%xmm4, %xmm6
	movss	%xmm6, (%rax,%r15,4)
	addl	$2, %edx
	incl	%ebx
	cmpl	%edi, %ebx
	jle	.LBB25_36	# bb57
.LBB25_37:	# bb59
	addl	%r10d, %r11d
	incl	%edi
	cmpl	%ecx, %edi
	je	.LBB25_12	# bb61
.LBB25_38:	# bb58.preheader
	testl	%edi, %edi
	js	.LBB25_37	# bb59
.LBB25_39:	# bb58.preheader.bb57_crit_edge
	xorl	%ebx, %ebx
	movl	%r11d, %edx
	jmp	.LBB25_36	# bb57
.LBB25_40:	# bb.nph171
	leal	1(%rdi), %ebx
	pxor	%xmm1, %xmm1
	xorl	%r14d, %r14d
	movl	20(%rsp), %r15d
	movaps	%xmm1, %xmm3
	.align	16
.LBB25_41:	# bb69
	leal	(%rdi,%r15), %r12d
	movslq	%r12d, %r12
	movss	(%r9,%r12,4), %xmm4
	leal	(%rbx,%r15), %r12d
	movslq	%r15d, %r13
	leal	1(%r15), %ebp
	movslq	%ebp, %rbp
	movss	(%r9,%rbp,4), %xmm5
	movaps	%xmm5, %xmm6
	mulss	%xmm4, %xmm6
	movss	(%r9,%r13,4), %xmm7
	movslq	%r12d, %r12
	movss	(%r9,%r12,4), %xmm8
	movaps	%xmm7, %xmm9
	mulss	%xmm8, %xmm9
	addss	%xmm6, %xmm9
	addss	%xmm9, %xmm3
	mulss	%xmm8, %xmm5
	mulss	%xmm4, %xmm7
	subss	%xmm5, %xmm7
	addss	%xmm7, %xmm1
	addl	$2, %r15d
	incl	%r14d
	cmpl	%r8d, %r14d
	jne	.LBB25_41	# bb69
.LBB25_42:	# bb71
	movaps	%xmm2, %xmm4
	mulss	%xmm3, %xmm4
	movaps	%xmm0, %xmm5
	mulss	%xmm1, %xmm5
	subss	%xmm4, %xmm5
	movslq	%r10d, %rbx
	addss	(%rax,%rbx,4), %xmm5
	movss	%xmm5, (%rax,%rbx,4)
	mulss	%xmm2, %xmm1
	mulss	%xmm0, %xmm3
	addss	%xmm1, %xmm3
	leal	1(%r10), %ebx
	movslq	%ebx, %rbx
	addss	(%rax,%rbx,4), %xmm3
	movss	%xmm3, (%rax,%rbx,4)
	addl	%edx, %edi
	addl	$2, %r10d
	incl	%r11d
	cmpl	%esi, %r11d
	jne	.LBB25_46	# bb70.preheader
.LBB25_43:	# bb73
	movl	20(%rsp), %edx
	addl	8(%rsp), %edx
	movl	%edx, 20(%rsp)
	movl	16(%rsp), %edx
	addl	4(%rsp), %edx
	movl	%edx, 16(%rsp)
	decl	%esi
	movl	12(%rsp), %edx
	incl	%edx
	movl	%edx, 12(%rsp)
	cmpl	%ecx, %edx
	je	.LBB25_85	# return
.LBB25_44:	# bb72.preheader
	cmpl	%ecx, 12(%rsp)
	jge	.LBB25_43	# bb73
.LBB25_45:	# bb.nph175
	movl	88(%rsp), %edx
	leal	(%rdx,%rdx), %edx
	xorl	%edi, %edi
	movl	16(%rsp), %r10d
	movl	%edi, %r11d
	.align	16
.LBB25_46:	# bb70.preheader
	testl	%r8d, %r8d
	jg	.LBB25_40	# bb.nph171
.LBB25_47:	# bb70.preheader.bb71_crit_edge
	pxor	%xmm1, %xmm1
	movaps	%xmm1, %xmm3
	jmp	.LBB25_42	# bb71
.LBB25_48:	# bb75
	cmpl	$121, %esi
	jne	.LBB25_60	# bb88
.LBB25_49:	# bb75
	cmpl	$112, 20(%rsp)
	jne	.LBB25_60	# bb88
.LBB25_50:	# bb87.preheader
	testl	%ecx, %ecx
	jle	.LBB25_85	# return
.LBB25_51:	# bb.nph167
	movl	112(%rsp), %edx
	leal	2(,%rdx,2), %edx
	movl	%edx, 8(%rsp)
	xorl	%edx, %edx
	movl	%edx, 20(%rsp)
	movl	%ecx, 16(%rsp)
	movl	%edx, 12(%rsp)
	jmp	.LBB25_56	# bb85.preheader
.LBB25_52:	# bb82.preheader
	leal	1(%rbx), %r15d
	movl	88(%rsp), %esi
	leal	(%rsi,%rsi), %esi
	pxor	%xmm3, %xmm3
	xorl	%r10d, %r10d
	movl	%edi, %r11d
	movaps	%xmm3, %xmm1
	.align	16
.LBB25_53:	# bb82
	leal	(%rbx,%r11), %r12d
	movslq	%r12d, %r12
	movss	(%r9,%r12,4), %xmm4
	leal	(%r15,%r11), %r12d
	movslq	%r11d, %r13
	leal	1(%r11), %ebp
	movslq	%ebp, %rbp
	movss	(%r9,%rbp,4), %xmm5
	movaps	%xmm5, %xmm6
	mulss	%xmm4, %xmm6
	movss	(%r9,%r13,4), %xmm7
	movslq	%r12d, %r12
	movss	(%r9,%r12,4), %xmm8
	movaps	%xmm7, %xmm9
	mulss	%xmm8, %xmm9
	addss	%xmm6, %xmm9
	addss	%xmm9, %xmm1
	mulss	%xmm8, %xmm5
	mulss	%xmm4, %xmm7
	subss	%xmm5, %xmm7
	addss	%xmm7, %xmm3
	addl	%esi, %r11d
	incl	%r10d
	cmpl	%r8d, %r10d
	jne	.LBB25_53	# bb82
.LBB25_54:	# bb84
	movaps	%xmm2, %xmm4
	mulss	%xmm1, %xmm4
	movaps	%xmm0, %xmm5
	mulss	%xmm3, %xmm5
	subss	%xmm4, %xmm5
	movl	20(%rsp), %esi
	leal	(%rsi,%rbx), %esi
	movslq	%esi, %rsi
	addss	(%rax,%rsi,4), %xmm5
	movss	%xmm5, (%rax,%rsi,4)
	mulss	%xmm2, %xmm3
	mulss	%xmm0, %xmm1
	addss	%xmm3, %xmm1
	leal	(%r14,%rbx), %esi
	movslq	%esi, %rsi
	addss	(%rax,%rsi,4), %xmm1
	movss	%xmm1, (%rax,%rsi,4)
	addl	$2, %ebx
	incl	%edx
	cmpl	16(%rsp), %edx
	jne	.LBB25_58	# bb83.preheader
.LBB25_55:	# bb86
	movl	20(%rsp), %edx
	addl	8(%rsp), %edx
	movl	%edx, 20(%rsp)
	decl	16(%rsp)
	movl	12(%rsp), %edx
	incl	%edx
	movl	%edx, 12(%rsp)
	cmpl	%ecx, %edx
	je	.LBB25_85	# return
.LBB25_56:	# bb85.preheader
	cmpl	%ecx, 12(%rsp)
	jge	.LBB25_55	# bb86
.LBB25_57:	# bb.nph165
	movl	12(%rsp), %edx
	leal	(%rdx,%rdx), %edi
	movl	20(%rsp), %edx
	leal	1(%rdx), %r14d
	xorl	%ebx, %ebx
	movl	%ebx, %edx
	.align	16
.LBB25_58:	# bb83.preheader
	testl	%r8d, %r8d
	jg	.LBB25_52	# bb82.preheader
.LBB25_59:	# bb83.preheader.bb84_crit_edge
	pxor	%xmm3, %xmm3
	movaps	%xmm3, %xmm1
	jmp	.LBB25_54	# bb84
.LBB25_60:	# bb88
	cmpl	$122, %esi
	jne	.LBB25_72	# bb101
.LBB25_61:	# bb88
	cmpl	$111, 20(%rsp)
	jne	.LBB25_72	# bb101
.LBB25_62:	# bb100.preheader
	testl	%ecx, %ecx
	jle	.LBB25_85	# return
.LBB25_63:	# bb.nph157
	movl	88(%rsp), %ebx
	leal	(%rbx,%rbx), %ebx
	movl	%ebx, 12(%rsp)
	movl	112(%rsp), %ebx
	addl	%ebx, %ebx
	movl	%ebx, 16(%rsp)
	xorl	%r14d, %r14d
	movl	%r14d, 20(%rsp)
	movl	%r14d, %ebx
	jmp	.LBB25_68	# bb98.preheader
.LBB25_64:	# bb.nph151
	leal	1(%rsi), %r15d
	pxor	%xmm1, %xmm1
	xorl	%r12d, %r12d
	movl	%r12d, %r13d
	movaps	%xmm1, %xmm3
	.align	16
.LBB25_65:	# bb95
	leal	(%rsi,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r9,%rbp,4), %xmm4
	leal	(%r11,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r9,%rbp,4), %xmm5
	movaps	%xmm5, %xmm6
	mulss	%xmm4, %xmm6
	leal	(%r15,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r9,%rbp,4), %xmm7
	leal	(%r14,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r9,%rbp,4), %xmm8
	movaps	%xmm8, %xmm9
	mulss	%xmm7, %xmm9
	addss	%xmm6, %xmm9
	addss	%xmm9, %xmm3
	mulss	%xmm7, %xmm5
	mulss	%xmm4, %xmm8
	subss	%xmm5, %xmm8
	addss	%xmm8, %xmm1
	addl	$2, %r12d
	incl	%r13d
	cmpl	%r8d, %r13d
	jne	.LBB25_65	# bb95
.LBB25_66:	# bb97
	movaps	%xmm2, %xmm4
	mulss	%xmm3, %xmm4
	movaps	%xmm0, %xmm5
	mulss	%xmm1, %xmm5
	subss	%xmm4, %xmm5
	movslq	%r10d, %r15
	addss	(%rax,%r15,4), %xmm5
	movss	%xmm5, (%rax,%r15,4)
	mulss	%xmm2, %xmm1
	mulss	%xmm0, %xmm3
	addss	%xmm1, %xmm3
	leal	1(%r10), %r15d
	movslq	%r15d, %r15
	addss	(%rax,%r15,4), %xmm3
	movss	%xmm3, (%rax,%r15,4)
	addl	%edx, %esi
	addl	$2, %r10d
	incl	%edi
	cmpl	%ebx, %edi
	jle	.LBB25_70	# bb96.preheader
.LBB25_67:	# bb99
	addl	12(%rsp), %r14d
	movl	20(%rsp), %edx
	addl	16(%rsp), %edx
	movl	%edx, 20(%rsp)
	incl	%ebx
	cmpl	%ecx, %ebx
	je	.LBB25_85	# return
.LBB25_68:	# bb98.preheader
	testl	%ebx, %ebx
	js	.LBB25_67	# bb99
.LBB25_69:	# bb.nph155
	leal	1(%r14), %r11d
	movl	88(%rsp), %edx
	leal	(%rdx,%rdx), %edx
	xorl	%esi, %esi
	movl	20(%rsp), %r10d
	movl	%esi, %edi
	.align	16
.LBB25_70:	# bb96.preheader
	testl	%r8d, %r8d
	jg	.LBB25_64	# bb.nph151
.LBB25_71:	# bb96.preheader.bb97_crit_edge
	pxor	%xmm1, %xmm1
	movaps	%xmm1, %xmm3
	jmp	.LBB25_66	# bb97
.LBB25_72:	# bb101
	cmpl	$122, %esi
	jne	.LBB25_84	# bb114
.LBB25_73:	# bb101
	cmpl	$112, 20(%rsp)
	jne	.LBB25_84	# bb114
.LBB25_74:	# bb113.preheader
	testl	%ecx, %ecx
	jle	.LBB25_85	# return
.LBB25_75:	# bb.nph147
	movl	112(%rsp), %esi
	addl	%esi, %esi
	movl	%esi, 16(%rsp)
	xorl	%r10d, %r10d
	movl	%r10d, 20(%rsp)
	movl	%r10d, %esi
	jmp	.LBB25_80	# bb111.preheader
.LBB25_76:	# bb108.preheader
	leal	1(%r13), %edi
	movl	88(%rsp), %edx
	leal	(%rdx,%rdx), %edx
	pxor	%xmm3, %xmm3
	xorl	%r12d, %r12d
	movl	%r12d, %r15d
	movaps	%xmm3, %xmm1
	.align	16
.LBB25_77:	# bb108
	leal	(%r13,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r9,%rbp,4), %xmm4
	leal	(%r11,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r9,%rbp,4), %xmm5
	movaps	%xmm5, %xmm6
	mulss	%xmm4, %xmm6
	leal	(%rdi,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r9,%rbp,4), %xmm7
	leal	(%rsi,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r9,%rbp,4), %xmm8
	movaps	%xmm8, %xmm9
	mulss	%xmm7, %xmm9
	addss	%xmm6, %xmm9
	addss	%xmm9, %xmm1
	mulss	%xmm7, %xmm5
	mulss	%xmm4, %xmm8
	subss	%xmm5, %xmm8
	addss	%xmm8, %xmm3
	addl	%edx, %r12d
	incl	%r15d
	cmpl	%r8d, %r15d
	jne	.LBB25_77	# bb108
.LBB25_78:	# bb110
	movaps	%xmm2, %xmm4
	mulss	%xmm1, %xmm4
	movaps	%xmm0, %xmm5
	mulss	%xmm3, %xmm5
	subss	%xmm4, %xmm5
	movl	20(%rsp), %edx
	leal	(%rdx,%r13), %edx
	movslq	%edx, %rdx
	addss	(%rax,%rdx,4), %xmm5
	movss	%xmm5, (%rax,%rdx,4)
	mulss	%xmm2, %xmm3
	mulss	%xmm0, %xmm1
	addss	%xmm3, %xmm1
	leal	(%rbx,%r13), %edx
	movslq	%edx, %rdx
	addss	(%rax,%rdx,4), %xmm1
	movss	%xmm1, (%rax,%rdx,4)
	addl	$2, %r13d
	incl	%r14d
	cmpl	%r10d, %r14d
	jle	.LBB25_82	# bb109.preheader
.LBB25_79:	# bb112
	movl	20(%rsp), %edx
	addl	16(%rsp), %edx
	movl	%edx, 20(%rsp)
	addl	$2, %esi
	incl	%r10d
	cmpl	%ecx, %r10d
	je	.LBB25_85	# return
.LBB25_80:	# bb111.preheader
	testl	%r10d, %r10d
	js	.LBB25_79	# bb112
.LBB25_81:	# bb.nph145
	leal	1(%rsi), %r11d
	movl	20(%rsp), %edx
	leal	1(%rdx), %ebx
	xorl	%r13d, %r13d
	movl	%r13d, %r14d
	.align	16
.LBB25_82:	# bb109.preheader
	testl	%r8d, %r8d
	jg	.LBB25_76	# bb108.preheader
.LBB25_83:	# bb109.preheader.bb110_crit_edge
	pxor	%xmm3, %xmm3
	movaps	%xmm3, %xmm1
	jmp	.LBB25_78	# bb110
.LBB25_84:	# bb114
	xorl	%edi, %edi
	leaq	.str34, %rsi
	leaq	.str135, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB25_85:	# return
	addq	$24, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB25_86:	# bb20
	cmpl	$111, %edx
	movl	$111, %edx
	movl	$112, %edi
	cmove	%edx, %edi
	movl	%edi, 20(%rsp)
	jmp	.LBB25_5	# bb31
	.size	cblas_csyrk, .-cblas_csyrk
.Leh_func_end18:


	.align	16
	.globl	cblas_ctbmv
	.type	cblas_ctbmv,@function
cblas_ctbmv:
.Leh_func_begin19:
.Llabel19:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$56, %rsp
	cmpl	$113, %edx
	movl	$4294967295, %eax
	movl	$1, %r10d
	cmove	%eax, %r10d
	movl	%r10d, 44(%rsp)
	movl	$112, %eax
	cmovne	%edx, %eax
	cmpl	$121, %esi
	sete	%dl
	setne	%r10b
	cmpl	$101, %edi
	sete	%r11b
	setne	%bl
	andb	%dl, %r11b
	orb	%r10b, %bl
	testb	%bl, %bl
	movl	136(%rsp), %edx
	movq	128(%rsp), %r10
	movq	112(%rsp), %rbx
	movl	%r9d, 52(%rsp)
	movl	%ecx, 48(%rsp)
	jne	.LBB26_2	# bb58
.LBB26_1:	# entry
	cmpl	$111, %eax
	je	.LBB26_4	# bb66
.LBB26_2:	# bb58
	cmpl	$122, %esi
	sete	%cl
	setne	%r9b
	cmpl	$102, %edi
	sete	%r14b
	setne	%r15b
	andb	%cl, %r14b
	orb	%r9b, %r15b
	testb	%r15b, %r15b
	jne	.LBB26_14	# bb81
.LBB26_3:	# bb58
	cmpl	$112, %eax
	jne	.LBB26_14	# bb81
.LBB26_4:	# bb66
	testl	%edx, %edx
	jg	.LBB26_55	# bb66.bb80.preheader_crit_edge
.LBB26_5:	# bb67
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
.LBB26_6:	# bb80.preheader
	testl	%r8d, %r8d
	jle	.LBB26_29	# bb118.thread
.LBB26_7:	# bb.nph275
	movl	$1, %ecx
	subl	%r8d, %ecx
	imull	%edx, %ecx
	movl	%ecx, 28(%rsp)
	addl	%eax, %eax
	movl	120(%rsp), %ecx
	addl	%ecx, %ecx
	movl	%ecx, 36(%rsp)
	movl	52(%rsp), %ecx
	leal	1(%rcx), %ecx
	movl	%ecx, 40(%rsp)
	cvtsi2ss	44(%rsp), %xmm0
	leal	(%rdx,%rdx), %ecx
	movl	%ecx, 32(%rsp)
	movl	$4294967294, 44(%rsp)
	xorl	%ecx, %ecx
	movl	%edx, %esi
	movl	%ecx, %edi
	movl	%ecx, %r9d
	.align	16
.LBB26_8:	# bb70
	testl	%edx, %edx
	movl	$0, %r11d
	cmovle	28(%rsp), %r11d
	movl	40(%rsp), %r14d
	leal	(%r14,%r9), %r14d
	cmpl	%r8d, %r14d
	cmovg	%r8d, %r14d
	leal	1(%r9), %r15d
	cmpl	%r14d, %r15d
	jge	.LBB26_56	# bb70.bb76_crit_edge
.LBB26_9:	# bb.nph269
	movl	$4294967295, %r14d
	subl	%r8d, %r14d
	movl	%edi, %r15d
	negl	%r15d
	subl	52(%rsp), %r15d
	addl	$4294967294, %r15d
	cmpl	%r15d, %r14d
	cmovg	%r14d, %r15d
	movl	44(%rsp), %r14d
	subl	%r15d, %r14d
	pxor	%xmm1, %xmm1
	xorl	%r15d, %r15d
	movl	%ecx, %r12d
	movl	%esi, %r13d
	movaps	%xmm1, %xmm2
	.align	16
.LBB26_10:	# bb74
	leal	3(%r12), %ebp
	movslq	%ebp, %rbp
	movaps	%xmm0, %xmm3
	mulss	(%rbx,%rbp,4), %xmm3
	addl	%r11d, %r13d
	leal	1(,%r13,2), %r11d
	leal	(%r13,%r13), %ebp
	movslq	%ebp, %rbp
	movss	(%r10,%rbp,4), %xmm4
	movaps	%xmm3, %xmm5
	mulss	%xmm4, %xmm5
	addl	$2, %r12d
	movslq	%r12d, %rbp
	movss	(%rbx,%rbp,4), %xmm6
	movslq	%r11d, %r11
	movss	(%r10,%r11,4), %xmm7
	movaps	%xmm6, %xmm8
	mulss	%xmm7, %xmm8
	addss	%xmm5, %xmm8
	addss	%xmm8, %xmm2
	mulss	%xmm7, %xmm3
	mulss	%xmm4, %xmm6
	subss	%xmm3, %xmm6
	addss	%xmm6, %xmm1
	incl	%r15d
	cmpl	%r14d, %r15d
	movl	%edx, %r11d
	jne	.LBB26_10	# bb74
.LBB26_11:	# bb76
	movslq	%eax, %r11
	movss	(%r10,%r11,4), %xmm3
	cmpl	$131, 48(%rsp)
	je	.LBB26_57	# bb77
.LBB26_12:	# bb78
	addss	%xmm1, %xmm3
	movss	%xmm3, (%r10,%r11,4)
	leal	1(%rax), %r11d
	movslq	%r11d, %r11
	addss	(%r10,%r11,4), %xmm2
	movss	%xmm2, (%r10,%r11,4)
.LBB26_13:	# bb79
	addl	32(%rsp), %eax
	addl	%edx, %esi
	addl	36(%rsp), %ecx
	incl	%edi
	decl	44(%rsp)
	incl	%r9d
	cmpl	%r8d, %r9d
	jne	.LBB26_8	# bb70
	jmp	.LBB26_29	# bb118.thread
.LBB26_14:	# bb81
	cmpl	$122, %esi
	sete	%cl
	setne	%r9b
	cmpl	$101, %edi
	sete	%r15b
	setne	%r12b
	andb	%cl, %r15b
	orb	%r9b, %r12b
	testb	%r12b, %r12b
	jne	.LBB26_16	# bb89
.LBB26_15:	# bb81
	cmpl	$111, %eax
	je	.LBB26_18	# bb97
.LBB26_16:	# bb89
	cmpl	$121, %esi
	sete	%cl
	setne	%sil
	cmpl	$102, %edi
	sete	%dil
	setne	%r9b
	andb	%cl, %dil
	orb	%sil, %r9b
	testb	%r9b, %r9b
	jne	.LBB26_30	# bb120
.LBB26_17:	# bb89
	cmpl	$112, %eax
	jne	.LBB26_30	# bb120
.LBB26_18:	# bb97
	testl	%edx, %edx
	jg	.LBB26_58	# bb97.bb100_crit_edge
.LBB26_19:	# bb98
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
.LBB26_20:	# bb100
	leal	-1(%r8), %ecx
	movl	120(%rsp), %esi
	movl	%esi, %edi
	imull	%ecx, %edi
	movl	$4294967295, %r11d
	movl	52(%rsp), %r9d
	subl	%r9d, %r11d
	movl	%r11d, 40(%rsp)
	leal	(%r9,%rdi), %r11d
	subl	%r8d, %r9d
	leal	1(%r9,%rdi), %edi
	addl	%r11d, %r11d
	imull	%edx, %ecx
	addl	%eax, %ecx
	addl	%ecx, %ecx
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
	movl	$1, %r9d
	subl	%esi, %r9d
	movl	%r9d, 32(%rsp)
	cvtsi2ss	44(%rsp), %xmm0
	addl	%esi, %esi
	movl	%esi, 36(%rsp)
	leal	(%rdx,%rdx), %esi
	movl	%esi, 44(%rsp)
	jmp	.LBB26_27	# bb114
.LBB26_21:	# bb101
	xorl	%r9d, %r9d
	testl	%edx, %edx
	movl	%eax, %r14d
	cmovg	%r9d, %r14d
	movl	40(%rsp), %r15d
	leal	(%r15,%r8), %r15d
	cmpl	52(%rsp), %esi
	cmovl	%r9d, %r15d
	movl	%r15d, %r9d
	imull	%edx, %r9d
	cmpl	%esi, %r15d
	jge	.LBB26_59	# bb101.bb110_crit_edge
.LBB26_22:	# bb.nph257
	movl	%r8d, %esi
	subl	%r15d, %esi
	addl	%edi, %r15d
	addl	%r15d, %r15d
	decl	%esi
	pxor	%xmm1, %xmm1
	xorl	%r12d, %r12d
	movaps	%xmm1, %xmm2
	.align	16
.LBB26_23:	# bb108
	movslq	%r15d, %r13
	leal	1(%r15), %ebp
	movslq	%ebp, %rbp
	movaps	%xmm0, %xmm3
	mulss	(%rbx,%rbp,4), %xmm3
	addl	%r14d, %r9d
	leal	1(,%r9,2), %r14d
	leal	(%r9,%r9), %ebp
	movslq	%ebp, %rbp
	movss	(%r10,%rbp,4), %xmm4
	movaps	%xmm3, %xmm5
	mulss	%xmm4, %xmm5
	movss	(%rbx,%r13,4), %xmm6
	movslq	%r14d, %r14
	movss	(%r10,%r14,4), %xmm7
	movaps	%xmm6, %xmm8
	mulss	%xmm7, %xmm8
	addss	%xmm5, %xmm8
	addss	%xmm8, %xmm2
	mulss	%xmm7, %xmm3
	mulss	%xmm4, %xmm6
	subss	%xmm3, %xmm6
	addss	%xmm6, %xmm1
	addl	$2, %r15d
	incl	%r12d
	cmpl	%esi, %r12d
	movl	%edx, %r14d
	jne	.LBB26_23	# bb108
.LBB26_24:	# bb110
	movslq	%ecx, %rsi
	movss	(%r10,%rsi,4), %xmm3
	cmpl	$131, 48(%rsp)
	je	.LBB26_60	# bb111
.LBB26_25:	# bb112
	addss	%xmm1, %xmm3
	movss	%xmm3, (%r10,%rsi,4)
	leal	1(%rcx), %esi
	movslq	%esi, %rsi
	addss	(%r10,%rsi,4), %xmm2
	movss	%xmm2, (%r10,%rsi,4)
.LBB26_26:	# bb113
	subl	36(%rsp), %r11d
	subl	44(%rsp), %ecx
	addl	32(%rsp), %edi
	decl	%r8d
.LBB26_27:	# bb114
	testl	%r8d, %r8d
	jle	.LBB26_29	# bb118.thread
.LBB26_28:	# bb115
	leal	-1(%r8), %esi
	testl	%r8d, %r8d
	jne	.LBB26_21	# bb101
.LBB26_29:	# bb118.thread
	addq	$56, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB26_30:	# bb120
	cmpl	$111, %eax
	sete	%cl
	andb	%cl, %r14b
	cmpl	$112, %eax
	sete	%cl
	testb	%cl, %r11b
	jne	.LBB26_32	# bb136
.LBB26_31:	# bb120
	notb	%r14b
	testb	$1, %r14b
	jne	.LBB26_43	# bb159
.LBB26_32:	# bb136
	testl	%edx, %edx
	jg	.LBB26_61	# bb136.bb139_crit_edge
.LBB26_33:	# bb137
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
.LBB26_34:	# bb139
	movl	$4294967295, %ecx
	subl	52(%rsp), %ecx
	movl	%ecx, 32(%rsp)
	leal	-1(%r8), %ecx
	movl	120(%rsp), %esi
	movl	%esi, %edi
	imull	%ecx, %edi
	addl	%edi, %edi
	imull	%edx, %ecx
	addl	%eax, %ecx
	addl	%ecx, %ecx
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
	cvtsi2ss	44(%rsp), %xmm0
	leal	(%rsi,%rsi), %esi
	movl	%esi, 44(%rsp)
	leal	(%rdx,%rdx), %esi
	movl	%esi, 40(%rsp)
	jmp	.LBB26_41	# bb153
.LBB26_35:	# bb140
	xorl	%r9d, %r9d
	testl	%edx, %edx
	movl	%eax, %r11d
	cmovg	%r9d, %r11d
	movl	32(%rsp), %r14d
	leal	(%r14,%r8), %r14d
	cmpl	52(%rsp), %esi
	cmovl	%r9d, %r14d
	movl	%r14d, %r9d
	imull	%edx, %r9d
	cmpl	%esi, %r14d
	jge	.LBB26_62	# bb140.bb149_crit_edge
.LBB26_36:	# bb.nph245
	movl	120(%rsp), %esi
	leal	-1(%rsi), %r15d
	imull	%r14d, %r15d
	leal	-1(%r8,%r15), %r15d
	addl	%r15d, %r15d
	leal	-1(%r8), %r12d
	subl	%r14d, %r12d
	leal	-2(,%rsi,2), %esi
	pxor	%xmm1, %xmm1
	xorl	%r14d, %r14d
	movaps	%xmm1, %xmm2
	.align	16
.LBB26_37:	# bb147
	movslq	%r15d, %r13
	leal	1(%r15), %ebp
	movslq	%ebp, %rbp
	movaps	%xmm0, %xmm3
	mulss	(%rbx,%rbp,4), %xmm3
	addl	%r11d, %r9d
	leal	1(,%r9,2), %r11d
	leal	(%r9,%r9), %ebp
	movslq	%ebp, %rbp
	movss	(%r10,%rbp,4), %xmm4
	movaps	%xmm3, %xmm5
	mulss	%xmm4, %xmm5
	movss	(%rbx,%r13,4), %xmm6
	movslq	%r11d, %r11
	movss	(%r10,%r11,4), %xmm7
	movaps	%xmm6, %xmm8
	mulss	%xmm7, %xmm8
	addss	%xmm5, %xmm8
	addss	%xmm8, %xmm2
	mulss	%xmm7, %xmm3
	mulss	%xmm4, %xmm6
	subss	%xmm3, %xmm6
	addss	%xmm6, %xmm1
	addl	%esi, %r15d
	incl	%r14d
	cmpl	%r12d, %r14d
	movl	%edx, %r11d
	jne	.LBB26_37	# bb147
.LBB26_38:	# bb149
	movslq	%ecx, %rsi
	movss	(%r10,%rsi,4), %xmm3
	cmpl	$131, 48(%rsp)
	je	.LBB26_63	# bb150
.LBB26_39:	# bb151
	addss	%xmm1, %xmm3
	movss	%xmm3, (%r10,%rsi,4)
	leal	1(%rcx), %esi
	movslq	%esi, %rsi
	addss	(%r10,%rsi,4), %xmm2
	movss	%xmm2, (%r10,%rsi,4)
.LBB26_40:	# bb152
	subl	44(%rsp), %edi
	subl	40(%rsp), %ecx
	decl	%r8d
.LBB26_41:	# bb153
	testl	%r8d, %r8d
	jle	.LBB26_29	# bb118.thread
.LBB26_42:	# bb154
	leal	-1(%r8), %esi
	testl	%r8d, %r8d
	jne	.LBB26_35	# bb140
	jmp	.LBB26_29	# bb118.thread
.LBB26_43:	# bb159
	cmpl	$111, %eax
	sete	%cl
	andb	%cl, %dil
	cmpl	$112, %eax
	sete	%al
	testb	%al, %r15b
	jne	.LBB26_45	# bb175
.LBB26_44:	# bb159
	notb	%dil
	testb	$1, %dil
	jne	.LBB26_67	# bb191
.LBB26_45:	# bb175
	testl	%edx, %edx
	jg	.LBB26_64	# bb175.bb190.preheader_crit_edge
.LBB26_46:	# bb176
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
.LBB26_47:	# bb190.preheader
	testl	%r8d, %r8d
	jle	.LBB26_29	# bb118.thread
.LBB26_48:	# bb.nph236
	movl	52(%rsp), %ecx
	leal	1(,%rcx,2), %esi
	movl	%esi, 28(%rsp)
	movl	120(%rsp), %esi
	leal	(%rcx,%rsi), %edi
	leal	-2(,%rdi,2), %edi
	movl	%edi, 24(%rsp)
	movl	$1, %edi
	subl	%r8d, %edi
	imull	%edx, %edi
	movl	%edi, 4(%rsp)
	addl	%eax, %eax
	leal	(%rsi,%rsi), %esi
	movl	%esi, 20(%rsp)
	cvtsi2ss	44(%rsp), %xmm0
	leal	(%rcx,%rcx), %esi
	movl	%esi, 16(%rsp)
	leal	1(%rcx), %ecx
	movl	%ecx, 12(%rsp)
	leal	(%rdx,%rdx), %ecx
	movl	%ecx, 8(%rsp)
	movl	$4294967294, 32(%rsp)
	xorl	%ecx, %ecx
	movl	%edx, 44(%rsp)
	movl	%ecx, 40(%rsp)
	movl	%ecx, %esi
	.align	16
.LBB26_49:	# bb179
	testl	%edx, %edx
	movl	$0, %edi
	cmovle	4(%rsp), %edi
	movl	12(%rsp), %r9d
	leal	(%r9,%rsi), %r9d
	cmpl	%r8d, %r9d
	cmovg	%r8d, %r9d
	leal	1(%rsi), %r11d
	cmpl	%r9d, %r11d
	jge	.LBB26_65	# bb179.bb186_crit_edge
.LBB26_50:	# bb.nph
	movl	$4294967295, %r9d
	subl	%r8d, %r9d
	movl	40(%rsp), %r11d
	negl	%r11d
	subl	52(%rsp), %r11d
	addl	$4294967294, %r11d
	cmpl	%r11d, %r9d
	cmovg	%r9d, %r11d
	movl	32(%rsp), %r9d
	subl	%r11d, %r9d
	movl	24(%rsp), %r11d
	leal	(%r11,%rcx), %r11d
	movl	120(%rsp), %r14d
	leal	-2(,%r14,2), %r14d
	pxor	%xmm1, %xmm1
	xorl	%r15d, %r15d
	movl	44(%rsp), %r12d
	movaps	%xmm1, %xmm2
	.align	16
.LBB26_51:	# bb184
	movslq	%r11d, %r13
	leal	1(%r11), %ebp
	movslq	%ebp, %rbp
	movaps	%xmm0, %xmm3
	mulss	(%rbx,%rbp,4), %xmm3
	addl	%edi, %r12d
	leal	1(,%r12,2), %edi
	leal	(%r12,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r10,%rbp,4), %xmm4
	movaps	%xmm3, %xmm5
	mulss	%xmm4, %xmm5
	movss	(%rbx,%r13,4), %xmm6
	movslq	%edi, %rdi
	movss	(%r10,%rdi,4), %xmm7
	movaps	%xmm6, %xmm8
	mulss	%xmm7, %xmm8
	addss	%xmm5, %xmm8
	addss	%xmm8, %xmm1
	mulss	%xmm7, %xmm3
	mulss	%xmm4, %xmm6
	subss	%xmm3, %xmm6
	addss	%xmm6, %xmm2
	addl	%r14d, %r11d
	incl	%r15d
	cmpl	%r9d, %r15d
	movl	%edx, %edi
	jne	.LBB26_51	# bb184
.LBB26_52:	# bb186
	movslq	%eax, %rdi
	movss	(%r10,%rdi,4), %xmm3
	cmpl	$131, 48(%rsp)
	je	.LBB26_66	# bb187
.LBB26_53:	# bb188
	addss	%xmm2, %xmm3
	movss	%xmm3, (%r10,%rdi,4)
	leal	1(%rax), %edi
	movslq	%edi, %rdi
	addss	(%r10,%rdi,4), %xmm1
	movss	%xmm1, (%r10,%rdi,4)
.LBB26_54:	# bb189
	addl	8(%rsp), %eax
	addl	%edx, 44(%rsp)
	addl	20(%rsp), %ecx
	incl	40(%rsp)
	decl	32(%rsp)
	incl	%esi
	cmpl	%r8d, %esi
	je	.LBB26_29	# bb118.thread
	jmp	.LBB26_49	# bb179
.LBB26_55:	# bb66.bb80.preheader_crit_edge
	xorl	%eax, %eax
	jmp	.LBB26_6	# bb80.preheader
.LBB26_56:	# bb70.bb76_crit_edge
	pxor	%xmm1, %xmm1
	movaps	%xmm1, %xmm2
	jmp	.LBB26_11	# bb76
.LBB26_57:	# bb77
	movslq	%ecx, %r14
	leal	1(%rcx), %r15d
	movslq	%r15d, %r15
	movaps	%xmm0, %xmm4
	mulss	(%rbx,%r15,4), %xmm4
	leal	1(%rax), %r15d
	movslq	%r15d, %r15
	movss	(%r10,%r15,4), %xmm5
	movaps	%xmm4, %xmm6
	mulss	%xmm5, %xmm6
	movss	(%rbx,%r14,4), %xmm7
	movaps	%xmm7, %xmm8
	mulss	%xmm3, %xmm8
	subss	%xmm6, %xmm8
	addss	%xmm1, %xmm8
	movss	%xmm8, (%r10,%r11,4)
	mulss	%xmm5, %xmm7
	mulss	%xmm3, %xmm4
	addss	%xmm7, %xmm4
	addss	%xmm2, %xmm4
	movss	%xmm4, (%r10,%r15,4)
	jmp	.LBB26_13	# bb79
.LBB26_58:	# bb97.bb100_crit_edge
	xorl	%eax, %eax
	jmp	.LBB26_20	# bb100
.LBB26_59:	# bb101.bb110_crit_edge
	pxor	%xmm1, %xmm1
	movaps	%xmm1, %xmm2
	jmp	.LBB26_24	# bb110
.LBB26_60:	# bb111
	movslq	%r11d, %r9
	leal	1(%r11), %r14d
	movslq	%r14d, %r14
	movaps	%xmm0, %xmm4
	mulss	(%rbx,%r14,4), %xmm4
	leal	1(%rcx), %r14d
	movslq	%r14d, %r14
	movss	(%r10,%r14,4), %xmm5
	movaps	%xmm4, %xmm6
	mulss	%xmm5, %xmm6
	movss	(%rbx,%r9,4), %xmm7
	movaps	%xmm7, %xmm8
	mulss	%xmm3, %xmm8
	subss	%xmm6, %xmm8
	addss	%xmm1, %xmm8
	movss	%xmm8, (%r10,%rsi,4)
	mulss	%xmm5, %xmm7
	mulss	%xmm3, %xmm4
	addss	%xmm7, %xmm4
	addss	%xmm2, %xmm4
	movss	%xmm4, (%r10,%r14,4)
	jmp	.LBB26_26	# bb113
.LBB26_61:	# bb136.bb139_crit_edge
	xorl	%eax, %eax
	jmp	.LBB26_34	# bb139
.LBB26_62:	# bb140.bb149_crit_edge
	pxor	%xmm1, %xmm1
	movaps	%xmm1, %xmm2
	jmp	.LBB26_38	# bb149
.LBB26_63:	# bb150
	movslq	%edi, %r9
	leal	1(%rdi), %r11d
	movslq	%r11d, %r11
	movaps	%xmm0, %xmm4
	mulss	(%rbx,%r11,4), %xmm4
	leal	1(%rcx), %r11d
	movslq	%r11d, %r11
	movss	(%r10,%r11,4), %xmm5
	movaps	%xmm4, %xmm6
	mulss	%xmm5, %xmm6
	movss	(%rbx,%r9,4), %xmm7
	movaps	%xmm7, %xmm8
	mulss	%xmm3, %xmm8
	subss	%xmm6, %xmm8
	addss	%xmm1, %xmm8
	movss	%xmm8, (%r10,%rsi,4)
	mulss	%xmm5, %xmm7
	mulss	%xmm3, %xmm4
	addss	%xmm7, %xmm4
	addss	%xmm2, %xmm4
	movss	%xmm4, (%r10,%r11,4)
	jmp	.LBB26_40	# bb152
.LBB26_64:	# bb175.bb190.preheader_crit_edge
	xorl	%eax, %eax
	jmp	.LBB26_47	# bb190.preheader
.LBB26_65:	# bb179.bb186_crit_edge
	pxor	%xmm1, %xmm1
	movaps	%xmm1, %xmm2
	jmp	.LBB26_52	# bb186
.LBB26_66:	# bb187
	leal	1(%rax), %r9d
	movslq	%r9d, %r9
	movss	(%r10,%r9,4), %xmm4
	movl	28(%rsp), %r11d
	leal	(%r11,%rcx), %r11d
	movslq	%r11d, %r11
	movaps	%xmm0, %xmm5
	mulss	(%rbx,%r11,4), %xmm5
	movaps	%xmm5, %xmm6
	mulss	%xmm4, %xmm6
	movl	16(%rsp), %r11d
	leal	(%r11,%rcx), %r11d
	movslq	%r11d, %r11
	movss	(%rbx,%r11,4), %xmm7
	movaps	%xmm7, %xmm8
	mulss	%xmm3, %xmm8
	subss	%xmm6, %xmm8
	addss	%xmm2, %xmm8
	movss	%xmm8, (%r10,%rdi,4)
	mulss	%xmm4, %xmm7
	mulss	%xmm3, %xmm5
	addss	%xmm7, %xmm5
	addss	%xmm1, %xmm5
	movss	%xmm5, (%r10,%r9,4)
	jmp	.LBB26_54	# bb189
.LBB26_67:	# bb191
	xorl	%edi, %edi
	leaq	.str36, %rsi
	leaq	.str137, %rdx
	xorb	%al, %al
	call	cblas_xerbla
	jmp	.LBB26_29	# bb118.thread
	.size	cblas_ctbmv, .-cblas_ctbmv
.Leh_func_end19:


	.align	16
	.globl	cblas_ctbsv
	.type	cblas_ctbsv,@function
cblas_ctbsv:
.Leh_func_begin20:
.Llabel20:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$88, %rsp
	cmpl	$113, %edx
	movl	$4294967295, %eax
	movl	$1, %r10d
	cmove	%eax, %r10d
	movl	$112, %eax
	cmovne	%edx, %eax
	testl	%r8d, %r8d
	movl	168(%rsp), %ebx
	movq	160(%rsp), %r14
	movq	144(%rsp), %r15
	movl	%r9d, 64(%rsp)
	movl	%r8d, 68(%rsp)
	movl	%ecx, 60(%rsp)
	je	.LBB27_16	# bb84.thread
.LBB27_1:	# bb51
	cmpl	$121, %esi
	sete	%cl
	setne	%dl
	cmpl	$101, %edi
	sete	%r8b
	setne	%r9b
	andb	%cl, %r8b
	orb	%dl, %r9b
	testb	%r9b, %r9b
	jne	.LBB27_3	# bb58
.LBB27_2:	# bb51
	cmpl	$111, %eax
	je	.LBB27_5	# bb66
.LBB27_3:	# bb58
	cmpl	$122, %esi
	sete	%cl
	setne	%dl
	cmpl	$102, %edi
	sete	%r9b
	setne	%r11b
	andb	%cl, %r9b
	orb	%dl, %r11b
	testb	%r11b, %r11b
	jne	.LBB27_17	# bb86
.LBB27_4:	# bb58
	cmpl	$112, %eax
	jne	.LBB27_17	# bb86
.LBB27_5:	# bb66
	testl	%ebx, %ebx
	jg	.LBB27_57	# bb66.bb69_crit_edge
.LBB27_6:	# bb67
	movl	$1, %eax
	subl	68(%rsp), %eax
	imull	%ebx, %eax
.LBB27_7:	# bb69
	movl	68(%rsp), %ecx
	leal	-1(%rcx), %edx
	movl	152(%rsp), %esi
	movl	%esi, %edi
	imull	%edx, %edi
	addl	%edi, %edi
	movl	%edi, 56(%rsp)
	imull	%ebx, %edx
	addl	%eax, %edx
	addl	%edx, %edx
	movl	%edx, 52(%rsp)
	movl	$1, %eax
	subl	%ecx, %eax
	imull	%ebx, %eax
	movl	%eax, 16(%rsp)
	movl	%ecx, %eax
	imull	%ebx, %eax
	movl	%eax, 44(%rsp)
	cvtsi2ss	%r10d, %xmm0
	movss	%xmm0, 76(%rsp)
	leal	(%rbx,%rbx), %eax
	movl	%eax, 20(%rsp)
	addl	%esi, %esi
	movl	%esi, 36(%rsp)
	xorl	%eax, %eax
	movl	%eax, 24(%rsp)
	movl	%ecx, %r12d
	movl	%eax, 40(%rsp)
	jmp	.LBB27_14	# bb80
.LBB27_8:	# bb70
	testl	%ebx, %ebx
	movl	$0, %eax
	cmovle	16(%rsp), %eax
	movl	64(%rsp), %r10d
	leal	(%r10,%r12), %r10d
	movl	68(%rsp), %ecx
	cmpl	%ecx, %r10d
	cmovg	%ecx, %r10d
	movl	52(%rsp), %ecx
	movslq	%ecx, %r13
	leal	1(%rcx), %ecx
	cmpl	%r10d, %r12d
	movss	(%r14,%r13,4), %xmm0
	movss	%xmm0, 84(%rsp)
	movslq	%ecx, %rbp
	movss	(%r14,%rbp,4), %xmm0
	movss	%xmm0, 80(%rsp)
	jge	.LBB27_11	# bb76
.LBB27_9:	# bb.nph264
	movl	$4294967295, %r10d
	movl	68(%rsp), %ecx
	subl	%ecx, %r10d
	movl	64(%rsp), %edx
	leal	1(%rdx,%rcx), %edx
	movl	24(%rsp), %esi
	subl	%edx, %esi
	cmpl	%esi, %r10d
	cmovg	%r10d, %esi
	addl	%ecx, %esi
	movl	40(%rsp), %r10d
	subl	%esi, %r10d
	decl	%r10d
	xorl	%ecx, %ecx
	movl	56(%rsp), %edx
	movl	44(%rsp), %esi
	.align	16
.LBB27_10:	# bb74
	leal	3(%rdx), %edi
	movslq	%edi, %rdi
	movss	76(%rsp), %xmm0
	mulss	(%r15,%rdi,4), %xmm0
	addl	%eax, %esi
	leal	1(,%rsi,2), %eax
	leal	(%rsi,%rsi), %edi
	movslq	%edi, %rdi
	movss	(%r14,%rdi,4), %xmm1
	movaps	%xmm0, %xmm2
	mulss	%xmm1, %xmm2
	addl	$2, %edx
	movslq	%edx, %rdi
	movss	(%r15,%rdi,4), %xmm3
	movslq	%eax, %rax
	movss	(%r14,%rax,4), %xmm4
	movaps	%xmm3, %xmm5
	mulss	%xmm4, %xmm5
	addss	%xmm2, %xmm5
	movss	80(%rsp), %xmm2
	subss	%xmm5, %xmm2
	movss	%xmm2, 80(%rsp)
	mulss	%xmm4, %xmm0
	mulss	%xmm1, %xmm3
	subss	%xmm0, %xmm3
	movss	84(%rsp), %xmm0
	subss	%xmm3, %xmm0
	movss	%xmm0, 84(%rsp)
	incl	%ecx
	cmpl	%r10d, %ecx
	movl	%ebx, %eax
	jne	.LBB27_10	# bb74
.LBB27_11:	# bb76
	cmpl	$131, 60(%rsp)
	je	.LBB27_58	# bb77
.LBB27_12:	# bb78
	movss	84(%rsp), %xmm0
	movss	%xmm0, (%r14,%r13,4)
	movss	80(%rsp), %xmm0
	movss	%xmm0, 48(%rsp)
.LBB27_13:	# bb79
	movss	48(%rsp), %xmm0
	movss	%xmm0, (%r14,%rbp,4)
	subl	%ebx, 44(%rsp)
	movl	52(%rsp), %eax
	subl	20(%rsp), %eax
	movl	%eax, 52(%rsp)
	movl	56(%rsp), %eax
	subl	36(%rsp), %eax
	movl	%eax, 56(%rsp)
	decl	%r12d
	incl	24(%rsp)
	incl	40(%rsp)
.LBB27_14:	# bb80
	testl	%r12d, %r12d
	jle	.LBB27_16	# bb84.thread
.LBB27_15:	# bb81
	testl	%r12d, %r12d
	jne	.LBB27_8	# bb70
.LBB27_16:	# bb84.thread
	addq	$88, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB27_17:	# bb86
	cmpl	$122, %esi
	sete	%cl
	setne	%dl
	cmpl	$101, %edi
	sete	%r11b
	setne	%r12b
	andb	%cl, %r11b
	orb	%dl, %r12b
	testb	%r12b, %r12b
	jne	.LBB27_19	# bb94
.LBB27_18:	# bb86
	cmpl	$111, %eax
	je	.LBB27_21	# bb102
.LBB27_19:	# bb94
	cmpl	$121, %esi
	sete	%cl
	setne	%dl
	cmpl	$102, %edi
	sete	%sil
	setne	%dil
	andb	%cl, %sil
	orb	%dl, %dil
	testb	%dil, %dil
	jne	.LBB27_31	# bb120
.LBB27_20:	# bb94
	cmpl	$112, %eax
	jne	.LBB27_31	# bb120
.LBB27_21:	# bb102
	testl	%ebx, %ebx
	jg	.LBB27_59	# bb102.bb119.preheader_crit_edge
.LBB27_22:	# bb103
	movl	$1, %eax
	subl	68(%rsp), %eax
	imull	%ebx, %eax
.LBB27_23:	# bb119.preheader
	cmpl	$0, 68(%rsp)
	jle	.LBB27_16	# bb84.thread
.LBB27_24:	# bb.nph256
	movl	$1, %ecx
	subl	68(%rsp), %ecx
	imull	%ebx, %ecx
	movl	%ecx, 20(%rsp)
	addl	%eax, %eax
	movl	152(%rsp), %ecx
	leal	(%rcx,%rcx), %edx
	movl	%edx, 44(%rsp)
	decl	%ecx
	movl	%ecx, 36(%rsp)
	movl	64(%rsp), %ecx
	leal	(%rcx,%rcx), %edx
	movl	%edx, 52(%rsp)
	movl	%ecx, %edx
	negl	%edx
	movl	%edx, 24(%rsp)
	cvtsi2ss	%r10d, %xmm0
	movss	%xmm0, 76(%rsp)
	leal	(%rbx,%rbx), %r10d
	movl	%r10d, 40(%rsp)
	xorl	%r12d, %r12d
	movl	%ecx, 56(%rsp)
	.align	16
.LBB27_25:	# bb106
	xorl	%r10d, %r10d
	testl	%ebx, %ebx
	movl	20(%rsp), %ecx
	cmovg	%r10d, %ecx
	movl	24(%rsp), %edx
	leal	(%rdx,%r12), %edx
	cmpl	64(%rsp), %r12d
	cmovl	%r10d, %edx
	movl	%edx, %r10d
	imull	%ebx, %r10d
	movslq	%eax, %r13
	incl	%eax
	cmpl	%r12d, %edx
	movss	(%r14,%r13,4), %xmm0
	movss	%xmm0, 84(%rsp)
	movslq	%eax, %rbp
	movss	(%r14,%rbp,4), %xmm0
	movss	%xmm0, 80(%rsp)
	jge	.LBB27_28	# bb115
.LBB27_26:	# bb.nph251
	movl	%r12d, %eax
	subl	%edx, %eax
	addl	56(%rsp), %edx
	addl	%edx, %edx
	xorl	%esi, %esi
	.align	16
.LBB27_27:	# bb113
	movslq	%edx, %rdi
	leal	1(%rdx), %r8d
	movslq	%r8d, %r8
	movss	76(%rsp), %xmm0
	mulss	(%r15,%r8,4), %xmm0
	addl	%ecx, %r10d
	leal	1(,%r10,2), %ecx
	leal	(%r10,%r10), %r8d
	movslq	%r8d, %r8
	movss	(%r14,%r8,4), %xmm1
	movaps	%xmm0, %xmm2
	mulss	%xmm1, %xmm2
	movss	(%r15,%rdi,4), %xmm3
	movslq	%ecx, %rcx
	movss	(%r14,%rcx,4), %xmm4
	movaps	%xmm3, %xmm5
	mulss	%xmm4, %xmm5
	addss	%xmm2, %xmm5
	movss	80(%rsp), %xmm2
	subss	%xmm5, %xmm2
	movss	%xmm2, 80(%rsp)
	mulss	%xmm4, %xmm0
	mulss	%xmm1, %xmm3
	subss	%xmm0, %xmm3
	movss	84(%rsp), %xmm0
	subss	%xmm3, %xmm0
	movss	%xmm0, 84(%rsp)
	addl	$2, %edx
	incl	%esi
	cmpl	%eax, %esi
	movl	%ebx, %ecx
	jne	.LBB27_27	# bb113
.LBB27_28:	# bb115
	cmpl	$131, 60(%rsp)
	je	.LBB27_60	# bb116
.LBB27_29:	# bb117
	movss	84(%rsp), %xmm0
	movss	%xmm0, (%r14,%r13,4)
	movss	80(%rsp), %xmm0
	movss	%xmm0, 48(%rsp)
.LBB27_30:	# bb118
	movss	48(%rsp), %xmm0
	movss	%xmm0, (%r14,%rbp,4)
	movl	%r13d, %eax
	addl	40(%rsp), %eax
	movl	52(%rsp), %r10d
	addl	44(%rsp), %r10d
	movl	%r10d, 52(%rsp)
	movl	56(%rsp), %r10d
	addl	36(%rsp), %r10d
	movl	%r10d, 56(%rsp)
	incl	%r12d
	cmpl	68(%rsp), %r12d
	je	.LBB27_16	# bb84.thread
	jmp	.LBB27_25	# bb106
.LBB27_31:	# bb120
	cmpl	$111, %eax
	sete	%cl
	andb	%cl, %r9b
	cmpl	$112, %eax
	sete	%cl
	testb	%cl, %r8b
	jne	.LBB27_33	# bb136
.LBB27_32:	# bb120
	notb	%r9b
	testb	$1, %r9b
	jne	.LBB27_43	# bb154
.LBB27_33:	# bb136
	testl	%ebx, %ebx
	jg	.LBB27_61	# bb136.bb153.preheader_crit_edge
.LBB27_34:	# bb137
	movl	$1, %eax
	subl	68(%rsp), %eax
	imull	%ebx, %eax
.LBB27_35:	# bb153.preheader
	cmpl	$0, 68(%rsp)
	jle	.LBB27_16	# bb84.thread
.LBB27_36:	# bb.nph243
	movl	$1, %ecx
	subl	68(%rsp), %ecx
	imull	%ebx, %ecx
	movl	%ecx, 24(%rsp)
	addl	%eax, %eax
	movl	152(%rsp), %ecx
	leal	(%rcx,%rcx), %ecx
	movl	%ecx, 48(%rsp)
	movl	64(%rsp), %ecx
	negl	%ecx
	movl	%ecx, 40(%rsp)
	cvtsi2ss	%r10d, %xmm0
	movss	%xmm0, 76(%rsp)
	leal	(%rbx,%rbx), %r10d
	movl	%r10d, 44(%rsp)
	xorl	%r10d, %r10d
	movl	%r10d, 56(%rsp)
	movl	%r10d, %r12d
	.align	16
.LBB27_37:	# bb140
	xorl	%r10d, %r10d
	testl	%ebx, %ebx
	movl	24(%rsp), %ecx
	cmovg	%r10d, %ecx
	movl	40(%rsp), %edx
	leal	(%rdx,%r12), %edx
	cmpl	64(%rsp), %r12d
	cmovl	%r10d, %edx
	movl	%edx, %r10d
	imull	%ebx, %r10d
	movslq	%eax, %r13
	incl	%eax
	cmpl	%r12d, %edx
	movss	(%r14,%r13,4), %xmm0
	movss	%xmm0, 84(%rsp)
	movslq	%eax, %rbp
	movss	(%r14,%rbp,4), %xmm0
	movss	%xmm0, 80(%rsp)
	jge	.LBB27_40	# bb149
.LBB27_38:	# bb.nph238
	movl	152(%rsp), %eax
	leal	-1(%rax), %esi
	imull	%edx, %esi
	addl	%r12d, %esi
	addl	%esi, %esi
	movl	%r12d, %edi
	subl	%edx, %edi
	leal	-2(,%rax,2), %eax
	xorl	%edx, %edx
	.align	16
.LBB27_39:	# bb147
	movslq	%esi, %r8
	leal	1(%rsi), %r9d
	movslq	%r9d, %r9
	movss	76(%rsp), %xmm0
	mulss	(%r15,%r9,4), %xmm0
	addl	%ecx, %r10d
	leal	1(,%r10,2), %ecx
	leal	(%r10,%r10), %r9d
	movslq	%r9d, %r9
	movss	(%r14,%r9,4), %xmm1
	movaps	%xmm0, %xmm2
	mulss	%xmm1, %xmm2
	movss	(%r15,%r8,4), %xmm3
	movslq	%ecx, %rcx
	movss	(%r14,%rcx,4), %xmm4
	movaps	%xmm3, %xmm5
	mulss	%xmm4, %xmm5
	addss	%xmm2, %xmm5
	movss	80(%rsp), %xmm2
	subss	%xmm5, %xmm2
	movss	%xmm2, 80(%rsp)
	mulss	%xmm4, %xmm0
	mulss	%xmm1, %xmm3
	subss	%xmm0, %xmm3
	movss	84(%rsp), %xmm0
	subss	%xmm3, %xmm0
	movss	%xmm0, 84(%rsp)
	addl	%eax, %esi
	incl	%edx
	cmpl	%edi, %edx
	movl	%ebx, %ecx
	jne	.LBB27_39	# bb147
.LBB27_40:	# bb149
	cmpl	$131, 60(%rsp)
	je	.LBB27_62	# bb150
.LBB27_41:	# bb151
	movss	84(%rsp), %xmm0
	movss	%xmm0, (%r14,%r13,4)
	movss	80(%rsp), %xmm0
	movss	%xmm0, 52(%rsp)
.LBB27_42:	# bb152
	movss	52(%rsp), %xmm0
	movss	%xmm0, (%r14,%rbp,4)
	movl	%r13d, %eax
	addl	44(%rsp), %eax
	movl	56(%rsp), %r10d
	addl	48(%rsp), %r10d
	movl	%r10d, 56(%rsp)
	incl	%r12d
	cmpl	68(%rsp), %r12d
	je	.LBB27_16	# bb84.thread
	jmp	.LBB27_37	# bb140
.LBB27_43:	# bb154
	cmpl	$111, %eax
	sete	%cl
	andb	%cl, %sil
	cmpl	$112, %eax
	sete	%al
	testb	%al, %r11b
	jne	.LBB27_45	# bb170
.LBB27_44:	# bb154
	notb	%sil
	testb	$1, %sil
	jne	.LBB27_56	# bb191
.LBB27_45:	# bb170
	testl	%ebx, %ebx
	jg	.LBB27_63	# bb170.bb173_crit_edge
.LBB27_46:	# bb171
	movl	$1, %eax
	subl	68(%rsp), %eax
	imull	%ebx, %eax
.LBB27_47:	# bb173
	movl	152(%rsp), %ecx
	movl	%ecx, %edx
	movl	68(%rsp), %esi
	imull	%esi, %edx
	movl	64(%rsp), %edi
	addl	%edi, %edx
	leal	-2(,%rdx,2), %edx
	movl	%edx, 20(%rsp)
	leal	-1(%rsi), %edx
	movl	%ebx, %r8d
	imull	%edx, %r8d
	addl	%eax, %r8d
	addl	%r8d, %r8d
	movl	%r8d, 56(%rsp)
	imull	%ecx, %edx
	addl	%edi, %edx
	leal	1(,%rdx,2), %eax
	movl	%eax, 16(%rsp)
	addl	%edx, %edx
	movl	%edx, 4(%rsp)
	movl	$1, %eax
	subl	%esi, %eax
	imull	%ebx, %eax
	movl	%eax, (%rsp)
	movl	%esi, %eax
	imull	%ebx, %eax
	movl	%eax, 48(%rsp)
	cvtsi2ss	%r10d, %xmm0
	movss	%xmm0, 76(%rsp)
	leal	(%rbx,%rbx), %eax
	movl	%eax, 12(%rsp)
	leal	(%rcx,%rcx), %eax
	movl	%eax, 8(%rsp)
	xorl	%r12d, %r12d
	movl	%esi, %r13d
	movl	%r12d, 44(%rsp)
	movl	%r12d, 40(%rsp)
	jmp	.LBB27_54	# bb185
.LBB27_48:	# bb174
	testl	%ebx, %ebx
	movl	$0, %eax
	cmovle	(%rsp), %eax
	movl	64(%rsp), %ecx
	leal	(%rcx,%r13), %ecx
	movl	68(%rsp), %edx
	cmpl	%edx, %ecx
	cmovg	%edx, %ecx
	movl	56(%rsp), %edx
	movslq	%edx, %rbp
	leal	1(%rdx), %edx
	cmpl	%ecx, %r13d
	movss	(%r14,%rbp,4), %xmm0
	movss	%xmm0, 84(%rsp)
	movslq	%edx, %rcx
	movq	%rcx, 24(%rsp)
	movss	(%r14,%rcx,4), %xmm0
	movss	%xmm0, 80(%rsp)
	jge	.LBB27_51	# bb181
.LBB27_49:	# bb.nph
	movl	$4294967295, %ecx
	movl	68(%rsp), %edx
	subl	%edx, %ecx
	movl	64(%rsp), %esi
	leal	1(%rsi,%rdx), %esi
	movl	44(%rsp), %edi
	subl	%esi, %edi
	cmpl	%edi, %ecx
	cmovg	%ecx, %edi
	addl	%edx, %edi
	movl	40(%rsp), %ecx
	subl	%edi, %ecx
	decl	%ecx
	movl	20(%rsp), %edx
	leal	(%rdx,%r12), %edx
	movl	152(%rsp), %esi
	leal	-2(,%rsi,2), %esi
	xorl	%edi, %edi
	movl	48(%rsp), %r8d
	.align	16
.LBB27_50:	# bb179
	movslq	%edx, %r9
	leal	1(%rdx), %r10d
	movslq	%r10d, %r10
	movss	76(%rsp), %xmm0
	mulss	(%r15,%r10,4), %xmm0
	addl	%eax, %r8d
	leal	1(,%r8,2), %eax
	leal	(%r8,%r8), %r10d
	movslq	%r10d, %r10
	movss	(%r14,%r10,4), %xmm1
	movaps	%xmm0, %xmm2
	mulss	%xmm1, %xmm2
	movss	(%r15,%r9,4), %xmm3
	movslq	%eax, %rax
	movss	(%r14,%rax,4), %xmm4
	movaps	%xmm3, %xmm5
	mulss	%xmm4, %xmm5
	addss	%xmm2, %xmm5
	movss	80(%rsp), %xmm2
	subss	%xmm5, %xmm2
	movss	%xmm2, 80(%rsp)
	mulss	%xmm4, %xmm0
	mulss	%xmm1, %xmm3
	subss	%xmm0, %xmm3
	movss	84(%rsp), %xmm0
	subss	%xmm3, %xmm0
	movss	%xmm0, 84(%rsp)
	addl	%esi, %edx
	incl	%edi
	cmpl	%ecx, %edi
	movl	%ebx, %eax
	jne	.LBB27_50	# bb179
.LBB27_51:	# bb181
	cmpl	$131, 60(%rsp)
	je	.LBB27_64	# bb182
.LBB27_52:	# bb183
	movss	84(%rsp), %xmm0
	movss	%xmm0, (%r14,%rbp,4)
	movss	80(%rsp), %xmm0
	movss	%xmm0, 52(%rsp)
.LBB27_53:	# bb184
	movss	52(%rsp), %xmm0
	movq	24(%rsp), %rax
	movss	%xmm0, (%r14,%rax,4)
	subl	%ebx, 48(%rsp)
	movl	56(%rsp), %eax
	subl	12(%rsp), %eax
	movl	%eax, 56(%rsp)
	subl	8(%rsp), %r12d
	decl	%r13d
	incl	44(%rsp)
	incl	40(%rsp)
.LBB27_54:	# bb185
	testl	%r13d, %r13d
	jle	.LBB27_16	# bb84.thread
.LBB27_55:	# bb186
	testl	%r13d, %r13d
	jne	.LBB27_48	# bb174
	jmp	.LBB27_16	# bb84.thread
.LBB27_56:	# bb191
	xorl	%edi, %edi
	leaq	.str38, %rsi
	leaq	.str139, %rdx
	xorb	%al, %al
	call	cblas_xerbla
	jmp	.LBB27_16	# bb84.thread
.LBB27_57:	# bb66.bb69_crit_edge
	xorl	%eax, %eax
	jmp	.LBB27_7	# bb69
.LBB27_58:	# bb77
	movl	56(%rsp), %eax
	movslq	%eax, %rcx
	leal	1(%rax), %eax
	movslq	%eax, %rax
	movss	76(%rsp), %xmm0
	mulss	(%r15,%rax,4), %xmm0
	movss	%xmm0, 72(%rsp)
	movss	(%r15,%rcx,4), %xmm0
	movss	%xmm0, 48(%rsp)
	cvtss2sd	72(%rsp), %xmm1
	cvtss2sd	%xmm0, %xmm0
	call	_ZL6xhypotdd
	cvtsd2ss	%xmm0, %xmm0
	movss	72(%rsp), %xmm1
	divss	%xmm0, %xmm1
	movss	%xmm1, 72(%rsp)
	movss	80(%rsp), %xmm1
	movaps	%xmm1, %xmm2
	mulss	72(%rsp), %xmm2
	movss	48(%rsp), %xmm3
	divss	%xmm0, %xmm3
	movss	%xmm3, 48(%rsp)
	movss	84(%rsp), %xmm3
	movaps	%xmm3, %xmm4
	mulss	48(%rsp), %xmm4
	addss	%xmm2, %xmm4
	divss	%xmm0, %xmm4
	movss	%xmm4, (%r14,%r13,4)
	movss	72(%rsp), %xmm2
	mulss	%xmm3, %xmm2
	movss	%xmm2, 72(%rsp)
	movss	48(%rsp), %xmm2
	mulss	%xmm1, %xmm2
	subss	72(%rsp), %xmm2
	divss	%xmm0, %xmm2
	movss	%xmm2, 48(%rsp)
	jmp	.LBB27_13	# bb79
.LBB27_59:	# bb102.bb119.preheader_crit_edge
	xorl	%eax, %eax
	jmp	.LBB27_23	# bb119.preheader
.LBB27_60:	# bb116
	movl	52(%rsp), %eax
	movslq	%eax, %rcx
	leal	1(%rax), %eax
	movslq	%eax, %rax
	movss	76(%rsp), %xmm0
	mulss	(%r15,%rax,4), %xmm0
	movss	%xmm0, 72(%rsp)
	movss	(%r15,%rcx,4), %xmm0
	movss	%xmm0, 48(%rsp)
	cvtss2sd	72(%rsp), %xmm1
	cvtss2sd	%xmm0, %xmm0
	call	_ZL6xhypotdd
	cvtsd2ss	%xmm0, %xmm0
	movss	72(%rsp), %xmm1
	divss	%xmm0, %xmm1
	movss	%xmm1, 72(%rsp)
	movss	80(%rsp), %xmm1
	movaps	%xmm1, %xmm2
	mulss	72(%rsp), %xmm2
	movss	48(%rsp), %xmm3
	divss	%xmm0, %xmm3
	movss	%xmm3, 48(%rsp)
	movss	84(%rsp), %xmm3
	movaps	%xmm3, %xmm4
	mulss	48(%rsp), %xmm4
	addss	%xmm2, %xmm4
	divss	%xmm0, %xmm4
	movss	%xmm4, (%r14,%r13,4)
	movss	72(%rsp), %xmm2
	mulss	%xmm3, %xmm2
	movss	%xmm2, 72(%rsp)
	movss	48(%rsp), %xmm2
	mulss	%xmm1, %xmm2
	subss	72(%rsp), %xmm2
	divss	%xmm0, %xmm2
	movss	%xmm2, 48(%rsp)
	jmp	.LBB27_30	# bb118
.LBB27_61:	# bb136.bb153.preheader_crit_edge
	xorl	%eax, %eax
	jmp	.LBB27_35	# bb153.preheader
.LBB27_62:	# bb150
	movl	56(%rsp), %eax
	movslq	%eax, %rcx
	leal	1(%rax), %eax
	movslq	%eax, %rax
	movss	76(%rsp), %xmm0
	mulss	(%r15,%rax,4), %xmm0
	movss	%xmm0, 72(%rsp)
	movss	(%r15,%rcx,4), %xmm0
	movss	%xmm0, 52(%rsp)
	cvtss2sd	72(%rsp), %xmm1
	cvtss2sd	%xmm0, %xmm0
	call	_ZL6xhypotdd
	cvtsd2ss	%xmm0, %xmm0
	movss	72(%rsp), %xmm1
	divss	%xmm0, %xmm1
	movss	%xmm1, 72(%rsp)
	movss	80(%rsp), %xmm1
	movaps	%xmm1, %xmm2
	mulss	72(%rsp), %xmm2
	movss	52(%rsp), %xmm3
	divss	%xmm0, %xmm3
	movss	%xmm3, 52(%rsp)
	movss	84(%rsp), %xmm3
	movaps	%xmm3, %xmm4
	mulss	52(%rsp), %xmm4
	addss	%xmm2, %xmm4
	divss	%xmm0, %xmm4
	movss	%xmm4, (%r14,%r13,4)
	movss	72(%rsp), %xmm2
	mulss	%xmm3, %xmm2
	movss	%xmm2, 72(%rsp)
	movss	52(%rsp), %xmm2
	mulss	%xmm1, %xmm2
	subss	72(%rsp), %xmm2
	divss	%xmm0, %xmm2
	movss	%xmm2, 52(%rsp)
	jmp	.LBB27_42	# bb152
.LBB27_63:	# bb170.bb173_crit_edge
	xorl	%eax, %eax
	jmp	.LBB27_47	# bb173
.LBB27_64:	# bb182
	movl	16(%rsp), %eax
	leal	(%rax,%r12), %eax
	movslq	%eax, %rax
	movss	76(%rsp), %xmm0
	mulss	(%r15,%rax,4), %xmm0
	movss	%xmm0, 72(%rsp)
	movl	4(%rsp), %eax
	leal	(%rax,%r12), %eax
	movslq	%eax, %rax
	movss	(%r15,%rax,4), %xmm0
	movss	%xmm0, 52(%rsp)
	cvtss2sd	%xmm0, %xmm0
	cvtss2sd	72(%rsp), %xmm1
	call	_ZL6xhypotdd
	cvtsd2ss	%xmm0, %xmm0
	movss	72(%rsp), %xmm1
	divss	%xmm0, %xmm1
	movss	%xmm1, 72(%rsp)
	movss	80(%rsp), %xmm1
	movaps	%xmm1, %xmm2
	mulss	72(%rsp), %xmm2
	movss	52(%rsp), %xmm3
	divss	%xmm0, %xmm3
	movss	%xmm3, 52(%rsp)
	movss	84(%rsp), %xmm3
	movaps	%xmm3, %xmm4
	mulss	52(%rsp), %xmm4
	addss	%xmm2, %xmm4
	divss	%xmm0, %xmm4
	movss	%xmm4, (%r14,%rbp,4)
	movss	72(%rsp), %xmm2
	mulss	%xmm3, %xmm2
	movss	%xmm2, 72(%rsp)
	movss	52(%rsp), %xmm2
	mulss	%xmm1, %xmm2
	subss	72(%rsp), %xmm2
	divss	%xmm0, %xmm2
	movss	%xmm2, 52(%rsp)
	jmp	.LBB27_53	# bb184
	.size	cblas_ctbsv, .-cblas_ctbsv
.Leh_func_end20:


	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI28_0:					
	.quad	9223372036854775807	# double value: nan
	.quad	9223372036854775807	# double value: nan
	.section	.rodata.cst8,"aM",@progbits,8
	.align	16
.LCPI28_1:					
	.quad	4607182418800017408	# double value: 1.000000e+00
	.text
	.align	16
	.type	_ZL6xhypotdd,@function
_ZL6xhypotdd:
	movsd	.LCPI28_0(%rip), %xmm2
	andpd	%xmm2, %xmm0
	andpd	%xmm2, %xmm1
	movapd	%xmm1, %xmm2
	maxsd	%xmm0, %xmm2
	minsd	%xmm1, %xmm0
	pxor	%xmm1, %xmm1
	ucomisd	%xmm1, %xmm0
	setnp	%al
	sete	%cl
	testb	%al, %cl
	jne	.LBB28_2	# bb5
.LBB28_1:	# bb4
	divsd	%xmm2, %xmm0
	mulsd	%xmm0, %xmm0
	addsd	.LCPI28_1(%rip), %xmm0
	sqrtsd	%xmm0, %xmm0
	mulsd	%xmm2, %xmm0
	ret
.LBB28_2:	# bb5
	movapd	%xmm2, %xmm0
	ret
	.size	_ZL6xhypotdd, .-_ZL6xhypotdd


	.align	16
	.globl	cblas_ctpmv
	.type	cblas_ctpmv,@function
cblas_ctpmv:
.Leh_func_begin21:
.Llabel21:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$24, %rsp
	cmpl	$113, %edx
	movl	$4294967295, %eax
	movl	$1, %r10d
	cmove	%eax, %r10d
	movl	$112, %eax
	cmovne	%edx, %eax
	cmpl	$121, %esi
	sete	%dl
	setne	%r11b
	cmpl	$101, %edi
	sete	%bl
	setne	%r14b
	andb	%dl, %bl
	orb	%r11b, %r14b
	testb	%r14b, %r14b
	movq	80(%rsp), %rdx
	movl	%ecx, 20(%rsp)
	jne	.LBB29_2	# bb48
.LBB29_1:	# entry
	cmpl	$111, %eax
	je	.LBB29_4	# bb56
.LBB29_2:	# bb48
	cmpl	$122, %esi
	sete	%cl
	setne	%r11b
	cmpl	$102, %edi
	sete	%r14b
	setne	%r15b
	andb	%cl, %r14b
	orb	%r11b, %r15b
	testb	%r15b, %r15b
	jne	.LBB29_14	# bb71
.LBB29_3:	# bb48
	cmpl	$112, %eax
	jne	.LBB29_14	# bb71
.LBB29_4:	# bb56
	cmpl	$0, 88(%rsp)
	jg	.LBB29_55	# bb56.bb70.preheader_crit_edge
.LBB29_5:	# bb57
	movl	$1, %eax
	subl	%r8d, %eax
	imull	88(%rsp), %eax
.LBB29_6:	# bb70.preheader
	testl	%r8d, %r8d
	jle	.LBB29_29	# bb105.thread
.LBB29_7:	# bb.nph257
	movl	$1, %ecx
	subl	%r8d, %ecx
	movl	88(%rsp), %esi
	imull	%esi, %ecx
	movl	%ecx, 4(%rsp)
	addl	%eax, %eax
	leal	1(,%r8,2), %ecx
	movl	%ecx, 16(%rsp)
	leal	-1(%r8), %ecx
	movl	%ecx, 12(%rsp)
	cvtsi2ss	%r10d, %xmm0
	leal	(%rsi,%rsi), %r10d
	movl	%r10d, 8(%rsp)
	xorl	%r10d, %r10d
	movl	%r10d, %ecx
	.align	16
.LBB29_8:	# bb60
	movl	16(%rsp), %edi
	leal	(%rdi,%r10), %edi
	imull	%ecx, %edi
	movl	%edi, %r11d
	shrl	$31, %r11d
	addl	%edi, %r11d
	movl	%r11d, %edi
	andl	$4294967294, %edi
	sarl	%r11d
	movslq	%edi, %rbx
	movss	(%r9,%rbx,4), %xmm1
	orl	$1, %edi
	movslq	%edi, %rdi
	movaps	%xmm0, %xmm2
	mulss	(%r9,%rdi,4), %xmm2
	movslq	%eax, %rdi
	movss	(%rdx,%rdi,4), %xmm3
	incl	%eax
	movslq	%eax, %rax
	movss	(%rdx,%rax,4), %xmm4
	cmpl	$131, 20(%rsp)
	jne	.LBB29_10	# bb63
.LBB29_9:	# bb61
	movaps	%xmm2, %xmm5
	mulss	%xmm3, %xmm5
	movaps	%xmm1, %xmm6
	mulss	%xmm4, %xmm6
	addss	%xmm5, %xmm6
	mulss	%xmm4, %xmm2
	mulss	%xmm3, %xmm1
	subss	%xmm2, %xmm1
	movaps	%xmm6, %xmm4
	movaps	%xmm1, %xmm3
.LBB29_10:	# bb63
	cmpl	$0, 88(%rsp)
	movl	$0, %ebx
	cmovle	4(%rsp), %ebx
	leal	1(%rcx), %r14d
	cmpl	%r8d, %r14d
	jge	.LBB29_13	# bb69
.LBB29_11:	# bb.nph252
	movl	12(%rsp), %r14d
	leal	(%r14,%r10), %r14d
	addl	%r11d, %r11d
	xorl	%r15d, %r15d
	movl	%esi, %r12d
	.align	16
.LBB29_12:	# bb67
	leal	3(%r11), %r13d
	movslq	%r13d, %r13
	movaps	%xmm0, %xmm1
	mulss	(%r9,%r13,4), %xmm1
	addl	%ebx, %r12d
	leal	1(,%r12,2), %ebx
	leal	(%r12,%r12), %r13d
	movslq	%r13d, %r13
	movss	(%rdx,%r13,4), %xmm2
	movaps	%xmm1, %xmm5
	mulss	%xmm2, %xmm5
	addl	$2, %r11d
	movslq	%r11d, %r13
	movss	(%r9,%r13,4), %xmm6
	movslq	%ebx, %rbx
	movss	(%rdx,%rbx,4), %xmm7
	movaps	%xmm6, %xmm8
	mulss	%xmm7, %xmm8
	addss	%xmm5, %xmm8
	addss	%xmm8, %xmm4
	mulss	%xmm7, %xmm1
	mulss	%xmm2, %xmm6
	subss	%xmm1, %xmm6
	addss	%xmm6, %xmm3
	incl	%r15d
	cmpl	%r14d, %r15d
	movl	88(%rsp), %ebx
	jne	.LBB29_12	# bb67
.LBB29_13:	# bb69
	movss	%xmm3, (%rdx,%rdi,4)
	movss	%xmm4, (%rdx,%rax,4)
	movl	%edi, %eax
	addl	8(%rsp), %eax
	addl	88(%rsp), %esi
	decl	%r10d
	incl	%ecx
	cmpl	%r8d, %ecx
	jne	.LBB29_8	# bb60
	jmp	.LBB29_29	# bb105.thread
.LBB29_14:	# bb71
	cmpl	$122, %esi
	sete	%cl
	setne	%r11b
	cmpl	$101, %edi
	sete	%r15b
	setne	%r12b
	andb	%cl, %r15b
	orb	%r11b, %r12b
	testb	%r12b, %r12b
	jne	.LBB29_16	# bb79
.LBB29_15:	# bb71
	cmpl	$111, %eax
	je	.LBB29_18	# bb87
.LBB29_16:	# bb79
	cmpl	$121, %esi
	sete	%cl
	setne	%sil
	cmpl	$102, %edi
	sete	%dil
	setne	%r11b
	andb	%cl, %dil
	orb	%sil, %r11b
	testb	%r11b, %r11b
	jne	.LBB29_30	# bb107
.LBB29_17:	# bb79
	cmpl	$112, %eax
	jne	.LBB29_30	# bb107
.LBB29_18:	# bb87
	cmpl	$0, 88(%rsp)
	jg	.LBB29_56	# bb87.bb90_crit_edge
.LBB29_19:	# bb88
	movl	$1, %eax
	subl	%r8d, %eax
	imull	88(%rsp), %eax
.LBB29_20:	# bb90
	leal	-1(%r8), %ecx
	movl	88(%rsp), %esi
	imull	%esi, %ecx
	addl	%eax, %ecx
	addl	%ecx, %ecx
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%esi, %eax
	cvtsi2ss	%r10d, %xmm0
	leal	(%rsi,%rsi), %r10d
	movl	%r10d, 16(%rsp)
	jmp	.LBB29_27	# bb101
.LBB29_21:	# bb91
	movl	%r10d, %esi
	imull	%r8d, %esi
	movl	%esi, %edi
	shrl	$31, %edi
	addl	%esi, %edi
	sarl	%edi
	leal	(%r8,%rdi), %esi
	leal	-1(,%rsi,2), %r11d
	movslq	%r11d, %r11
	movaps	%xmm0, %xmm1
	mulss	(%r9,%r11,4), %xmm1
	leal	-2(,%rsi,2), %esi
	movslq	%esi, %rsi
	movss	(%r9,%rsi,4), %xmm2
	movslq	%ecx, %rsi
	movss	(%rdx,%rsi,4), %xmm3
	leal	1(%rcx), %r11d
	movslq	%r11d, %r11
	movss	(%rdx,%r11,4), %xmm4
	cmpl	$131, 20(%rsp)
	jne	.LBB29_23	# bb94
.LBB29_22:	# bb92
	movaps	%xmm1, %xmm5
	mulss	%xmm3, %xmm5
	movaps	%xmm2, %xmm6
	mulss	%xmm4, %xmm6
	addss	%xmm5, %xmm6
	mulss	%xmm4, %xmm1
	mulss	%xmm3, %xmm2
	subss	%xmm1, %xmm2
	movaps	%xmm6, %xmm4
	movaps	%xmm2, %xmm3
.LBB29_23:	# bb94
	cmpl	$0, 88(%rsp)
	movl	$0, %ebx
	cmovle	%eax, %ebx
	testl	%r10d, %r10d
	jle	.LBB29_26	# bb100
.LBB29_24:	# bb.nph240
	movl	88(%rsp), %r10d
	leal	(%r10,%r10), %r10d
	addl	%ebx, %ebx
	addl	%edi, %edi
	leal	-1(%r8), %r14d
	xorl	%r15d, %r15d
	.align	16
.LBB29_25:	# bb98
	movslq	%edi, %r12
	leal	1(%rdi), %r13d
	movslq	%r13d, %r13
	movaps	%xmm0, %xmm1
	mulss	(%r9,%r13,4), %xmm1
	movslq	%ebx, %r13
	movss	(%rdx,%r13,4), %xmm2
	movaps	%xmm1, %xmm5
	mulss	%xmm2, %xmm5
	movss	(%r9,%r12,4), %xmm6
	leal	1(%rbx), %r12d
	movslq	%r12d, %r12
	movss	(%rdx,%r12,4), %xmm7
	movaps	%xmm6, %xmm8
	mulss	%xmm7, %xmm8
	addss	%xmm5, %xmm8
	addss	%xmm8, %xmm4
	mulss	%xmm2, %xmm6
	mulss	%xmm7, %xmm1
	subss	%xmm1, %xmm6
	addss	%xmm6, %xmm3
	addl	%r10d, %ebx
	addl	$2, %edi
	incl	%r15d
	cmpl	%r14d, %r15d
	jne	.LBB29_25	# bb98
.LBB29_26:	# bb100
	movss	%xmm3, (%rdx,%rsi,4)
	movss	%xmm4, (%rdx,%r11,4)
	subl	16(%rsp), %ecx
	decl	%r8d
.LBB29_27:	# bb101
	testl	%r8d, %r8d
	jle	.LBB29_29	# bb105.thread
.LBB29_28:	# bb102
	leal	-1(%r8), %r10d
	testl	%r8d, %r8d
	jne	.LBB29_21	# bb91
.LBB29_29:	# bb105.thread
	addq	$24, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB29_30:	# bb107
	cmpl	$111, %eax
	sete	%cl
	andb	%cl, %r14b
	cmpl	$112, %eax
	sete	%cl
	testb	%cl, %bl
	jne	.LBB29_32	# bb123
.LBB29_31:	# bb107
	notb	%r14b
	testb	$1, %r14b
	jne	.LBB29_43	# bb143
.LBB29_32:	# bb123
	cmpl	$0, 88(%rsp)
	jg	.LBB29_57	# bb123.bb126_crit_edge
.LBB29_33:	# bb124
	movl	$1, %eax
	subl	%r8d, %eax
	imull	88(%rsp), %eax
.LBB29_34:	# bb126
	leal	-1(%r8), %ecx
	movl	88(%rsp), %esi
	imull	%esi, %ecx
	addl	%eax, %ecx
	addl	%ecx, %ecx
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%esi, %eax
	movl	%eax, 8(%rsp)
	leal	2(%r8), %eax
	movl	%eax, 16(%rsp)
	cvtsi2ss	%r10d, %xmm0
	leal	(%rsi,%rsi), %eax
	movl	%eax, 12(%rsp)
	movl	%r8d, %eax
	jmp	.LBB29_41	# bb137
.LBB29_35:	# bb127
	movl	16(%rsp), %esi
	imull	%r10d, %esi
	movl	%esi, %edi
	shrl	$31, %edi
	addl	%esi, %edi
	andl	$4294967294, %edi
	movslq	%edi, %rsi
	movss	(%r9,%rsi,4), %xmm1
	orl	$1, %edi
	movslq	%edi, %rsi
	movaps	%xmm0, %xmm2
	mulss	(%r9,%rsi,4), %xmm2
	movslq	%ecx, %rsi
	movss	(%rdx,%rsi,4), %xmm3
	leal	1(%rcx), %edi
	movslq	%edi, %rdi
	movss	(%rdx,%rdi,4), %xmm4
	cmpl	$131, 20(%rsp)
	jne	.LBB29_37	# bb130
.LBB29_36:	# bb128
	movaps	%xmm2, %xmm5
	mulss	%xmm3, %xmm5
	movaps	%xmm1, %xmm6
	mulss	%xmm4, %xmm6
	addss	%xmm5, %xmm6
	mulss	%xmm4, %xmm2
	mulss	%xmm3, %xmm1
	subss	%xmm2, %xmm1
	movaps	%xmm1, %xmm3
	movaps	%xmm6, %xmm4
.LBB29_37:	# bb130
	cmpl	$0, 88(%rsp)
	movl	$0, %r11d
	cmovle	8(%rsp), %r11d
	testl	%r10d, %r10d
	jle	.LBB29_40	# bb136
.LBB29_38:	# bb.nph232
	movl	88(%rsp), %r10d
	leal	(%r10,%r10), %r10d
	addl	%r11d, %r11d
	leal	1(,%r8,2), %ebx
	leal	-1(%rax), %r14d
	xorl	%r15d, %r15d
	movl	%r15d, %r12d
	.align	16
.LBB29_39:	# bb134
	leal	(%rbx,%r15), %r13d
	imull	%r12d, %r13d
	movl	%r13d, %ebp
	shrl	$31, %ebp
	addl	%r13d, %ebp
	sarl	%ebp
	leal	(%r14,%r15), %r13d
	addl	%ebp, %r13d
	leal	1(,%r13,2), %ebp
	movslq	%ebp, %rbp
	movaps	%xmm0, %xmm1
	mulss	(%r9,%rbp,4), %xmm1
	movslq	%r11d, %rbp
	movss	(%rdx,%rbp,4), %xmm2
	movaps	%xmm1, %xmm5
	mulss	%xmm2, %xmm5
	addl	%r13d, %r13d
	movslq	%r13d, %r13
	movss	(%r9,%r13,4), %xmm6
	leal	1(%r11), %r13d
	movslq	%r13d, %r13
	movss	(%rdx,%r13,4), %xmm7
	movaps	%xmm6, %xmm8
	mulss	%xmm7, %xmm8
	addss	%xmm5, %xmm8
	addss	%xmm8, %xmm4
	mulss	%xmm7, %xmm1
	mulss	%xmm2, %xmm6
	subss	%xmm1, %xmm6
	addss	%xmm6, %xmm3
	addl	%r10d, %r11d
	decl	%r15d
	incl	%r12d
	cmpl	%r14d, %r12d
	jne	.LBB29_39	# bb134
.LBB29_40:	# bb136
	movss	%xmm3, (%rdx,%rsi,4)
	movss	%xmm4, (%rdx,%rdi,4)
	subl	12(%rsp), %ecx
	decl	%eax
	incl	16(%rsp)
.LBB29_41:	# bb137
	testl	%eax, %eax
	jle	.LBB29_29	# bb105.thread
.LBB29_42:	# bb138
	leal	-1(%rax), %r10d
	testl	%eax, %eax
	jne	.LBB29_35	# bb127
	jmp	.LBB29_29	# bb105.thread
.LBB29_43:	# bb143
	cmpl	$111, %eax
	sete	%cl
	andb	%cl, %dil
	cmpl	$112, %eax
	sete	%al
	testb	%al, %r15b
	jne	.LBB29_45	# bb159
.LBB29_44:	# bb143
	notb	%dil
	testb	$1, %dil
	jne	.LBB29_59	# bb174
.LBB29_45:	# bb159
	cmpl	$0, 88(%rsp)
	jg	.LBB29_58	# bb159.bb173.preheader_crit_edge
.LBB29_46:	# bb160
	movl	$1, %eax
	subl	%r8d, %eax
	imull	88(%rsp), %eax
.LBB29_47:	# bb173.preheader
	testl	%r8d, %r8d
	jle	.LBB29_29	# bb105.thread
.LBB29_48:	# bb.nph226
	movl	$1, %ecx
	subl	%r8d, %ecx
	movl	88(%rsp), %esi
	imull	%esi, %ecx
	movl	%ecx, 12(%rsp)
	addl	%eax, %eax
	leal	-1(%r8), %ecx
	cvtsi2ss	%r10d, %xmm0
	leal	(%rsi,%rsi), %edi
	movl	%edi, 16(%rsp)
	xorl	%edi, %edi
	.align	16
.LBB29_49:	# bb163
	leal	1(%rdi), %r10d
	movl	%r10d, %r11d
	imull	%edi, %r11d
	movl	%r11d, %ebx
	shrl	$31, %ebx
	addl	%r11d, %ebx
	sarl	%ebx
	addl	%edi, %ebx
	leal	1(,%rbx,2), %r11d
	movslq	%r11d, %r11
	movaps	%xmm0, %xmm1
	mulss	(%r9,%r11,4), %xmm1
	addl	%ebx, %ebx
	movslq	%ebx, %r11
	movss	(%r9,%r11,4), %xmm2
	movslq	%eax, %r11
	movss	(%rdx,%r11,4), %xmm3
	incl	%eax
	movslq	%eax, %rax
	movss	(%rdx,%rax,4), %xmm4
	cmpl	$131, 20(%rsp)
	jne	.LBB29_51	# bb166
.LBB29_50:	# bb164
	movaps	%xmm1, %xmm5
	mulss	%xmm3, %xmm5
	movaps	%xmm2, %xmm6
	mulss	%xmm4, %xmm6
	addss	%xmm5, %xmm6
	mulss	%xmm4, %xmm1
	mulss	%xmm3, %xmm2
	subss	%xmm1, %xmm2
	movaps	%xmm2, %xmm3
	movaps	%xmm6, %xmm4
.LBB29_51:	# bb166
	cmpl	$0, 88(%rsp)
	movl	$0, %ebx
	cmovle	12(%rsp), %ebx
	cmpl	%r8d, %r10d
	jge	.LBB29_54	# bb172
.LBB29_52:	# bb.nph
	leal	1(%rdi), %r10d
	leal	2(%rdi), %r14d
	xorl	%r15d, %r15d
	movl	%esi, %r12d
	.align	16
.LBB29_53:	# bb170
	leal	(%r10,%r15), %r13d
	leal	(%r14,%r15), %ebp
	imull	%r13d, %ebp
	movl	%ebp, %r13d
	shrl	$31, %r13d
	addl	%ebp, %r13d
	sarl	%r13d
	addl	%edi, %r13d
	leal	1(,%r13,2), %ebp
	movslq	%ebp, %rbp
	movaps	%xmm0, %xmm1
	mulss	(%r9,%rbp,4), %xmm1
	addl	%ebx, %r12d
	leal	1(,%r12,2), %ebx
	leal	(%r12,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%rdx,%rbp,4), %xmm2
	movaps	%xmm1, %xmm5
	mulss	%xmm2, %xmm5
	addl	%r13d, %r13d
	movslq	%r13d, %r13
	movss	(%r9,%r13,4), %xmm6
	movslq	%ebx, %rbx
	movss	(%rdx,%rbx,4), %xmm7
	movaps	%xmm6, %xmm8
	mulss	%xmm7, %xmm8
	addss	%xmm5, %xmm8
	addss	%xmm8, %xmm4
	mulss	%xmm7, %xmm1
	mulss	%xmm2, %xmm6
	subss	%xmm1, %xmm6
	addss	%xmm6, %xmm3
	incl	%r15d
	cmpl	%ecx, %r15d
	movl	88(%rsp), %ebx
	jne	.LBB29_53	# bb170
.LBB29_54:	# bb172
	movss	%xmm3, (%rdx,%r11,4)
	movss	%xmm4, (%rdx,%rax,4)
	movl	%r11d, %eax
	addl	16(%rsp), %eax
	addl	88(%rsp), %esi
	decl	%ecx
	incl	%edi
	cmpl	%r8d, %edi
	je	.LBB29_29	# bb105.thread
	jmp	.LBB29_49	# bb163
.LBB29_55:	# bb56.bb70.preheader_crit_edge
	xorl	%eax, %eax
	jmp	.LBB29_6	# bb70.preheader
.LBB29_56:	# bb87.bb90_crit_edge
	xorl	%eax, %eax
	jmp	.LBB29_20	# bb90
.LBB29_57:	# bb123.bb126_crit_edge
	xorl	%eax, %eax
	jmp	.LBB29_34	# bb126
.LBB29_58:	# bb159.bb173.preheader_crit_edge
	xorl	%eax, %eax
	jmp	.LBB29_47	# bb173.preheader
.LBB29_59:	# bb174
	xorl	%edi, %edi
	leaq	.str40, %rsi
	leaq	.str141, %rdx
	xorb	%al, %al
	call	cblas_xerbla
	jmp	.LBB29_29	# bb105.thread
	.size	cblas_ctpmv, .-cblas_ctpmv
.Leh_func_end21:


	.align	16
	.globl	cblas_ctpsv
	.type	cblas_ctpsv,@function
cblas_ctpsv:
.Leh_func_begin22:
.Llabel22:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$72, %rsp
	cmpl	$113, %edx
	movl	$4294967295, %eax
	movl	$1, %r10d
	cmove	%eax, %r10d
	movl	%r10d, 56(%rsp)
	movl	$112, %eax
	cmovne	%edx, %eax
	testl	%r8d, %r8d
	movq	128(%rsp), %rbx
	movq	%r9, %r14
	movl	%r8d, 52(%rsp)
	movl	%ecx, 48(%rsp)
	je	.LBB30_18	# bb105.thread
.LBB30_1:	# bb73
	cmpl	$121, %esi
	sete	%cl
	setne	%dl
	cmpl	$101, %edi
	sete	%r8b
	setne	%r9b
	andb	%cl, %r8b
	orb	%dl, %r9b
	testb	%r9b, %r9b
	jne	.LBB30_3	# bb80
.LBB30_2:	# bb73
	cmpl	$111, %eax
	je	.LBB30_5	# bb88
.LBB30_3:	# bb80
	cmpl	$122, %esi
	sete	%cl
	setne	%dl
	cmpl	$102, %edi
	sete	%r9b
	setne	%r10b
	andb	%cl, %r9b
	orb	%dl, %r10b
	testb	%r10b, %r10b
	jne	.LBB30_19	# bb107
.LBB30_4:	# bb80
	cmpl	$112, %eax
	jne	.LBB30_19	# bb107
.LBB30_5:	# bb88
	cmpl	$0, 136(%rsp)
	jg	.LBB30_65	# bb88.bb91_crit_edge
.LBB30_6:	# bb89
	movl	$1, %r15d
	subl	52(%rsp), %r15d
	imull	136(%rsp), %r15d
.LBB30_7:	# bb91
	movl	52(%rsp), %eax
	leal	-1(%rax), %eax
	movl	%eax, %ecx
	imull	136(%rsp), %ecx
	addl	%r15d, %ecx
	cmpl	$131, 48(%rsp)
	jne	.LBB30_9	# bb93
.LBB30_8:	# bb92
	movl	52(%rsp), %edx
	leal	(%rdx,%rdx), %esi
	leal	-2(%rdx), %edx
	subl	%edx, %esi
	imull	%eax, %esi
	movl	%esi, %eax
	shrl	$31, %eax
	addl	%esi, %eax
	andl	$4294967294, %eax
	movslq	%eax, %rdx
	orl	$1, %eax
	movslq	%eax, %rax
	cvtsi2ss	56(%rsp), %xmm0
	mulss	(%r14,%rax,4), %xmm0
	movss	%xmm0, 64(%rsp)
	movss	(%r14,%rdx,4), %xmm0
	movss	%xmm0, 68(%rsp)
	leal	1(,%rcx,2), %eax
	movslq	%eax, %r12
	movss	(%rbx,%r12,4), %xmm0
	movss	%xmm0, 44(%rsp)
	addl	%ecx, %ecx
	movslq	%ecx, %r13
	movss	(%rbx,%r13,4), %xmm0
	movss	%xmm0, 60(%rsp)
	cvtss2sd	68(%rsp), %xmm0
	cvtss2sd	64(%rsp), %xmm1
	call	_ZL6xhypotdd44
	cvtsd2ss	%xmm0, %xmm0
	movss	68(%rsp), %xmm1
	divss	%xmm0, %xmm1
	movss	%xmm1, 68(%rsp)
	movss	64(%rsp), %xmm1
	divss	%xmm0, %xmm1
	movss	%xmm1, 64(%rsp)
	movss	44(%rsp), %xmm1
	mulss	64(%rsp), %xmm1
	movss	64(%rsp), %xmm2
	mulss	60(%rsp), %xmm2
	movss	%xmm2, 64(%rsp)
	movss	60(%rsp), %xmm2
	mulss	68(%rsp), %xmm2
	addss	%xmm1, %xmm2
	divss	%xmm0, %xmm2
	movss	%xmm2, (%rbx,%r13,4)
	movss	68(%rsp), %xmm1
	mulss	44(%rsp), %xmm1
	subss	64(%rsp), %xmm1
	divss	%xmm0, %xmm1
	movss	%xmm1, (%rbx,%r12,4)
.LBB30_9:	# bb93
	movl	52(%rsp), %eax
	leal	-1(%rax), %ecx
	movl	136(%rsp), %edx
	imull	%edx, %ecx
	addl	%r15d, %ecx
	addl	%ecx, %ecx
	movl	%ecx, 16(%rsp)
	leal	-2(%rax), %ecx
	imull	%edx, %ecx
	addl	%r15d, %ecx
	leal	1(,%rcx,2), %esi
	movl	%esi, 24(%rsp)
	addl	%ecx, %ecx
	movl	%ecx, 12(%rsp)
	leal	3(%rax), %ecx
	movl	%ecx, 28(%rsp)
	cvtsi2ss	56(%rsp), %xmm0
	movss	%xmm0, 60(%rsp)
	leal	(%rdx,%rdx), %ecx
	movl	%ecx, 20(%rsp)
	xorl	%r15d, %r15d
	movl	%eax, 44(%rsp)
	movl	%r15d, %r12d
	jmp	.LBB30_16	# bb101
.LBB30_10:	# bb94
	movl	12(%rsp), %edx
	leal	(%rdx,%r15), %edx
	movl	24(%rsp), %esi
	leal	(%rsi,%r15), %esi
	cmpl	52(%rsp), %ecx
	movslq	%esi, %r13
	movss	(%rbx,%r13,4), %xmm0
	movss	%xmm0, 68(%rsp)
	movslq	%edx, %rbp
	movss	(%rbx,%rbp,4), %xmm0
	movss	%xmm0, 64(%rsp)
	jge	.LBB30_13	# bb97
.LBB30_11:	# bb.nph294
	movl	28(%rsp), %ecx
	leal	(%rcx,%r12), %ecx
	imull	%eax, %ecx
	movl	%ecx, %edx
	shrl	$31, %edx
	addl	%ecx, %edx
	andl	$4294967294, %edx
	movl	16(%rsp), %ecx
	leal	(%rcx,%r15), %ecx
	movl	136(%rsp), %esi
	leal	(%rsi,%rsi), %esi
	leal	1(%r12), %edi
	xorl	%r8d, %r8d
	.align	16
.LBB30_12:	# bb95
	leal	3(%rdx), %r9d
	movslq	%r9d, %r9
	movss	60(%rsp), %xmm0
	mulss	(%r14,%r9,4), %xmm0
	movslq	%ecx, %r9
	movss	(%rbx,%r9,4), %xmm1
	movaps	%xmm0, %xmm2
	mulss	%xmm1, %xmm2
	leal	1(%rcx), %r9d
	movslq	%r9d, %r9
	movss	(%rbx,%r9,4), %xmm3
	addl	$2, %edx
	movslq	%edx, %r9
	movss	(%r14,%r9,4), %xmm4
	movaps	%xmm4, %xmm5
	mulss	%xmm3, %xmm5
	addss	%xmm2, %xmm5
	movss	68(%rsp), %xmm2
	subss	%xmm5, %xmm2
	movss	%xmm2, 68(%rsp)
	mulss	%xmm3, %xmm0
	mulss	%xmm1, %xmm4
	subss	%xmm0, %xmm4
	movss	64(%rsp), %xmm0
	subss	%xmm4, %xmm0
	movss	%xmm0, 64(%rsp)
	addl	%esi, %ecx
	incl	%r8d
	cmpl	%edi, %r8d
	jne	.LBB30_12	# bb95
.LBB30_13:	# bb97
	cmpl	$131, 48(%rsp)
	je	.LBB30_66	# bb98
.LBB30_14:	# bb99
	movss	64(%rsp), %xmm0
	movss	%xmm0, (%rbx,%rbp,4)
	movss	68(%rsp), %xmm0
	movss	%xmm0, 32(%rsp)
.LBB30_15:	# bb101.backedge
	movss	32(%rsp), %xmm0
	movss	%xmm0, (%rbx,%r13,4)
	subl	20(%rsp), %r15d
	decl	44(%rsp)
	incl	%r12d
.LBB30_16:	# bb101
	movl	44(%rsp), %eax
	leal	-1(%rax), %ecx
	testl	%ecx, %ecx
	jle	.LBB30_18	# bb105.thread
.LBB30_17:	# bb102
	movl	44(%rsp), %eax
	leal	-2(%rax), %eax
	cmpl	$4294967295, %eax
	jne	.LBB30_10	# bb94
.LBB30_18:	# bb105.thread
	addq	$72, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB30_19:	# bb107
	cmpl	$122, %esi
	sete	%cl
	setne	%dl
	cmpl	$101, %edi
	sete	%r10b
	setne	%r11b
	andb	%cl, %r10b
	orb	%dl, %r11b
	testb	%r11b, %r11b
	jne	.LBB30_21	# bb115
.LBB30_20:	# bb107
	cmpl	$111, %eax
	je	.LBB30_23	# bb123
.LBB30_21:	# bb115
	cmpl	$121, %esi
	sete	%cl
	setne	%dl
	cmpl	$102, %edi
	sete	%sil
	setne	%dil
	andb	%cl, %sil
	orb	%dl, %dil
	testb	%dil, %dil
	jne	.LBB30_35	# bb140
.LBB30_22:	# bb115
	cmpl	$112, %eax
	jne	.LBB30_35	# bb140
.LBB30_23:	# bb123
	cmpl	$0, 136(%rsp)
	jg	.LBB30_67	# bb123.bb126_crit_edge
.LBB30_24:	# bb124
	movl	$1, %r15d
	subl	52(%rsp), %r15d
	imull	136(%rsp), %r15d
.LBB30_25:	# bb126
	cmpl	$131, 48(%rsp)
	jne	.LBB30_27	# bb139.preheader
.LBB30_26:	# bb127
	cvtsi2ss	56(%rsp), %xmm0
	mulss	4(%r14), %xmm0
	movss	%xmm0, 64(%rsp)
	leal	1(,%r15,2), %eax
	movslq	%eax, %r12
	movss	(%rbx,%r12,4), %xmm0
	movss	%xmm0, 44(%rsp)
	leal	(%r15,%r15), %eax
	movslq	%eax, %r13
	movss	(%rbx,%r13,4), %xmm0
	movss	%xmm0, 60(%rsp)
	movss	(%r14), %xmm0
	movss	%xmm0, 68(%rsp)
	cvtss2sd	64(%rsp), %xmm1
	cvtss2sd	%xmm0, %xmm0
	call	_ZL6xhypotdd44
	cvtsd2ss	%xmm0, %xmm0
	movss	64(%rsp), %xmm1
	divss	%xmm0, %xmm1
	movss	%xmm1, 64(%rsp)
	movss	44(%rsp), %xmm1
	mulss	64(%rsp), %xmm1
	movss	64(%rsp), %xmm2
	mulss	60(%rsp), %xmm2
	movss	%xmm2, 64(%rsp)
	movss	68(%rsp), %xmm2
	divss	%xmm0, %xmm2
	movss	%xmm2, 68(%rsp)
	movss	60(%rsp), %xmm2
	mulss	68(%rsp), %xmm2
	addss	%xmm1, %xmm2
	divss	%xmm0, %xmm2
	movss	%xmm2, (%rbx,%r13,4)
	movss	68(%rsp), %xmm1
	mulss	44(%rsp), %xmm1
	subss	64(%rsp), %xmm1
	divss	%xmm0, %xmm1
	movss	%xmm1, (%rbx,%r12,4)
.LBB30_27:	# bb139.preheader
	cmpl	$2, 52(%rsp)
	jl	.LBB30_18	# bb105.thread
.LBB30_28:	# bb.nph285
	movl	136(%rsp), %eax
	addl	%eax, %r15d
	addl	%r15d, %r15d
	movl	$1, %r12d
	movl	52(%rsp), %ecx
	subl	%ecx, %r12d
	imull	%eax, %r12d
	decl	%ecx
	movl	%ecx, 52(%rsp)
	cvtsi2ss	56(%rsp), %xmm0
	movss	%xmm0, 60(%rsp)
	leal	(%rax,%rax), %eax
	movl	%eax, 32(%rsp)
	xorl	%r13d, %r13d
	.align	16
.LBB30_29:	# bb129
	cmpl	$0, 136(%rsp)
	movl	$0, %eax
	cmovle	%r12d, %eax
	movslq	%r15d, %rbp
	movss	(%rbx,%rbp,4), %xmm0
	movss	%xmm0, 68(%rsp)
	incl	%r15d
	movslq	%r15d, %r15
	movss	(%rbx,%r15,4), %xmm0
	movss	%xmm0, 64(%rsp)
	leal	1(%r13), %ecx
	testl	%ecx, %ecx
	jle	.LBB30_32	# bb135
.LBB30_30:	# bb.nph278
	leal	2(%r13), %edx
	imull	%ecx, %edx
	movl	%edx, %esi
	shrl	$31, %esi
	addl	%edx, %esi
	andl	$4294967294, %esi
	movl	136(%rsp), %edx
	leal	(%rdx,%rdx), %edx
	addl	%eax, %eax
	xorl	%edi, %edi
	.align	16
.LBB30_31:	# bb133
	movslq	%esi, %r8
	leal	1(%rsi), %r9d
	movslq	%r9d, %r9
	movss	60(%rsp), %xmm0
	mulss	(%r14,%r9,4), %xmm0
	movslq	%eax, %r9
	movss	(%rbx,%r9,4), %xmm1
	movaps	%xmm0, %xmm2
	mulss	%xmm1, %xmm2
	movss	(%r14,%r8,4), %xmm3
	leal	1(%rax), %r8d
	movslq	%r8d, %r8
	movss	(%rbx,%r8,4), %xmm4
	movaps	%xmm3, %xmm5
	mulss	%xmm4, %xmm5
	addss	%xmm2, %xmm5
	movss	64(%rsp), %xmm2
	subss	%xmm5, %xmm2
	movss	%xmm2, 64(%rsp)
	mulss	%xmm1, %xmm3
	mulss	%xmm4, %xmm0
	subss	%xmm0, %xmm3
	movss	68(%rsp), %xmm0
	subss	%xmm3, %xmm0
	movss	%xmm0, 68(%rsp)
	addl	%edx, %eax
	addl	$2, %esi
	incl	%edi
	cmpl	%ecx, %edi
	jne	.LBB30_31	# bb133
.LBB30_32:	# bb135
	cmpl	$131, 48(%rsp)
	je	.LBB30_68	# bb136
.LBB30_33:	# bb137
	movss	68(%rsp), %xmm0
	movss	%xmm0, (%rbx,%rbp,4)
	movss	64(%rsp), %xmm0
	movss	%xmm0, 44(%rsp)
.LBB30_34:	# bb138
	movss	44(%rsp), %xmm0
	movss	%xmm0, (%rbx,%r15,4)
	movl	%ebp, %r15d
	addl	32(%rsp), %r15d
	incl	%r13d
	cmpl	52(%rsp), %r13d
	je	.LBB30_18	# bb105.thread
	jmp	.LBB30_29	# bb129
.LBB30_35:	# bb140
	cmpl	$111, %eax
	sete	%cl
	andb	%cl, %r9b
	cmpl	$112, %eax
	sete	%cl
	testb	%cl, %r8b
	jne	.LBB30_37	# bb156
.LBB30_36:	# bb140
	notb	%r9b
	testb	$1, %r9b
	jne	.LBB30_49	# bb173
.LBB30_37:	# bb156
	cmpl	$0, 136(%rsp)
	jg	.LBB30_69	# bb156.bb159_crit_edge
.LBB30_38:	# bb157
	movl	$1, %r15d
	subl	52(%rsp), %r15d
	imull	136(%rsp), %r15d
.LBB30_39:	# bb159
	cmpl	$131, 48(%rsp)
	jne	.LBB30_41	# bb172.preheader
.LBB30_40:	# bb160
	cvtsi2ss	56(%rsp), %xmm0
	mulss	4(%r14), %xmm0
	movss	%xmm0, 64(%rsp)
	leal	1(,%r15,2), %eax
	movslq	%eax, %r12
	movss	(%rbx,%r12,4), %xmm0
	movss	%xmm0, 44(%rsp)
	leal	(%r15,%r15), %eax
	movslq	%eax, %r13
	movss	(%rbx,%r13,4), %xmm0
	movss	%xmm0, 60(%rsp)
	movss	(%r14), %xmm0
	movss	%xmm0, 68(%rsp)
	cvtss2sd	64(%rsp), %xmm1
	cvtss2sd	%xmm0, %xmm0
	call	_ZL6xhypotdd44
	cvtsd2ss	%xmm0, %xmm0
	movss	64(%rsp), %xmm1
	divss	%xmm0, %xmm1
	movss	%xmm1, 64(%rsp)
	movss	44(%rsp), %xmm1
	mulss	64(%rsp), %xmm1
	movss	64(%rsp), %xmm2
	mulss	60(%rsp), %xmm2
	movss	%xmm2, 64(%rsp)
	movss	68(%rsp), %xmm2
	divss	%xmm0, %xmm2
	movss	%xmm2, 68(%rsp)
	movss	60(%rsp), %xmm2
	mulss	68(%rsp), %xmm2
	addss	%xmm1, %xmm2
	divss	%xmm0, %xmm2
	movss	%xmm2, (%rbx,%r13,4)
	movss	68(%rsp), %xmm1
	mulss	44(%rsp), %xmm1
	subss	64(%rsp), %xmm1
	divss	%xmm0, %xmm1
	movss	%xmm1, (%rbx,%r12,4)
.LBB30_41:	# bb172.preheader
	cmpl	$2, 52(%rsp)
	jl	.LBB30_18	# bb105.thread
.LBB30_42:	# bb.nph273
	movl	136(%rsp), %eax
	addl	%eax, %r15d
	addl	%r15d, %r15d
	movl	$1, %edx
	movl	52(%rsp), %ecx
	subl	%ecx, %edx
	imull	%eax, %edx
	movl	%edx, 24(%rsp)
	leal	(%rcx,%rcx), %r12d
	leal	-1(%rcx), %ecx
	movl	%ecx, 32(%rsp)
	cvtsi2ss	56(%rsp), %xmm0
	movss	%xmm0, 60(%rsp)
	leal	(%rax,%rax), %eax
	movl	%eax, 28(%rsp)
	xorl	%r13d, %r13d
	.align	16
.LBB30_43:	# bb162
	cmpl	$0, 136(%rsp)
	movl	$0, %eax
	cmovle	24(%rsp), %eax
	movslq	%r15d, %rbp
	movss	(%rbx,%rbp,4), %xmm0
	movss	%xmm0, 68(%rsp)
	incl	%r15d
	movslq	%r15d, %r15
	movss	(%rbx,%r15,4), %xmm0
	movss	%xmm0, 64(%rsp)
	leal	1(%r13), %ecx
	testl	%ecx, %ecx
	jle	.LBB30_46	# bb168
.LBB30_44:	# bb166.preheader
	movl	136(%rsp), %edx
	leal	(%rdx,%rdx), %edx
	addl	%eax, %eax
	movl	52(%rsp), %esi
	leal	(%rsi,%rsi), %esi
	xorl	%edi, %edi
	movl	$1, %r8d
	.align	16
.LBB30_45:	# bb166
	leal	(%rsi,%r8), %r9d
	imull	%edi, %r9d
	movl	%r9d, %r10d
	shrl	$31, %r10d
	addl	%r9d, %r10d
	sarl	%r10d
	leal	(%r13,%r8), %r9d
	addl	%r10d, %r9d
	leal	1(,%r9,2), %r10d
	movslq	%r10d, %r10
	movss	60(%rsp), %xmm0
	mulss	(%r14,%r10,4), %xmm0
	movslq	%eax, %r10
	movss	(%rbx,%r10,4), %xmm1
	movaps	%xmm0, %xmm2
	mulss	%xmm1, %xmm2
	addl	%r9d, %r9d
	movslq	%r9d, %r9
	movss	(%r14,%r9,4), %xmm3
	leal	1(%rax), %r9d
	movslq	%r9d, %r9
	movss	(%rbx,%r9,4), %xmm4
	movaps	%xmm3, %xmm5
	mulss	%xmm4, %xmm5
	addss	%xmm2, %xmm5
	movss	64(%rsp), %xmm2
	subss	%xmm5, %xmm2
	movss	%xmm2, 64(%rsp)
	mulss	%xmm4, %xmm0
	mulss	%xmm1, %xmm3
	subss	%xmm0, %xmm3
	movss	68(%rsp), %xmm0
	subss	%xmm3, %xmm0
	movss	%xmm0, 68(%rsp)
	addl	%edx, %eax
	decl	%r8d
	incl	%edi
	cmpl	%ecx, %edi
	jne	.LBB30_45	# bb166
.LBB30_46:	# bb168
	cmpl	$131, 48(%rsp)
	je	.LBB30_70	# bb169
.LBB30_47:	# bb170
	movss	68(%rsp), %xmm0
	movss	%xmm0, (%rbx,%rbp,4)
	movss	64(%rsp), %xmm0
	movss	%xmm0, 44(%rsp)
.LBB30_48:	# bb171
	movss	44(%rsp), %xmm0
	movss	%xmm0, (%rbx,%r15,4)
	movl	%ebp, %r15d
	addl	28(%rsp), %r15d
	decl	%r12d
	incl	%r13d
	cmpl	32(%rsp), %r13d
	je	.LBB30_18	# bb105.thread
	jmp	.LBB30_43	# bb162
.LBB30_49:	# bb173
	cmpl	$111, %eax
	sete	%cl
	andb	%cl, %sil
	cmpl	$112, %eax
	sete	%al
	testb	%al, %r10b
	jne	.LBB30_51	# bb189
.LBB30_50:	# bb173
	notb	%sil
	testb	$1, %sil
	jne	.LBB30_64	# bb208
.LBB30_51:	# bb189
	cmpl	$0, 136(%rsp)
	jg	.LBB30_71	# bb189.bb192_crit_edge
.LBB30_52:	# bb190
	movl	$1, %r15d
	subl	52(%rsp), %r15d
	imull	136(%rsp), %r15d
.LBB30_53:	# bb192
	movl	52(%rsp), %eax
	leal	-1(%rax), %eax
	movl	%eax, %ecx
	imull	136(%rsp), %ecx
	addl	%r15d, %ecx
	cmpl	$131, 48(%rsp)
	jne	.LBB30_55	# bb194
.LBB30_54:	# bb193
	movl	52(%rsp), %edx
	imull	%edx, %eax
	movl	%eax, %esi
	shrl	$31, %esi
	addl	%eax, %esi
	sarl	%esi
	leal	2147483647(%rdx,%rsi), %eax
	leal	1(,%rax,2), %edx
	movslq	%edx, %rdx
	cvtsi2ss	56(%rsp), %xmm0
	mulss	(%r14,%rdx,4), %xmm0
	movss	%xmm0, 64(%rsp)
	addl	%eax, %eax
	movslq	%eax, %rax
	movss	(%r14,%rax,4), %xmm0
	movss	%xmm0, 68(%rsp)
	leal	1(,%rcx,2), %eax
	movslq	%eax, %r12
	movss	(%rbx,%r12,4), %xmm0
	movss	%xmm0, 44(%rsp)
	addl	%ecx, %ecx
	movslq	%ecx, %r13
	movss	(%rbx,%r13,4), %xmm0
	movss	%xmm0, 60(%rsp)
	cvtss2sd	68(%rsp), %xmm0
	cvtss2sd	64(%rsp), %xmm1
	call	_ZL6xhypotdd44
	cvtsd2ss	%xmm0, %xmm0
	movss	68(%rsp), %xmm1
	divss	%xmm0, %xmm1
	movss	%xmm1, 68(%rsp)
	movss	64(%rsp), %xmm1
	divss	%xmm0, %xmm1
	movss	%xmm1, 64(%rsp)
	movss	44(%rsp), %xmm1
	mulss	64(%rsp), %xmm1
	movss	64(%rsp), %xmm2
	mulss	60(%rsp), %xmm2
	movss	%xmm2, 64(%rsp)
	movss	60(%rsp), %xmm2
	mulss	68(%rsp), %xmm2
	addss	%xmm1, %xmm2
	divss	%xmm0, %xmm2
	movss	%xmm2, (%rbx,%r13,4)
	movss	68(%rsp), %xmm1
	mulss	44(%rsp), %xmm1
	subss	64(%rsp), %xmm1
	divss	%xmm0, %xmm1
	movss	%xmm1, (%rbx,%r12,4)
.LBB30_55:	# bb194
	movl	52(%rsp), %eax
	leal	-1(%rax), %ecx
	movl	136(%rsp), %edx
	imull	%edx, %ecx
	addl	%r15d, %ecx
	addl	%ecx, %ecx
	movl	%ecx, 20(%rsp)
	leal	-2(%rax), %ecx
	imull	%edx, %ecx
	addl	%r15d, %ecx
	leal	1(,%rcx,2), %esi
	movl	%esi, 28(%rsp)
	addl	%ecx, %ecx
	movl	%ecx, 16(%rsp)
	cvtsi2ss	56(%rsp), %xmm0
	movss	%xmm0, 60(%rsp)
	leal	(%rdx,%rdx), %ecx
	movl	%ecx, 24(%rsp)
	movl	$1, %r15d
	xorl	%r12d, %r12d
	movl	%eax, %r13d
	jmp	.LBB30_62	# bb202
.LBB30_56:	# bb195
	movl	16(%rsp), %edx
	leal	(%rdx,%r12), %edx
	movl	28(%rsp), %esi
	leal	(%rsi,%r12), %esi
	cmpl	52(%rsp), %eax
	movslq	%esi, %rsi
	movq	%rsi, 32(%rsp)
	movss	(%rbx,%rsi,4), %xmm0
	movss	%xmm0, 68(%rsp)
	movslq	%edx, %rbp
	movss	(%rbx,%rbp,4), %xmm0
	movss	%xmm0, 64(%rsp)
	jge	.LBB30_59	# bb198
.LBB30_57:	# bb.nph
	movl	20(%rsp), %edx
	leal	(%rdx,%r12), %edx
	leal	-2(%r13), %esi
	movl	136(%rsp), %edi
	leal	(%rdi,%rdi), %edi
	leal	-1(%r13), %r8d
	xorl	%r9d, %r9d
	.align	16
.LBB30_58:	# bb196
	leal	(%r8,%r9), %r10d
	leal	(%r13,%r9), %r11d
	imull	%r10d, %r11d
	movl	%r11d, %r10d
	shrl	$31, %r10d
	addl	%r11d, %r10d
	sarl	%r10d
	addl	%esi, %r10d
	leal	1(,%r10,2), %r11d
	movslq	%r11d, %r11
	movss	60(%rsp), %xmm0
	mulss	(%r14,%r11,4), %xmm0
	movslq	%edx, %r11
	movss	(%rbx,%r11,4), %xmm1
	movaps	%xmm0, %xmm2
	mulss	%xmm1, %xmm2
	addl	%r10d, %r10d
	movslq	%r10d, %r10
	movss	(%r14,%r10,4), %xmm3
	leal	1(%rdx), %r10d
	movslq	%r10d, %r10
	movss	(%rbx,%r10,4), %xmm4
	movaps	%xmm3, %xmm5
	mulss	%xmm4, %xmm5
	addss	%xmm2, %xmm5
	movss	68(%rsp), %xmm2
	subss	%xmm5, %xmm2
	movss	%xmm2, 68(%rsp)
	mulss	%xmm4, %xmm0
	mulss	%xmm1, %xmm3
	subss	%xmm0, %xmm3
	movss	64(%rsp), %xmm0
	subss	%xmm3, %xmm0
	movss	%xmm0, 64(%rsp)
	addl	%edi, %edx
	incl	%r9d
	cmpl	%r15d, %r9d
	jne	.LBB30_58	# bb196
.LBB30_59:	# bb198
	cmpl	$131, 48(%rsp)
	je	.LBB30_72	# bb199
.LBB30_60:	# bb200
	movss	64(%rsp), %xmm0
	movss	%xmm0, (%rbx,%rbp,4)
	movss	68(%rsp), %xmm0
	movss	%xmm0, 44(%rsp)
.LBB30_61:	# bb202.backedge
	movss	44(%rsp), %xmm0
	movq	32(%rsp), %rax
	movss	%xmm0, (%rbx,%rax,4)
	subl	24(%rsp), %r12d
	decl	%r13d
	incl	%r15d
.LBB30_62:	# bb202
	leal	-1(%r13), %eax
	testl	%eax, %eax
	jle	.LBB30_18	# bb105.thread
.LBB30_63:	# bb203
	leal	-2(%r13), %ecx
	cmpl	$4294967295, %ecx
	jne	.LBB30_56	# bb195
	jmp	.LBB30_18	# bb105.thread
.LBB30_64:	# bb208
	xorl	%edi, %edi
	leaq	.str42, %rsi
	leaq	.str143, %rdx
	xorb	%al, %al
	call	cblas_xerbla
	jmp	.LBB30_18	# bb105.thread
.LBB30_65:	# bb88.bb91_crit_edge
	xorl	%r15d, %r15d
	jmp	.LBB30_7	# bb91
.LBB30_66:	# bb98
	movl	28(%rsp), %ecx
	leal	(%rcx,%r12), %ecx
	imull	%eax, %ecx
	movl	%ecx, %eax
	shrl	$31, %eax
	addl	%ecx, %eax
	andl	$4294967294, %eax
	movslq	%eax, %rcx
	movss	(%r14,%rcx,4), %xmm0
	movss	%xmm0, 32(%rsp)
	orl	$1, %eax
	movslq	%eax, %rax
	movss	60(%rsp), %xmm0
	mulss	(%r14,%rax,4), %xmm0
	movss	%xmm0, 56(%rsp)
	cvtss2sd	32(%rsp), %xmm0
	cvtss2sd	56(%rsp), %xmm1
	call	_ZL6xhypotdd44
	cvtsd2ss	%xmm0, %xmm0
	movss	56(%rsp), %xmm1
	divss	%xmm0, %xmm1
	movss	%xmm1, 56(%rsp)
	movss	68(%rsp), %xmm1
	movaps	%xmm1, %xmm2
	mulss	56(%rsp), %xmm2
	movss	32(%rsp), %xmm3
	divss	%xmm0, %xmm3
	movss	%xmm3, 32(%rsp)
	movss	64(%rsp), %xmm3
	movaps	%xmm3, %xmm4
	mulss	32(%rsp), %xmm4
	addss	%xmm2, %xmm4
	divss	%xmm0, %xmm4
	movss	%xmm4, (%rbx,%rbp,4)
	movss	56(%rsp), %xmm2
	mulss	%xmm3, %xmm2
	movss	%xmm2, 56(%rsp)
	movss	32(%rsp), %xmm2
	mulss	%xmm1, %xmm2
	subss	56(%rsp), %xmm2
	divss	%xmm0, %xmm2
	movss	%xmm2, 32(%rsp)
	jmp	.LBB30_15	# bb101.backedge
.LBB30_67:	# bb123.bb126_crit_edge
	xorl	%r15d, %r15d
	jmp	.LBB30_25	# bb126
.LBB30_68:	# bb136
	leal	2(%r13), %eax
	imull	%ecx, %eax
	movl	%eax, %ecx
	shrl	$31, %ecx
	addl	%eax, %ecx
	sarl	%ecx
	addl	%r13d, %ecx
	leal	3(,%rcx,2), %eax
	movslq	%eax, %rax
	movss	60(%rsp), %xmm0
	mulss	(%r14,%rax,4), %xmm0
	movss	%xmm0, 56(%rsp)
	leal	2(,%rcx,2), %eax
	movslq	%eax, %rax
	movss	(%r14,%rax,4), %xmm0
	movss	%xmm0, 44(%rsp)
	cvtss2sd	%xmm0, %xmm0
	cvtss2sd	56(%rsp), %xmm1
	call	_ZL6xhypotdd44
	cvtsd2ss	%xmm0, %xmm0
	movss	56(%rsp), %xmm1
	divss	%xmm0, %xmm1
	movss	%xmm1, 56(%rsp)
	movss	64(%rsp), %xmm1
	movaps	%xmm1, %xmm2
	mulss	56(%rsp), %xmm2
	movss	44(%rsp), %xmm3
	divss	%xmm0, %xmm3
	movss	%xmm3, 44(%rsp)
	movss	68(%rsp), %xmm3
	movaps	%xmm3, %xmm4
	mulss	44(%rsp), %xmm4
	addss	%xmm2, %xmm4
	divss	%xmm0, %xmm4
	movss	%xmm4, (%rbx,%rbp,4)
	movss	56(%rsp), %xmm2
	mulss	%xmm3, %xmm2
	movss	%xmm2, 56(%rsp)
	movss	44(%rsp), %xmm2
	mulss	%xmm1, %xmm2
	subss	56(%rsp), %xmm2
	divss	%xmm0, %xmm2
	movss	%xmm2, 44(%rsp)
	jmp	.LBB30_34	# bb138
.LBB30_69:	# bb156.bb159_crit_edge
	xorl	%r15d, %r15d
	jmp	.LBB30_39	# bb159
.LBB30_70:	# bb169
	imull	%r12d, %ecx
	movl	%ecx, %eax
	shrl	$31, %eax
	addl	%ecx, %eax
	andl	$4294967294, %eax
	movslq	%eax, %rcx
	movss	(%r14,%rcx,4), %xmm0
	movss	%xmm0, 44(%rsp)
	orl	$1, %eax
	movslq	%eax, %rax
	movss	60(%rsp), %xmm0
	mulss	(%r14,%rax,4), %xmm0
	movss	%xmm0, 56(%rsp)
	cvtss2sd	44(%rsp), %xmm0
	cvtss2sd	56(%rsp), %xmm1
	call	_ZL6xhypotdd44
	cvtsd2ss	%xmm0, %xmm0
	movss	56(%rsp), %xmm1
	divss	%xmm0, %xmm1
	movss	%xmm1, 56(%rsp)
	movss	64(%rsp), %xmm1
	movaps	%xmm1, %xmm2
	mulss	56(%rsp), %xmm2
	movss	44(%rsp), %xmm3
	divss	%xmm0, %xmm3
	movss	%xmm3, 44(%rsp)
	movss	68(%rsp), %xmm3
	movaps	%xmm3, %xmm4
	mulss	44(%rsp), %xmm4
	addss	%xmm2, %xmm4
	divss	%xmm0, %xmm4
	movss	%xmm4, (%rbx,%rbp,4)
	movss	56(%rsp), %xmm2
	mulss	%xmm3, %xmm2
	movss	%xmm2, 56(%rsp)
	movss	44(%rsp), %xmm2
	mulss	%xmm1, %xmm2
	subss	56(%rsp), %xmm2
	divss	%xmm0, %xmm2
	movss	%xmm2, 44(%rsp)
	jmp	.LBB30_48	# bb171
.LBB30_71:	# bb189.bb192_crit_edge
	xorl	%r15d, %r15d
	jmp	.LBB30_53	# bb192
.LBB30_72:	# bb199
	imull	%eax, %ecx
	movl	%ecx, %eax
	shrl	$31, %eax
	addl	%ecx, %eax
	sarl	%eax
	addl	%r13d, %eax
	leal	-3(,%rax,2), %ecx
	movslq	%ecx, %rcx
	movss	60(%rsp), %xmm0
	mulss	(%r14,%rcx,4), %xmm0
	movss	%xmm0, 56(%rsp)
	leal	-4(,%rax,2), %eax
	movslq	%eax, %rax
	movss	(%r14,%rax,4), %xmm0
	movss	%xmm0, 44(%rsp)
	cvtss2sd	%xmm0, %xmm0
	cvtss2sd	56(%rsp), %xmm1
	call	_ZL6xhypotdd44
	cvtsd2ss	%xmm0, %xmm0
	movss	56(%rsp), %xmm1
	divss	%xmm0, %xmm1
	movss	%xmm1, 56(%rsp)
	movss	68(%rsp), %xmm1
	movaps	%xmm1, %xmm2
	mulss	56(%rsp), %xmm2
	movss	44(%rsp), %xmm3
	divss	%xmm0, %xmm3
	movss	%xmm3, 44(%rsp)
	movss	64(%rsp), %xmm3
	movaps	%xmm3, %xmm4
	mulss	44(%rsp), %xmm4
	addss	%xmm2, %xmm4
	divss	%xmm0, %xmm4
	movss	%xmm4, (%rbx,%rbp,4)
	movss	56(%rsp), %xmm2
	mulss	%xmm3, %xmm2
	movss	%xmm2, 56(%rsp)
	movss	44(%rsp), %xmm2
	mulss	%xmm1, %xmm2
	subss	56(%rsp), %xmm2
	divss	%xmm0, %xmm2
	movss	%xmm2, 44(%rsp)
	jmp	.LBB30_61	# bb202.backedge
	.size	cblas_ctpsv, .-cblas_ctpsv
.Leh_func_end22:


	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI31_0:					
	.quad	9223372036854775807	# double value: nan
	.quad	9223372036854775807	# double value: nan
	.section	.rodata.cst8,"aM",@progbits,8
	.align	16
.LCPI31_1:					
	.quad	4607182418800017408	# double value: 1.000000e+00
	.text
	.align	16
	.type	_ZL6xhypotdd44,@function
_ZL6xhypotdd44:
	movsd	.LCPI31_0(%rip), %xmm2
	andpd	%xmm2, %xmm0
	andpd	%xmm2, %xmm1
	movapd	%xmm1, %xmm2
	maxsd	%xmm0, %xmm2
	minsd	%xmm1, %xmm0
	pxor	%xmm1, %xmm1
	ucomisd	%xmm1, %xmm0
	setnp	%al
	sete	%cl
	testb	%al, %cl
	jne	.LBB31_2	# bb5
.LBB31_1:	# bb4
	divsd	%xmm2, %xmm0
	mulsd	%xmm0, %xmm0
	addsd	.LCPI31_1(%rip), %xmm0
	sqrtsd	%xmm0, %xmm0
	mulsd	%xmm2, %xmm0
	ret
.LBB31_2:	# bb5
	movapd	%xmm2, %xmm0
	ret
	.size	_ZL6xhypotdd44, .-_ZL6xhypotdd44


	.align	16
	.globl	cblas_ctrmm
	.type	cblas_ctrmm,@function
cblas_ctrmm:
.Leh_func_begin23:
.Llabel23:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$56, %rsp
	cmpl	$113, %ecx
	movl	$4294967295, %eax
	movl	$1, %r10d
	cmove	%eax, %r10d
	cmpl	$101, %edi
	movq	120(%rsp), %rax
	movss	4(%rax), %xmm0
	movss	(%rax), %xmm1
	movq	144(%rsp), %rax
	movq	128(%rsp), %rdi
	movl	112(%rsp), %r11d
	movl	%r8d, 48(%rsp)
	je	.LBB32_112	# bb63
.LBB32_1:	# bb67
	cmpl	$111, %ecx
	movl	$111, %ecx
	movl	$112, %r8d
	cmove	%ecx, %r8d
	cmpl	$121, %edx
	movl	$122, %ecx
	movl	$121, %edx
	cmove	%ecx, %edx
	cmpl	$141, %esi
	movl	$142, %ecx
	movl	$141, %esi
	cmove	%ecx, %esi
	movl	%r9d, 52(%rsp)
	movl	%r11d, %r9d
.LBB32_2:	# bb77
	movl	%r9d, 44(%rsp)
	cmpl	$121, %edx
	sete	%cl
	setne	%r9b
	cmpl	$111, %r8d
	sete	%r11b
	setne	%bl
	andb	%cl, %r11b
	orb	%r9b, %bl
	testb	%bl, %bl
	jne	.LBB32_15	# bb96
.LBB32_3:	# bb77
	cmpl	$141, %esi
	jne	.LBB32_15	# bb96
.LBB32_4:	# bb95.preheader
	cmpl	$0, 44(%rsp)
	jle	.LBB32_111	# bb118.thread
.LBB32_5:	# bb.nph397
	cmpl	$0, 52(%rsp)
	cvtsi2ss	%r10d, %xmm2
	jle	.LBB32_111	# bb118.thread
.LBB32_6:	# bb93.preheader.preheader
	movl	136(%rsp), %r8d
	leal	2(,%r8,2), %r8d
	movl	%r8d, 24(%rsp)
	movl	152(%rsp), %r8d
	leal	(%r8,%r8), %r8d
	movl	%r8d, 20(%rsp)
	movl	44(%rsp), %r8d
	leal	-1(%r8), %edx
	xorl	%r9d, %r9d
	movl	%r9d, 40(%rsp)
	movl	%r9d, 32(%rsp)
	jmp	.LBB32_14	# bb93.preheader
	.align	16
.LBB32_7:	# bb86
	cmpl	$131, 48(%rsp)
	je	.LBB32_113	# bb87
.LBB32_8:	# bb88
	movslq	%r11d, %r14
	movss	(%rax,%r14,4), %xmm7
	leal	1(%r11), %r14d
	movslq	%r14d, %r14
	movss	(%rax,%r14,4), %xmm8
.LBB32_9:	# bb91.preheader
	cmpl	44(%rsp), %r8d
	jge	.LBB32_12	# bb92
.LBB32_10:	# bb.nph391
	leal	(%r10,%r11), %r14d
	movl	152(%rsp), %r15d
	leal	(%r15,%r15), %r15d
	xorl	%r12d, %r12d
	movl	%r9d, %r13d
	.align	16
.LBB32_11:	# bb90
	leal	3(%r13), %ebp
	movslq	%ebp, %rbp
	movaps	%xmm2, %xmm3
	mulss	(%rdi,%rbp,4), %xmm3
	movslq	%r14d, %rbp
	movss	(%rax,%rbp,4), %xmm4
	movaps	%xmm3, %xmm5
	mulss	%xmm4, %xmm5
	leal	1(%r14), %ebp
	movslq	%ebp, %rbp
	movss	(%rax,%rbp,4), %xmm6
	addl	$2, %r13d
	movslq	%r13d, %rbp
	movss	(%rdi,%rbp,4), %xmm9
	movaps	%xmm9, %xmm10
	mulss	%xmm6, %xmm10
	addss	%xmm5, %xmm10
	addss	%xmm10, %xmm8
	mulss	%xmm6, %xmm3
	mulss	%xmm4, %xmm9
	subss	%xmm3, %xmm9
	addss	%xmm9, %xmm7
	addl	%r15d, %r14d
	incl	%r12d
	cmpl	%edx, %r12d
	jne	.LBB32_11	# bb90
.LBB32_12:	# bb92
	movaps	%xmm0, %xmm3
	mulss	%xmm8, %xmm3
	movaps	%xmm1, %xmm4
	mulss	%xmm7, %xmm4
	subss	%xmm3, %xmm4
	movslq	%r11d, %r14
	movss	%xmm4, (%rax,%r14,4)
	mulss	%xmm0, %xmm7
	mulss	%xmm1, %xmm8
	addss	%xmm7, %xmm8
	leal	1(%r11), %r14d
	movslq	%r14d, %r14
	movss	%xmm8, (%rax,%r14,4)
	addl	$2, %r11d
	incl	%ecx
	cmpl	52(%rsp), %ecx
	jne	.LBB32_7	# bb86
.LBB32_13:	# bb94
	addl	24(%rsp), %r9d
	movl	40(%rsp), %r8d
	addl	20(%rsp), %r8d
	movl	%r8d, 40(%rsp)
	decl	%edx
	movl	32(%rsp), %r8d
	incl	%r8d
	movl	%r8d, 32(%rsp)
	cmpl	44(%rsp), %r8d
	je	.LBB32_111	# bb118.thread
.LBB32_14:	# bb93.preheader
	movslq	%r9d, %rsi
	leal	1(%r9), %r8d
	movslq	%r8d, %rbx
	movl	152(%rsp), %r8d
	leal	(%r8,%r8), %r10d
	movl	32(%rsp), %r8d
	leal	1(%r8), %r8d
	xorl	%ecx, %ecx
	movl	40(%rsp), %r11d
	jmp	.LBB32_7	# bb86
.LBB32_15:	# bb96
	cmpl	$121, %edx
	sete	%cl
	setne	%r9b
	cmpl	$112, %r8d
	sete	%bl
	setne	%r14b
	andb	%cl, %bl
	orb	%r9b, %r14b
	testb	%r14b, %r14b
	jne	.LBB32_30	# bb120
.LBB32_16:	# bb96
	cmpl	$141, %esi
	jne	.LBB32_30	# bb120
.LBB32_17:	# bb114.preheader
	movl	136(%rsp), %r8d
	leal	2(,%r8,2), %ecx
	leal	(%r8,%r8), %r8d
	movl	$4294967294, %edx
	subl	%r8d, %edx
	movl	%edx, 16(%rsp)
	movl	44(%rsp), %r8d
	leal	-1(%r8), %edx
	movl	152(%rsp), %esi
	movl	%esi, %r9d
	imull	%edx, %r9d
	imull	%edx, %ecx
	movl	%ecx, 24(%rsp)
	addl	%r9d, %r9d
	leal	(%r8,%r8), %r8d
	movl	%r8d, 40(%rsp)
	cvtsi2ss	%r10d, %xmm2
	leal	(%rsi,%rsi), %r8d
	movl	%r8d, 20(%rsp)
	.align	16
.LBB32_18:	# bb114
	cmpl	$0, 44(%rsp)
	jle	.LBB32_111	# bb118.thread
.LBB32_19:	# bb115
	movl	44(%rsp), %r8d
	leal	-1(%r8), %r10d
	movl	%r10d, 32(%rsp)
	testl	%r8d, %r8d
	je	.LBB32_111	# bb118.thread
.LBB32_20:	# bb113.preheader
	cmpl	$0, 52(%rsp)
	jle	.LBB32_28	# bb114.loopexit
.LBB32_21:	# bb.nph385
	movl	24(%rsp), %r8d
	movslq	%r8d, %rcx
	leal	1(%r8), %r8d
	movslq	%r8d, %rdx
	movl	44(%rsp), %r8d
	leal	-1(%r8), %r11d
	leal	1(%r9), %r8d
	xorl	%esi, %esi
	movl	%esi, %r10d
	.align	16
.LBB32_22:	# bb108.preheader
	cmpl	$0, 32(%rsp)
	jle	.LBB32_29	# bb108.preheader.bb109_crit_edge
.LBB32_23:	# bb.nph381
	movl	136(%rsp), %ebx
	leal	(%rbx,%rbx), %ebx
	movl	152(%rsp), %r14d
	leal	(%r14,%r14), %r14d
	pxor	%xmm3, %xmm3
	xorl	%r15d, %r15d
	movl	40(%rsp), %r12d
	movl	%esi, %r13d
	movaps	%xmm3, %xmm4
	.align	16
.LBB32_24:	# bb107
	leal	-1(%r12), %ebp
	movslq	%ebp, %rbp
	movaps	%xmm2, %xmm5
	mulss	(%rdi,%rbp,4), %xmm5
	movslq	%r13d, %rbp
	movss	(%rax,%rbp,4), %xmm6
	movaps	%xmm5, %xmm7
	mulss	%xmm6, %xmm7
	leal	1(%r13), %ebp
	movslq	%ebp, %rbp
	movss	(%rax,%rbp,4), %xmm8
	leal	-2(%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%rdi,%rbp,4), %xmm9
	movaps	%xmm9, %xmm10
	mulss	%xmm8, %xmm10
	addss	%xmm7, %xmm10
	addss	%xmm10, %xmm4
	mulss	%xmm8, %xmm5
	mulss	%xmm6, %xmm9
	subss	%xmm5, %xmm9
	addss	%xmm9, %xmm3
	addl	%ebx, %r12d
	addl	%r14d, %r13d
	incl	%r15d
	cmpl	%r11d, %r15d
	jne	.LBB32_24	# bb107
.LBB32_25:	# bb109
	cmpl	$131, 48(%rsp)
	je	.LBB32_114	# bb110
.LBB32_26:	# bb111
	leal	(%r8,%rsi), %ebx
	movslq	%ebx, %rbx
	movss	(%rax,%rbx,4), %xmm10
	leal	(%r9,%rsi), %ebx
	movslq	%ebx, %rbx
	movss	(%rax,%rbx,4), %xmm9
.LBB32_27:	# bb112
	addss	%xmm4, %xmm10
	movaps	%xmm0, %xmm4
	mulss	%xmm10, %xmm4
	addss	%xmm3, %xmm9
	movaps	%xmm1, %xmm3
	mulss	%xmm9, %xmm3
	subss	%xmm4, %xmm3
	leal	(%r9,%rsi), %ebx
	movslq	%ebx, %rbx
	movss	%xmm3, (%rax,%rbx,4)
	mulss	%xmm0, %xmm9
	mulss	%xmm1, %xmm10
	addss	%xmm9, %xmm10
	leal	(%r8,%rsi), %ebx
	movslq	%ebx, %rbx
	movss	%xmm10, (%rax,%rbx,4)
	addl	$2, %esi
	incl	%r10d
	cmpl	52(%rsp), %r10d
	jne	.LBB32_22	# bb108.preheader
.LBB32_28:	# bb114.loopexit
	movl	24(%rsp), %r8d
	addl	16(%rsp), %r8d
	movl	%r8d, 24(%rsp)
	subl	20(%rsp), %r9d
	addl	$4294967294, 40(%rsp)
	decl	44(%rsp)
	jmp	.LBB32_18	# bb114
.LBB32_29:	# bb108.preheader.bb109_crit_edge
	pxor	%xmm3, %xmm3
	movaps	%xmm3, %xmm4
	jmp	.LBB32_25	# bb109
.LBB32_30:	# bb120
	cmpl	$122, %edx
	sete	%cl
	setne	%r9b
	cmpl	$111, %r8d
	sete	%r14b
	setne	%r15b
	andb	%cl, %r14b
	orb	%r9b, %r15b
	testb	%r15b, %r15b
	jne	.LBB32_45	# bb144
.LBB32_31:	# bb120
	cmpl	$141, %esi
	jne	.LBB32_45	# bb144
.LBB32_32:	# bb138.preheader
	movl	44(%rsp), %r8d
	leal	-1(%r8), %r8d
	movl	152(%rsp), %ecx
	movl	%ecx, %edx
	imull	%r8d, %edx
	movl	136(%rsp), %esi
	leal	2(,%rsi,2), %r9d
	movl	%esi, %r11d
	imull	%r8d, %r11d
	imull	%r8d, %r9d
	movl	%r9d, 24(%rsp)
	addl	%esi, %esi
	movl	%esi, 4(%rsp)
	movl	$4294967294, %r8d
	subl	%esi, %r8d
	movl	%r8d, 16(%rsp)
	addl	%r11d, %r11d
	movl	%r11d, 40(%rsp)
	addl	%edx, %edx
	cvtsi2ss	%r10d, %xmm2
	leal	(%rcx,%rcx), %r8d
	movl	%r8d, 20(%rsp)
	.align	16
.LBB32_33:	# bb138
	cmpl	$0, 44(%rsp)
	jle	.LBB32_111	# bb118.thread
.LBB32_34:	# bb139
	movl	44(%rsp), %r8d
	leal	-1(%r8), %r10d
	movl	%r10d, 32(%rsp)
	testl	%r8d, %r8d
	je	.LBB32_111	# bb118.thread
.LBB32_35:	# bb137.preheader
	cmpl	$0, 52(%rsp)
	jle	.LBB32_43	# bb138.loopexit
.LBB32_36:	# bb.nph377
	movl	24(%rsp), %r8d
	movslq	%r8d, %rcx
	leal	1(%r8), %r8d
	movslq	%r8d, %rsi
	movl	44(%rsp), %r8d
	leal	-1(%r8), %r11d
	leal	1(%rdx), %r9d
	xorl	%r8d, %r8d
	movl	%r8d, %r10d
	.align	16
.LBB32_37:	# bb132.preheader
	cmpl	$0, 32(%rsp)
	jle	.LBB32_44	# bb132.preheader.bb133_crit_edge
.LBB32_38:	# bb.nph373
	movl	152(%rsp), %ebx
	leal	(%rbx,%rbx), %ebx
	pxor	%xmm3, %xmm3
	xorl	%r14d, %r14d
	movl	%r8d, %r15d
	movl	40(%rsp), %r12d
	movaps	%xmm3, %xmm4
	.align	16
.LBB32_39:	# bb131
	movslq	%r12d, %r13
	leal	1(%r12), %ebp
	movslq	%ebp, %rbp
	movaps	%xmm2, %xmm5
	mulss	(%rdi,%rbp,4), %xmm5
	movslq	%r15d, %rbp
	movss	(%rax,%rbp,4), %xmm6
	movaps	%xmm5, %xmm7
	mulss	%xmm6, %xmm7
	movss	(%rdi,%r13,4), %xmm8
	leal	1(%r15), %r13d
	movslq	%r13d, %r13
	movss	(%rax,%r13,4), %xmm9
	movaps	%xmm8, %xmm10
	mulss	%xmm9, %xmm10
	addss	%xmm7, %xmm10
	addss	%xmm10, %xmm4
	mulss	%xmm6, %xmm8
	mulss	%xmm9, %xmm5
	subss	%xmm5, %xmm8
	addss	%xmm8, %xmm3
	addl	%ebx, %r15d
	addl	$2, %r12d
	incl	%r14d
	cmpl	%r11d, %r14d
	jne	.LBB32_39	# bb131
.LBB32_40:	# bb133
	cmpl	$131, 48(%rsp)
	je	.LBB32_115	# bb134
.LBB32_41:	# bb135
	leal	(%r9,%r8), %ebx
	movslq	%ebx, %rbx
	movss	(%rax,%rbx,4), %xmm10
	leal	(%rdx,%r8), %ebx
	movslq	%ebx, %rbx
	movss	(%rax,%rbx,4), %xmm9
.LBB32_42:	# bb136
	addss	%xmm4, %xmm10
	movaps	%xmm0, %xmm4
	mulss	%xmm10, %xmm4
	addss	%xmm3, %xmm9
	movaps	%xmm1, %xmm3
	mulss	%xmm9, %xmm3
	subss	%xmm4, %xmm3
	leal	(%rdx,%r8), %ebx
	movslq	%ebx, %rbx
	movss	%xmm3, (%rax,%rbx,4)
	mulss	%xmm0, %xmm9
	mulss	%xmm1, %xmm10
	addss	%xmm9, %xmm10
	leal	(%r9,%r8), %ebx
	movslq	%ebx, %rbx
	movss	%xmm10, (%rax,%rbx,4)
	addl	$2, %r8d
	incl	%r10d
	cmpl	52(%rsp), %r10d
	jne	.LBB32_37	# bb132.preheader
.LBB32_43:	# bb138.loopexit
	movl	24(%rsp), %r8d
	addl	16(%rsp), %r8d
	movl	%r8d, 24(%rsp)
	movl	40(%rsp), %r8d
	subl	4(%rsp), %r8d
	movl	%r8d, 40(%rsp)
	subl	20(%rsp), %edx
	decl	44(%rsp)
	jmp	.LBB32_33	# bb138
.LBB32_44:	# bb132.preheader.bb133_crit_edge
	pxor	%xmm3, %xmm3
	movaps	%xmm3, %xmm4
	jmp	.LBB32_40	# bb133
.LBB32_45:	# bb144
	cmpl	$122, %edx
	sete	%cl
	setne	%dl
	cmpl	$112, %r8d
	sete	%r8b
	setne	%r9b
	andb	%cl, %r8b
	orb	%dl, %r9b
	testb	%r9b, %r9b
	jne	.LBB32_58	# bb164
.LBB32_46:	# bb144
	cmpl	$141, %esi
	jne	.LBB32_58	# bb164
.LBB32_47:	# bb163.preheader
	cmpl	$0, 44(%rsp)
	jle	.LBB32_111	# bb118.thread
.LBB32_48:	# bb.nph369
	cmpl	$0, 52(%rsp)
	cvtsi2ss	%r10d, %xmm2
	jle	.LBB32_111	# bb118.thread
.LBB32_49:	# bb161.preheader.preheader
	movl	136(%rsp), %r10d
	leal	2(,%r10,2), %ecx
	movl	%ecx, 16(%rsp)
	leal	(%r10,%r10), %r10d
	movl	%r10d, 12(%rsp)
	movl	152(%rsp), %r10d
	leal	(%r10,%r10), %r10d
	movl	%r10d, 8(%rsp)
	movl	44(%rsp), %r10d
	leal	-1(%r10), %r9d
	xorl	%r10d, %r10d
	movl	%r10d, 24(%rsp)
	movl	%r10d, 20(%rsp)
	jmp	.LBB32_57	# bb161.preheader
	.align	16
.LBB32_50:	# bb154
	cmpl	$131, 48(%rsp)
	je	.LBB32_116	# bb155
.LBB32_51:	# bb156
	movslq	%r10d, %r11
	movss	(%rax,%r11,4), %xmm7
	leal	1(%r10), %r11d
	movslq	%r11d, %r11
	movss	(%rax,%r11,4), %xmm8
.LBB32_52:	# bb159.preheader
	cmpl	44(%rsp), %r8d
	jge	.LBB32_55	# bb160
.LBB32_53:	# bb.nph363
	leal	(%rcx,%r10), %r11d
	movl	136(%rsp), %ebx
	leal	(%rbx,%rbx), %ebx
	movl	152(%rsp), %r14d
	leal	(%r14,%r14), %r14d
	xorl	%r15d, %r15d
	movl	40(%rsp), %r12d
	.align	16
.LBB32_54:	# bb158
	movslq	%r12d, %r13
	leal	1(%r12), %ebp
	movslq	%ebp, %rbp
	movaps	%xmm2, %xmm3
	mulss	(%rdi,%rbp,4), %xmm3
	movslq	%r11d, %rbp
	movss	(%rax,%rbp,4), %xmm4
	movaps	%xmm3, %xmm5
	mulss	%xmm4, %xmm5
	movss	(%rdi,%r13,4), %xmm6
	leal	1(%r11), %r13d
	movslq	%r13d, %r13
	movss	(%rax,%r13,4), %xmm9
	movaps	%xmm6, %xmm10
	mulss	%xmm9, %xmm10
	addss	%xmm5, %xmm10
	addss	%xmm10, %xmm8
	mulss	%xmm4, %xmm6
	mulss	%xmm9, %xmm3
	subss	%xmm3, %xmm6
	addss	%xmm6, %xmm7
	addl	%ebx, %r12d
	addl	%r14d, %r11d
	incl	%r15d
	cmpl	%r9d, %r15d
	jne	.LBB32_54	# bb158
.LBB32_55:	# bb160
	movaps	%xmm0, %xmm3
	mulss	%xmm8, %xmm3
	movaps	%xmm1, %xmm4
	mulss	%xmm7, %xmm4
	subss	%xmm3, %xmm4
	movslq	%r10d, %r11
	movss	%xmm4, (%rax,%r11,4)
	mulss	%xmm0, %xmm7
	mulss	%xmm1, %xmm8
	addss	%xmm7, %xmm8
	leal	1(%r10), %r11d
	movslq	%r11d, %r11
	movss	%xmm8, (%rax,%r11,4)
	addl	$2, %r10d
	incl	%esi
	cmpl	52(%rsp), %esi
	jne	.LBB32_50	# bb154
.LBB32_56:	# bb162
	movl	%edx, %r10d
	addl	16(%rsp), %r10d
	movl	24(%rsp), %ecx
	addl	8(%rsp), %ecx
	movl	%ecx, 24(%rsp)
	decl	%r9d
	movl	20(%rsp), %ecx
	incl	%ecx
	movl	%ecx, 20(%rsp)
	cmpl	44(%rsp), %ecx
	je	.LBB32_111	# bb118.thread
.LBB32_57:	# bb161.preheader
	movl	12(%rsp), %ecx
	leal	(%rcx,%r10), %ecx
	movl	%ecx, 40(%rsp)
	movslq	%r10d, %rdx
	incl	%r10d
	movslq	%r10d, %rcx
	movq	%rcx, 32(%rsp)
	movl	152(%rsp), %r10d
	leal	(%r10,%r10), %ecx
	movl	20(%rsp), %r10d
	leal	1(%r10), %r8d
	xorl	%esi, %esi
	movl	24(%rsp), %r10d
	jmp	.LBB32_50	# bb154
.LBB32_58:	# bb164
	cmpl	$142, %esi
	setne	%cl
	notb	%r11b
	orb	%cl, %r11b
	testb	$1, %r11b
	jne	.LBB32_71	# bb189
.LBB32_59:	# bb188.preheader
	cmpl	$0, 44(%rsp)
	jle	.LBB32_111	# bb118.thread
.LBB32_60:	# bb.nph357
	movl	152(%rsp), %ecx
	addl	%ecx, %ecx
	movl	%ecx, 28(%rsp)
	cvtsi2ss	%r10d, %xmm2
	xorl	%r8d, %r8d
	movl	%r8d, 40(%rsp)
	.align	16
.LBB32_61:	# bb181.preheader
	movl	136(%rsp), %r10d
	leal	2(,%r10,2), %ecx
	leal	(%r10,%r10), %r10d
	movl	$4294967294, %r11d
	subl	%r10d, %r11d
	movl	52(%rsp), %edx
	leal	-1(%rdx), %r10d
	imull	%ecx, %r10d
	leal	-1(%r8), %r9d
	leal	-2(%r8), %esi
	leal	(%rdx,%rdx), %ecx
	jmp	.LBB32_67	# bb181
.LBB32_62:	# bb.nph353
	movl	136(%rsp), %ebx
	leal	(%rbx,%rbx), %ebx
	leal	-1(%rdx), %r14d
	pxor	%xmm3, %xmm3
	xorl	%r15d, %r15d
	movl	%ecx, %r12d
	movl	%r8d, %r13d
	movaps	%xmm3, %xmm4
	.align	16
.LBB32_63:	# bb175
	leal	-1(%r12), %ebp
	movslq	%ebp, %rbp
	movaps	%xmm2, %xmm5
	mulss	(%rdi,%rbp,4), %xmm5
	movslq	%r13d, %rbp
	movss	(%rax,%rbp,4), %xmm6
	movaps	%xmm5, %xmm7
	mulss	%xmm6, %xmm7
	leal	1(%r13), %ebp
	movslq	%ebp, %rbp
	movss	(%rax,%rbp,4), %xmm8
	leal	-2(%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%rdi,%rbp,4), %xmm9
	movaps	%xmm9, %xmm10
	mulss	%xmm8, %xmm10
	addss	%xmm7, %xmm10
	addss	%xmm10, %xmm4
	mulss	%xmm8, %xmm5
	mulss	%xmm6, %xmm9
	subss	%xmm5, %xmm9
	addss	%xmm9, %xmm3
	addl	%ebx, %r12d
	addl	$2, %r13d
	incl	%r15d
	cmpl	%r14d, %r15d
	jne	.LBB32_63	# bb175
.LBB32_64:	# bb177
	cmpl	$131, 48(%rsp)
	je	.LBB32_117	# bb178
.LBB32_65:	# bb179
	leal	(%r9,%rcx), %ebx
	movslq	%ebx, %rbx
	movss	(%rax,%rbx,4), %xmm10
	leal	(%rsi,%rcx), %ebx
	movslq	%ebx, %rbx
	movss	(%rax,%rbx,4), %xmm8
.LBB32_66:	# bb180
	addss	%xmm4, %xmm10
	movaps	%xmm0, %xmm4
	mulss	%xmm10, %xmm4
	addss	%xmm3, %xmm8
	movaps	%xmm1, %xmm3
	mulss	%xmm8, %xmm3
	subss	%xmm4, %xmm3
	leal	(%rsi,%rcx), %ebx
	movslq	%ebx, %rbx
	movss	%xmm3, (%rax,%rbx,4)
	mulss	%xmm0, %xmm8
	mulss	%xmm1, %xmm10
	addss	%xmm8, %xmm10
	leal	(%r9,%rcx), %ebx
	movslq	%ebx, %rbx
	movss	%xmm10, (%rax,%rbx,4)
	addl	%r11d, %r10d
	addl	$4294967294, %ecx
	decl	%edx
.LBB32_67:	# bb181
	testl	%edx, %edx
	jle	.LBB32_110	# bb187
.LBB32_68:	# bb182
	leal	-1(%rdx), %ebx
	testl	%edx, %edx
	je	.LBB32_110	# bb187
.LBB32_69:	# bb176.preheader
	testl	%ebx, %ebx
	jg	.LBB32_62	# bb.nph353
.LBB32_70:	# bb176.preheader.bb177_crit_edge
	pxor	%xmm3, %xmm3
	movaps	%xmm3, %xmm4
	jmp	.LBB32_64	# bb177
.LBB32_71:	# bb189
	cmpl	$142, %esi
	setne	%cl
	notb	%bl
	orb	%cl, %bl
	testb	$1, %bl
	jne	.LBB32_83	# bb209
.LBB32_72:	# bb208.preheader
	cmpl	$0, 44(%rsp)
	jle	.LBB32_111	# bb118.thread
.LBB32_73:	# bb.nph349
	cmpl	$0, 52(%rsp)
	cvtsi2ss	%r10d, %xmm2
	jle	.LBB32_111	# bb118.thread
.LBB32_74:	# bb206.preheader.preheader
	movl	152(%rsp), %r10d
	addl	%r10d, %r10d
	movl	%r10d, 28(%rsp)
	xorl	%r8d, %r8d
	movl	%r8d, 40(%rsp)
	jmp	.LBB32_82	# bb206.preheader
	.align	16
.LBB32_75:	# bb199
	cmpl	$131, 48(%rsp)
	je	.LBB32_118	# bb200
.LBB32_76:	# bb201
	movslq	%edx, %r11
	movss	(%rax,%r11,4), %xmm6
	leal	1(%rdx), %r11d
	movslq	%r11d, %r11
	movss	(%rax,%r11,4), %xmm8
.LBB32_77:	# bb204.preheader
	leal	1(%r10), %r11d
	cmpl	52(%rsp), %r11d
	jge	.LBB32_80	# bb205
.LBB32_78:	# bb.nph343
	leal	3(%rdx), %r11d
	leal	2(%rdx), %ebx
	leal	3(%rsi), %r14d
	leal	2(%rsi), %r15d
	xorl	%r12d, %r12d
	movl	%r12d, %r13d
	.align	16
.LBB32_79:	# bb203
	leal	(%rbx,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%rax,%rbp,4), %xmm3
	leal	(%r14,%r12), %ebp
	movslq	%ebp, %rbp
	movaps	%xmm2, %xmm4
	mulss	(%rdi,%rbp,4), %xmm4
	movaps	%xmm4, %xmm5
	mulss	%xmm3, %xmm5
	leal	(%r11,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%rax,%rbp,4), %xmm7
	leal	(%r15,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%rdi,%rbp,4), %xmm9
	movaps	%xmm9, %xmm10
	mulss	%xmm7, %xmm10
	addss	%xmm5, %xmm10
	addss	%xmm10, %xmm8
	mulss	%xmm7, %xmm4
	mulss	%xmm3, %xmm9
	subss	%xmm4, %xmm9
	addss	%xmm9, %xmm6
	addl	$2, %r12d
	incl	%r13d
	cmpl	%ecx, %r13d
	jne	.LBB32_79	# bb203
.LBB32_80:	# bb205
	movaps	%xmm0, %xmm3
	mulss	%xmm8, %xmm3
	movaps	%xmm1, %xmm4
	mulss	%xmm6, %xmm4
	subss	%xmm3, %xmm4
	movslq	%edx, %r11
	movss	%xmm4, (%rax,%r11,4)
	mulss	%xmm0, %xmm6
	mulss	%xmm1, %xmm8
	addss	%xmm6, %xmm8
	leal	1(%rdx), %r11d
	movslq	%r11d, %r11
	movss	%xmm8, (%rax,%r11,4)
	addl	%r9d, %esi
	addl	$2, %edx
	decl	%ecx
	incl	%r10d
	cmpl	52(%rsp), %r10d
	jne	.LBB32_75	# bb199
.LBB32_81:	# bb207
	addl	28(%rsp), %r8d
	movl	40(%rsp), %r10d
	incl	%r10d
	movl	%r10d, 40(%rsp)
	cmpl	44(%rsp), %r10d
	je	.LBB32_111	# bb118.thread
.LBB32_82:	# bb206.preheader
	movl	136(%rsp), %r10d
	leal	2(,%r10,2), %r9d
	movl	52(%rsp), %r10d
	leal	-1(%r10), %ecx
	xorl	%esi, %esi
	movl	%r8d, %edx
	movl	%esi, %r10d
	jmp	.LBB32_75	# bb199
.LBB32_83:	# bb209
	cmpl	$142, %esi
	setne	%cl
	notb	%r14b
	orb	%cl, %r14b
	testb	$1, %r14b
	jne	.LBB32_95	# bb229
.LBB32_84:	# bb228.preheader
	cmpl	$0, 44(%rsp)
	jle	.LBB32_111	# bb118.thread
.LBB32_85:	# bb.nph337
	cmpl	$0, 52(%rsp)
	cvtsi2ss	%r10d, %xmm2
	jle	.LBB32_111	# bb118.thread
.LBB32_86:	# bb226.preheader.preheader
	movl	152(%rsp), %r10d
	addl	%r10d, %r10d
	movl	%r10d, 28(%rsp)
	xorl	%r10d, %r10d
	movl	%r10d, 40(%rsp)
	movl	%r10d, 32(%rsp)
	jmp	.LBB32_94	# bb226.preheader
	.align	16
.LBB32_87:	# bb219
	cmpl	$131, 48(%rsp)
	je	.LBB32_119	# bb220
.LBB32_88:	# bb221
	movslq	%esi, %r11
	movss	(%rax,%r11,4), %xmm6
	leal	1(%rsi), %r11d
	movslq	%r11d, %r11
	movss	(%rax,%r11,4), %xmm8
.LBB32_89:	# bb224.preheader
	leal	1(%r8), %r11d
	cmpl	52(%rsp), %r11d
	jge	.LBB32_92	# bb225
.LBB32_90:	# bb.nph331
	leal	(%rcx,%r9), %r11d
	movl	136(%rsp), %ebx
	leal	(%rbx,%rbx), %ebx
	xorl	%r14d, %r14d
	movl	%esi, %r15d
	.align	16
.LBB32_91:	# bb223
	movslq	%r11d, %r12
	leal	1(%r11), %r13d
	movslq	%r13d, %r13
	movaps	%xmm2, %xmm3
	mulss	(%rdi,%r13,4), %xmm3
	leal	3(%r15), %r13d
	leal	2(%r15), %r15d
	movslq	%r15d, %rbp
	movss	(%rax,%rbp,4), %xmm4
	movaps	%xmm3, %xmm5
	mulss	%xmm4, %xmm5
	movss	(%rdi,%r12,4), %xmm7
	movslq	%r13d, %r12
	movss	(%rax,%r12,4), %xmm9
	movaps	%xmm7, %xmm10
	mulss	%xmm9, %xmm10
	addss	%xmm5, %xmm10
	addss	%xmm10, %xmm8
	mulss	%xmm9, %xmm3
	mulss	%xmm4, %xmm7
	subss	%xmm3, %xmm7
	addss	%xmm7, %xmm6
	addl	%ebx, %r11d
	incl	%r14d
	cmpl	%r10d, %r14d
	jne	.LBB32_91	# bb223
.LBB32_92:	# bb225
	movaps	%xmm0, %xmm3
	mulss	%xmm8, %xmm3
	movaps	%xmm1, %xmm4
	mulss	%xmm6, %xmm4
	subss	%xmm3, %xmm4
	movslq	%esi, %r11
	movss	%xmm4, (%rax,%r11,4)
	mulss	%xmm0, %xmm6
	mulss	%xmm1, %xmm8
	addss	%xmm6, %xmm8
	leal	1(%rsi), %r11d
	movslq	%r11d, %r11
	movss	%xmm8, (%rax,%r11,4)
	addl	%edx, %r9d
	addl	$2, %esi
	decl	%r10d
	incl	%r8d
	cmpl	52(%rsp), %r8d
	jne	.LBB32_87	# bb219
.LBB32_93:	# bb227
	movl	40(%rsp), %r10d
	addl	28(%rsp), %r10d
	movl	%r10d, 40(%rsp)
	movl	32(%rsp), %r10d
	incl	%r10d
	movl	%r10d, 32(%rsp)
	cmpl	44(%rsp), %r10d
	je	.LBB32_111	# bb118.thread
.LBB32_94:	# bb226.preheader
	movl	136(%rsp), %r10d
	leal	2(,%r10,2), %edx
	leal	(%r10,%r10), %ecx
	movl	52(%rsp), %r10d
	leal	-1(%r10), %r10d
	xorl	%r9d, %r9d
	movl	40(%rsp), %esi
	movl	%r9d, %r8d
	jmp	.LBB32_87	# bb219
.LBB32_95:	# bb229
	cmpl	$142, %esi
	setne	%cl
	notb	%r8b
	orb	%cl, %r8b
	testb	$1, %r8b
	jne	.LBB32_109	# bb254
.LBB32_96:	# bb253.preheader
	cmpl	$0, 44(%rsp)
	jle	.LBB32_111	# bb118.thread
.LBB32_97:	# bb.nph325
	movl	52(%rsp), %ecx
	leal	(%rcx,%rcx), %ecx
	movl	%ecx, 32(%rsp)
	movl	152(%rsp), %ecx
	addl	%ecx, %ecx
	movl	%ecx, 28(%rsp)
	cvtsi2ss	%r10d, %xmm2
	xorl	%esi, %esi
	movl	%esi, 40(%rsp)
	jmp	.LBB32_108	# bb246.preheader
.LBB32_98:	# bb.nph
	leal	1(%rdx), %r14d
	leal	-1(%r9), %r15d
	pxor	%xmm3, %xmm3
	xorl	%r12d, %r12d
	movl	%r12d, %r13d
	movaps	%xmm3, %xmm4
	.align	16
.LBB32_99:	# bb240
	leal	(%rsi,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%rax,%rbp,4), %xmm5
	leal	(%r14,%r12), %ebp
	movslq	%ebp, %rbp
	movaps	%xmm2, %xmm6
	mulss	(%rdi,%rbp,4), %xmm6
	movaps	%xmm6, %xmm7
	mulss	%xmm5, %xmm7
	leal	(%rbx,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%rax,%rbp,4), %xmm8
	leal	(%rdx,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%rdi,%rbp,4), %xmm9
	movaps	%xmm9, %xmm10
	mulss	%xmm8, %xmm10
	addss	%xmm7, %xmm10
	addss	%xmm10, %xmm3
	mulss	%xmm8, %xmm6
	mulss	%xmm5, %xmm9
	subss	%xmm6, %xmm9
	addss	%xmm9, %xmm4
	addl	$2, %r12d
	incl	%r13d
	cmpl	%r15d, %r13d
	jne	.LBB32_99	# bb240
.LBB32_100:	# bb242
	cmpl	$131, 48(%rsp)
	je	.LBB32_120	# bb243
.LBB32_101:	# bb244
	leal	-1(%r8), %r14d
	movslq	%r14d, %r14
	movss	(%rax,%r14,4), %xmm10
	leal	-2(%r8), %r14d
	movslq	%r14d, %r14
	movss	(%rax,%r14,4), %xmm8
.LBB32_102:	# bb245
	addss	%xmm3, %xmm10
	movaps	%xmm0, %xmm3
	mulss	%xmm10, %xmm3
	addss	%xmm4, %xmm8
	movaps	%xmm1, %xmm4
	mulss	%xmm8, %xmm4
	subss	%xmm3, %xmm4
	leal	-1(%r8), %r14d
	leal	-2(%r8), %r8d
	movslq	%r8d, %r15
	movss	%xmm4, (%rax,%r15,4)
	mulss	%xmm0, %xmm8
	mulss	%xmm1, %xmm10
	addss	%xmm8, %xmm10
	movslq	%r14d, %r14
	movss	%xmm10, (%rax,%r14,4)
	addl	%r10d, %ecx
	subl	%r11d, %edx
	decl	%r9d
.LBB32_103:	# bb246
	testl	%r9d, %r9d
	jle	.LBB32_107	# bb252
.LBB32_104:	# bb247
	leal	-1(%r9), %r14d
	testl	%r9d, %r9d
	je	.LBB32_107	# bb252
.LBB32_105:	# bb241.preheader
	testl	%r14d, %r14d
	jg	.LBB32_98	# bb.nph
.LBB32_106:	# bb241.preheader.bb242_crit_edge
	pxor	%xmm3, %xmm3
	movaps	%xmm3, %xmm4
	jmp	.LBB32_100	# bb242
.LBB32_107:	# bb252
	addl	28(%rsp), %esi
	movl	40(%rsp), %r10d
	incl	%r10d
	movl	%r10d, 40(%rsp)
	cmpl	44(%rsp), %r10d
	je	.LBB32_111	# bb118.thread
.LBB32_108:	# bb246.preheader
	movl	52(%rsp), %r9d
	leal	-1(%r9), %r8d
	movl	136(%rsp), %r10d
	leal	2(,%r10,2), %ecx
	movl	%r10d, %edx
	imull	%r8d, %edx
	imull	%r8d, %ecx
	leal	(%r10,%r10), %r11d
	movl	$4294967294, %r10d
	subl	%r11d, %r10d
	addl	%edx, %edx
	movl	32(%rsp), %r8d
	leal	(%r8,%rsi), %r8d
	leal	1(%rsi), %ebx
	jmp	.LBB32_103	# bb246
.LBB32_109:	# bb254
	xorl	%edi, %edi
	leaq	.str45, %rsi
	leaq	.str146, %rdx
	xorb	%al, %al
	call	cblas_xerbla
	jmp	.LBB32_111	# bb118.thread
.LBB32_110:	# bb187
	addl	28(%rsp), %r8d
	movl	40(%rsp), %r10d
	incl	%r10d
	movl	%r10d, 40(%rsp)
	cmpl	44(%rsp), %r10d
	jne	.LBB32_61	# bb181.preheader
.LBB32_111:	# bb118.thread
	addq	$56, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB32_112:	# bb63
	cmpl	$111, %ecx
	movl	$111, %ecx
	movl	$112, %r8d
	cmove	%ecx, %r8d
	movl	%r11d, 52(%rsp)
	jmp	.LBB32_2	# bb77
.LBB32_113:	# bb87
	movaps	%xmm2, %xmm3
	mulss	(%rdi,%rbx,4), %xmm3
	movslq	%r11d, %r14
	movss	(%rax,%r14,4), %xmm4
	movaps	%xmm3, %xmm5
	mulss	%xmm4, %xmm5
	leal	1(%r11), %r14d
	movslq	%r14d, %r14
	movss	(%rax,%r14,4), %xmm6
	movss	(%rdi,%rsi,4), %xmm7
	movaps	%xmm7, %xmm8
	mulss	%xmm6, %xmm8
	addss	%xmm5, %xmm8
	mulss	%xmm4, %xmm7
	mulss	%xmm6, %xmm3
	subss	%xmm3, %xmm7
	jmp	.LBB32_9	# bb91.preheader
.LBB32_114:	# bb110
	movaps	%xmm2, %xmm5
	mulss	(%rdi,%rdx,4), %xmm5
	leal	(%r9,%rsi), %ebx
	movslq	%ebx, %rbx
	movss	(%rax,%rbx,4), %xmm6
	movaps	%xmm5, %xmm7
	mulss	%xmm6, %xmm7
	leal	(%r8,%rsi), %ebx
	movslq	%ebx, %rbx
	movss	(%rax,%rbx,4), %xmm8
	movss	(%rdi,%rcx,4), %xmm9
	movaps	%xmm9, %xmm10
	mulss	%xmm8, %xmm10
	addss	%xmm7, %xmm10
	mulss	%xmm8, %xmm5
	mulss	%xmm6, %xmm9
	subss	%xmm5, %xmm9
	jmp	.LBB32_27	# bb112
.LBB32_115:	# bb134
	movaps	%xmm2, %xmm5
	mulss	(%rdi,%rsi,4), %xmm5
	leal	(%rdx,%r8), %ebx
	movslq	%ebx, %rbx
	movss	(%rax,%rbx,4), %xmm6
	movaps	%xmm5, %xmm7
	mulss	%xmm6, %xmm7
	leal	(%r9,%r8), %ebx
	movslq	%ebx, %rbx
	movss	(%rax,%rbx,4), %xmm8
	movss	(%rdi,%rcx,4), %xmm9
	movaps	%xmm9, %xmm10
	mulss	%xmm8, %xmm10
	addss	%xmm7, %xmm10
	mulss	%xmm8, %xmm5
	mulss	%xmm6, %xmm9
	subss	%xmm5, %xmm9
	jmp	.LBB32_42	# bb136
.LBB32_116:	# bb155
	movaps	%xmm2, %xmm3
	movq	32(%rsp), %r11
	mulss	(%rdi,%r11,4), %xmm3
	movslq	%r10d, %r11
	movss	(%rax,%r11,4), %xmm4
	movaps	%xmm3, %xmm5
	mulss	%xmm4, %xmm5
	leal	1(%r10), %r11d
	movslq	%r11d, %r11
	movss	(%rax,%r11,4), %xmm6
	movss	(%rdi,%rdx,4), %xmm7
	movaps	%xmm7, %xmm8
	mulss	%xmm6, %xmm8
	addss	%xmm5, %xmm8
	mulss	%xmm4, %xmm7
	mulss	%xmm6, %xmm3
	subss	%xmm3, %xmm7
	jmp	.LBB32_52	# bb159.preheader
.LBB32_117:	# bb178
	movslq	%r10d, %rbx
	leal	1(%r10), %r14d
	movslq	%r14d, %r14
	movaps	%xmm2, %xmm5
	mulss	(%rdi,%r14,4), %xmm5
	leal	(%rsi,%rcx), %r14d
	movslq	%r14d, %r14
	movss	(%rax,%r14,4), %xmm6
	movaps	%xmm5, %xmm7
	mulss	%xmm6, %xmm7
	movss	(%rdi,%rbx,4), %xmm8
	leal	(%r9,%rcx), %ebx
	movslq	%ebx, %rbx
	movss	(%rax,%rbx,4), %xmm9
	movaps	%xmm8, %xmm10
	mulss	%xmm9, %xmm10
	addss	%xmm7, %xmm10
	mulss	%xmm9, %xmm5
	mulss	%xmm6, %xmm8
	subss	%xmm5, %xmm8
	jmp	.LBB32_66	# bb180
.LBB32_118:	# bb200
	movslq	%esi, %r11
	leal	1(%rsi), %ebx
	movslq	%ebx, %rbx
	movaps	%xmm2, %xmm3
	mulss	(%rdi,%rbx,4), %xmm3
	movslq	%edx, %rbx
	movss	(%rax,%rbx,4), %xmm4
	movaps	%xmm3, %xmm5
	mulss	%xmm4, %xmm5
	movss	(%rdi,%r11,4), %xmm6
	leal	1(%rdx), %r11d
	movslq	%r11d, %r11
	movss	(%rax,%r11,4), %xmm7
	movaps	%xmm6, %xmm8
	mulss	%xmm7, %xmm8
	addss	%xmm5, %xmm8
	mulss	%xmm4, %xmm6
	mulss	%xmm7, %xmm3
	subss	%xmm3, %xmm6
	jmp	.LBB32_77	# bb204.preheader
.LBB32_119:	# bb220
	movslq	%r9d, %r11
	leal	1(%r9), %ebx
	movslq	%ebx, %rbx
	movaps	%xmm2, %xmm3
	mulss	(%rdi,%rbx,4), %xmm3
	movslq	%esi, %rbx
	movss	(%rax,%rbx,4), %xmm4
	movaps	%xmm3, %xmm5
	mulss	%xmm4, %xmm5
	movss	(%rdi,%r11,4), %xmm6
	leal	1(%rsi), %r11d
	movslq	%r11d, %r11
	movss	(%rax,%r11,4), %xmm7
	movaps	%xmm6, %xmm8
	mulss	%xmm7, %xmm8
	addss	%xmm5, %xmm8
	mulss	%xmm4, %xmm6
	mulss	%xmm7, %xmm3
	subss	%xmm3, %xmm6
	jmp	.LBB32_89	# bb224.preheader
.LBB32_120:	# bb243
	movslq	%ecx, %r14
	leal	1(%rcx), %r15d
	movslq	%r15d, %r15
	movaps	%xmm2, %xmm5
	mulss	(%rdi,%r15,4), %xmm5
	leal	-2(%r8), %r15d
	movslq	%r15d, %r15
	movss	(%rax,%r15,4), %xmm6
	movaps	%xmm5, %xmm7
	mulss	%xmm6, %xmm7
	movss	(%rdi,%r14,4), %xmm8
	leal	-1(%r8), %r14d
	movslq	%r14d, %r14
	movss	(%rax,%r14,4), %xmm9
	movaps	%xmm8, %xmm10
	mulss	%xmm9, %xmm10
	addss	%xmm7, %xmm10
	mulss	%xmm9, %xmm5
	mulss	%xmm6, %xmm8
	subss	%xmm5, %xmm8
	jmp	.LBB32_102	# bb245
	.size	cblas_ctrmm, .-cblas_ctrmm
.Leh_func_end23:


	.align	16
	.globl	cblas_ctrmv
	.type	cblas_ctrmv,@function
cblas_ctrmv:
.Leh_func_begin24:
.Llabel24:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	cmpl	$113, %edx
	movl	$4294967295, %eax
	movl	$1, %r10d
	cmove	%eax, %r10d
	movl	$112, %eax
	cmovne	%edx, %eax
	cmpl	$121, %esi
	sete	%dl
	setne	%r11b
	cmpl	$101, %edi
	sete	%bl
	setne	%r14b
	andb	%dl, %bl
	orb	%r11b, %r14b
	testb	%r14b, %r14b
	movl	112(%rsp), %edx
	movq	104(%rsp), %r11
	movl	%ecx, 36(%rsp)
	jne	.LBB33_2	# bb54
.LBB33_1:	# entry
	cmpl	$111, %eax
	je	.LBB33_4	# bb62
.LBB33_2:	# bb54
	cmpl	$122, %esi
	sete	%cl
	setne	%r14b
	cmpl	$102, %edi
	sete	%r15b
	setne	%r12b
	andb	%cl, %r15b
	orb	%r14b, %r12b
	testb	%r12b, %r12b
	jne	.LBB33_14	# bb77
.LBB33_3:	# bb54
	cmpl	$112, %eax
	jne	.LBB33_14	# bb77
.LBB33_4:	# bb62
	testl	%edx, %edx
	jg	.LBB33_55	# bb62.bb76.preheader_crit_edge
.LBB33_5:	# bb63
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
.LBB33_6:	# bb76.preheader
	testl	%r8d, %r8d
	jle	.LBB33_29	# bb111.thread
.LBB33_7:	# bb.nph246
	movl	$1, %ecx
	subl	%r8d, %ecx
	imull	%edx, %ecx
	addl	%eax, %eax
	movl	96(%rsp), %esi
	leal	2(,%rsi,2), %esi
	movl	%esi, 32(%rsp)
	leal	-1(%r8), %esi
	cvtsi2ss	%r10d, %xmm0
	leal	(%rdx,%rdx), %r10d
	movl	%r10d, 28(%rsp)
	xorl	%r10d, %r10d
	movl	%edx, %edi
	movl	%r10d, %ebx
	.align	16
.LBB33_8:	# bb66
	testl	%edx, %edx
	movl	$0, %r14d
	cmovle	%ecx, %r14d
	leal	1(%rbx), %r15d
	cmpl	%r8d, %r15d
	jge	.LBB33_56	# bb66.bb72_crit_edge
.LBB33_9:	# bb66.bb70_crit_edge
	pxor	%xmm1, %xmm1
	xorl	%r15d, %r15d
	movl	%r10d, %r12d
	movl	%edi, %r13d
	movaps	%xmm1, %xmm2
	.align	16
.LBB33_10:	# bb70
	leal	3(%r12), %ebp
	movslq	%ebp, %rbp
	movaps	%xmm0, %xmm3
	mulss	(%r9,%rbp,4), %xmm3
	addl	%r14d, %r13d
	leal	1(,%r13,2), %r14d
	leal	(%r13,%r13), %ebp
	movslq	%ebp, %rbp
	movss	(%r11,%rbp,4), %xmm4
	movaps	%xmm3, %xmm5
	mulss	%xmm4, %xmm5
	addl	$2, %r12d
	movslq	%r12d, %rbp
	movss	(%r9,%rbp,4), %xmm6
	movslq	%r14d, %r14
	movss	(%r11,%r14,4), %xmm7
	movaps	%xmm6, %xmm8
	mulss	%xmm7, %xmm8
	addss	%xmm5, %xmm8
	addss	%xmm8, %xmm2
	mulss	%xmm7, %xmm3
	mulss	%xmm4, %xmm6
	subss	%xmm3, %xmm6
	addss	%xmm6, %xmm1
	incl	%r15d
	cmpl	%esi, %r15d
	movl	%edx, %r14d
	jne	.LBB33_10	# bb70
.LBB33_11:	# bb72
	movslq	%eax, %r14
	movss	(%r11,%r14,4), %xmm3
	cmpl	$131, 36(%rsp)
	je	.LBB33_57	# bb73
.LBB33_12:	# bb74
	addss	%xmm1, %xmm3
	movss	%xmm3, (%r11,%r14,4)
	leal	1(%rax), %r14d
	movslq	%r14d, %r14
	addss	(%r11,%r14,4), %xmm2
	movss	%xmm2, (%r11,%r14,4)
.LBB33_13:	# bb75
	addl	28(%rsp), %eax
	addl	%edx, %edi
	addl	32(%rsp), %r10d
	decl	%esi
	incl	%ebx
	cmpl	%r8d, %ebx
	jne	.LBB33_8	# bb66
	jmp	.LBB33_29	# bb111.thread
.LBB33_14:	# bb77
	cmpl	$122, %esi
	sete	%cl
	setne	%r14b
	cmpl	$101, %edi
	sete	%r12b
	setne	%r13b
	andb	%cl, %r12b
	orb	%r14b, %r13b
	testb	%r13b, %r13b
	jne	.LBB33_16	# bb85
.LBB33_15:	# bb77
	cmpl	$111, %eax
	je	.LBB33_18	# bb93
.LBB33_16:	# bb85
	cmpl	$121, %esi
	sete	%cl
	setne	%sil
	cmpl	$102, %edi
	sete	%dil
	setne	%r14b
	andb	%cl, %dil
	orb	%sil, %r14b
	testb	%r14b, %r14b
	jne	.LBB33_30	# bb113
.LBB33_17:	# bb85
	cmpl	$112, %eax
	jne	.LBB33_30	# bb113
.LBB33_18:	# bb93
	testl	%edx, %edx
	jg	.LBB33_58	# bb93.bb96_crit_edge
.LBB33_19:	# bb94
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
.LBB33_20:	# bb96
	leal	-1(%r8), %ecx
	movl	96(%rsp), %esi
	leal	2(,%rsi,2), %edi
	movl	%esi, %ebx
	imull	%ecx, %ebx
	movl	%edx, %r14d
	imull	%ecx, %r14d
	imull	%ecx, %edi
	addl	%esi, %esi
	movl	%esi, 16(%rsp)
	movl	$4294967294, %ecx
	subl	%esi, %ecx
	movl	%ecx, 28(%rsp)
	addl	%ebx, %ebx
	addl	%eax, %r14d
	addl	%r14d, %r14d
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
	cvtsi2ss	%r10d, %xmm0
	leal	(%rdx,%rdx), %r10d
	movl	%r10d, 32(%rsp)
	jmp	.LBB33_27	# bb107
.LBB33_21:	# bb97
	testl	%edx, %edx
	movl	$0, %ecx
	cmovle	%eax, %ecx
	testl	%r10d, %r10d
	jle	.LBB33_59	# bb97.bb103_crit_edge
.LBB33_22:	# bb.nph228
	leal	(%rdx,%rdx), %r10d
	addl	%ecx, %ecx
	leal	-1(%r8), %esi
	pxor	%xmm1, %xmm1
	xorl	%r15d, %r15d
	movl	%ebx, %r12d
	movaps	%xmm1, %xmm2
	.align	16
.LBB33_23:	# bb101
	movslq	%r12d, %r13
	leal	1(%r12), %ebp
	movslq	%ebp, %rbp
	movaps	%xmm0, %xmm3
	mulss	(%r9,%rbp,4), %xmm3
	movslq	%ecx, %rbp
	movss	(%r11,%rbp,4), %xmm4
	movaps	%xmm3, %xmm5
	mulss	%xmm4, %xmm5
	movss	(%r9,%r13,4), %xmm6
	leal	1(%rcx), %r13d
	movslq	%r13d, %r13
	movss	(%r11,%r13,4), %xmm7
	movaps	%xmm6, %xmm8
	mulss	%xmm7, %xmm8
	addss	%xmm5, %xmm8
	addss	%xmm8, %xmm2
	mulss	%xmm4, %xmm6
	mulss	%xmm7, %xmm3
	subss	%xmm3, %xmm6
	addss	%xmm6, %xmm1
	addl	%r10d, %ecx
	addl	$2, %r12d
	incl	%r15d
	cmpl	%esi, %r15d
	jne	.LBB33_23	# bb101
.LBB33_24:	# bb103
	movslq	%r14d, %rcx
	movss	(%r11,%rcx,4), %xmm3
	cmpl	$131, 36(%rsp)
	je	.LBB33_60	# bb104
.LBB33_25:	# bb105
	addss	%xmm1, %xmm3
	movss	%xmm3, (%r11,%rcx,4)
	leal	1(%r14), %r10d
	movslq	%r10d, %rcx
	addss	(%r11,%rcx,4), %xmm2
	movss	%xmm2, (%r11,%rcx,4)
.LBB33_26:	# bb106
	addl	28(%rsp), %edi
	subl	32(%rsp), %r14d
	subl	16(%rsp), %ebx
	decl	%r8d
.LBB33_27:	# bb107
	testl	%r8d, %r8d
	jle	.LBB33_29	# bb111.thread
.LBB33_28:	# bb108
	leal	-1(%r8), %r10d
	testl	%r8d, %r8d
	jne	.LBB33_21	# bb97
.LBB33_29:	# bb111.thread
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB33_30:	# bb113
	cmpl	$111, %eax
	sete	%cl
	andb	%cl, %r15b
	cmpl	$112, %eax
	sete	%cl
	testb	%cl, %bl
	jne	.LBB33_32	# bb129
.LBB33_31:	# bb113
	notb	%r15b
	testb	$1, %r15b
	jne	.LBB33_43	# bb149
.LBB33_32:	# bb129
	testl	%edx, %edx
	jg	.LBB33_61	# bb129.bb132_crit_edge
.LBB33_33:	# bb130
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
.LBB33_34:	# bb132
	movl	96(%rsp), %ecx
	leal	2(,%rcx,2), %esi
	leal	(%rcx,%rcx), %ecx
	movl	$4294967294, %edi
	subl	%ecx, %edi
	movl	%edi, 28(%rsp)
	leal	-1(%r8), %ecx
	movl	%edx, %edi
	imull	%ecx, %edi
	imull	%ecx, %esi
	addl	%eax, %edi
	addl	%edi, %edi
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
	leal	(%r8,%r8), %ecx
	cvtsi2ss	%r10d, %xmm0
	leal	(%rdx,%rdx), %r10d
	movl	%r10d, 32(%rsp)
	jmp	.LBB33_41	# bb143
.LBB33_35:	# bb133
	testl	%edx, %edx
	movl	$0, %ebx
	cmovle	%eax, %ebx
	testl	%r10d, %r10d
	jle	.LBB33_62	# bb133.bb139_crit_edge
.LBB33_36:	# bb.nph219
	leal	(%rdx,%rdx), %r10d
	addl	%ebx, %ebx
	movl	96(%rsp), %r14d
	leal	(%r14,%r14), %r14d
	leal	-1(%r8), %r15d
	pxor	%xmm1, %xmm1
	xorl	%r12d, %r12d
	movl	%ecx, %r13d
	movaps	%xmm1, %xmm2
	.align	16
.LBB33_37:	# bb137
	leal	-1(%r13), %ebp
	movslq	%ebp, %rbp
	movaps	%xmm0, %xmm3
	mulss	(%r9,%rbp,4), %xmm3
	movslq	%ebx, %rbp
	movss	(%r11,%rbp,4), %xmm4
	movaps	%xmm3, %xmm5
	mulss	%xmm4, %xmm5
	leal	1(%rbx), %ebp
	movslq	%ebp, %rbp
	movss	(%r11,%rbp,4), %xmm6
	leal	-2(%r13), %ebp
	movslq	%ebp, %rbp
	movss	(%r9,%rbp,4), %xmm7
	movaps	%xmm7, %xmm8
	mulss	%xmm6, %xmm8
	addss	%xmm5, %xmm8
	addss	%xmm8, %xmm2
	mulss	%xmm6, %xmm3
	mulss	%xmm4, %xmm7
	subss	%xmm3, %xmm7
	addss	%xmm7, %xmm1
	addl	%r10d, %ebx
	addl	%r14d, %r13d
	incl	%r12d
	cmpl	%r15d, %r12d
	jne	.LBB33_37	# bb137
.LBB33_38:	# bb139
	movslq	%edi, %r10
	movss	(%r11,%r10,4), %xmm3
	cmpl	$131, 36(%rsp)
	je	.LBB33_63	# bb140
.LBB33_39:	# bb141
	addss	%xmm1, %xmm3
	movss	%xmm3, (%r11,%r10,4)
	leal	1(%rdi), %r10d
	movslq	%r10d, %r10
	addss	(%r11,%r10,4), %xmm2
	movss	%xmm2, (%r11,%r10,4)
.LBB33_40:	# bb142
	addl	28(%rsp), %esi
	subl	32(%rsp), %edi
	addl	$4294967294, %ecx
	decl	%r8d
.LBB33_41:	# bb143
	testl	%r8d, %r8d
	jle	.LBB33_29	# bb111.thread
.LBB33_42:	# bb144
	leal	-1(%r8), %r10d
	testl	%r8d, %r8d
	jne	.LBB33_35	# bb133
	jmp	.LBB33_29	# bb111.thread
.LBB33_43:	# bb149
	cmpl	$111, %eax
	sete	%cl
	andb	%cl, %dil
	cmpl	$112, %eax
	sete	%al
	testb	%al, %r12b
	jne	.LBB33_45	# bb165
.LBB33_44:	# bb149
	notb	%dil
	testb	$1, %dil
	jne	.LBB33_67	# bb180
.LBB33_45:	# bb165
	testl	%edx, %edx
	jg	.LBB33_64	# bb165.bb179.preheader_crit_edge
.LBB33_46:	# bb166
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
.LBB33_47:	# bb179.preheader
	testl	%r8d, %r8d
	jle	.LBB33_29	# bb111.thread
.LBB33_48:	# bb.nph213
	movl	$1, %ecx
	subl	%r8d, %ecx
	imull	%edx, %ecx
	movl	%ecx, 12(%rsp)
	addl	%eax, %eax
	movl	96(%rsp), %ecx
	leal	2(,%rcx,2), %esi
	movl	%esi, 28(%rsp)
	leal	(%rcx,%rcx), %ecx
	movl	%ecx, 24(%rsp)
	leal	-1(%r8), %ecx
	cvtsi2ss	%r10d, %xmm0
	leal	(%rdx,%rdx), %esi
	movl	%esi, 20(%rsp)
	xorl	%esi, %esi
	movl	%edx, 32(%rsp)
	movl	%esi, %edi
	.align	16
.LBB33_49:	# bb169
	testl	%edx, %edx
	movl	$0, %r10d
	cmovle	12(%rsp), %r10d
	leal	1(%rdi), %ebx
	cmpl	%r8d, %ebx
	jge	.LBB33_65	# bb169.bb175_crit_edge
.LBB33_50:	# bb.nph
	movl	24(%rsp), %ebx
	leal	(%rbx,%rsi), %ebx
	movl	96(%rsp), %r14d
	leal	(%r14,%r14), %r14d
	pxor	%xmm1, %xmm1
	xorl	%r15d, %r15d
	movl	32(%rsp), %r12d
	movaps	%xmm1, %xmm2
	.align	16
.LBB33_51:	# bb173
	movslq	%ebx, %r13
	leal	1(%rbx), %ebp
	movslq	%ebp, %rbp
	movaps	%xmm0, %xmm3
	mulss	(%r9,%rbp,4), %xmm3
	addl	%r10d, %r12d
	leal	1(,%r12,2), %r10d
	leal	(%r12,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r11,%rbp,4), %xmm4
	movaps	%xmm3, %xmm5
	mulss	%xmm4, %xmm5
	movss	(%r9,%r13,4), %xmm6
	movslq	%r10d, %r10
	movss	(%r11,%r10,4), %xmm7
	movaps	%xmm6, %xmm8
	mulss	%xmm7, %xmm8
	addss	%xmm5, %xmm8
	addss	%xmm8, %xmm1
	mulss	%xmm7, %xmm3
	mulss	%xmm4, %xmm6
	subss	%xmm3, %xmm6
	addss	%xmm6, %xmm2
	addl	%r14d, %ebx
	incl	%r15d
	cmpl	%ecx, %r15d
	movl	%edx, %r10d
	jne	.LBB33_51	# bb173
.LBB33_52:	# bb175
	movslq	%eax, %r10
	movss	(%r11,%r10,4), %xmm3
	cmpl	$131, 36(%rsp)
	je	.LBB33_66	# bb176
.LBB33_53:	# bb177
	addss	%xmm2, %xmm3
	movss	%xmm3, (%r11,%r10,4)
	leal	1(%rax), %r10d
	movslq	%r10d, %r10
	addss	(%r11,%r10,4), %xmm1
	movss	%xmm1, (%r11,%r10,4)
.LBB33_54:	# bb178
	addl	20(%rsp), %eax
	addl	%edx, 32(%rsp)
	addl	28(%rsp), %esi
	decl	%ecx
	incl	%edi
	cmpl	%r8d, %edi
	je	.LBB33_29	# bb111.thread
	jmp	.LBB33_49	# bb169
.LBB33_55:	# bb62.bb76.preheader_crit_edge
	xorl	%eax, %eax
	jmp	.LBB33_6	# bb76.preheader
.LBB33_56:	# bb66.bb72_crit_edge
	pxor	%xmm1, %xmm1
	movaps	%xmm1, %xmm2
	jmp	.LBB33_11	# bb72
.LBB33_57:	# bb73
	movslq	%r10d, %r15
	leal	1(%r10), %r12d
	movslq	%r12d, %r12
	movaps	%xmm0, %xmm4
	mulss	(%r9,%r12,4), %xmm4
	leal	1(%rax), %r12d
	movslq	%r12d, %r12
	movss	(%r11,%r12,4), %xmm5
	movaps	%xmm4, %xmm6
	mulss	%xmm5, %xmm6
	movss	(%r9,%r15,4), %xmm7
	movaps	%xmm7, %xmm8
	mulss	%xmm3, %xmm8
	subss	%xmm6, %xmm8
	addss	%xmm1, %xmm8
	movss	%xmm8, (%r11,%r14,4)
	mulss	%xmm5, %xmm7
	mulss	%xmm3, %xmm4
	addss	%xmm7, %xmm4
	addss	%xmm2, %xmm4
	movss	%xmm4, (%r11,%r12,4)
	jmp	.LBB33_13	# bb75
.LBB33_58:	# bb93.bb96_crit_edge
	xorl	%eax, %eax
	jmp	.LBB33_20	# bb96
.LBB33_59:	# bb97.bb103_crit_edge
	pxor	%xmm1, %xmm1
	movaps	%xmm1, %xmm2
	jmp	.LBB33_24	# bb103
.LBB33_60:	# bb104
	movslq	%edi, %rsi
	leal	1(%rdi), %r10d
	movslq	%r10d, %r10
	movaps	%xmm0, %xmm4
	mulss	(%r9,%r10,4), %xmm4
	leal	1(%r14), %r10d
	movslq	%r10d, %r10
	movss	(%r11,%r10,4), %xmm5
	movaps	%xmm4, %xmm6
	mulss	%xmm5, %xmm6
	movss	(%r9,%rsi,4), %xmm7
	movaps	%xmm7, %xmm8
	mulss	%xmm3, %xmm8
	subss	%xmm6, %xmm8
	addss	%xmm1, %xmm8
	movss	%xmm8, (%r11,%rcx,4)
	mulss	%xmm5, %xmm7
	mulss	%xmm3, %xmm4
	addss	%xmm7, %xmm4
	addss	%xmm2, %xmm4
	movss	%xmm4, (%r11,%r10,4)
	jmp	.LBB33_26	# bb106
.LBB33_61:	# bb129.bb132_crit_edge
	xorl	%eax, %eax
	jmp	.LBB33_34	# bb132
.LBB33_62:	# bb133.bb139_crit_edge
	pxor	%xmm1, %xmm1
	movaps	%xmm1, %xmm2
	jmp	.LBB33_38	# bb139
.LBB33_63:	# bb140
	movslq	%esi, %rbx
	leal	1(%rsi), %r14d
	movslq	%r14d, %r14
	movaps	%xmm0, %xmm4
	mulss	(%r9,%r14,4), %xmm4
	leal	1(%rdi), %r14d
	movslq	%r14d, %r14
	movss	(%r11,%r14,4), %xmm5
	movaps	%xmm4, %xmm6
	mulss	%xmm5, %xmm6
	movss	(%r9,%rbx,4), %xmm7
	movaps	%xmm7, %xmm8
	mulss	%xmm3, %xmm8
	subss	%xmm6, %xmm8
	addss	%xmm1, %xmm8
	movss	%xmm8, (%r11,%r10,4)
	mulss	%xmm5, %xmm7
	mulss	%xmm3, %xmm4
	addss	%xmm7, %xmm4
	addss	%xmm2, %xmm4
	movss	%xmm4, (%r11,%r14,4)
	jmp	.LBB33_40	# bb142
.LBB33_64:	# bb165.bb179.preheader_crit_edge
	xorl	%eax, %eax
	jmp	.LBB33_47	# bb179.preheader
.LBB33_65:	# bb169.bb175_crit_edge
	pxor	%xmm1, %xmm1
	movaps	%xmm1, %xmm2
	jmp	.LBB33_52	# bb175
.LBB33_66:	# bb176
	movslq	%esi, %rbx
	leal	1(%rsi), %r14d
	movslq	%r14d, %r14
	movaps	%xmm0, %xmm4
	mulss	(%r9,%r14,4), %xmm4
	leal	1(%rax), %r14d
	movslq	%r14d, %r14
	movss	(%r11,%r14,4), %xmm5
	movaps	%xmm4, %xmm6
	mulss	%xmm5, %xmm6
	movss	(%r9,%rbx,4), %xmm7
	movaps	%xmm7, %xmm8
	mulss	%xmm3, %xmm8
	subss	%xmm6, %xmm8
	addss	%xmm2, %xmm8
	movss	%xmm8, (%r11,%r10,4)
	mulss	%xmm5, %xmm7
	mulss	%xmm3, %xmm4
	addss	%xmm7, %xmm4
	addss	%xmm1, %xmm4
	movss	%xmm4, (%r11,%r14,4)
	jmp	.LBB33_54	# bb178
.LBB33_67:	# bb180
	xorl	%edi, %edi
	leaq	.str47, %rsi
	leaq	.str148, %rdx
	xorb	%al, %al
	call	cblas_xerbla
	jmp	.LBB33_29	# bb111.thread
	.size	cblas_ctrmv, .-cblas_ctrmv
.Leh_func_end24:


	.section	.rodata.cst4,"aM",@progbits,4
	.align	4
.LCPI34_0:					
	.long	1065353216	# float 1.000000e+00
	.text
	.align	16
	.globl	cblas_ctrsm
	.type	cblas_ctrsm,@function
cblas_ctrsm:
.Leh_func_begin25:
.Llabel25:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$56, %rsp
	cmpl	$113, %ecx
	movl	$4294967295, %eax
	movl	$1, %r10d
	cmove	%eax, %r10d
	cmpl	$101, %edi
	movq	120(%rsp), %rax
	movss	4(%rax), %xmm0
	movss	(%rax), %xmm1
	movq	144(%rsp), %rbx
	movl	112(%rsp), %eax
	movl	%r8d, 36(%rsp)
	je	.LBB34_170	# bb90
.LBB34_1:	# bb94
	cmpl	$111, %ecx
	movl	$111, %ecx
	movl	$112, %edi
	cmove	%ecx, %edi
	cmpl	$121, %edx
	movl	$122, %ecx
	movl	$121, %edx
	cmove	%ecx, %edx
	cmpl	$141, %esi
	movl	$142, %ecx
	movl	$141, %esi
	cmove	%ecx, %esi
	movl	%r9d, 52(%rsp)
	movl	%eax, %r9d
.LBB34_2:	# bb104
	movl	%r9d, 28(%rsp)
	cmpl	$121, %edx
	sete	%al
	setne	%cl
	cmpl	$111, %edi
	sete	%r8b
	setne	%r9b
	andb	%al, %r8b
	orb	%cl, %r9b
	testb	%r9b, %r9b
	jne	.LBB34_25	# bb136
.LBB34_3:	# bb104
	cmpl	$141, %esi
	jne	.LBB34_25	# bb136
.LBB34_4:	# bb111
	pxor	%xmm2, %xmm2
	ucomiss	%xmm2, %xmm0
	setnp	%r8b
	sete	%al
	andb	%r8b, %al
	ucomiss	.LCPI34_0(%rip), %xmm1
	setnp	%r8b
	sete	%cl
	andb	%r8b, %cl
	testb	%cl, %al
	jne	.LBB34_171	# bb130.preheader
.LBB34_5:	# bb111
	cmpl	$0, 28(%rsp)
	jle	.LBB34_171	# bb130.preheader
.LBB34_6:	# bb.nph497
	cmpl	$0, 52(%rsp)
	jle	.LBB34_171	# bb130.preheader
.LBB34_7:	# bb116.preheader.preheader
	movl	152(%rsp), %edi
	leal	(%rdi,%rdi), %edi
	xorl	%edx, %edx
	movl	%edx, %esi
	jmp	.LBB34_10	# bb116.preheader
	.align	16
.LBB34_8:	# bb115
	movslq	%eax, %r8
	movss	(%rbx,%r8,4), %xmm2
	movaps	%xmm1, %xmm3
	mulss	%xmm2, %xmm3
	leal	1(%rax), %r9d
	movslq	%r9d, %r9
	movss	(%rbx,%r9,4), %xmm4
	movaps	%xmm0, %xmm5
	mulss	%xmm4, %xmm5
	subss	%xmm5, %xmm3
	movss	%xmm3, (%rbx,%r8,4)
	mulss	%xmm0, %xmm2
	mulss	%xmm1, %xmm4
	addss	%xmm2, %xmm4
	movss	%xmm4, (%rbx,%r9,4)
	addl	$2, %eax
	incl	%ecx
	cmpl	52(%rsp), %ecx
	jne	.LBB34_8	# bb115
.LBB34_9:	# bb117
	addl	%edi, %edx
	incl	%esi
	cmpl	28(%rsp), %esi
	je	.LBB34_171	# bb130.preheader
.LBB34_10:	# bb116.preheader
	xorl	%ecx, %ecx
	movl	%edx, %eax
	jmp	.LBB34_8	# bb115
.LBB34_11:	# bb120
	cmpl	$131, 36(%rsp)
	jne	.LBB34_15	# bb129.preheader
.LBB34_12:	# bb121
	movl	32(%rsp), %edi
	movslq	%edi, %rax
	leal	1(%rdi), %edi
	movslq	%edi, %rcx
	movss	48(%rsp), %xmm0
	movq	128(%rsp), %rdx
	mulss	(%rdx,%rcx,4), %xmm0
	movss	%xmm0, 44(%rsp)
	movss	(%rdx,%rax,4), %xmm0
	movss	%xmm0, 40(%rsp)
	cvtss2sd	44(%rsp), %xmm1
	cvtss2sd	%xmm0, %xmm0
	call	_ZL6xhypotdd51
	cvtsd2ss	%xmm0, %xmm0
	movss	44(%rsp), %xmm1
	divss	%xmm0, %xmm1
	movss	%xmm1, 44(%rsp)
	movss	40(%rsp), %xmm1
	divss	%xmm0, %xmm1
	movss	%xmm1, 40(%rsp)
	cmpl	$0, 52(%rsp)
	jle	.LBB34_15	# bb129.preheader
.LBB34_13:	# bb121.bb122_crit_edge
	xorl	%edi, %edi
	movl	%r14d, %r10d
	.align	16
.LBB34_14:	# bb122
	movslq	%r10d, %rax
	movss	(%rbx,%rax,4), %xmm1
	movaps	%xmm1, %xmm2
	movss	40(%rsp), %xmm3
	mulss	%xmm3, %xmm2
	leal	1(%r10), %ecx
	movslq	%ecx, %rcx
	movss	(%rbx,%rcx,4), %xmm4
	movaps	%xmm4, %xmm5
	movss	44(%rsp), %xmm6
	mulss	%xmm6, %xmm5
	addss	%xmm2, %xmm5
	divss	%xmm0, %xmm5
	movss	%xmm5, (%rbx,%rax,4)
	mulss	%xmm6, %xmm1
	mulss	%xmm3, %xmm4
	subss	%xmm1, %xmm4
	divss	%xmm0, %xmm4
	movss	%xmm4, (%rbx,%rcx,4)
	addl	$2, %r10d
	incl	%edi
	cmpl	52(%rsp), %edi
	jne	.LBB34_14	# bb122
.LBB34_15:	# bb129.preheader
	testl	%r15d, %r15d
	jle	.LBB34_21	# bb130.loopexit404
.LBB34_16:	# bb.nph493
	cmpl	$0, 52(%rsp)
	jle	.LBB34_21	# bb130.loopexit404
.LBB34_17:	# bb.nph493.split
	leal	1(%r14), %edi
	movl	136(%rsp), %r10d
	leal	(%r10,%r10), %r10d
	movl	152(%rsp), %r15d
	leal	(%r15,%r15), %r15d
	movl	28(%rsp), %eax
	leal	-1(%rax), %eax
	xorl	%ecx, %ecx
	movl	24(%rsp), %edx
	movl	%ecx, %esi
	.align	16
.LBB34_18:	# bb125
	leal	-1(%rdx), %r8d
	movslq	%r8d, %r8
	movss	48(%rsp), %xmm0
	movq	128(%rsp), %r9
	mulss	(%r9,%r8,4), %xmm0
	leal	-2(%rdx), %r8d
	movslq	%r8d, %r8
	movss	(%r9,%r8,4), %xmm1
	leal	1(%rcx), %r8d
	xorl	%r9d, %r9d
	movl	%r9d, %r11d
	.align	16
.LBB34_19:	# bb126
	leal	(%rdi,%r9), %r12d
	movslq	%r12d, %r12
	movss	(%rbx,%r12,4), %xmm2
	movaps	%xmm0, %xmm3
	mulss	%xmm2, %xmm3
	leal	(%r14,%r9), %r12d
	movslq	%r12d, %r12
	movss	(%rbx,%r12,4), %xmm4
	movaps	%xmm1, %xmm5
	mulss	%xmm4, %xmm5
	subss	%xmm3, %xmm5
	leal	(%rcx,%r9), %r12d
	movslq	%r12d, %r12
	movss	(%rbx,%r12,4), %xmm3
	subss	%xmm5, %xmm3
	movss	%xmm3, (%rbx,%r12,4)
	mulss	%xmm0, %xmm4
	mulss	%xmm1, %xmm2
	addss	%xmm4, %xmm2
	leal	(%r8,%r9), %r12d
	movslq	%r12d, %r12
	movss	(%rbx,%r12,4), %xmm3
	subss	%xmm2, %xmm3
	movss	%xmm3, (%rbx,%r12,4)
	addl	$2, %r9d
	incl	%r11d
	cmpl	52(%rsp), %r11d
	jne	.LBB34_19	# bb126
.LBB34_20:	# bb128
	addl	%r10d, %edx
	addl	%r15d, %ecx
	incl	%esi
	cmpl	%eax, %esi
	jne	.LBB34_18	# bb125
.LBB34_21:	# bb130.loopexit404
	movl	16(%rsp), %edi
	addl	%edi, 32(%rsp)
	subl	20(%rsp), %r14d
	addl	$4294967294, 24(%rsp)
	decl	28(%rsp)
.LBB34_22:	# bb130
	cmpl	$0, 28(%rsp)
	jle	.LBB34_24	# bb134.thread
.LBB34_23:	# bb131
	movl	28(%rsp), %edi
	leal	-1(%rdi), %r15d
	testl	%edi, %edi
	jne	.LBB34_11	# bb120
.LBB34_24:	# bb134.thread
	addq	$56, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB34_25:	# bb136
	cmpl	$121, %edx
	sete	%al
	setne	%cl
	cmpl	$112, %edi
	sete	%r9b
	setne	%r11b
	andb	%al, %r9b
	orb	%cl, %r11b
	testb	%r11b, %r11b
	jne	.LBB34_47	# bb165
.LBB34_26:	# bb136
	cmpl	$141, %esi
	jne	.LBB34_47	# bb165
.LBB34_27:	# bb144
	pxor	%xmm2, %xmm2
	ucomiss	%xmm2, %xmm0
	setnp	%r8b
	sete	%r9b
	andb	%r8b, %r9b
	ucomiss	.LCPI34_0(%rip), %xmm1
	setnp	%r8b
	sete	%al
	andb	%r8b, %al
	testb	%al, %r9b
	jne	.LBB34_34	# bb164.preheader
.LBB34_28:	# bb144
	cmpl	$0, 28(%rsp)
	jle	.LBB34_34	# bb164.preheader
.LBB34_29:	# bb.nph487
	cmpl	$0, 52(%rsp)
	jle	.LBB34_34	# bb164.preheader
.LBB34_30:	# bb149.preheader.preheader
	movl	152(%rsp), %edi
	leal	(%rdi,%rdi), %esi
	xorl	%eax, %eax
	movl	%eax, %edi
	.align	16
.LBB34_31:	# bb149.preheader
	xorl	%ecx, %ecx
	movl	%eax, %edx
	.align	16
.LBB34_32:	# bb148
	movslq	%edx, %r8
	movss	(%rbx,%r8,4), %xmm2
	movaps	%xmm1, %xmm3
	mulss	%xmm2, %xmm3
	leal	1(%rdx), %r9d
	movslq	%r9d, %r9
	movss	(%rbx,%r9,4), %xmm4
	movaps	%xmm0, %xmm5
	mulss	%xmm4, %xmm5
	subss	%xmm5, %xmm3
	movss	%xmm3, (%rbx,%r8,4)
	mulss	%xmm0, %xmm2
	mulss	%xmm1, %xmm4
	addss	%xmm2, %xmm4
	movss	%xmm4, (%rbx,%r9,4)
	addl	$2, %edx
	incl	%ecx
	cmpl	52(%rsp), %ecx
	jne	.LBB34_32	# bb148
.LBB34_33:	# bb150
	addl	%esi, %eax
	incl	%edi
	cmpl	28(%rsp), %edi
	jne	.LBB34_31	# bb149.preheader
.LBB34_34:	# bb164.preheader
	cmpl	$0, 28(%rsp)
	jle	.LBB34_24	# bb134.thread
.LBB34_35:	# bb.nph483
	movl	136(%rsp), %edi
	leal	2(,%rdi,2), %edi
	movl	%edi, 24(%rsp)
	movl	152(%rsp), %edi
	leal	(%rdi,%rdi), %edi
	movl	%edi, 20(%rsp)
	movl	28(%rsp), %edi
	leal	-1(%rdi), %r14d
	cvtsi2ss	%r10d, %xmm0
	movss	%xmm0, 48(%rsp)
	xorl	%r15d, %r15d
	movl	%r15d, %r12d
	movl	%r15d, 32(%rsp)
	.align	16
.LBB34_36:	# bb153
	cmpl	$131, 36(%rsp)
	jne	.LBB34_40	# bb162.preheader
.LBB34_37:	# bb154
	movslq	%r15d, %rax
	leal	1(%r15), %edi
	movslq	%edi, %rcx
	movss	48(%rsp), %xmm0
	movq	128(%rsp), %rdx
	mulss	(%rdx,%rcx,4), %xmm0
	movss	%xmm0, 44(%rsp)
	movss	(%rdx,%rax,4), %xmm0
	movss	%xmm0, 40(%rsp)
	cvtss2sd	44(%rsp), %xmm1
	cvtss2sd	%xmm0, %xmm0
	call	_ZL6xhypotdd51
	cvtsd2ss	%xmm0, %xmm0
	movss	44(%rsp), %xmm1
	divss	%xmm0, %xmm1
	movss	%xmm1, 44(%rsp)
	movss	40(%rsp), %xmm1
	divss	%xmm0, %xmm1
	movss	%xmm1, 40(%rsp)
	cmpl	$0, 52(%rsp)
	jle	.LBB34_40	# bb162.preheader
.LBB34_38:	# bb154.bb155_crit_edge
	xorl	%edi, %edi
	movl	%r12d, %r10d
	.align	16
.LBB34_39:	# bb155
	movslq	%r10d, %rax
	movss	(%rbx,%rax,4), %xmm1
	movaps	%xmm1, %xmm2
	movss	40(%rsp), %xmm3
	mulss	%xmm3, %xmm2
	leal	1(%r10), %ecx
	movslq	%ecx, %rcx
	movss	(%rbx,%rcx,4), %xmm4
	movaps	%xmm4, %xmm5
	movss	44(%rsp), %xmm6
	mulss	%xmm6, %xmm5
	addss	%xmm2, %xmm5
	divss	%xmm0, %xmm5
	movss	%xmm5, (%rbx,%rax,4)
	mulss	%xmm6, %xmm1
	mulss	%xmm3, %xmm4
	subss	%xmm1, %xmm4
	divss	%xmm0, %xmm4
	movss	%xmm4, (%rbx,%rcx,4)
	addl	$2, %r10d
	incl	%edi
	cmpl	52(%rsp), %edi
	jne	.LBB34_39	# bb155
.LBB34_40:	# bb162.preheader
	movl	32(%rsp), %edi
	leal	1(%rdi), %edi
	cmpl	28(%rsp), %edi
	jge	.LBB34_46	# bb163
.LBB34_41:	# bb.nph481
	cmpl	$0, 52(%rsp)
	jle	.LBB34_46	# bb163
.LBB34_42:	# bb.nph481.split
	movl	152(%rsp), %edi
	leal	(%rdi,%rdi), %edi
	xorl	%r10d, %r10d
	movl	%edi, %eax
	movl	%r15d, %ecx
	.align	16
.LBB34_43:	# bb158
	leal	3(%rcx), %edx
	movslq	%edx, %rdx
	movss	48(%rsp), %xmm0
	movq	128(%rsp), %rsi
	mulss	(%rsi,%rdx,4), %xmm0
	leal	2(%rcx), %edx
	movslq	%edx, %rdx
	movss	(%rsi,%rdx,4), %xmm1
	leal	1(%rax), %edx
	xorl	%esi, %esi
	movl	%r12d, %r8d
	.align	16
.LBB34_44:	# bb159
	movslq	%r8d, %r9
	movss	(%rbx,%r9,4), %xmm2
	movaps	%xmm1, %xmm3
	mulss	%xmm2, %xmm3
	leal	1(%r8), %r9d
	movslq	%r9d, %r9
	movss	(%rbx,%r9,4), %xmm4
	movaps	%xmm0, %xmm5
	mulss	%xmm4, %xmm5
	subss	%xmm5, %xmm3
	leal	(%rax,%r8), %r9d
	movslq	%r9d, %r9
	movss	(%rbx,%r9,4), %xmm5
	subss	%xmm3, %xmm5
	movss	%xmm5, (%rbx,%r9,4)
	mulss	%xmm0, %xmm2
	mulss	%xmm1, %xmm4
	addss	%xmm2, %xmm4
	leal	(%rdx,%r8), %r9d
	movslq	%r9d, %r9
	movss	(%rbx,%r9,4), %xmm2
	subss	%xmm4, %xmm2
	movss	%xmm2, (%rbx,%r9,4)
	addl	$2, %r8d
	incl	%esi
	cmpl	52(%rsp), %esi
	jne	.LBB34_44	# bb159
.LBB34_45:	# bb162.loopexit
	addl	%edi, %eax
	addl	$2, %ecx
	incl	%r10d
	cmpl	%r14d, %r10d
	jne	.LBB34_43	# bb158
.LBB34_46:	# bb163
	addl	24(%rsp), %r15d
	addl	20(%rsp), %r12d
	decl	%r14d
	movl	32(%rsp), %edi
	incl	%edi
	movl	%edi, 32(%rsp)
	cmpl	28(%rsp), %edi
	je	.LBB34_24	# bb134.thread
	jmp	.LBB34_36	# bb153
.LBB34_47:	# bb165
	cmpl	$122, %edx
	sete	%al
	setne	%cl
	cmpl	$111, %edi
	sete	%r11b
	setne	%r14b
	andb	%al, %r11b
	orb	%cl, %r14b
	testb	%r14b, %r14b
	jne	.LBB34_69	# bb194
.LBB34_48:	# bb165
	cmpl	$141, %esi
	jne	.LBB34_69	# bb194
.LBB34_49:	# bb173
	pxor	%xmm2, %xmm2
	ucomiss	%xmm2, %xmm0
	setnp	%r8b
	sete	%r9b
	andb	%r8b, %r9b
	ucomiss	.LCPI34_0(%rip), %xmm1
	setnp	%r8b
	sete	%r11b
	andb	%r8b, %r11b
	testb	%r11b, %r9b
	jne	.LBB34_56	# bb193.preheader
.LBB34_50:	# bb173
	cmpl	$0, 28(%rsp)
	jle	.LBB34_56	# bb193.preheader
.LBB34_51:	# bb.nph473
	cmpl	$0, 52(%rsp)
	jle	.LBB34_56	# bb193.preheader
.LBB34_52:	# bb178.preheader.preheader
	movl	152(%rsp), %edi
	leal	(%rdi,%rdi), %esi
	xorl	%eax, %eax
	movl	%eax, %edi
	.align	16
.LBB34_53:	# bb178.preheader
	xorl	%ecx, %ecx
	movl	%eax, %edx
	.align	16
.LBB34_54:	# bb177
	movslq	%edx, %r8
	movss	(%rbx,%r8,4), %xmm2
	movaps	%xmm1, %xmm3
	mulss	%xmm2, %xmm3
	leal	1(%rdx), %r9d
	movslq	%r9d, %r9
	movss	(%rbx,%r9,4), %xmm4
	movaps	%xmm0, %xmm5
	mulss	%xmm4, %xmm5
	subss	%xmm5, %xmm3
	movss	%xmm3, (%rbx,%r8,4)
	mulss	%xmm0, %xmm2
	mulss	%xmm1, %xmm4
	addss	%xmm2, %xmm4
	movss	%xmm4, (%rbx,%r9,4)
	addl	$2, %edx
	incl	%ecx
	cmpl	52(%rsp), %ecx
	jne	.LBB34_54	# bb177
.LBB34_55:	# bb179
	addl	%esi, %eax
	incl	%edi
	cmpl	28(%rsp), %edi
	jne	.LBB34_53	# bb178.preheader
.LBB34_56:	# bb193.preheader
	cmpl	$0, 28(%rsp)
	jle	.LBB34_24	# bb134.thread
.LBB34_57:	# bb.nph469
	movl	136(%rsp), %edi
	leal	2(,%rdi,2), %eax
	movl	%eax, 24(%rsp)
	leal	(%rdi,%rdi), %edi
	movl	%edi, 20(%rsp)
	movl	152(%rsp), %edi
	leal	(%rdi,%rdi), %edi
	movl	%edi, 16(%rsp)
	movl	28(%rsp), %edi
	leal	-1(%rdi), %r14d
	cvtsi2ss	%r10d, %xmm0
	movss	%xmm0, 48(%rsp)
	xorl	%r15d, %r15d
	movl	%r15d, %r12d
	movl	%r15d, 32(%rsp)
	.align	16
.LBB34_58:	# bb182
	cmpl	$131, 36(%rsp)
	jne	.LBB34_62	# bb191.preheader
.LBB34_59:	# bb183
	movslq	%r15d, %rax
	leal	1(%r15), %edi
	movslq	%edi, %rcx
	movss	48(%rsp), %xmm0
	movq	128(%rsp), %rdx
	mulss	(%rdx,%rcx,4), %xmm0
	movss	%xmm0, 44(%rsp)
	movss	(%rdx,%rax,4), %xmm0
	movss	%xmm0, 40(%rsp)
	cvtss2sd	44(%rsp), %xmm1
	cvtss2sd	%xmm0, %xmm0
	call	_ZL6xhypotdd51
	cvtsd2ss	%xmm0, %xmm0
	movss	44(%rsp), %xmm1
	divss	%xmm0, %xmm1
	movss	%xmm1, 44(%rsp)
	movss	40(%rsp), %xmm1
	divss	%xmm0, %xmm1
	movss	%xmm1, 40(%rsp)
	cmpl	$0, 52(%rsp)
	jle	.LBB34_62	# bb191.preheader
.LBB34_60:	# bb183.bb184_crit_edge
	xorl	%edi, %edi
	movl	%r12d, %r10d
	.align	16
.LBB34_61:	# bb184
	movslq	%r10d, %rax
	movss	(%rbx,%rax,4), %xmm1
	movaps	%xmm1, %xmm2
	movss	40(%rsp), %xmm3
	mulss	%xmm3, %xmm2
	leal	1(%r10), %ecx
	movslq	%ecx, %rcx
	movss	(%rbx,%rcx,4), %xmm4
	movaps	%xmm4, %xmm5
	movss	44(%rsp), %xmm6
	mulss	%xmm6, %xmm5
	addss	%xmm2, %xmm5
	divss	%xmm0, %xmm5
	movss	%xmm5, (%rbx,%rax,4)
	mulss	%xmm6, %xmm1
	mulss	%xmm3, %xmm4
	subss	%xmm1, %xmm4
	divss	%xmm0, %xmm4
	movss	%xmm4, (%rbx,%rcx,4)
	addl	$2, %r10d
	incl	%edi
	cmpl	52(%rsp), %edi
	jne	.LBB34_61	# bb184
.LBB34_62:	# bb191.preheader
	movl	32(%rsp), %edi
	leal	1(%rdi), %edi
	cmpl	28(%rsp), %edi
	jge	.LBB34_68	# bb192
.LBB34_63:	# bb.nph467
	cmpl	$0, 52(%rsp)
	jle	.LBB34_68	# bb192
.LBB34_64:	# bb.nph467.split
	movl	20(%rsp), %edi
	leal	(%rdi,%r15), %edi
	movl	136(%rsp), %r10d
	leal	(%r10,%r10), %r10d
	movl	152(%rsp), %eax
	leal	(%rax,%rax), %eax
	xorl	%ecx, %ecx
	movl	%eax, %edx
	.align	16
.LBB34_65:	# bb187
	movslq	%edi, %rsi
	leal	1(%rdi), %r8d
	movslq	%r8d, %r8
	movss	48(%rsp), %xmm0
	movq	128(%rsp), %r9
	mulss	(%r9,%r8,4), %xmm0
	movss	(%r9,%rsi,4), %xmm1
	leal	1(%rdx), %esi
	xorl	%r8d, %r8d
	movl	%r12d, %r9d
	.align	16
.LBB34_66:	# bb188
	movslq	%r9d, %r11
	movss	(%rbx,%r11,4), %xmm2
	movaps	%xmm1, %xmm3
	mulss	%xmm2, %xmm3
	leal	1(%r9), %r11d
	movslq	%r11d, %r11
	movss	(%rbx,%r11,4), %xmm4
	movaps	%xmm0, %xmm5
	mulss	%xmm4, %xmm5
	subss	%xmm5, %xmm3
	leal	(%rdx,%r9), %r11d
	movslq	%r11d, %r11
	movss	(%rbx,%r11,4), %xmm5
	subss	%xmm3, %xmm5
	movss	%xmm5, (%rbx,%r11,4)
	mulss	%xmm0, %xmm2
	mulss	%xmm1, %xmm4
	addss	%xmm2, %xmm4
	leal	(%rsi,%r9), %r11d
	movslq	%r11d, %r11
	movss	(%rbx,%r11,4), %xmm2
	subss	%xmm4, %xmm2
	movss	%xmm2, (%rbx,%r11,4)
	addl	$2, %r9d
	incl	%r8d
	cmpl	52(%rsp), %r8d
	jne	.LBB34_66	# bb188
.LBB34_67:	# bb191.loopexit
	addl	%r10d, %edi
	addl	%eax, %edx
	incl	%ecx
	cmpl	%r14d, %ecx
	jne	.LBB34_65	# bb187
.LBB34_68:	# bb192
	addl	24(%rsp), %r15d
	addl	16(%rsp), %r12d
	decl	%r14d
	movl	32(%rsp), %edi
	incl	%edi
	movl	%edi, 32(%rsp)
	cmpl	28(%rsp), %edi
	je	.LBB34_24	# bb134.thread
	jmp	.LBB34_58	# bb182
.LBB34_69:	# bb194
	cmpl	$122, %edx
	sete	%al
	setne	%cl
	cmpl	$112, %edi
	sete	%dl
	setne	%dil
	andb	%al, %dl
	orb	%cl, %dil
	testb	%dil, %dil
	jne	.LBB34_91	# bb227
.LBB34_70:	# bb194
	cmpl	$141, %esi
	jne	.LBB34_91	# bb227
.LBB34_71:	# bb202
	pxor	%xmm2, %xmm2
	ucomiss	%xmm2, %xmm0
	setnp	%dl
	sete	%r8b
	andb	%dl, %r8b
	ucomiss	.LCPI34_0(%rip), %xmm1
	setnp	%dl
	sete	%r9b
	andb	%dl, %r9b
	testb	%r9b, %r8b
	jne	.LBB34_172	# bb221.preheader
.LBB34_72:	# bb202
	cmpl	$0, 28(%rsp)
	jle	.LBB34_172	# bb221.preheader
.LBB34_73:	# bb.nph459
	cmpl	$0, 52(%rsp)
	jle	.LBB34_172	# bb221.preheader
.LBB34_74:	# bb207.preheader.preheader
	movl	152(%rsp), %eax
	leal	(%rax,%rax), %eax
	xorl	%edx, %edx
	movl	%edx, %ecx
	jmp	.LBB34_77	# bb207.preheader
	.align	16
.LBB34_75:	# bb206
	movslq	%edi, %r8
	movss	(%rbx,%r8,4), %xmm2
	movaps	%xmm1, %xmm3
	mulss	%xmm2, %xmm3
	leal	1(%rdi), %r9d
	movslq	%r9d, %r9
	movss	(%rbx,%r9,4), %xmm4
	movaps	%xmm0, %xmm5
	mulss	%xmm4, %xmm5
	subss	%xmm5, %xmm3
	movss	%xmm3, (%rbx,%r8,4)
	mulss	%xmm0, %xmm2
	mulss	%xmm1, %xmm4
	addss	%xmm2, %xmm4
	movss	%xmm4, (%rbx,%r9,4)
	addl	$2, %edi
	incl	%esi
	cmpl	52(%rsp), %esi
	jne	.LBB34_75	# bb206
.LBB34_76:	# bb208
	addl	%eax, %edx
	incl	%ecx
	cmpl	28(%rsp), %ecx
	je	.LBB34_172	# bb221.preheader
.LBB34_77:	# bb207.preheader
	xorl	%esi, %esi
	movl	%edx, %edi
	jmp	.LBB34_75	# bb206
.LBB34_78:	# bb211
	cmpl	$131, 36(%rsp)
	jne	.LBB34_82	# bb220.preheader
.LBB34_79:	# bb212
	movslq	%r12d, %rax
	leal	1(%r12), %r10d
	movslq	%r10d, %rcx
	movss	48(%rsp), %xmm0
	movq	128(%rsp), %rdx
	mulss	(%rdx,%rcx,4), %xmm0
	movss	%xmm0, 44(%rsp)
	movss	(%rdx,%rax,4), %xmm0
	movss	%xmm0, 40(%rsp)
	cvtss2sd	44(%rsp), %xmm1
	cvtss2sd	%xmm0, %xmm0
	call	_ZL6xhypotdd51
	cvtsd2ss	%xmm0, %xmm0
	movss	44(%rsp), %xmm1
	divss	%xmm0, %xmm1
	movss	%xmm1, 44(%rsp)
	movss	40(%rsp), %xmm1
	divss	%xmm0, %xmm1
	movss	%xmm1, 40(%rsp)
	cmpl	$0, 52(%rsp)
	jle	.LBB34_82	# bb220.preheader
.LBB34_80:	# bb212.bb213_crit_edge
	xorl	%r10d, %r10d
	movl	%r15d, %eax
	.align	16
.LBB34_81:	# bb213
	movslq	%eax, %rcx
	movss	(%rbx,%rcx,4), %xmm1
	movaps	%xmm1, %xmm2
	movss	40(%rsp), %xmm3
	mulss	%xmm3, %xmm2
	leal	1(%rax), %edx
	movslq	%edx, %rdx
	movss	(%rbx,%rdx,4), %xmm4
	movaps	%xmm4, %xmm5
	movss	44(%rsp), %xmm6
	mulss	%xmm6, %xmm5
	addss	%xmm2, %xmm5
	divss	%xmm0, %xmm5
	movss	%xmm5, (%rbx,%rcx,4)
	mulss	%xmm6, %xmm1
	mulss	%xmm3, %xmm4
	subss	%xmm1, %xmm4
	divss	%xmm0, %xmm4
	movss	%xmm4, (%rbx,%rdx,4)
	addl	$2, %eax
	incl	%r10d
	cmpl	52(%rsp), %r10d
	jne	.LBB34_81	# bb213
.LBB34_82:	# bb220.preheader
	testl	%r14d, %r14d
	jle	.LBB34_88	# bb221.loopexit406
.LBB34_83:	# bb.nph455
	cmpl	$0, 52(%rsp)
	jle	.LBB34_88	# bb221.loopexit406
.LBB34_84:	# bb.nph455.split
	leal	1(%r15), %r10d
	movl	152(%rsp), %r14d
	leal	(%r14,%r14), %r14d
	movl	28(%rsp), %eax
	leal	-1(%rax), %eax
	xorl	%ecx, %ecx
	movl	32(%rsp), %edx
	movl	%ecx, %esi
	.align	16
.LBB34_85:	# bb216
	movslq	%edx, %rdi
	leal	1(%rdx), %r8d
	movslq	%r8d, %r8
	movss	48(%rsp), %xmm0
	movq	128(%rsp), %r9
	mulss	(%r9,%r8,4), %xmm0
	movss	(%r9,%rdi,4), %xmm1
	leal	1(%rcx), %edi
	xorl	%r8d, %r8d
	movl	%r8d, %r9d
	.align	16
.LBB34_86:	# bb217
	leal	(%r10,%r8), %r11d
	movslq	%r11d, %r11
	movss	(%rbx,%r11,4), %xmm2
	movaps	%xmm0, %xmm3
	mulss	%xmm2, %xmm3
	leal	(%r15,%r8), %r11d
	movslq	%r11d, %r11
	movss	(%rbx,%r11,4), %xmm4
	movaps	%xmm1, %xmm5
	mulss	%xmm4, %xmm5
	subss	%xmm3, %xmm5
	leal	(%rcx,%r8), %r11d
	movslq	%r11d, %r11
	movss	(%rbx,%r11,4), %xmm3
	subss	%xmm5, %xmm3
	movss	%xmm3, (%rbx,%r11,4)
	mulss	%xmm0, %xmm4
	mulss	%xmm1, %xmm2
	addss	%xmm4, %xmm2
	leal	(%rdi,%r8), %r11d
	movslq	%r11d, %r11
	movss	(%rbx,%r11,4), %xmm3
	subss	%xmm2, %xmm3
	movss	%xmm3, (%rbx,%r11,4)
	addl	$2, %r8d
	incl	%r9d
	cmpl	52(%rsp), %r9d
	jne	.LBB34_86	# bb217
.LBB34_87:	# bb219
	addl	%r14d, %ecx
	addl	$2, %edx
	incl	%esi
	cmpl	%eax, %esi
	jne	.LBB34_85	# bb216
.LBB34_88:	# bb221.loopexit406
	addl	20(%rsp), %r12d
	movl	32(%rsp), %r10d
	subl	(%rsp), %r10d
	movl	%r10d, 32(%rsp)
	subl	24(%rsp), %r15d
	decl	28(%rsp)
.LBB34_89:	# bb221
	cmpl	$0, 28(%rsp)
	jle	.LBB34_24	# bb134.thread
.LBB34_90:	# bb222
	movl	28(%rsp), %r10d
	leal	-1(%r10), %r14d
	testl	%r10d, %r10d
	jne	.LBB34_78	# bb211
	jmp	.LBB34_24	# bb134.thread
.LBB34_91:	# bb227
	cmpl	$142, %esi
	setne	%al
	notb	%r8b
	orb	%al, %r8b
	testb	$1, %r8b
	jne	.LBB34_110	# bb254
.LBB34_92:	# bb235
	pxor	%xmm2, %xmm2
	ucomiss	%xmm2, %xmm0
	setnp	%dl
	sete	%r9b
	andb	%dl, %r9b
	ucomiss	.LCPI34_0(%rip), %xmm1
	setnp	%dl
	sete	%r11b
	andb	%dl, %r11b
	testb	%r11b, %r9b
	jne	.LBB34_106	# bb253.preheader
.LBB34_93:	# bb235
	cmpl	$0, 28(%rsp)
	jle	.LBB34_106	# bb253.preheader
.LBB34_94:	# bb.nph449
	cmpl	$0, 52(%rsp)
	jle	.LBB34_106	# bb253.preheader
.LBB34_95:	# bb240.preheader.preheader
	movl	152(%rsp), %eax
	leal	(%rax,%rax), %edx
	xorl	%edi, %edi
	movl	%edi, %ecx
	jmp	.LBB34_98	# bb240.preheader
	.align	16
.LBB34_96:	# bb239
	movslq	%eax, %r8
	movss	(%rbx,%r8,4), %xmm2
	movaps	%xmm1, %xmm3
	mulss	%xmm2, %xmm3
	leal	1(%rax), %r9d
	movslq	%r9d, %r9
	movss	(%rbx,%r9,4), %xmm4
	movaps	%xmm0, %xmm5
	mulss	%xmm4, %xmm5
	subss	%xmm5, %xmm3
	movss	%xmm3, (%rbx,%r8,4)
	mulss	%xmm0, %xmm2
	mulss	%xmm1, %xmm4
	addss	%xmm2, %xmm4
	movss	%xmm4, (%rbx,%r9,4)
	addl	$2, %eax
	incl	%esi
	cmpl	52(%rsp), %esi
	jne	.LBB34_96	# bb239
.LBB34_97:	# bb241
	addl	%edx, %edi
	incl	%ecx
	cmpl	28(%rsp), %ecx
	je	.LBB34_106	# bb253.preheader
.LBB34_98:	# bb240.preheader
	xorl	%esi, %esi
	movl	%edi, %eax
	jmp	.LBB34_96	# bb239
	.align	16
.LBB34_99:	# bb245
	cmpl	$131, 36(%rsp)
	jne	.LBB34_101	# bb247
.LBB34_100:	# bb246
	movslq	%r15d, %rax
	leal	1(%r15), %r10d
	movslq	%r10d, %rcx
	movss	44(%rsp), %xmm0
	movq	128(%rsp), %rdx
	mulss	(%rdx,%rcx,4), %xmm0
	movss	%xmm0, 40(%rsp)
	movss	(%rdx,%rax,4), %xmm0
	movss	%xmm0, 48(%rsp)
	cvtss2sd	40(%rsp), %xmm1
	cvtss2sd	%xmm0, %xmm0
	call	_ZL6xhypotdd51
	cvtsd2ss	%xmm0, %xmm0
	movss	40(%rsp), %xmm1
	divss	%xmm0, %xmm1
	movss	%xmm1, 40(%rsp)
	movslq	%r14d, %rax
	leal	1(%r14), %r10d
	movslq	%r10d, %rcx
	movss	(%rbx,%rcx,4), %xmm1
	movaps	%xmm1, %xmm2
	mulss	40(%rsp), %xmm2
	movss	48(%rsp), %xmm3
	divss	%xmm0, %xmm3
	movss	%xmm3, 48(%rsp)
	movss	(%rbx,%rax,4), %xmm3
	movaps	%xmm3, %xmm4
	mulss	48(%rsp), %xmm4
	addss	%xmm2, %xmm4
	divss	%xmm0, %xmm4
	movss	%xmm4, (%rbx,%rax,4)
	movss	40(%rsp), %xmm2
	mulss	%xmm3, %xmm2
	movss	%xmm2, 40(%rsp)
	movss	48(%rsp), %xmm2
	mulss	%xmm1, %xmm2
	subss	40(%rsp), %xmm2
	divss	%xmm0, %xmm2
	movss	%xmm2, (%rbx,%rcx,4)
.LBB34_101:	# bb247
	movslq	%r14d, %rax
	leal	1(%r14), %r10d
	leal	1(%r13), %ecx
	cmpl	52(%rsp), %ecx
	movss	(%rbx,%rax,4), %xmm0
	movslq	%r10d, %rax
	movss	(%rbx,%rax,4), %xmm1
	jge	.LBB34_104	# bb250
.LBB34_102:	# bb.nph441
	leal	3(%r14), %r10d
	leal	2(%r14), %eax
	leal	3(%r15), %ecx
	leal	2(%r15), %edx
	xorl	%esi, %esi
	movl	%esi, %edi
	.align	16
.LBB34_103:	# bb248
	leal	(%rcx,%rsi), %r8d
	movslq	%r8d, %r8
	movss	44(%rsp), %xmm2
	movq	128(%rsp), %r9
	mulss	(%r9,%r8,4), %xmm2
	movaps	%xmm2, %xmm3
	mulss	%xmm1, %xmm3
	leal	(%rdx,%rsi), %r8d
	movslq	%r8d, %r8
	movss	(%r9,%r8,4), %xmm4
	movaps	%xmm4, %xmm5
	mulss	%xmm0, %xmm5
	subss	%xmm3, %xmm5
	leal	(%rax,%rsi), %r8d
	movslq	%r8d, %r8
	movss	(%rbx,%r8,4), %xmm3
	subss	%xmm5, %xmm3
	movss	%xmm3, (%rbx,%r8,4)
	mulss	%xmm0, %xmm2
	mulss	%xmm1, %xmm4
	addss	%xmm2, %xmm4
	leal	(%r10,%rsi), %r8d
	movslq	%r8d, %r8
	movss	(%rbx,%r8,4), %xmm2
	subss	%xmm4, %xmm2
	movss	%xmm2, (%rbx,%r8,4)
	addl	$2, %esi
	incl	%edi
	cmpl	%r12d, %edi
	jne	.LBB34_103	# bb248
.LBB34_104:	# bb250
	addl	32(%rsp), %r15d
	addl	$2, %r14d
	decl	%r12d
	incl	%r13d
	cmpl	52(%rsp), %r13d
	jne	.LBB34_99	# bb245
.LBB34_105:	# bb252
	movl	24(%rsp), %r10d
	addl	12(%rsp), %r10d
	movl	%r10d, 24(%rsp)
	movl	20(%rsp), %r10d
	incl	%r10d
	movl	%r10d, 20(%rsp)
	cmpl	28(%rsp), %r10d
	je	.LBB34_24	# bb134.thread
	jmp	.LBB34_109	# bb251.preheader
.LBB34_106:	# bb253.preheader
	cmpl	$0, 28(%rsp)
	jle	.LBB34_24	# bb134.thread
.LBB34_107:	# bb.nph445
	cmpl	$0, 52(%rsp)
	cvtsi2ss	%r10d, %xmm0
	movss	%xmm0, 44(%rsp)
	jle	.LBB34_24	# bb134.thread
.LBB34_108:	# bb251.preheader.preheader
	movl	152(%rsp), %r10d
	addl	%r10d, %r10d
	movl	%r10d, 12(%rsp)
	xorl	%r10d, %r10d
	movl	%r10d, 24(%rsp)
	movl	%r10d, 20(%rsp)
	.align	16
.LBB34_109:	# bb251.preheader
	movl	136(%rsp), %r10d
	leal	2(,%r10,2), %r10d
	movl	%r10d, 32(%rsp)
	movl	52(%rsp), %r10d
	leal	-1(%r10), %r12d
	xorl	%r15d, %r15d
	movl	24(%rsp), %r14d
	movl	%r15d, %r13d
	jmp	.LBB34_99	# bb245
.LBB34_110:	# bb254
	cmpl	$142, %esi
	setne	%al
	notb	%r9b
	orb	%al, %r9b
	testb	$1, %r9b
	jne	.LBB34_130	# bb285
.LBB34_111:	# bb262
	pxor	%xmm2, %xmm2
	ucomiss	%xmm2, %xmm0
	setnp	%dl
	sete	%r11b
	andb	%dl, %r11b
	ucomiss	.LCPI34_0(%rip), %xmm1
	setnp	%dl
	sete	%al
	andb	%dl, %al
	testb	%al, %r11b
	jne	.LBB34_127	# bb284.preheader
.LBB34_112:	# bb262
	cmpl	$0, 28(%rsp)
	jle	.LBB34_127	# bb284.preheader
.LBB34_113:	# bb.nph437
	cmpl	$0, 52(%rsp)
	jle	.LBB34_127	# bb284.preheader
.LBB34_114:	# bb267.preheader.preheader
	movl	152(%rsp), %eax
	leal	(%rax,%rax), %edx
	xorl	%eax, %eax
	movl	%eax, %edi
	jmp	.LBB34_117	# bb267.preheader
	.align	16
.LBB34_115:	# bb266
	movslq	%ecx, %r8
	movss	(%rbx,%r8,4), %xmm2
	movaps	%xmm1, %xmm3
	mulss	%xmm2, %xmm3
	leal	1(%rcx), %r9d
	movslq	%r9d, %r9
	movss	(%rbx,%r9,4), %xmm4
	movaps	%xmm0, %xmm5
	mulss	%xmm4, %xmm5
	subss	%xmm5, %xmm3
	movss	%xmm3, (%rbx,%r8,4)
	mulss	%xmm0, %xmm2
	mulss	%xmm1, %xmm4
	addss	%xmm2, %xmm4
	movss	%xmm4, (%rbx,%r9,4)
	addl	$2, %ecx
	incl	%esi
	cmpl	52(%rsp), %esi
	jne	.LBB34_115	# bb266
.LBB34_116:	# bb268
	addl	%edx, %eax
	incl	%edi
	cmpl	28(%rsp), %edi
	je	.LBB34_127	# bb284.preheader
.LBB34_117:	# bb267.preheader
	xorl	%esi, %esi
	movl	%eax, %ecx
	jmp	.LBB34_115	# bb266
.LBB34_118:	# bb272
	cmpl	$131, 36(%rsp)
	jne	.LBB34_120	# bb274
.LBB34_119:	# bb273
	movslq	%r15d, %rax
	leal	1(%r15), %r10d
	movslq	%r10d, %rcx
	movss	44(%rsp), %xmm0
	movq	128(%rsp), %rdx
	mulss	(%rdx,%rcx,4), %xmm0
	movss	%xmm0, 40(%rsp)
	movss	(%rdx,%rax,4), %xmm0
	movss	%xmm0, 48(%rsp)
	cvtss2sd	40(%rsp), %xmm1
	cvtss2sd	%xmm0, %xmm0
	call	_ZL6xhypotdd51
	cvtsd2ss	%xmm0, %xmm0
	movss	40(%rsp), %xmm1
	divss	%xmm0, %xmm1
	movss	%xmm1, 40(%rsp)
	movl	32(%rsp), %r10d
	leal	(%r10,%r13), %r10d
	movslq	%r10d, %rax
	movss	(%rbx,%rax,4), %xmm1
	movaps	%xmm1, %xmm2
	mulss	40(%rsp), %xmm2
	movss	48(%rsp), %xmm3
	divss	%xmm0, %xmm3
	movss	%xmm3, 48(%rsp)
	movl	24(%rsp), %r10d
	leal	(%r10,%r13), %r10d
	movslq	%r10d, %rcx
	movss	(%rbx,%rcx,4), %xmm3
	movaps	%xmm3, %xmm4
	mulss	48(%rsp), %xmm4
	addss	%xmm2, %xmm4
	divss	%xmm0, %xmm4
	movss	%xmm4, (%rbx,%rcx,4)
	mulss	40(%rsp), %xmm3
	movss	48(%rsp), %xmm2
	mulss	%xmm1, %xmm2
	subss	%xmm3, %xmm2
	divss	%xmm0, %xmm2
	movss	%xmm2, (%rbx,%rax,4)
.LBB34_120:	# bb274
	movl	32(%rsp), %r10d
	leal	(%r10,%r13), %r10d
	movslq	%r10d, %rax
	movss	(%rbx,%rax,4), %xmm0
	movl	24(%rsp), %r10d
	leal	(%r10,%r13), %r10d
	movslq	%r10d, %rax
	movss	(%rbx,%rax,4), %xmm1
	testl	%r12d, %r12d
	jle	.LBB34_123	# bb277.loopexit
.LBB34_121:	# bb.nph431
	movl	136(%rsp), %r10d
	leal	(%r10,%r10), %r10d
	leal	-1(%r14), %r12d
	xorl	%eax, %eax
	movl	%r13d, %ecx
	movl	20(%rsp), %edx
	.align	16
.LBB34_122:	# bb275
	leal	-1(%rcx), %esi
	movslq	%esi, %rsi
	movss	44(%rsp), %xmm2
	movq	128(%rsp), %rdi
	mulss	(%rdi,%rsi,4), %xmm2
	movaps	%xmm2, %xmm3
	mulss	%xmm0, %xmm3
	leal	-2(%rcx), %esi
	movslq	%esi, %rsi
	movss	(%rdi,%rsi,4), %xmm4
	movaps	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	subss	%xmm3, %xmm5
	movslq	%edx, %rsi
	movss	(%rbx,%rsi,4), %xmm3
	subss	%xmm5, %xmm3
	movss	%xmm3, (%rbx,%rsi,4)
	mulss	%xmm1, %xmm2
	mulss	%xmm0, %xmm4
	addss	%xmm2, %xmm4
	leal	1(%rdx), %esi
	movslq	%esi, %rsi
	movss	(%rbx,%rsi,4), %xmm2
	subss	%xmm4, %xmm2
	movss	%xmm2, (%rbx,%rsi,4)
	addl	%r10d, %ecx
	addl	$2, %edx
	incl	%eax
	cmpl	%r12d, %eax
	jne	.LBB34_122	# bb275
.LBB34_123:	# bb277.loopexit
	addl	16(%rsp), %r15d
	addl	$4294967294, %r13d
	decl	%r14d
.LBB34_124:	# bb277
	testl	%r14d, %r14d
	jle	.LBB34_126	# bb283
.LBB34_125:	# bb278
	leal	-1(%r14), %r12d
	testl	%r14d, %r14d
	jne	.LBB34_118	# bb272
.LBB34_126:	# bb283
	movl	20(%rsp), %r10d
	addl	12(%rsp), %r10d
	movl	%r10d, 20(%rsp)
	movl	8(%rsp), %r10d
	incl	%r10d
	movl	%r10d, 8(%rsp)
	cmpl	28(%rsp), %r10d
	je	.LBB34_24	# bb134.thread
	jmp	.LBB34_129	# bb277.preheader
.LBB34_127:	# bb284.preheader
	cmpl	$0, 28(%rsp)
	jle	.LBB34_24	# bb134.thread
.LBB34_128:	# bb.nph433
	movl	152(%rsp), %r14d
	addl	%r14d, %r14d
	movl	%r14d, 12(%rsp)
	cvtsi2ss	%r10d, %xmm0
	movss	%xmm0, 44(%rsp)
	xorl	%r10d, %r10d
	movl	%r10d, 20(%rsp)
	movl	%r10d, 8(%rsp)
	.align	16
.LBB34_129:	# bb277.preheader
	movl	136(%rsp), %r10d
	leal	2(,%r10,2), %r14d
	leal	(%r10,%r10), %r10d
	movl	$4294967294, %r15d
	subl	%r10d, %r15d
	movl	%r15d, 16(%rsp)
	movl	52(%rsp), %r10d
	leal	-1(%r10), %r15d
	imull	%r14d, %r15d
	movl	20(%rsp), %r14d
	leal	-1(%r14), %r13d
	movl	%r13d, 32(%rsp)
	leal	-2(%r14), %r14d
	movl	%r14d, 24(%rsp)
	leal	(%r10,%r10), %r13d
	movl	%r10d, %r14d
	jmp	.LBB34_124	# bb277
.LBB34_130:	# bb285
	cmpl	$142, %esi
	setne	%al
	notb	%r11b
	orb	%al, %r11b
	testb	$1, %r11b
	jne	.LBB34_150	# bb316
.LBB34_131:	# bb293
	pxor	%xmm2, %xmm2
	ucomiss	%xmm2, %xmm0
	setnp	%dl
	sete	%al
	andb	%dl, %al
	ucomiss	.LCPI34_0(%rip), %xmm1
	setnp	%dl
	sete	%cl
	andb	%dl, %cl
	testb	%cl, %al
	jne	.LBB34_147	# bb315.preheader
.LBB34_132:	# bb293
	cmpl	$0, 28(%rsp)
	jle	.LBB34_147	# bb315.preheader
.LBB34_133:	# bb.nph429
	cmpl	$0, 52(%rsp)
	jle	.LBB34_147	# bb315.preheader
.LBB34_134:	# bb298.preheader.preheader
	movl	152(%rsp), %eax
	leal	(%rax,%rax), %ecx
	xorl	%esi, %esi
	movl	%esi, %edi
	jmp	.LBB34_137	# bb298.preheader
	.align	16
.LBB34_135:	# bb297
	movslq	%eax, %r8
	movss	(%rbx,%r8,4), %xmm2
	movaps	%xmm1, %xmm3
	mulss	%xmm2, %xmm3
	leal	1(%rax), %r9d
	movslq	%r9d, %r9
	movss	(%rbx,%r9,4), %xmm4
	movaps	%xmm0, %xmm5
	mulss	%xmm4, %xmm5
	subss	%xmm5, %xmm3
	movss	%xmm3, (%rbx,%r8,4)
	mulss	%xmm0, %xmm2
	mulss	%xmm1, %xmm4
	addss	%xmm2, %xmm4
	movss	%xmm4, (%rbx,%r9,4)
	addl	$2, %eax
	incl	%edx
	cmpl	52(%rsp), %edx
	jne	.LBB34_135	# bb297
.LBB34_136:	# bb299
	addl	%ecx, %esi
	incl	%edi
	cmpl	28(%rsp), %edi
	je	.LBB34_147	# bb315.preheader
.LBB34_137:	# bb298.preheader
	xorl	%edx, %edx
	movl	%esi, %eax
	jmp	.LBB34_135	# bb297
.LBB34_138:	# bb303
	cmpl	$131, 36(%rsp)
	jne	.LBB34_140	# bb305
.LBB34_139:	# bb304
	movl	32(%rsp), %r10d
	movslq	%r10d, %rax
	leal	1(%r10), %r10d
	movslq	%r10d, %rcx
	movss	44(%rsp), %xmm0
	movq	128(%rsp), %rdx
	mulss	(%rdx,%rcx,4), %xmm0
	movss	%xmm0, 40(%rsp)
	movss	(%rdx,%rax,4), %xmm0
	movss	%xmm0, 48(%rsp)
	cvtss2sd	40(%rsp), %xmm1
	cvtss2sd	%xmm0, %xmm0
	call	_ZL6xhypotdd51
	cvtsd2ss	%xmm0, %xmm0
	movss	40(%rsp), %xmm1
	divss	%xmm0, %xmm1
	movss	%xmm1, 40(%rsp)
	leal	-1(%r12), %r10d
	movslq	%r10d, %rax
	movss	(%rbx,%rax,4), %xmm1
	movaps	%xmm1, %xmm2
	mulss	40(%rsp), %xmm2
	movss	48(%rsp), %xmm3
	divss	%xmm0, %xmm3
	movss	%xmm3, 48(%rsp)
	leal	-2(%r12), %r10d
	movslq	%r10d, %rcx
	movss	(%rbx,%rcx,4), %xmm3
	movaps	%xmm3, %xmm4
	mulss	48(%rsp), %xmm4
	addss	%xmm2, %xmm4
	divss	%xmm0, %xmm4
	movss	%xmm4, (%rbx,%rcx,4)
	mulss	40(%rsp), %xmm3
	movss	48(%rsp), %xmm2
	mulss	%xmm1, %xmm2
	subss	%xmm3, %xmm2
	divss	%xmm0, %xmm2
	movss	%xmm2, (%rbx,%rax,4)
.LBB34_140:	# bb305
	leal	-1(%r12), %r10d
	movslq	%r10d, %rax
	movss	(%rbx,%rax,4), %xmm0
	leal	-2(%r12), %r10d
	movslq	%r10d, %rax
	movss	(%rbx,%rax,4), %xmm1
	cmpl	$0, 24(%rsp)
	jle	.LBB34_143	# bb308.loopexit
.LBB34_141:	# bb.nph423
	leal	1(%r14), %r10d
	leal	-1(%rbp), %eax
	xorl	%ecx, %ecx
	movl	%ecx, %edx
	.align	16
.LBB34_142:	# bb306
	leal	(%r10,%rcx), %esi
	movslq	%esi, %rsi
	movss	44(%rsp), %xmm2
	movq	128(%rsp), %rdi
	mulss	(%rdi,%rsi,4), %xmm2
	movaps	%xmm2, %xmm3
	mulss	%xmm0, %xmm3
	leal	(%r14,%rcx), %esi
	movslq	%esi, %rsi
	movss	(%rdi,%rsi,4), %xmm4
	movaps	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	subss	%xmm3, %xmm5
	leal	(%r13,%rcx), %esi
	movslq	%esi, %rsi
	movss	(%rbx,%rsi,4), %xmm3
	subss	%xmm5, %xmm3
	movss	%xmm3, (%rbx,%rsi,4)
	mulss	%xmm1, %xmm2
	mulss	%xmm0, %xmm4
	addss	%xmm2, %xmm4
	leal	(%r15,%rcx), %esi
	movslq	%esi, %rsi
	movss	(%rbx,%rsi,4), %xmm2
	subss	%xmm4, %xmm2
	movss	%xmm2, (%rbx,%rsi,4)
	addl	$2, %ecx
	incl	%edx
	cmpl	%eax, %edx
	jne	.LBB34_142	# bb306
.LBB34_143:	# bb308.loopexit
	movl	32(%rsp), %r10d
	addl	16(%rsp), %r10d
	movl	%r10d, 32(%rsp)
	subl	20(%rsp), %r14d
	addl	$4294967294, %r12d
	decl	%ebp
.LBB34_144:	# bb308
	testl	%ebp, %ebp
	jle	.LBB34_146	# bb314
.LBB34_145:	# bb309
	leal	-1(%rbp), %r10d
	movl	%r10d, 24(%rsp)
	testl	%ebp, %ebp
	jne	.LBB34_138	# bb303
.LBB34_146:	# bb314
	addl	12(%rsp), %r13d
	movl	8(%rsp), %r10d
	incl	%r10d
	movl	%r10d, 8(%rsp)
	cmpl	28(%rsp), %r10d
	je	.LBB34_24	# bb134.thread
	jmp	.LBB34_149	# bb308.preheader
.LBB34_147:	# bb315.preheader
	cmpl	$0, 28(%rsp)
	jle	.LBB34_24	# bb134.thread
.LBB34_148:	# bb.nph425
	movl	52(%rsp), %r14d
	leal	(%r14,%r14), %r14d
	movl	%r14d, 4(%rsp)
	movl	152(%rsp), %r14d
	addl	%r14d, %r14d
	movl	%r14d, 12(%rsp)
	cvtsi2ss	%r10d, %xmm0
	movss	%xmm0, 44(%rsp)
	xorl	%r13d, %r13d
	movl	%r13d, 8(%rsp)
	.align	16
.LBB34_149:	# bb308.preheader
	movl	52(%rsp), %r10d
	leal	-1(%r10), %r15d
	movl	136(%rsp), %r12d
	leal	2(,%r12,2), %ebp
	movl	%r12d, %r14d
	imull	%r15d, %r14d
	imull	%r15d, %ebp
	movl	%ebp, 32(%rsp)
	leal	(%r12,%r12), %r15d
	movl	%r15d, 20(%rsp)
	movl	$4294967294, %r12d
	subl	%r15d, %r12d
	movl	%r12d, 16(%rsp)
	addl	%r14d, %r14d
	movl	4(%rsp), %r15d
	leal	(%r15,%r13), %r12d
	leal	1(%r13), %r15d
	movl	%r10d, %ebp
	jmp	.LBB34_144	# bb308
.LBB34_150:	# bb316
	cmpl	$142, %esi
	setne	%al
	notb	%dl
	orb	%al, %dl
	testb	$1, %dl
	jne	.LBB34_169	# bb343
.LBB34_151:	# bb324
	pxor	%xmm2, %xmm2
	ucomiss	%xmm2, %xmm0
	setnp	%al
	sete	%cl
	andb	%al, %cl
	ucomiss	.LCPI34_0(%rip), %xmm1
	setnp	%al
	sete	%dl
	andb	%al, %dl
	testb	%dl, %cl
	jne	.LBB34_165	# bb342.preheader
.LBB34_152:	# bb324
	cmpl	$0, 28(%rsp)
	jle	.LBB34_165	# bb342.preheader
.LBB34_153:	# bb.nph421
	cmpl	$0, 52(%rsp)
	jle	.LBB34_165	# bb342.preheader
.LBB34_154:	# bb329.preheader.preheader
	movl	152(%rsp), %eax
	leal	(%rax,%rax), %edx
	xorl	%ecx, %ecx
	movl	%ecx, %edi
	jmp	.LBB34_157	# bb329.preheader
	.align	16
.LBB34_155:	# bb328
	movslq	%eax, %r8
	movss	(%rbx,%r8,4), %xmm2
	movaps	%xmm1, %xmm3
	mulss	%xmm2, %xmm3
	leal	1(%rax), %r9d
	movslq	%r9d, %r9
	movss	(%rbx,%r9,4), %xmm4
	movaps	%xmm0, %xmm5
	mulss	%xmm4, %xmm5
	subss	%xmm5, %xmm3
	movss	%xmm3, (%rbx,%r8,4)
	mulss	%xmm0, %xmm2
	mulss	%xmm1, %xmm4
	addss	%xmm2, %xmm4
	movss	%xmm4, (%rbx,%r9,4)
	addl	$2, %eax
	incl	%esi
	cmpl	52(%rsp), %esi
	jne	.LBB34_155	# bb328
.LBB34_156:	# bb330
	addl	%edx, %ecx
	incl	%edi
	cmpl	28(%rsp), %edi
	je	.LBB34_165	# bb342.preheader
.LBB34_157:	# bb329.preheader
	xorl	%esi, %esi
	movl	%ecx, %eax
	jmp	.LBB34_155	# bb328
	.align	16
.LBB34_158:	# bb334
	cmpl	$131, 36(%rsp)
	jne	.LBB34_160	# bb336
.LBB34_159:	# bb335
	movslq	%r12d, %rax
	leal	1(%r12), %r10d
	movslq	%r10d, %rcx
	movss	44(%rsp), %xmm0
	movq	128(%rsp), %rdx
	mulss	(%rdx,%rcx,4), %xmm0
	movss	%xmm0, 40(%rsp)
	movss	(%rdx,%rax,4), %xmm0
	movss	%xmm0, 48(%rsp)
	cvtss2sd	40(%rsp), %xmm1
	cvtss2sd	%xmm0, %xmm0
	call	_ZL6xhypotdd51
	cvtsd2ss	%xmm0, %xmm0
	movss	40(%rsp), %xmm1
	divss	%xmm0, %xmm1
	movss	%xmm1, 40(%rsp)
	movslq	%r15d, %rax
	leal	1(%r15), %r10d
	movslq	%r10d, %rcx
	movss	(%rbx,%rcx,4), %xmm1
	movaps	%xmm1, %xmm2
	mulss	40(%rsp), %xmm2
	movss	48(%rsp), %xmm3
	divss	%xmm0, %xmm3
	movss	%xmm3, 48(%rsp)
	movss	(%rbx,%rax,4), %xmm3
	movaps	%xmm3, %xmm4
	mulss	48(%rsp), %xmm4
	addss	%xmm2, %xmm4
	divss	%xmm0, %xmm4
	movss	%xmm4, (%rbx,%rax,4)
	movss	40(%rsp), %xmm2
	mulss	%xmm3, %xmm2
	movss	%xmm2, 40(%rsp)
	movss	48(%rsp), %xmm2
	mulss	%xmm1, %xmm2
	subss	40(%rsp), %xmm2
	divss	%xmm0, %xmm2
	movss	%xmm2, (%rbx,%rcx,4)
.LBB34_160:	# bb336
	movslq	%r15d, %rax
	leal	1(%r15), %r10d
	leal	1(%rbp), %ecx
	cmpl	52(%rsp), %ecx
	movss	(%rbx,%rax,4), %xmm0
	movslq	%r10d, %rax
	movss	(%rbx,%rax,4), %xmm1
	jge	.LBB34_163	# bb339
.LBB34_161:	# bb.nph
	leal	(%r13,%r12), %r10d
	movl	136(%rsp), %eax
	leal	(%rax,%rax), %eax
	xorl	%ecx, %ecx
	movl	%r15d, %edx
	.align	16
.LBB34_162:	# bb337
	movslq	%r10d, %rsi
	leal	1(%r10), %edi
	movslq	%edi, %rdi
	movss	44(%rsp), %xmm2
	movq	128(%rsp), %r8
	mulss	(%r8,%rdi,4), %xmm2
	movaps	%xmm2, %xmm3
	mulss	%xmm1, %xmm3
	movss	(%r8,%rsi,4), %xmm4
	movaps	%xmm4, %xmm5
	mulss	%xmm0, %xmm5
	subss	%xmm3, %xmm5
	leal	2(%rdx), %esi
	movslq	%esi, %rdi
	movss	(%rbx,%rdi,4), %xmm3
	subss	%xmm5, %xmm3
	movss	%xmm3, (%rbx,%rdi,4)
	mulss	%xmm0, %xmm2
	mulss	%xmm1, %xmm4
	addss	%xmm2, %xmm4
	addl	$3, %edx
	movslq	%edx, %rdx
	movss	(%rbx,%rdx,4), %xmm2
	subss	%xmm4, %xmm2
	movss	%xmm2, (%rbx,%rdx,4)
	addl	%eax, %r10d
	incl	%ecx
	cmpl	%r14d, %ecx
	movl	%esi, %edx
	jne	.LBB34_162	# bb337
.LBB34_163:	# bb339
	addl	32(%rsp), %r12d
	addl	$2, %r15d
	decl	%r14d
	incl	%ebp
	cmpl	52(%rsp), %ebp
	jne	.LBB34_158	# bb334
.LBB34_164:	# bb341
	movl	24(%rsp), %r10d
	addl	12(%rsp), %r10d
	movl	%r10d, 24(%rsp)
	movl	20(%rsp), %r10d
	incl	%r10d
	movl	%r10d, 20(%rsp)
	cmpl	28(%rsp), %r10d
	je	.LBB34_24	# bb134.thread
	jmp	.LBB34_168	# bb340.preheader
.LBB34_165:	# bb342.preheader
	cmpl	$0, 28(%rsp)
	jle	.LBB34_24	# bb134.thread
.LBB34_166:	# bb.nph417
	cmpl	$0, 52(%rsp)
	cvtsi2ss	%r10d, %xmm0
	movss	%xmm0, 44(%rsp)
	jle	.LBB34_24	# bb134.thread
.LBB34_167:	# bb340.preheader.preheader
	movl	152(%rsp), %r14d
	addl	%r14d, %r14d
	movl	%r14d, 12(%rsp)
	xorl	%r14d, %r14d
	movl	%r14d, 24(%rsp)
	movl	%r14d, 20(%rsp)
	.align	16
.LBB34_168:	# bb340.preheader
	movl	136(%rsp), %r14d
	leal	2(,%r14,2), %r15d
	movl	%r15d, 32(%rsp)
	leal	(%r14,%r14), %r13d
	movl	52(%rsp), %r14d
	leal	-1(%r14), %r14d
	xorl	%r12d, %r12d
	movl	24(%rsp), %r15d
	movl	%r12d, %ebp
	jmp	.LBB34_158	# bb334
.LBB34_169:	# bb343
	xorl	%edi, %edi
	leaq	.str49, %rsi
	leaq	.str150, %rdx
	xorb	%al, %al
	call	cblas_xerbla
	jmp	.LBB34_24	# bb134.thread
.LBB34_170:	# bb90
	cmpl	$111, %ecx
	movl	$111, %ecx
	movl	$112, %edi
	cmove	%ecx, %edi
	movl	%eax, 52(%rsp)
	jmp	.LBB34_2	# bb104
.LBB34_171:	# bb130.preheader
	movl	136(%rsp), %edi
	leal	2(,%rdi,2), %r15d
	leal	(%rdi,%rdi), %edi
	movl	$4294967294, %r14d
	subl	%edi, %r14d
	movl	%r14d, 16(%rsp)
	movl	28(%rsp), %edi
	leal	-1(%rdi), %eax
	movl	152(%rsp), %ecx
	movl	%ecx, %r14d
	imull	%eax, %r14d
	imull	%eax, %r15d
	movl	%r15d, 32(%rsp)
	addl	%r14d, %r14d
	leal	(%rdi,%rdi), %edi
	movl	%edi, 24(%rsp)
	cvtsi2ss	%r10d, %xmm0
	movss	%xmm0, 48(%rsp)
	leal	(%rcx,%rcx), %edi
	movl	%edi, 20(%rsp)
	jmp	.LBB34_22	# bb130
.LBB34_172:	# bb221.preheader
	movl	28(%rsp), %r14d
	leal	-1(%r14), %r14d
	movl	152(%rsp), %eax
	movl	%eax, %r15d
	imull	%r14d, %r15d
	movl	136(%rsp), %ecx
	leal	2(,%rcx,2), %r12d
	movl	%ecx, %edx
	imull	%r14d, %edx
	imull	%r14d, %r12d
	addl	%ecx, %ecx
	movl	%ecx, (%rsp)
	movl	$4294967294, %r14d
	subl	%ecx, %r14d
	movl	%r14d, 20(%rsp)
	addl	%edx, %edx
	movl	%edx, 32(%rsp)
	addl	%r15d, %r15d
	cvtsi2ss	%r10d, %xmm0
	movss	%xmm0, 48(%rsp)
	leal	(%rax,%rax), %r10d
	movl	%r10d, 24(%rsp)
	jmp	.LBB34_89	# bb221
	.size	cblas_ctrsm, .-cblas_ctrsm
.Leh_func_end25:


	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI35_0:					
	.quad	9223372036854775807	# double value: nan
	.quad	9223372036854775807	# double value: nan
	.section	.rodata.cst8,"aM",@progbits,8
	.align	16
.LCPI35_1:					
	.quad	4607182418800017408	# double value: 1.000000e+00
	.text
	.align	16
	.type	_ZL6xhypotdd51,@function
_ZL6xhypotdd51:
	movsd	.LCPI35_0(%rip), %xmm2
	andpd	%xmm2, %xmm0
	andpd	%xmm2, %xmm1
	movapd	%xmm1, %xmm2
	maxsd	%xmm0, %xmm2
	minsd	%xmm1, %xmm0
	pxor	%xmm1, %xmm1
	ucomisd	%xmm1, %xmm0
	setnp	%al
	sete	%cl
	testb	%al, %cl
	jne	.LBB35_2	# bb5
.LBB35_1:	# bb4
	divsd	%xmm2, %xmm0
	mulsd	%xmm0, %xmm0
	addsd	.LCPI35_1(%rip), %xmm0
	sqrtsd	%xmm0, %xmm0
	mulsd	%xmm2, %xmm0
	ret
.LBB35_2:	# bb5
	movapd	%xmm2, %xmm0
	ret
	.size	_ZL6xhypotdd51, .-_ZL6xhypotdd51


	.align	16
	.globl	cblas_ctrsv
	.type	cblas_ctrsv,@function
cblas_ctrsv:
.Leh_func_begin26:
.Llabel26:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$88, %rsp
	cmpl	$113, %edx
	movl	$4294967295, %eax
	movl	$1, %r10d
	cmove	%eax, %r10d
	movl	%r10d, 72(%rsp)
	movl	$112, %eax
	cmovne	%edx, %eax
	testl	%r8d, %r8d
	movq	152(%rsp), %rbx
	movq	%r9, %r14
	movl	%r8d, 68(%rsp)
	movl	%ecx, 64(%rsp)
	je	.LBB36_18	# bb99.thread
.LBB36_1:	# bb67
	cmpl	$121, %esi
	sete	%cl
	setne	%dl
	cmpl	$101, %edi
	sete	%r8b
	setne	%r9b
	andb	%cl, %r8b
	orb	%dl, %r9b
	testb	%r9b, %r9b
	jne	.LBB36_3	# bb74
.LBB36_2:	# bb67
	cmpl	$111, %eax
	je	.LBB36_5	# bb82
.LBB36_3:	# bb74
	cmpl	$122, %esi
	sete	%cl
	setne	%dl
	cmpl	$102, %edi
	sete	%r9b
	setne	%r10b
	andb	%cl, %r9b
	orb	%dl, %r10b
	testb	%r10b, %r10b
	jne	.LBB36_19	# bb101
.LBB36_4:	# bb74
	cmpl	$112, %eax
	jne	.LBB36_19	# bb101
.LBB36_5:	# bb82
	cmpl	$0, 160(%rsp)
	jg	.LBB36_65	# bb82.bb85_crit_edge
.LBB36_6:	# bb83
	movl	$1, %r15d
	subl	68(%rsp), %r15d
	imull	160(%rsp), %r15d
.LBB36_7:	# bb85
	movl	68(%rsp), %eax
	leal	-1(%rax), %eax
	movl	%eax, %ecx
	imull	160(%rsp), %ecx
	addl	%r15d, %ecx
	cmpl	$131, 64(%rsp)
	jne	.LBB36_9	# bb87
.LBB36_8:	# bb86
	movl	144(%rsp), %edx
	leal	2(,%rdx,2), %edx
	imull	%eax, %edx
	movslq	%edx, %rax
	orl	$1, %edx
	movslq	%edx, %rdx
	cvtsi2ss	72(%rsp), %xmm0
	mulss	(%r14,%rdx,4), %xmm0
	movss	%xmm0, 80(%rsp)
	leal	1(,%rcx,2), %edx
	movslq	%edx, %r12
	movss	(%rbx,%r12,4), %xmm0
	movss	%xmm0, 60(%rsp)
	addl	%ecx, %ecx
	movslq	%ecx, %r13
	movss	(%rbx,%r13,4), %xmm0
	movss	%xmm0, 76(%rsp)
	movss	(%r14,%rax,4), %xmm0
	movss	%xmm0, 84(%rsp)
	cvtss2sd	%xmm0, %xmm0
	cvtss2sd	80(%rsp), %xmm1
	call	_ZL6xhypotdd54
	cvtsd2ss	%xmm0, %xmm0
	movss	84(%rsp), %xmm1
	divss	%xmm0, %xmm1
	movss	%xmm1, 84(%rsp)
	movss	80(%rsp), %xmm1
	divss	%xmm0, %xmm1
	movss	%xmm1, 80(%rsp)
	movss	60(%rsp), %xmm1
	mulss	80(%rsp), %xmm1
	movss	80(%rsp), %xmm2
	mulss	76(%rsp), %xmm2
	movss	%xmm2, 80(%rsp)
	movss	76(%rsp), %xmm2
	mulss	84(%rsp), %xmm2
	addss	%xmm1, %xmm2
	divss	%xmm0, %xmm2
	movss	%xmm2, (%rbx,%r13,4)
	movss	84(%rsp), %xmm1
	mulss	60(%rsp), %xmm1
	subss	80(%rsp), %xmm1
	divss	%xmm0, %xmm1
	movss	%xmm1, (%rbx,%r12,4)
.LBB36_9:	# bb87
	movl	144(%rsp), %eax
	leal	2(,%rax,2), %ecx
	leal	(%rax,%rax), %edx
	movl	$4294967294, %esi
	subl	%edx, %esi
	movl	%esi, 12(%rsp)
	movl	68(%rsp), %edx
	leal	-2(%rdx), %esi
	movl	160(%rsp), %edi
	movl	%edi, %r8d
	imull	%esi, %r8d
	imull	%esi, %eax
	imull	%esi, %ecx
	movl	%ecx, 32(%rsp)
	orl	$1, %ecx
	movl	%ecx, 40(%rsp)
	addl	%edx, %eax
	leal	-2(,%rax,2), %eax
	movl	%eax, 44(%rsp)
	leal	-1(%rdx), %eax
	imull	%edi, %eax
	addl	%r15d, %eax
	addl	%eax, %eax
	movl	%eax, 20(%rsp)
	addl	%r15d, %r8d
	leal	1(,%r8,2), %eax
	movl	%eax, 36(%rsp)
	addl	%r8d, %r8d
	movl	%r8d, 24(%rsp)
	cvtsi2ss	72(%rsp), %xmm0
	movss	%xmm0, 76(%rsp)
	leal	(%rdi,%rdi), %eax
	movl	%eax, 28(%rsp)
	movl	$1, %r15d
	xorl	%r12d, %r12d
	movl	%r12d, %r13d
	movl	%edx, 60(%rsp)
	jmp	.LBB36_16	# bb95
.LBB36_10:	# bb88
	movl	24(%rsp), %ecx
	leal	(%rcx,%r13), %ecx
	movl	36(%rsp), %edx
	leal	(%rdx,%r13), %edx
	cmpl	68(%rsp), %eax
	movslq	%edx, %rax
	movq	%rax, 48(%rsp)
	movss	(%rbx,%rax,4), %xmm0
	movss	%xmm0, 84(%rsp)
	movslq	%ecx, %rbp
	movss	(%rbx,%rbp,4), %xmm0
	movss	%xmm0, 80(%rsp)
	jge	.LBB36_13	# bb91
.LBB36_11:	# bb.nph267
	movl	44(%rsp), %eax
	leal	(%rax,%r12), %eax
	movl	20(%rsp), %ecx
	leal	(%rcx,%r13), %ecx
	movl	160(%rsp), %edx
	leal	(%rdx,%rdx), %edx
	xorl	%esi, %esi
	.align	16
.LBB36_12:	# bb89
	movslq	%eax, %rdi
	leal	1(%rax), %r8d
	movslq	%r8d, %r8
	movss	76(%rsp), %xmm0
	mulss	(%r14,%r8,4), %xmm0
	movslq	%ecx, %r8
	movss	(%rbx,%r8,4), %xmm1
	movaps	%xmm0, %xmm2
	mulss	%xmm1, %xmm2
	movss	(%r14,%rdi,4), %xmm3
	leal	1(%rcx), %edi
	movslq	%edi, %rdi
	movss	(%rbx,%rdi,4), %xmm4
	movaps	%xmm3, %xmm5
	mulss	%xmm4, %xmm5
	addss	%xmm2, %xmm5
	movss	84(%rsp), %xmm2
	subss	%xmm5, %xmm2
	movss	%xmm2, 84(%rsp)
	mulss	%xmm1, %xmm3
	mulss	%xmm4, %xmm0
	subss	%xmm0, %xmm3
	movss	80(%rsp), %xmm0
	subss	%xmm3, %xmm0
	movss	%xmm0, 80(%rsp)
	addl	%edx, %ecx
	addl	$2, %eax
	incl	%esi
	cmpl	%r15d, %esi
	jne	.LBB36_12	# bb89
.LBB36_13:	# bb91
	cmpl	$131, 64(%rsp)
	je	.LBB36_66	# bb92
.LBB36_14:	# bb93
	movss	80(%rsp), %xmm0
	movss	%xmm0, (%rbx,%rbp,4)
	movss	84(%rsp), %xmm0
	movss	%xmm0, 56(%rsp)
.LBB36_15:	# bb95.backedge
	movss	56(%rsp), %xmm0
	movq	48(%rsp), %rax
	movss	%xmm0, (%rbx,%rax,4)
	addl	12(%rsp), %r12d
	subl	28(%rsp), %r13d
	decl	60(%rsp)
	incl	%r15d
.LBB36_16:	# bb95
	movl	60(%rsp), %eax
	leal	-1(%rax), %eax
	testl	%eax, %eax
	jle	.LBB36_18	# bb99.thread
.LBB36_17:	# bb96
	cmpl	$1, 60(%rsp)
	jne	.LBB36_10	# bb88
.LBB36_18:	# bb99.thread
	addq	$88, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB36_19:	# bb101
	cmpl	$122, %esi
	sete	%cl
	setne	%dl
	cmpl	$101, %edi
	sete	%r10b
	setne	%r11b
	andb	%cl, %r10b
	orb	%dl, %r11b
	testb	%r11b, %r11b
	jne	.LBB36_21	# bb109
.LBB36_20:	# bb101
	cmpl	$111, %eax
	je	.LBB36_23	# bb117
.LBB36_21:	# bb109
	cmpl	$121, %esi
	sete	%cl
	setne	%dl
	cmpl	$102, %edi
	sete	%sil
	setne	%dil
	andb	%cl, %sil
	orb	%dl, %dil
	testb	%dil, %dil
	jne	.LBB36_35	# bb134
.LBB36_22:	# bb109
	cmpl	$112, %eax
	jne	.LBB36_35	# bb134
.LBB36_23:	# bb117
	cmpl	$0, 160(%rsp)
	jg	.LBB36_67	# bb117.bb120_crit_edge
.LBB36_24:	# bb118
	movl	$1, %r15d
	subl	68(%rsp), %r15d
	imull	160(%rsp), %r15d
.LBB36_25:	# bb120
	cmpl	$131, 64(%rsp)
	jne	.LBB36_27	# bb133.preheader
.LBB36_26:	# bb121
	cvtsi2ss	72(%rsp), %xmm0
	mulss	4(%r14), %xmm0
	movss	%xmm0, 80(%rsp)
	leal	1(,%r15,2), %eax
	movslq	%eax, %r12
	movss	(%rbx,%r12,4), %xmm0
	movss	%xmm0, 60(%rsp)
	leal	(%r15,%r15), %eax
	movslq	%eax, %r13
	movss	(%rbx,%r13,4), %xmm0
	movss	%xmm0, 76(%rsp)
	movss	(%r14), %xmm0
	movss	%xmm0, 84(%rsp)
	cvtss2sd	80(%rsp), %xmm1
	cvtss2sd	%xmm0, %xmm0
	call	_ZL6xhypotdd54
	cvtsd2ss	%xmm0, %xmm0
	movss	80(%rsp), %xmm1
	divss	%xmm0, %xmm1
	movss	%xmm1, 80(%rsp)
	movss	60(%rsp), %xmm1
	mulss	80(%rsp), %xmm1
	movss	80(%rsp), %xmm2
	mulss	76(%rsp), %xmm2
	movss	%xmm2, 80(%rsp)
	movss	84(%rsp), %xmm2
	divss	%xmm0, %xmm2
	movss	%xmm2, 84(%rsp)
	movss	76(%rsp), %xmm2
	mulss	84(%rsp), %xmm2
	addss	%xmm1, %xmm2
	divss	%xmm0, %xmm2
	movss	%xmm2, (%rbx,%r13,4)
	movss	84(%rsp), %xmm1
	mulss	60(%rsp), %xmm1
	subss	80(%rsp), %xmm1
	divss	%xmm0, %xmm1
	movss	%xmm1, (%rbx,%r12,4)
.LBB36_27:	# bb133.preheader
	cmpl	$2, 68(%rsp)
	jl	.LBB36_18	# bb99.thread
.LBB36_28:	# bb.nph258
	movl	160(%rsp), %eax
	addl	%eax, %r15d
	addl	%r15d, %r15d
	movl	$1, %edx
	movl	68(%rsp), %ecx
	subl	%ecx, %edx
	imull	%eax, %edx
	movl	%edx, 40(%rsp)
	movl	144(%rsp), %edx
	leal	2(,%rdx,2), %esi
	movl	%esi, 48(%rsp)
	addl	%edx, %edx
	movl	%edx, 16(%rsp)
	decl	%ecx
	movl	%ecx, 68(%rsp)
	cvtsi2ss	72(%rsp), %xmm0
	movss	%xmm0, 76(%rsp)
	leal	(%rax,%rax), %eax
	movl	%eax, 44(%rsp)
	xorl	%r12d, %r12d
	movl	%edx, %r13d
	movl	%edx, 60(%rsp)
	.align	16
.LBB36_29:	# bb123
	cmpl	$0, 160(%rsp)
	movl	$0, %eax
	cmovle	40(%rsp), %eax
	movslq	%r15d, %rbp
	movss	(%rbx,%rbp,4), %xmm0
	movss	%xmm0, 84(%rsp)
	incl	%r15d
	movslq	%r15d, %r15
	movss	(%rbx,%r15,4), %xmm0
	movss	%xmm0, 80(%rsp)
	leal	1(%r12), %ecx
	testl	%ecx, %ecx
	jle	.LBB36_32	# bb129
.LBB36_30:	# bb.nph251
	movl	160(%rsp), %edx
	leal	(%rdx,%rdx), %edx
	addl	%eax, %eax
	xorl	%esi, %esi
	movl	60(%rsp), %edi
	.align	16
.LBB36_31:	# bb127
	movslq	%edi, %r8
	leal	1(%rdi), %r9d
	movslq	%r9d, %r9
	movss	76(%rsp), %xmm0
	mulss	(%r14,%r9,4), %xmm0
	movslq	%eax, %r9
	movss	(%rbx,%r9,4), %xmm1
	movaps	%xmm0, %xmm2
	mulss	%xmm1, %xmm2
	movss	(%r14,%r8,4), %xmm3
	leal	1(%rax), %r8d
	movslq	%r8d, %r8
	movss	(%rbx,%r8,4), %xmm4
	movaps	%xmm3, %xmm5
	mulss	%xmm4, %xmm5
	addss	%xmm2, %xmm5
	movss	80(%rsp), %xmm2
	subss	%xmm5, %xmm2
	movss	%xmm2, 80(%rsp)
	mulss	%xmm1, %xmm3
	mulss	%xmm4, %xmm0
	subss	%xmm0, %xmm3
	movss	84(%rsp), %xmm0
	subss	%xmm3, %xmm0
	movss	%xmm0, 84(%rsp)
	addl	%edx, %eax
	addl	$2, %edi
	incl	%esi
	cmpl	%ecx, %esi
	jne	.LBB36_31	# bb127
.LBB36_32:	# bb129
	cmpl	$131, 64(%rsp)
	je	.LBB36_68	# bb130
.LBB36_33:	# bb131
	movss	84(%rsp), %xmm0
	movss	%xmm0, (%rbx,%rbp,4)
	movss	80(%rsp), %xmm0
	movss	%xmm0, 56(%rsp)
.LBB36_34:	# bb132
	movss	56(%rsp), %xmm0
	movss	%xmm0, (%rbx,%r15,4)
	movl	%ebp, %r15d
	addl	44(%rsp), %r15d
	addl	48(%rsp), %r13d
	movl	60(%rsp), %eax
	addl	16(%rsp), %eax
	movl	%eax, 60(%rsp)
	incl	%r12d
	cmpl	68(%rsp), %r12d
	je	.LBB36_18	# bb99.thread
	jmp	.LBB36_29	# bb123
.LBB36_35:	# bb134
	cmpl	$111, %eax
	sete	%cl
	andb	%cl, %r9b
	cmpl	$112, %eax
	sete	%cl
	testb	%cl, %r8b
	jne	.LBB36_37	# bb150
.LBB36_36:	# bb134
	notb	%r9b
	testb	$1, %r9b
	jne	.LBB36_49	# bb167
.LBB36_37:	# bb150
	cmpl	$0, 160(%rsp)
	jg	.LBB36_69	# bb150.bb153_crit_edge
.LBB36_38:	# bb151
	movl	$1, %r15d
	subl	68(%rsp), %r15d
	imull	160(%rsp), %r15d
.LBB36_39:	# bb153
	cmpl	$131, 64(%rsp)
	jne	.LBB36_41	# bb166.preheader
.LBB36_40:	# bb154
	cvtsi2ss	72(%rsp), %xmm0
	mulss	4(%r14), %xmm0
	movss	%xmm0, 80(%rsp)
	leal	1(,%r15,2), %eax
	movslq	%eax, %r12
	movss	(%rbx,%r12,4), %xmm0
	movss	%xmm0, 60(%rsp)
	leal	(%r15,%r15), %eax
	movslq	%eax, %r13
	movss	(%rbx,%r13,4), %xmm0
	movss	%xmm0, 76(%rsp)
	movss	(%r14), %xmm0
	movss	%xmm0, 84(%rsp)
	cvtss2sd	80(%rsp), %xmm1
	cvtss2sd	%xmm0, %xmm0
	call	_ZL6xhypotdd54
	cvtsd2ss	%xmm0, %xmm0
	movss	80(%rsp), %xmm1
	divss	%xmm0, %xmm1
	movss	%xmm1, 80(%rsp)
	movss	60(%rsp), %xmm1
	mulss	80(%rsp), %xmm1
	movss	80(%rsp), %xmm2
	mulss	76(%rsp), %xmm2
	movss	%xmm2, 80(%rsp)
	movss	84(%rsp), %xmm2
	divss	%xmm0, %xmm2
	movss	%xmm2, 84(%rsp)
	movss	76(%rsp), %xmm2
	mulss	84(%rsp), %xmm2
	addss	%xmm1, %xmm2
	divss	%xmm0, %xmm2
	movss	%xmm2, (%rbx,%r13,4)
	movss	84(%rsp), %xmm1
	mulss	60(%rsp), %xmm1
	subss	80(%rsp), %xmm1
	divss	%xmm0, %xmm1
	movss	%xmm1, (%rbx,%r12,4)
.LBB36_41:	# bb166.preheader
	cmpl	$2, 68(%rsp)
	jl	.LBB36_18	# bb99.thread
.LBB36_42:	# bb.nph246
	movl	160(%rsp), %eax
	addl	%eax, %r15d
	addl	%r15d, %r15d
	movl	$1, %edx
	movl	68(%rsp), %ecx
	subl	%ecx, %edx
	imull	%eax, %edx
	movl	%edx, 44(%rsp)
	movl	144(%rsp), %edx
	leal	2(,%rdx,2), %esi
	movl	%esi, 56(%rsp)
	leal	(%rdx,%rdx), %r12d
	decl	%ecx
	movl	%ecx, 68(%rsp)
	cvtsi2ss	72(%rsp), %xmm0
	movss	%xmm0, 76(%rsp)
	leal	(%rax,%rax), %eax
	movl	%eax, 48(%rsp)
	xorl	%r13d, %r13d
	.align	16
.LBB36_43:	# bb156
	cmpl	$0, 160(%rsp)
	movl	$0, %eax
	cmovle	44(%rsp), %eax
	movslq	%r15d, %rbp
	movss	(%rbx,%rbp,4), %xmm0
	movss	%xmm0, 84(%rsp)
	incl	%r15d
	movslq	%r15d, %r15
	movss	(%rbx,%r15,4), %xmm0
	movss	%xmm0, 80(%rsp)
	leal	1(%r13), %ecx
	testl	%ecx, %ecx
	jle	.LBB36_46	# bb162
.LBB36_44:	# bb160.preheader
	leal	(%r13,%r13), %edx
	movl	160(%rsp), %esi
	leal	(%rsi,%rsi), %esi
	addl	%eax, %eax
	movl	144(%rsp), %edi
	leal	(%rdi,%rdi), %edi
	xorl	%r8d, %r8d
	.align	16
.LBB36_45:	# bb160
	leal	3(%rdx), %r9d
	movslq	%r9d, %r9
	movss	76(%rsp), %xmm0
	mulss	(%r14,%r9,4), %xmm0
	movslq	%eax, %r9
	movss	(%rbx,%r9,4), %xmm1
	movaps	%xmm0, %xmm2
	mulss	%xmm1, %xmm2
	leal	1(%rax), %r9d
	movslq	%r9d, %r9
	movss	(%rbx,%r9,4), %xmm3
	leal	2(%rdx), %r9d
	movslq	%r9d, %r9
	movss	(%r14,%r9,4), %xmm4
	movaps	%xmm4, %xmm5
	mulss	%xmm3, %xmm5
	addss	%xmm2, %xmm5
	movss	80(%rsp), %xmm2
	subss	%xmm5, %xmm2
	movss	%xmm2, 80(%rsp)
	mulss	%xmm3, %xmm0
	mulss	%xmm1, %xmm4
	subss	%xmm0, %xmm4
	movss	84(%rsp), %xmm0
	subss	%xmm4, %xmm0
	movss	%xmm0, 84(%rsp)
	addl	%esi, %eax
	addl	%edi, %edx
	incl	%r8d
	cmpl	%ecx, %r8d
	jne	.LBB36_45	# bb160
.LBB36_46:	# bb162
	cmpl	$131, 64(%rsp)
	je	.LBB36_70	# bb163
.LBB36_47:	# bb164
	movss	84(%rsp), %xmm0
	movss	%xmm0, (%rbx,%rbp,4)
	movss	80(%rsp), %xmm0
	movss	%xmm0, 60(%rsp)
.LBB36_48:	# bb165
	movss	60(%rsp), %xmm0
	movss	%xmm0, (%rbx,%r15,4)
	movl	%ebp, %r15d
	addl	48(%rsp), %r15d
	addl	56(%rsp), %r12d
	incl	%r13d
	cmpl	68(%rsp), %r13d
	je	.LBB36_18	# bb99.thread
	jmp	.LBB36_43	# bb156
.LBB36_49:	# bb167
	cmpl	$111, %eax
	sete	%cl
	andb	%cl, %sil
	cmpl	$112, %eax
	sete	%al
	testb	%al, %r10b
	jne	.LBB36_51	# bb183
.LBB36_50:	# bb167
	notb	%sil
	testb	$1, %sil
	jne	.LBB36_64	# bb202
.LBB36_51:	# bb183
	cmpl	$0, 160(%rsp)
	jg	.LBB36_71	# bb183.bb186_crit_edge
.LBB36_52:	# bb184
	movl	$1, %r15d
	subl	68(%rsp), %r15d
	imull	160(%rsp), %r15d
.LBB36_53:	# bb186
	movl	68(%rsp), %eax
	leal	-1(%rax), %eax
	movl	%eax, %ecx
	imull	160(%rsp), %ecx
	addl	%r15d, %ecx
	cmpl	$131, 64(%rsp)
	jne	.LBB36_55	# bb188
.LBB36_54:	# bb187
	movl	144(%rsp), %edx
	leal	2(,%rdx,2), %edx
	imull	%eax, %edx
	movslq	%edx, %rax
	orl	$1, %edx
	movslq	%edx, %rdx
	cvtsi2ss	72(%rsp), %xmm0
	mulss	(%r14,%rdx,4), %xmm0
	movss	%xmm0, 80(%rsp)
	leal	1(,%rcx,2), %edx
	movslq	%edx, %r12
	movss	(%rbx,%r12,4), %xmm0
	movss	%xmm0, 60(%rsp)
	addl	%ecx, %ecx
	movslq	%ecx, %r13
	movss	(%rbx,%r13,4), %xmm0
	movss	%xmm0, 76(%rsp)
	movss	(%r14,%rax,4), %xmm0
	movss	%xmm0, 84(%rsp)
	cvtss2sd	%xmm0, %xmm0
	cvtss2sd	80(%rsp), %xmm1
	call	_ZL6xhypotdd54
	cvtsd2ss	%xmm0, %xmm0
	movss	84(%rsp), %xmm1
	divss	%xmm0, %xmm1
	movss	%xmm1, 84(%rsp)
	movss	80(%rsp), %xmm1
	divss	%xmm0, %xmm1
	movss	%xmm1, 80(%rsp)
	movss	60(%rsp), %xmm1
	mulss	80(%rsp), %xmm1
	movss	80(%rsp), %xmm2
	mulss	76(%rsp), %xmm2
	movss	%xmm2, 80(%rsp)
	movss	76(%rsp), %xmm2
	mulss	84(%rsp), %xmm2
	addss	%xmm1, %xmm2
	divss	%xmm0, %xmm2
	movss	%xmm2, (%rbx,%r13,4)
	movss	84(%rsp), %xmm1
	mulss	60(%rsp), %xmm1
	subss	80(%rsp), %xmm1
	divss	%xmm0, %xmm1
	movss	%xmm1, (%rbx,%r12,4)
.LBB36_55:	# bb188
	movl	144(%rsp), %eax
	leal	2(,%rax,2), %ecx
	leal	(%rax,%rax), %edx
	movl	$4294967294, %esi
	subl	%edx, %esi
	movl	%esi, 12(%rsp)
	movl	68(%rsp), %edx
	leal	-2(%rdx), %esi
	movl	160(%rsp), %edi
	movl	%edi, %r8d
	imull	%esi, %r8d
	imull	%esi, %ecx
	movl	%ecx, 32(%rsp)
	orl	$1, %ecx
	movl	%ecx, 44(%rsp)
	leal	-1(%rdx), %ecx
	imull	%ecx, %eax
	addl	%edx, %eax
	leal	-4(,%rax,2), %eax
	movl	%eax, 40(%rsp)
	imull	%edi, %ecx
	addl	%r15d, %ecx
	addl	%ecx, %ecx
	movl	%ecx, 20(%rsp)
	addl	%r15d, %r8d
	leal	1(,%r8,2), %eax
	movl	%eax, 36(%rsp)
	addl	%r8d, %r8d
	movl	%r8d, 24(%rsp)
	cvtsi2ss	72(%rsp), %xmm0
	movss	%xmm0, 76(%rsp)
	leal	(%rdi,%rdi), %eax
	movl	%eax, 28(%rsp)
	movl	$1, %r15d
	xorl	%r12d, %r12d
	movl	%r12d, %r13d
	movl	%edx, 60(%rsp)
	jmp	.LBB36_62	# bb196
.LBB36_56:	# bb189
	movl	24(%rsp), %ecx
	leal	(%rcx,%r13), %ecx
	movl	36(%rsp), %edx
	leal	(%rdx,%r13), %edx
	cmpl	68(%rsp), %eax
	movslq	%edx, %rax
	movq	%rax, 48(%rsp)
	movss	(%rbx,%rax,4), %xmm0
	movss	%xmm0, 84(%rsp)
	movslq	%ecx, %rbp
	movss	(%rbx,%rbp,4), %xmm0
	movss	%xmm0, 80(%rsp)
	jge	.LBB36_59	# bb192
.LBB36_57:	# bb.nph
	movl	40(%rsp), %eax
	leal	(%rax,%r12), %eax
	movl	20(%rsp), %ecx
	leal	(%rcx,%r13), %ecx
	movl	160(%rsp), %edx
	leal	(%rdx,%rdx), %edx
	movl	144(%rsp), %esi
	leal	(%rsi,%rsi), %esi
	xorl	%edi, %edi
	.align	16
.LBB36_58:	# bb190
	movslq	%eax, %r8
	leal	1(%rax), %r9d
	movslq	%r9d, %r9
	movss	76(%rsp), %xmm0
	mulss	(%r14,%r9,4), %xmm0
	movslq	%ecx, %r9
	movss	(%rbx,%r9,4), %xmm1
	movaps	%xmm0, %xmm2
	mulss	%xmm1, %xmm2
	movss	(%r14,%r8,4), %xmm3
	leal	1(%rcx), %r8d
	movslq	%r8d, %r8
	movss	(%rbx,%r8,4), %xmm4
	movaps	%xmm3, %xmm5
	mulss	%xmm4, %xmm5
	addss	%xmm2, %xmm5
	movss	84(%rsp), %xmm2
	subss	%xmm5, %xmm2
	movss	%xmm2, 84(%rsp)
	mulss	%xmm1, %xmm3
	mulss	%xmm4, %xmm0
	subss	%xmm0, %xmm3
	movss	80(%rsp), %xmm0
	subss	%xmm3, %xmm0
	movss	%xmm0, 80(%rsp)
	addl	%edx, %ecx
	addl	%esi, %eax
	incl	%edi
	cmpl	%r15d, %edi
	jne	.LBB36_58	# bb190
.LBB36_59:	# bb192
	cmpl	$131, 64(%rsp)
	je	.LBB36_72	# bb193
.LBB36_60:	# bb194
	movss	80(%rsp), %xmm0
	movss	%xmm0, (%rbx,%rbp,4)
	movss	84(%rsp), %xmm0
	movss	%xmm0, 56(%rsp)
.LBB36_61:	# bb196.backedge
	movss	56(%rsp), %xmm0
	movq	48(%rsp), %rax
	movss	%xmm0, (%rbx,%rax,4)
	addl	12(%rsp), %r12d
	subl	28(%rsp), %r13d
	decl	60(%rsp)
	incl	%r15d
.LBB36_62:	# bb196
	movl	60(%rsp), %eax
	leal	-1(%rax), %eax
	testl	%eax, %eax
	jle	.LBB36_18	# bb99.thread
.LBB36_63:	# bb197
	cmpl	$1, 60(%rsp)
	jne	.LBB36_56	# bb189
	jmp	.LBB36_18	# bb99.thread
.LBB36_64:	# bb202
	xorl	%edi, %edi
	leaq	.str52, %rsi
	leaq	.str153, %rdx
	xorb	%al, %al
	call	cblas_xerbla
	jmp	.LBB36_18	# bb99.thread
.LBB36_65:	# bb82.bb85_crit_edge
	xorl	%r15d, %r15d
	jmp	.LBB36_7	# bb85
.LBB36_66:	# bb92
	movl	40(%rsp), %eax
	leal	(%rax,%r12), %eax
	movslq	%eax, %rax
	movss	76(%rsp), %xmm0
	mulss	(%r14,%rax,4), %xmm0
	movss	%xmm0, 72(%rsp)
	movl	32(%rsp), %eax
	leal	(%rax,%r12), %eax
	movslq	%eax, %rax
	movss	(%r14,%rax,4), %xmm0
	movss	%xmm0, 56(%rsp)
	cvtss2sd	%xmm0, %xmm0
	cvtss2sd	72(%rsp), %xmm1
	call	_ZL6xhypotdd54
	cvtsd2ss	%xmm0, %xmm0
	movss	72(%rsp), %xmm1
	divss	%xmm0, %xmm1
	movss	%xmm1, 72(%rsp)
	movss	84(%rsp), %xmm1
	movaps	%xmm1, %xmm2
	mulss	72(%rsp), %xmm2
	movss	56(%rsp), %xmm3
	divss	%xmm0, %xmm3
	movss	%xmm3, 56(%rsp)
	movss	80(%rsp), %xmm3
	movaps	%xmm3, %xmm4
	mulss	56(%rsp), %xmm4
	addss	%xmm2, %xmm4
	divss	%xmm0, %xmm4
	movss	%xmm4, (%rbx,%rbp,4)
	movss	72(%rsp), %xmm2
	mulss	%xmm3, %xmm2
	movss	%xmm2, 72(%rsp)
	movss	56(%rsp), %xmm2
	mulss	%xmm1, %xmm2
	subss	72(%rsp), %xmm2
	divss	%xmm0, %xmm2
	movss	%xmm2, 56(%rsp)
	jmp	.LBB36_15	# bb95.backedge
.LBB36_67:	# bb117.bb120_crit_edge
	xorl	%r15d, %r15d
	jmp	.LBB36_25	# bb120
.LBB36_68:	# bb130
	leal	3(%r13), %eax
	movslq	%eax, %rax
	movss	76(%rsp), %xmm0
	mulss	(%r14,%rax,4), %xmm0
	movss	%xmm0, 72(%rsp)
	leal	2(%r13), %eax
	movslq	%eax, %rax
	movss	(%r14,%rax,4), %xmm0
	movss	%xmm0, 56(%rsp)
	cvtss2sd	72(%rsp), %xmm1
	cvtss2sd	%xmm0, %xmm0
	call	_ZL6xhypotdd54
	cvtsd2ss	%xmm0, %xmm0
	movss	72(%rsp), %xmm1
	divss	%xmm0, %xmm1
	movss	%xmm1, 72(%rsp)
	movss	80(%rsp), %xmm1
	movaps	%xmm1, %xmm2
	mulss	72(%rsp), %xmm2
	movss	56(%rsp), %xmm3
	divss	%xmm0, %xmm3
	movss	%xmm3, 56(%rsp)
	movss	84(%rsp), %xmm3
	movaps	%xmm3, %xmm4
	mulss	56(%rsp), %xmm4
	addss	%xmm2, %xmm4
	divss	%xmm0, %xmm4
	movss	%xmm4, (%rbx,%rbp,4)
	movss	72(%rsp), %xmm2
	mulss	%xmm3, %xmm2
	movss	%xmm2, 72(%rsp)
	movss	56(%rsp), %xmm2
	mulss	%xmm1, %xmm2
	subss	72(%rsp), %xmm2
	divss	%xmm0, %xmm2
	movss	%xmm2, 56(%rsp)
	jmp	.LBB36_34	# bb132
.LBB36_69:	# bb150.bb153_crit_edge
	xorl	%r15d, %r15d
	jmp	.LBB36_39	# bb153
.LBB36_70:	# bb163
	leal	3(%r12), %eax
	movslq	%eax, %rax
	movss	76(%rsp), %xmm0
	mulss	(%r14,%rax,4), %xmm0
	movss	%xmm0, 72(%rsp)
	leal	2(%r12), %eax
	movslq	%eax, %rax
	movss	(%r14,%rax,4), %xmm0
	movss	%xmm0, 60(%rsp)
	cvtss2sd	72(%rsp), %xmm1
	cvtss2sd	%xmm0, %xmm0
	call	_ZL6xhypotdd54
	cvtsd2ss	%xmm0, %xmm0
	movss	72(%rsp), %xmm1
	divss	%xmm0, %xmm1
	movss	%xmm1, 72(%rsp)
	movss	80(%rsp), %xmm1
	movaps	%xmm1, %xmm2
	mulss	72(%rsp), %xmm2
	movss	60(%rsp), %xmm3
	divss	%xmm0, %xmm3
	movss	%xmm3, 60(%rsp)
	movss	84(%rsp), %xmm3
	movaps	%xmm3, %xmm4
	mulss	60(%rsp), %xmm4
	addss	%xmm2, %xmm4
	divss	%xmm0, %xmm4
	movss	%xmm4, (%rbx,%rbp,4)
	movss	72(%rsp), %xmm2
	mulss	%xmm3, %xmm2
	movss	%xmm2, 72(%rsp)
	movss	60(%rsp), %xmm2
	mulss	%xmm1, %xmm2
	subss	72(%rsp), %xmm2
	divss	%xmm0, %xmm2
	movss	%xmm2, 60(%rsp)
	jmp	.LBB36_48	# bb165
.LBB36_71:	# bb183.bb186_crit_edge
	xorl	%r15d, %r15d
	jmp	.LBB36_53	# bb186
.LBB36_72:	# bb193
	movl	44(%rsp), %eax
	leal	(%rax,%r12), %eax
	movslq	%eax, %rax
	movss	76(%rsp), %xmm0
	mulss	(%r14,%rax,4), %xmm0
	movss	%xmm0, 72(%rsp)
	movl	32(%rsp), %eax
	leal	(%rax,%r12), %eax
	movslq	%eax, %rax
	movss	(%r14,%rax,4), %xmm0
	movss	%xmm0, 56(%rsp)
	cvtss2sd	%xmm0, %xmm0
	cvtss2sd	72(%rsp), %xmm1
	call	_ZL6xhypotdd54
	cvtsd2ss	%xmm0, %xmm0
	movss	72(%rsp), %xmm1
	divss	%xmm0, %xmm1
	movss	%xmm1, 72(%rsp)
	movss	84(%rsp), %xmm1
	movaps	%xmm1, %xmm2
	mulss	72(%rsp), %xmm2
	movss	56(%rsp), %xmm3
	divss	%xmm0, %xmm3
	movss	%xmm3, 56(%rsp)
	movss	80(%rsp), %xmm3
	movaps	%xmm3, %xmm4
	mulss	56(%rsp), %xmm4
	addss	%xmm2, %xmm4
	divss	%xmm0, %xmm4
	movss	%xmm4, (%rbx,%rbp,4)
	movss	72(%rsp), %xmm2
	mulss	%xmm3, %xmm2
	movss	%xmm2, 72(%rsp)
	movss	56(%rsp), %xmm2
	mulss	%xmm1, %xmm2
	subss	72(%rsp), %xmm2
	divss	%xmm0, %xmm2
	movss	%xmm2, 56(%rsp)
	jmp	.LBB36_61	# bb196.backedge
	.size	cblas_ctrsv, .-cblas_ctrsv
.Leh_func_end26:


	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI37_0:					
	.quad	9223372036854775807	# double value: nan
	.quad	9223372036854775807	# double value: nan
	.section	.rodata.cst8,"aM",@progbits,8
	.align	16
.LCPI37_1:					
	.quad	4607182418800017408	# double value: 1.000000e+00
	.text
	.align	16
	.type	_ZL6xhypotdd54,@function
_ZL6xhypotdd54:
	movsd	.LCPI37_0(%rip), %xmm2
	andpd	%xmm2, %xmm0
	andpd	%xmm2, %xmm1
	movapd	%xmm1, %xmm2
	maxsd	%xmm0, %xmm2
	minsd	%xmm1, %xmm0
	pxor	%xmm1, %xmm1
	ucomisd	%xmm1, %xmm0
	setnp	%al
	sete	%cl
	testb	%al, %cl
	jne	.LBB37_2	# bb5
.LBB37_1:	# bb4
	divsd	%xmm2, %xmm0
	mulsd	%xmm0, %xmm0
	addsd	.LCPI37_1(%rip), %xmm0
	sqrtsd	%xmm0, %xmm0
	mulsd	%xmm2, %xmm0
	ret
.LBB37_2:	# bb5
	movapd	%xmm2, %xmm0
	ret
	.size	_ZL6xhypotdd54, .-_ZL6xhypotdd54


	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI38_0:					
	.quad	9223372036854775807	# double value: nan
	.quad	9223372036854775807	# double value: nan
	.text
	.align	16
	.globl	cblas_dasum
	.type	cblas_dasum,@function
cblas_dasum:
	testl	%edx, %edx
	jle	.LBB38_5	# entry.bb5_crit_edge
.LBB38_1:	# entry
	testl	%edi, %edi
	jle	.LBB38_5	# entry.bb5_crit_edge
.LBB38_2:	# entry.bb2_crit_edge
	pxor	%xmm0, %xmm0
	xorl	%eax, %eax
	movl	%eax, %ecx
	.align	16
.LBB38_3:	# bb2
	leal	(%rdx,%rax), %r8d
	incl	%ecx
	cmpl	%edi, %ecx
	movslq	%eax, %rax
	movsd	(%rsi,%rax,8), %xmm1
	andpd	.LCPI38_0(%rip), %xmm1
	addsd	%xmm1, %xmm0
	movl	%r8d, %eax
	jne	.LBB38_3	# bb2
.LBB38_4:	# bb5
	ret
.LBB38_5:	# entry.bb5_crit_edge
	pxor	%xmm0, %xmm0
	jmp	.LBB38_4	# bb5
	.size	cblas_dasum, .-cblas_dasum


	.align	16
	.globl	cblas_daxpy
	.type	cblas_daxpy,@function
cblas_daxpy:
.Leh_func_begin27:
.Llabel27:
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$16, %rsp
	movl	%r8d, %ebx
	movq	%rcx, %r14
	movl	%edx, %r15d
	movq	%rsi, %r12
	movsd	%xmm0, 8(%rsp)
	movl	%edi, %r13d
	xorl	%edi, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB39_18	# real_catch0
.LBB39_1:	# jump0
	xorl	%edi, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB39_19	# real_end0
.LBB39_2:	# real_try0
	xorl	%edi, %edi
	call	llvm_real_try
	pxor	%xmm0, %xmm0
	movsd	8(%rsp), %xmm1
	ucomisd	%xmm0, %xmm1
	setnp	%al
	sete	%cl
	testb	%al, %cl
	jne	.LBB39_20	# return
.LBB39_3:	# bb
	cmpl	$1, %ebx
	jne	.LBB39_11	# bb9
.LBB39_4:	# bb
	cmpl	$1, %r15d
	jne	.LBB39_11	# bb9
.LBB39_5:	# bb3
	movl	%r13d, %ebx
	sarl	$31, %ebx
	shrl	$30, %ebx
	addl	%r13d, %ebx
	andl	$4294967292, %ebx
	movl	%r13d, %r15d
	subl	%ebx, %r15d
	testl	%r15d, %r15d
	jle	.LBB39_8	# bb8.loopexit
.LBB39_6:	# bb3.bb4_crit_edge
	xorl	%ebx, %ebx
	.align	16
.LBB39_7:	# bb4
	movslq	%ebx, %rax
	movsd	8(%rsp), %xmm0
	mulsd	(%r12,%rax,8), %xmm0
	addsd	(%r14,%rax,8), %xmm0
	movsd	%xmm0, (%r14,%rax,8)
	incl	%ebx
	cmpl	%r15d, %ebx
	jne	.LBB39_7	# bb4
.LBB39_8:	# bb8.loopexit
	leal	3(%r15), %ebx
	.align	16
.LBB39_9:	# bb8.loopexit
	cmpl	%r13d, %ebx
	jge	.LBB39_19	# real_end0
.LBB39_10:	# bb7
	movslq	%r15d, %rax
	movsd	8(%rsp), %xmm0
	movapd	%xmm0, %xmm1
	mulsd	(%r12,%rax,8), %xmm1
	addsd	(%r14,%rax,8), %xmm1
	movsd	%xmm1, (%r14,%rax,8)
	leal	1(%r15), %ebx
	movslq	%ebx, %rax
	movapd	%xmm0, %xmm1
	mulsd	(%r12,%rax,8), %xmm1
	addsd	(%r14,%rax,8), %xmm1
	movsd	%xmm1, (%r14,%rax,8)
	leal	2(%r15), %ebx
	movslq	%ebx, %rax
	movapd	%xmm0, %xmm1
	mulsd	(%r12,%rax,8), %xmm1
	addsd	(%r14,%rax,8), %xmm1
	movsd	%xmm1, (%r14,%rax,8)
	leal	3(%r15), %ebx
	movslq	%ebx, %rax
	mulsd	(%r12,%rax,8), %xmm0
	addsd	(%r14,%rax,8), %xmm0
	movsd	%xmm0, (%r14,%rax,8)
	leal	7(%r15), %ebx
	leal	4(%r15), %r15d
	jmp	.LBB39_9	# bb8.loopexit
.LBB39_11:	# bb9
	testl	%r15d, %r15d
	jg	.LBB39_21	# bb9.bb12_crit_edge
.LBB39_12:	# bb10
	movl	$1, %eax
	subl	%r13d, %eax
	imull	%r15d, %eax
.LBB39_13:	# bb12
	testl	%ebx, %ebx
	jg	.LBB39_22	# bb12.bb17.preheader_crit_edge
.LBB39_14:	# bb13
	movl	$1, %ecx
	subl	%r13d, %ecx
	imull	%ebx, %ecx
.LBB39_15:	# bb17.preheader
	testl	%r13d, %r13d
	jle	.LBB39_19	# real_end0
.LBB39_16:	# bb17.preheader.bb16_crit_edge
	xorl	%edx, %edx
	.align	16
.LBB39_17:	# bb16
	movslq	%eax, %rsi
	movsd	8(%rsp), %xmm0
	mulsd	(%r12,%rsi,8), %xmm0
	movslq	%ecx, %rsi
	addsd	(%r14,%rsi,8), %xmm0
	movsd	%xmm0, (%r14,%rsi,8)
	addl	%ebx, %ecx
	addl	%r15d, %eax
	incl	%edx
	cmpl	%r13d, %edx
	jne	.LBB39_17	# bb16
	jmp	.LBB39_19	# real_end0
.LBB39_18:	# real_catch0
	xorl	%edi, %edi
	call	llvm_real_catch
.LBB39_19:	# real_end0
	xorl	%edi, %edi
	call	llvm_real_end
.LBB39_20:	# return
	addq	$16, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	ret
.LBB39_21:	# bb9.bb12_crit_edge
	xorl	%eax, %eax
	jmp	.LBB39_13	# bb12
.LBB39_22:	# bb12.bb17.preheader_crit_edge
	xorl	%ecx, %ecx
	jmp	.LBB39_15	# bb17.preheader
	.size	cblas_daxpy, .-cblas_daxpy
.Leh_func_end27:


	.align	16
	.globl	cblas_dcopy
	.type	cblas_dcopy,@function
cblas_dcopy:
	testl	%edx, %edx
	jg	.LBB40_8	# entry.bb2_crit_edge
.LBB40_1:	# bb
	movl	$1, %eax
	subl	%edi, %eax
	imull	%edx, %eax
.LBB40_2:	# bb2
	testl	%r8d, %r8d
	jg	.LBB40_9	# bb2.bb7.preheader_crit_edge
.LBB40_3:	# bb3
	movl	$1, %r9d
	subl	%edi, %r9d
	imull	%r8d, %r9d
.LBB40_4:	# bb7.preheader
	testl	%edi, %edi
	jle	.LBB40_7	# return
.LBB40_5:	# bb7.preheader.bb6_crit_edge
	xorl	%r10d, %r10d
	.align	16
.LBB40_6:	# bb6
	movslq	%eax, %r11
	movsd	(%rsi,%r11,8), %xmm0
	movslq	%r9d, %r11
	movsd	%xmm0, (%rcx,%r11,8)
	addl	%r8d, %r9d
	addl	%edx, %eax
	incl	%r10d
	cmpl	%edi, %r10d
	jne	.LBB40_6	# bb6
.LBB40_7:	# return
	ret
.LBB40_8:	# entry.bb2_crit_edge
	xorl	%eax, %eax
	jmp	.LBB40_2	# bb2
.LBB40_9:	# bb2.bb7.preheader_crit_edge
	xorl	%r9d, %r9d
	jmp	.LBB40_4	# bb7.preheader
	.size	cblas_dcopy, .-cblas_dcopy


	.align	16
	.globl	cblas_ddot
	.type	cblas_ddot,@function
cblas_ddot:
.Leh_func_begin28:
.Llabel28:
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	movl	%r8d, %ebx
	movq	%rcx, %r14
	movl	%edx, %r15d
	movq	%rsi, %r12
	movl	%edi, %r13d
	xorl	%edi, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB41_9	# real_catch0
.LBB41_1:	# jump0
	xorl	%edi, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB41_10	# real_end0
.LBB41_2:	# real_try0
	xorl	%edi, %edi
	call	llvm_real_try
	testl	%r15d, %r15d
	jg	.LBB41_12	# real_try0.bb2_crit_edge
.LBB41_3:	# bb
	movl	$1, %eax
	subl	%r13d, %eax
	imull	%r15d, %eax
.LBB41_4:	# bb2
	testl	%ebx, %ebx
	jg	.LBB41_13	# bb2.bb7.preheader_crit_edge
.LBB41_5:	# bb3
	movl	$1, %ecx
	subl	%r13d, %ecx
	imull	%ebx, %ecx
.LBB41_6:	# bb7.preheader
	testl	%r13d, %r13d
	jle	.LBB41_14	# bb7.preheader.return_crit_edge
.LBB41_7:	# bb7.preheader.bb6_crit_edge
	pxor	%xmm0, %xmm0
	xorl	%edx, %edx
	.align	16
.LBB41_8:	# bb6
	leal	(%rbx,%rcx), %esi
	leal	(%r15,%rax), %edi
	incl	%edx
	cmpl	%r13d, %edx
	movslq	%eax, %rax
	movsd	(%r12,%rax,8), %xmm1
	movslq	%ecx, %rax
	mulsd	(%r14,%rax,8), %xmm1
	addsd	%xmm1, %xmm0
	movl	%esi, %ecx
	movl	%edi, %eax
	jne	.LBB41_8	# bb6
	jmp	.LBB41_11	# return
.LBB41_9:	# real_catch0
	xorl	%edi, %edi
	call	llvm_real_catch
.LBB41_10:	# real_end0
	xorl	%edi, %edi
	call	llvm_real_end
	# implicit-def: xmm0
.LBB41_11:	# return
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	ret
.LBB41_12:	# real_try0.bb2_crit_edge
	xorl	%eax, %eax
	jmp	.LBB41_4	# bb2
.LBB41_13:	# bb2.bb7.preheader_crit_edge
	xorl	%ecx, %ecx
	jmp	.LBB41_6	# bb7.preheader
.LBB41_14:	# bb7.preheader.return_crit_edge
	pxor	%xmm0, %xmm0
	jmp	.LBB41_11	# return
	.size	cblas_ddot, .-cblas_ddot
.Leh_func_end28:


	.section	.rodata.cst8,"aM",@progbits,8
	.align	8
.LCPI42_0:					
	.quad	4607182418800017408	# double value: 1.000000e+00
	.text
	.align	16
	.globl	cblas_dgbmv
	.type	cblas_dgbmv,@function
cblas_dgbmv:
.Leh_func_begin29:
.Llabel29:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$24, %rsp
	cmpl	$113, %esi
	movl	$112, %eax
	cmovne	%esi, %eax
	testl	%ecx, %ecx
	movl	120(%rsp), %esi
	movq	112(%rsp), %r10
	movq	80(%rsp), %r11
	je	.LBB42_43	# return
.LBB42_1:	# entry
	testl	%edx, %edx
	je	.LBB42_43	# return
.LBB42_2:	# bb10
	ucomisd	.LCPI42_0(%rip), %xmm1
	jne	.LBB42_4	# bb12
	jp	.LBB42_4	# bb12
.LBB42_3:	# bb10
	pxor	%xmm2, %xmm2
	ucomisd	%xmm2, %xmm0
	setnp	%bl
	sete	%r14b
	testb	%bl, %r14b
	jne	.LBB42_43	# return
.LBB42_4:	# bb12
	cmpl	$111, %eax
	je	.LBB42_44	# bb12.bb15_crit_edge
.LBB42_5:	# bb14
	movl	%r8d, 16(%rsp)
	movl	%r9d, %r8d
	movl	%ecx, %r9d
	movl	%edx, %ecx
.LBB42_6:	# bb15
	movl	%r8d, 12(%rsp)
	movl	%ecx, 20(%rsp)
	pxor	%xmm2, %xmm2
	ucomisd	%xmm2, %xmm1
	jne	.LBB42_12	# bb22
	jp	.LBB42_12	# bb22
.LBB42_7:	# bb16
	testl	%esi, %esi
	jg	.LBB42_45	# bb16.bb21.preheader_crit_edge
.LBB42_8:	# bb17
	movl	$1, %ecx
	subl	%r9d, %ecx
	imull	%esi, %ecx
.LBB42_9:	# bb21.preheader
	testl	%r9d, %r9d
	jle	.LBB42_18	# bb29
.LBB42_10:	# bb21.preheader.bb20_crit_edge
	xorl	%edx, %edx
	.align	16
.LBB42_11:	# bb20
	movslq	%ecx, %r8
	movq	$0, (%r10,%r8,8)
	addl	%esi, %ecx
	incl	%edx
	cmpl	%r9d, %edx
	jne	.LBB42_11	# bb20
	jmp	.LBB42_18	# bb29
.LBB42_12:	# bb22
	ucomisd	.LCPI42_0(%rip), %xmm1
	setnp	%cl
	sete	%dl
	testb	%cl, %dl
	jne	.LBB42_18	# bb29
.LBB42_13:	# bb23
	testl	%esi, %esi
	jg	.LBB42_46	# bb23.bb28.preheader_crit_edge
.LBB42_14:	# bb24
	movl	$1, %ecx
	subl	%r9d, %ecx
	imull	%esi, %ecx
.LBB42_15:	# bb28.preheader
	testl	%r9d, %r9d
	jle	.LBB42_18	# bb29
.LBB42_16:	# bb28.preheader.bb27_crit_edge
	xorl	%edx, %edx
	.align	16
.LBB42_17:	# bb27
	movslq	%ecx, %r8
	movapd	%xmm1, %xmm2
	mulsd	(%r10,%r8,8), %xmm2
	movsd	%xmm2, (%r10,%r8,8)
	addl	%esi, %ecx
	incl	%edx
	cmpl	%r9d, %edx
	jne	.LBB42_17	# bb27
.LBB42_18:	# bb29
	pxor	%xmm1, %xmm1
	ucomisd	%xmm1, %xmm0
	setnp	%cl
	sete	%dl
	testb	%cl, %dl
	jne	.LBB42_43	# return
.LBB42_19:	# bb30
	cmpl	$111, %eax
	jne	.LBB42_21	# bb34
.LBB42_20:	# bb30
	cmpl	$101, %edi
	je	.LBB42_23	# bb38
.LBB42_21:	# bb34
	cmpl	$112, %eax
	jne	.LBB42_31	# bb53
.LBB42_22:	# bb34
	cmpl	$102, %edi
	jne	.LBB42_31	# bb53
.LBB42_23:	# bb38
	testl	%esi, %esi
	jg	.LBB42_47	# bb38.bb52.preheader_crit_edge
.LBB42_24:	# bb39
	movl	$1, %eax
	subl	%r9d, %eax
	imull	%esi, %eax
.LBB42_25:	# bb52.preheader
	testl	%r9d, %r9d
	jle	.LBB42_43	# return
.LBB42_26:	# bb.nph104
	movl	$1, %edi
	subl	20(%rsp), %edi
	imull	104(%rsp), %edi
	movl	%edi, (%rsp)
	movl	$4294967294, %edi
	movl	16(%rsp), %ecx
	subl	%ecx, %edi
	movl	88(%rsp), %edx
	decl	%edx
	movl	%edx, 8(%rsp)
	movl	12(%rsp), %edx
	movl	%edx, %r8d
	negl	%r8d
	movl	%r8d, 4(%rsp)
	incl	%ecx
	movl	%ecx, 16(%rsp)
	xorl	%ecx, %ecx
	.align	16
.LBB42_27:	# bb42
	xorl	%r8d, %r8d
	movl	104(%rsp), %ebx
	testl	%ebx, %ebx
	movl	(%rsp), %r14d
	cmovg	%r8d, %r14d
	movl	4(%rsp), %r15d
	leal	(%r15,%rcx), %r15d
	cmpl	12(%rsp), %ecx
	cmovle	%r8d, %r15d
	movl	%r15d, %r8d
	imull	%ebx, %r8d
	movl	16(%rsp), %ebx
	leal	(%rbx,%rcx), %ebx
	movl	20(%rsp), %r12d
	cmpl	%r12d, %ebx
	cmovg	%r12d, %ebx
	cmpl	%ebx, %r15d
	jge	.LBB42_48	# bb42.bb51_crit_edge
.LBB42_28:	# bb.nph100
	movl	20(%rsp), %ebx
	notl	%ebx
	cmpl	%ebx, %edi
	cmovge	%edi, %ebx
	addl	%r15d, %ebx
	notl	%ebx
	addl	%edx, %r15d
	pxor	%xmm1, %xmm1
	xorl	%r12d, %r12d
	.align	16
.LBB42_29:	# bb49
	leal	(%r15,%r12), %r13d
	movslq	%r13d, %r13
	addl	%r14d, %r8d
	movslq	%r8d, %r14
	movq	96(%rsp), %rbp
	movsd	(%rbp,%r14,8), %xmm2
	mulsd	(%r11,%r13,8), %xmm2
	addsd	%xmm2, %xmm1
	incl	%r12d
	cmpl	%ebx, %r12d
	movl	104(%rsp), %r14d
	jne	.LBB42_29	# bb49
.LBB42_30:	# bb51
	mulsd	%xmm0, %xmm1
	movslq	%eax, %r8
	addsd	(%r10,%r8,8), %xmm1
	movsd	%xmm1, (%r10,%r8,8)
	addl	%esi, %eax
	addl	8(%rsp), %edx
	decl	%edi
	incl	%ecx
	cmpl	%r9d, %ecx
	jne	.LBB42_27	# bb42
	jmp	.LBB42_43	# return
.LBB42_31:	# bb53
	cmpl	$102, %edi
	sete	%cl
	cmpl	$111, %eax
	sete	%dl
	andb	%cl, %dl
	cmpl	$101, %edi
	sete	%cl
	cmpl	$112, %eax
	sete	%al
	testb	%cl, %al
	jne	.LBB42_33	# bb61
.LBB42_32:	# bb53
	notb	%dl
	testb	$1, %dl
	jne	.LBB42_42	# bb78
.LBB42_33:	# bb61
	cmpl	$0, 104(%rsp)
	jg	.LBB42_49	# bb61.bb77.preheader_crit_edge
.LBB42_34:	# bb62
	movl	$1, %eax
	subl	20(%rsp), %eax
	imull	104(%rsp), %eax
.LBB42_35:	# bb77.preheader
	cmpl	$0, 20(%rsp)
	jle	.LBB42_43	# return
.LBB42_36:	# bb.nph93
	movl	$1, %ecx
	subl	%r9d, %ecx
	imull	%esi, %ecx
	movl	%ecx, 4(%rsp)
	movl	$4294967294, %ecx
	movl	12(%rsp), %edx
	subl	%edx, %ecx
	movl	88(%rsp), %edi
	decl	%edi
	movl	%edi, 8(%rsp)
	movl	16(%rsp), %edi
	movl	%edi, %r8d
	negl	%r8d
	incl	%edx
	movl	%edx, 12(%rsp)
	xorl	%edx, %edx
	.align	16
.LBB42_37:	# bb65
	movslq	%eax, %rbx
	movapd	%xmm0, %xmm1
	movq	96(%rsp), %r14
	mulsd	(%r14,%rbx,8), %xmm1
	pxor	%xmm2, %xmm2
	ucomisd	%xmm2, %xmm1
	setnp	%bl
	sete	%r14b
	testb	%bl, %r14b
	jne	.LBB42_41	# bb76
.LBB42_38:	# bb66
	xorl	%ebx, %ebx
	testl	%esi, %esi
	movl	4(%rsp), %r14d
	cmovg	%ebx, %r14d
	leal	(%r8,%rdx), %r15d
	cmpl	16(%rsp), %edx
	cmovle	%ebx, %r15d
	movl	%r15d, %ebx
	imull	%esi, %ebx
	movl	12(%rsp), %r12d
	leal	(%r12,%rdx), %r12d
	cmpl	%r9d, %r12d
	cmovg	%r9d, %r12d
	cmpl	%r12d, %r15d
	jge	.LBB42_41	# bb76
.LBB42_39:	# bb.nph90
	movl	%r9d, %r12d
	notl	%r12d
	cmpl	%r12d, %ecx
	cmovge	%ecx, %r12d
	addl	%r15d, %r12d
	notl	%r12d
	addl	%edi, %r15d
	xorl	%r13d, %r13d
	.align	16
.LBB42_40:	# bb74
	addl	%r14d, %ebx
	movslq	%ebx, %r14
	leal	(%r15,%r13), %ebp
	movslq	%ebp, %rbp
	movapd	%xmm1, %xmm2
	mulsd	(%r11,%rbp,8), %xmm2
	addsd	(%r10,%r14,8), %xmm2
	movsd	%xmm2, (%r10,%r14,8)
	incl	%r13d
	cmpl	%r12d, %r13d
	movl	%esi, %r14d
	jne	.LBB42_40	# bb74
.LBB42_41:	# bb76
	addl	104(%rsp), %eax
	addl	8(%rsp), %edi
	decl	%ecx
	incl	%edx
	cmpl	20(%rsp), %edx
	jne	.LBB42_37	# bb65
	jmp	.LBB42_43	# return
.LBB42_42:	# bb78
	xorl	%edi, %edi
	leaq	.str55, %rsi
	leaq	.str156, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB42_43:	# return
	addq	$24, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB42_44:	# bb12.bb15_crit_edge
	movl	%r9d, 16(%rsp)
	movl	%edx, %r9d
	jmp	.LBB42_6	# bb15
.LBB42_45:	# bb16.bb21.preheader_crit_edge
	xorl	%ecx, %ecx
	jmp	.LBB42_9	# bb21.preheader
.LBB42_46:	# bb23.bb28.preheader_crit_edge
	xorl	%ecx, %ecx
	jmp	.LBB42_15	# bb28.preheader
.LBB42_47:	# bb38.bb52.preheader_crit_edge
	xorl	%eax, %eax
	jmp	.LBB42_25	# bb52.preheader
.LBB42_48:	# bb42.bb51_crit_edge
	pxor	%xmm1, %xmm1
	jmp	.LBB42_30	# bb51
.LBB42_49:	# bb61.bb77.preheader_crit_edge
	xorl	%eax, %eax
	jmp	.LBB42_35	# bb77.preheader
	.size	cblas_dgbmv, .-cblas_dgbmv
.Leh_func_end29:


	.section	.rodata.cst8,"aM",@progbits,8
	.align	8
.LCPI43_0:					
	.quad	4607182418800017408	# double value: 1.000000e+00
	.text
	.align	16
	.globl	cblas_dgemm
	.type	cblas_dgemm,@function
cblas_dgemm:
.Leh_func_begin30:
.Llabel30:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	ucomisd	.LCPI43_0(%rip), %xmm1
	movq	128(%rsp), %rbx
	movl	104(%rsp), %r14d
	movq	96(%rsp), %r15
	movsd	%xmm1, 16(%rsp)
	movsd	%xmm0, 32(%rsp)
	movl	%r9d, %r12d
	movl	%r8d, 4(%rsp)
	movl	%ecx, 28(%rsp)
	movl	%edx, 12(%rsp)
	movl	%esi, 8(%rsp)
	jne	.LBB43_2	# bb4
	jp	.LBB43_2	# bb4
.LBB43_1:	# entry
	pxor	%xmm0, %xmm0
	movsd	32(%rsp), %xmm1
	ucomisd	%xmm0, %xmm1
	setnp	%al
	sete	%cl
	testb	%al, %cl
	jne	.LBB43_97	# return
.LBB43_2:	# bb4
	cmpl	$101, %edi
	jne	.LBB43_8	# bb12
.LBB43_3:	# bb5
	movl	$1, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB43_6	# real_catch1
.LBB43_4:	# jump1
	movl	$1, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB43_7	# real_end1
.LBB43_5:	# real_try1
	movl	$1, %edi
	call	llvm_real_try
	movl	12(%rsp), %eax
	cmpl	$113, %eax
	movl	$112, %ecx
	cmove	%ecx, %eax
	movl	%eax, 12(%rsp)
	movl	8(%rsp), %eax
	cmpl	$113, %eax
	cmove	%ecx, %eax
	movl	%eax, 8(%rsp)
	jmp	.LBB43_7	# real_end1
.LBB43_6:	# real_catch1
	movl	$1, %edi
	call	llvm_real_catch
.LBB43_7:	# real_end1
	movl	$1, %edi
	call	llvm_real_end
	movq	112(%rsp), %r13
	movl	120(%rsp), %eax
	movl	%eax, 24(%rsp)
	movl	4(%rsp), %ebp
	jmp	.LBB43_13	# bb19
.LBB43_8:	# bb12
	xorl	%edi, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB43_11	# real_catch0
.LBB43_9:	# jump0
	xorl	%edi, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB43_12	# real_end0
.LBB43_10:	# real_try0
	xorl	%edi, %edi
	call	llvm_real_try
	movl	8(%rsp), %ebp
	cmpl	$113, %ebp
	movl	$112, %eax
	cmove	%eax, %ebp
	movl	%ebp, 8(%rsp)
	movl	12(%rsp), %ebp
	cmpl	$113, %ebp
	cmove	%eax, %ebp
	movl	%ebp, 12(%rsp)
	jmp	.LBB43_12	# real_end0
.LBB43_11:	# real_catch0
	xorl	%edi, %edi
	call	llvm_real_catch
.LBB43_12:	# real_end0
	movl	12(%rsp), %ebp
	xorl	%edi, %edi
	call	llvm_real_end
	movq	%r15, %r13
	movq	112(%rsp), %r15
	movl	8(%rsp), %eax
	movl	%eax, 12(%rsp)
	movl	%ebp, 8(%rsp)
	movl	%r14d, 24(%rsp)
	movl	120(%rsp), %r14d
	movl	28(%rsp), %ebp
	movl	4(%rsp), %eax
	movl	%eax, 28(%rsp)
.LBB43_13:	# bb19
	movl	$2, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB43_29	# real_catch2
.LBB43_14:	# jump2
	movl	$2, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB43_30	# real_end2
.LBB43_15:	# real_try2
	movl	$2, %edi
	call	llvm_real_try
	pxor	%xmm0, %xmm0
	movsd	16(%rsp), %xmm1
	ucomisd	%xmm0, %xmm1
	jne	.LBB43_22	# bb26
	jp	.LBB43_22	# bb26
.LBB43_16:	# bb25.preheader
	cmpl	$0, 28(%rsp)
	jle	.LBB43_30	# real_end2
.LBB43_17:	# bb25.preheader
	testl	%ebp, %ebp
	jle	.LBB43_30	# real_end2
.LBB43_18:	# bb25.preheader.bb23.preheader_crit_edge
	xorl	%eax, %eax
	movl	%eax, %ecx
	jmp	.LBB43_21	# bb23.preheader
	.align	16
.LBB43_19:	# bb22
	leal	(%rax,%rdx), %esi
	movslq	%esi, %rsi
	movq	$0, (%rbx,%rsi,8)
	incl	%edx
	cmpl	%ebp, %edx
	jne	.LBB43_19	# bb22
.LBB43_20:	# bb24
	addl	136(%rsp), %eax
	incl	%ecx
	cmpl	28(%rsp), %ecx
	je	.LBB43_30	# real_end2
.LBB43_21:	# bb23.preheader
	xorl	%edx, %edx
	jmp	.LBB43_19	# bb22
.LBB43_22:	# bb26
	movsd	16(%rsp), %xmm0
	ucomisd	.LCPI43_0(%rip), %xmm0
	setnp	%al
	sete	%cl
	testb	%al, %cl
	jne	.LBB43_30	# real_end2
.LBB43_23:	# bb26
	cmpl	$0, 28(%rsp)
	jle	.LBB43_30	# real_end2
.LBB43_24:	# bb26
	testl	%ebp, %ebp
	jle	.LBB43_30	# real_end2
.LBB43_25:	# bb26.bb30.preheader_crit_edge
	xorl	%eax, %eax
	movl	%eax, %ecx
	jmp	.LBB43_28	# bb30.preheader
	.align	16
.LBB43_26:	# bb29
	leal	(%rax,%rdx), %esi
	movslq	%esi, %rsi
	movsd	16(%rsp), %xmm0
	mulsd	(%rbx,%rsi,8), %xmm0
	movsd	%xmm0, (%rbx,%rsi,8)
	incl	%edx
	cmpl	%ebp, %edx
	jne	.LBB43_26	# bb29
.LBB43_27:	# bb31
	addl	136(%rsp), %eax
	incl	%ecx
	cmpl	28(%rsp), %ecx
	je	.LBB43_30	# real_end2
.LBB43_28:	# bb30.preheader
	xorl	%edx, %edx
	jmp	.LBB43_26	# bb29
.LBB43_29:	# real_catch2
	movl	$2, %edi
	call	llvm_real_catch
.LBB43_30:	# real_end2
	movl	$2, %edi
	call	llvm_real_end
	pxor	%xmm0, %xmm0
	movsd	32(%rsp), %xmm1
	ucomisd	%xmm0, %xmm1
	setnp	%al
	sete	%cl
	testb	%al, %cl
	jne	.LBB43_97	# return
.LBB43_31:	# bb33
	cmpl	$111, 8(%rsp)
	jne	.LBB43_48	# bb46
.LBB43_32:	# bb33
	cmpl	$111, 12(%rsp)
	jne	.LBB43_48	# bb46
.LBB43_33:	# bb36
	movl	$3, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB43_45	# real_catch3
.LBB43_34:	# jump3
	movl	$3, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB43_46	# real_end3
.LBB43_35:	# real_try3
	movl	$3, %edi
	call	llvm_real_try
	testl	%r12d, %r12d
	jle	.LBB43_46	# real_end3
.LBB43_36:	# bb.nph123
	cmpl	$0, 28(%rsp)
	jle	.LBB43_46	# real_end3
.LBB43_37:	# bb.nph123.bb43.preheader_crit_edge
	xorl	%eax, %eax
	movl	%eax, %ecx
	jmp	.LBB43_44	# bb43.preheader
	.align	16
.LBB43_38:	# bb38
	movslq	%edi, %r8
	movsd	32(%rsp), %xmm0
	mulsd	(%r15,%r8,8), %xmm0
	pxor	%xmm1, %xmm1
	ucomisd	%xmm1, %xmm0
	setnp	%r8b
	sete	%r9b
	testl	%ebp, %ebp
	setle	%r10b
	testb	%r8b, %r9b
	jne	.LBB43_42	# bb42
.LBB43_39:	# bb38
	testb	$1, %r10b
	jne	.LBB43_42	# bb42
.LBB43_40:	# bb38.bb40_crit_edge
	xorl	%r8d, %r8d
	.align	16
.LBB43_41:	# bb40
	leal	(%rsi,%r8), %r9d
	movslq	%r9d, %r9
	leal	(%rax,%r8), %r10d
	movslq	%r10d, %r10
	movapd	%xmm0, %xmm1
	mulsd	(%r13,%r10,8), %xmm1
	addsd	(%rbx,%r9,8), %xmm1
	movsd	%xmm1, (%rbx,%r9,8)
	incl	%r8d
	cmpl	%ebp, %r8d
	jne	.LBB43_41	# bb40
.LBB43_42:	# bb42
	addl	%r14d, %edi
	addl	136(%rsp), %esi
	incl	%edx
	cmpl	28(%rsp), %edx
	jne	.LBB43_38	# bb38
.LBB43_43:	# bb44
	addl	24(%rsp), %eax
	incl	%ecx
	cmpl	%r12d, %ecx
	je	.LBB43_46	# real_end3
.LBB43_44:	# bb43.preheader
	xorl	%esi, %esi
	movl	%ecx, %edi
	movl	%esi, %edx
	jmp	.LBB43_38	# bb38
.LBB43_45:	# real_catch3
	movl	$3, %edi
	call	llvm_real_catch
.LBB43_46:	# real_end3
	movl	$3, %edi
.LBB43_47:	# real_end3
	call	llvm_real_end
	jmp	.LBB43_97	# return
.LBB43_48:	# bb46
	cmpl	$111, 8(%rsp)
	jne	.LBB43_64	# bb59
.LBB43_49:	# bb46
	cmpl	$112, 12(%rsp)
	jne	.LBB43_64	# bb59
.LBB43_50:	# bb50
	movl	$4, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB43_62	# real_catch4
.LBB43_51:	# jump4
	movl	$4, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB43_63	# real_end4
.LBB43_52:	# real_try4
	movl	$4, %edi
	call	llvm_real_try
	cmpl	$0, 28(%rsp)
	jle	.LBB43_63	# real_end4
.LBB43_53:	# bb.nph117
	testl	%ebp, %ebp
	jle	.LBB43_63	# real_end4
.LBB43_54:	# bb.nph117.bb56.preheader_crit_edge
	xorl	%edx, %edx
	movl	%edx, %ecx
	movl	%edx, %edi
	jmp	.LBB43_61	# bb56.preheader
.LBB43_55:	# bb54.preheader.bb53_crit_edge
	pxor	%xmm0, %xmm0
	xorl	%eax, %eax
	.align	16
.LBB43_56:	# bb53
	leal	(%r8,%rax), %r9d
	movslq	%r9d, %r9
	leal	(%rdx,%rax), %r10d
	movslq	%r10d, %r10
	movsd	(%r15,%r10,8), %xmm1
	mulsd	(%r13,%r9,8), %xmm1
	addsd	%xmm1, %xmm0
	incl	%eax
	cmpl	%r12d, %eax
	jne	.LBB43_56	# bb53
.LBB43_57:	# bb55
	leal	(%rcx,%rsi), %eax
	movslq	%eax, %rax
	mulsd	32(%rsp), %xmm0
	addsd	(%rbx,%rax,8), %xmm0
	movsd	%xmm0, (%rbx,%rax,8)
	addl	24(%rsp), %r8d
	incl	%esi
	cmpl	%ebp, %esi
	je	.LBB43_60	# bb57
.LBB43_58:	# bb54.preheader
	testl	%r12d, %r12d
	jg	.LBB43_55	# bb54.preheader.bb53_crit_edge
.LBB43_59:	# bb54.preheader.bb55_crit_edge
	pxor	%xmm0, %xmm0
	jmp	.LBB43_57	# bb55
.LBB43_60:	# bb57
	addl	%r14d, %edx
	addl	136(%rsp), %ecx
	incl	%edi
	cmpl	28(%rsp), %edi
	je	.LBB43_63	# real_end4
.LBB43_61:	# bb56.preheader
	xorl	%r8d, %r8d
	movl	%r8d, %esi
	jmp	.LBB43_58	# bb54.preheader
.LBB43_62:	# real_catch4
	movl	$4, %edi
	call	llvm_real_catch
.LBB43_63:	# real_end4
	movl	$4, %edi
	jmp	.LBB43_47	# real_end3
.LBB43_64:	# bb59
	cmpl	$112, 8(%rsp)
	jne	.LBB43_80	# bb73
.LBB43_65:	# bb59
	cmpl	$111, 12(%rsp)
	jne	.LBB43_80	# bb73
.LBB43_66:	# bb63
	movl	$5, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB43_78	# real_catch5
.LBB43_67:	# jump5
	movl	$5, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB43_79	# real_end5
.LBB43_68:	# real_try5
	movl	$5, %edi
	call	llvm_real_try
	testl	%r12d, %r12d
	jle	.LBB43_79	# real_end5
.LBB43_69:	# bb.nph109
	cmpl	$0, 28(%rsp)
	jle	.LBB43_79	# real_end5
.LBB43_70:	# bb.nph109.bb70.preheader_crit_edge
	xorl	%edi, %edi
	movl	%edi, %eax
	movl	%edi, %edx
	jmp	.LBB43_77	# bb70.preheader
	.align	16
.LBB43_71:	# bb65
	leal	(%rax,%rcx), %r8d
	movslq	%r8d, %r8
	movsd	32(%rsp), %xmm0
	mulsd	(%r15,%r8,8), %xmm0
	pxor	%xmm1, %xmm1
	ucomisd	%xmm1, %xmm0
	setnp	%r8b
	sete	%r9b
	testl	%ebp, %ebp
	setle	%r10b
	testb	%r8b, %r9b
	jne	.LBB43_75	# bb69
.LBB43_72:	# bb65
	testb	$1, %r10b
	jne	.LBB43_75	# bb69
.LBB43_73:	# bb65.bb67_crit_edge
	xorl	%r8d, %r8d
	.align	16
.LBB43_74:	# bb67
	leal	(%rsi,%r8), %r9d
	movslq	%r9d, %r9
	leal	(%rdi,%r8), %r10d
	movslq	%r10d, %r10
	movapd	%xmm0, %xmm1
	mulsd	(%r13,%r10,8), %xmm1
	addsd	(%rbx,%r9,8), %xmm1
	movsd	%xmm1, (%rbx,%r9,8)
	incl	%r8d
	cmpl	%ebp, %r8d
	jne	.LBB43_74	# bb67
.LBB43_75:	# bb69
	addl	136(%rsp), %esi
	incl	%ecx
	cmpl	28(%rsp), %ecx
	jne	.LBB43_71	# bb65
.LBB43_76:	# bb71
	addl	24(%rsp), %edi
	addl	%r14d, %eax
	incl	%edx
	cmpl	%r12d, %edx
	je	.LBB43_79	# real_end5
.LBB43_77:	# bb70.preheader
	xorl	%esi, %esi
	movl	%esi, %ecx
	jmp	.LBB43_71	# bb65
.LBB43_78:	# real_catch5
	movl	$5, %edi
	call	llvm_real_catch
.LBB43_79:	# real_end5
	movl	$5, %edi
	jmp	.LBB43_47	# real_end3
.LBB43_80:	# bb73
	cmpl	$112, 8(%rsp)
	jne	.LBB43_96	# bb86
.LBB43_81:	# bb73
	cmpl	$112, 12(%rsp)
	jne	.LBB43_96	# bb86
.LBB43_82:	# bb77
	movl	$6, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB43_94	# real_catch6
.LBB43_83:	# jump6
	movl	$6, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB43_95	# real_end6
.LBB43_84:	# real_try6
	movl	$6, %edi
	call	llvm_real_try
	cmpl	$0, 28(%rsp)
	jle	.LBB43_95	# real_end6
.LBB43_85:	# bb.nph103
	testl	%ebp, %ebp
	jle	.LBB43_95	# real_end6
.LBB43_86:	# bb.nph103.bb83.preheader_crit_edge
	xorl	%edi, %edi
	movl	%edi, %edx
	jmp	.LBB43_93	# bb83.preheader
.LBB43_87:	# bb81.preheader.bb80_crit_edge
	pxor	%xmm0, %xmm0
	xorl	%ecx, %ecx
	movl	%edx, %eax
	.align	16
.LBB43_88:	# bb80
	leal	(%rsi,%rcx), %r9d
	movslq	%r9d, %r9
	movslq	%eax, %r10
	movsd	(%r15,%r10,8), %xmm1
	mulsd	(%r13,%r9,8), %xmm1
	addsd	%xmm1, %xmm0
	addl	%r14d, %eax
	incl	%ecx
	cmpl	%r12d, %ecx
	jne	.LBB43_88	# bb80
.LBB43_89:	# bb82
	leal	(%rdi,%r8), %eax
	movslq	%eax, %rax
	mulsd	32(%rsp), %xmm0
	addsd	(%rbx,%rax,8), %xmm0
	movsd	%xmm0, (%rbx,%rax,8)
	addl	24(%rsp), %esi
	incl	%r8d
	cmpl	%ebp, %r8d
	je	.LBB43_92	# bb84
.LBB43_90:	# bb81.preheader
	testl	%r12d, %r12d
	jg	.LBB43_87	# bb81.preheader.bb80_crit_edge
.LBB43_91:	# bb81.preheader.bb82_crit_edge
	pxor	%xmm0, %xmm0
	jmp	.LBB43_89	# bb82
.LBB43_92:	# bb84
	addl	136(%rsp), %edi
	incl	%edx
	cmpl	28(%rsp), %edx
	je	.LBB43_95	# real_end6
.LBB43_93:	# bb83.preheader
	xorl	%esi, %esi
	movl	%esi, %r8d
	jmp	.LBB43_90	# bb81.preheader
.LBB43_94:	# real_catch6
	movl	$6, %edi
	call	llvm_real_catch
.LBB43_95:	# real_end6
	movl	$6, %edi
	jmp	.LBB43_47	# real_end3
.LBB43_96:	# bb86
	xorl	%edi, %edi
	leaq	.str57, %rsi
	leaq	.str158, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB43_97:	# return
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
	.size	cblas_dgemm, .-cblas_dgemm
.Leh_func_end30:


	.section	.rodata.cst8,"aM",@progbits,8
	.align	8
.LCPI44_0:					
	.quad	4607182418800017408	# double value: 1.000000e+00
	.text
	.align	16
	.globl	cblas_dgemv
	.type	cblas_dgemv,@function
cblas_dgemv:
.Leh_func_begin31:
.Llabel31:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	cmpl	$113, %esi
	movl	$112, %eax
	cmovne	%esi, %eax
	movl	%eax, 12(%rsp)
	testl	%ecx, %ecx
	movl	120(%rsp), %ebx
	movq	112(%rsp), %r14
	movl	104(%rsp), %r15d
	movsd	%xmm1, 16(%rsp)
	movl	%r9d, 28(%rsp)
	movq	%r8, %r12
	movsd	%xmm0, 32(%rsp)
	movl	%ecx, %r13d
	movl	%edx, 4(%rsp)
	movl	%edi, 8(%rsp)
	je	.LBB44_53	# return
.LBB44_1:	# entry
	cmpl	$0, 4(%rsp)
	je	.LBB44_53	# return
.LBB44_2:	# bb10
	movsd	16(%rsp), %xmm0
	ucomisd	.LCPI44_0(%rip), %xmm0
	jne	.LBB44_4	# bb12
	jp	.LBB44_4	# bb12
.LBB44_3:	# bb10
	pxor	%xmm0, %xmm0
	movsd	32(%rsp), %xmm1
	ucomisd	%xmm0, %xmm1
	setnp	%al
	sete	%cl
	testb	%al, %cl
	jne	.LBB44_53	# return
.LBB44_4:	# bb12
	xorl	%edi, %edi
	call	llvm_real_split
	cmpl	$111, 12(%rsp)
	movl	4(%rsp), %eax
	movl	%eax, %ebp
	cmove	%r13d, %ebp
	cmove	%eax, %r13d
	cmpl	$0, dummy
	je	.LBB44_18	# real_catch0
.LBB44_5:	# jump0
	xorl	%edi, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB44_19	# real_end0
.LBB44_6:	# real_try0
	xorl	%edi, %edi
	call	llvm_real_try
	pxor	%xmm0, %xmm0
	movsd	16(%rsp), %xmm1
	ucomisd	%xmm0, %xmm1
	jne	.LBB44_12	# bb22
	jp	.LBB44_12	# bb22
.LBB44_7:	# bb16
	testl	%ebx, %ebx
	jg	.LBB44_54	# bb16.bb21.preheader_crit_edge
.LBB44_8:	# bb17
	movl	$1, %eax
	subl	%r13d, %eax
	imull	%ebx, %eax
.LBB44_9:	# bb21.preheader
	testl	%r13d, %r13d
	jle	.LBB44_19	# real_end0
.LBB44_10:	# bb21.preheader.bb20_crit_edge
	xorl	%ecx, %ecx
	.align	16
.LBB44_11:	# bb20
	movslq	%eax, %rdx
	movq	$0, (%r14,%rdx,8)
	addl	%ebx, %eax
	incl	%ecx
	cmpl	%r13d, %ecx
	jne	.LBB44_11	# bb20
	jmp	.LBB44_19	# real_end0
.LBB44_12:	# bb22
	movsd	16(%rsp), %xmm0
	ucomisd	.LCPI44_0(%rip), %xmm0
	setnp	%al
	sete	%cl
	testb	%al, %cl
	jne	.LBB44_19	# real_end0
.LBB44_13:	# bb23
	testl	%ebx, %ebx
	jg	.LBB44_55	# bb23.bb28.preheader_crit_edge
.LBB44_14:	# bb24
	movl	$1, %eax
	subl	%r13d, %eax
	imull	%ebx, %eax
.LBB44_15:	# bb28.preheader
	testl	%r13d, %r13d
	jle	.LBB44_19	# real_end0
.LBB44_16:	# bb28.preheader.bb27_crit_edge
	xorl	%ecx, %ecx
	.align	16
.LBB44_17:	# bb27
	movslq	%eax, %rdx
	movsd	16(%rsp), %xmm0
	mulsd	(%r14,%rdx,8), %xmm0
	movsd	%xmm0, (%r14,%rdx,8)
	addl	%ebx, %eax
	incl	%ecx
	cmpl	%r13d, %ecx
	jne	.LBB44_17	# bb27
	jmp	.LBB44_19	# real_end0
.LBB44_18:	# real_catch0
	xorl	%edi, %edi
	call	llvm_real_catch
.LBB44_19:	# real_end0
	xorl	%edi, %edi
	call	llvm_real_end
	pxor	%xmm0, %xmm0
	movsd	32(%rsp), %xmm1
	ucomisd	%xmm0, %xmm1
	setnp	%al
	sete	%cl
	testb	%al, %cl
	jne	.LBB44_53	# return
.LBB44_20:	# bb29
	cmpl	$111, 12(%rsp)
	jne	.LBB44_22	# bb33
.LBB44_21:	# bb29
	cmpl	$101, 8(%rsp)
	je	.LBB44_24	# bb37
.LBB44_22:	# bb33
	cmpl	$112, 12(%rsp)
	jne	.LBB44_37	# bb49
.LBB44_23:	# bb33
	cmpl	$102, 8(%rsp)
	jne	.LBB44_37	# bb49
.LBB44_24:	# bb37
	movl	$1, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB44_34	# real_catch1
.LBB44_25:	# jump1
	movl	$1, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB44_35	# real_end1
.LBB44_26:	# real_try1
	movl	$1, %edi
	call	llvm_real_try
	testl	%ebx, %ebx
	jg	.LBB44_56	# real_try1.bb48.preheader_crit_edge
.LBB44_27:	# bb38
	movl	$1, %eax
	subl	%r13d, %eax
	imull	%ebx, %eax
.LBB44_28:	# bb48.preheader
	testl	%r13d, %r13d
	jle	.LBB44_35	# real_end1
.LBB44_29:	# bb.nph91
	movl	$1, %ecx
	subl	%ebp, %ecx
	imull	%r15d, %ecx
	xorl	%edx, %edx
	movl	%edx, %esi
	.align	16
.LBB44_30:	# bb41
	testl	%r15d, %r15d
	movl	$0, %edi
	cmovle	%ecx, %edi
	testl	%ebp, %ebp
	jle	.LBB44_57	# bb41.bb47_crit_edge
.LBB44_31:	# bb41.bb45_crit_edge
	pxor	%xmm0, %xmm0
	xorl	%r8d, %r8d
	.align	16
.LBB44_32:	# bb45
	leal	(%rdx,%r8), %r9d
	movslq	%r9d, %r9
	movslq	%edi, %r10
	movq	96(%rsp), %r11
	movsd	(%r11,%r10,8), %xmm1
	mulsd	(%r12,%r9,8), %xmm1
	addsd	%xmm1, %xmm0
	addl	%r15d, %edi
	incl	%r8d
	cmpl	%ebp, %r8d
	jne	.LBB44_32	# bb45
.LBB44_33:	# bb47
	mulsd	32(%rsp), %xmm0
	movslq	%eax, %rdi
	addsd	(%r14,%rdi,8), %xmm0
	movsd	%xmm0, (%r14,%rdi,8)
	addl	%ebx, %eax
	addl	28(%rsp), %edx
	incl	%esi
	cmpl	%r13d, %esi
	jne	.LBB44_30	# bb41
	jmp	.LBB44_35	# real_end1
.LBB44_34:	# real_catch1
	movl	$1, %edi
	call	llvm_real_catch
.LBB44_35:	# real_end1
	movl	$1, %edi
.LBB44_36:	# real_end1
	call	llvm_real_end
	jmp	.LBB44_53	# return
.LBB44_37:	# bb49
	movl	8(%rsp), %eax
	cmpl	$102, %eax
	sete	%cl
	movl	12(%rsp), %edx
	cmpl	$111, %edx
	sete	%sil
	andb	%cl, %sil
	cmpl	$101, %eax
	sete	%al
	cmpl	$112, %edx
	sete	%cl
	testb	%al, %cl
	jne	.LBB44_39	# bb57
.LBB44_38:	# bb49
	notb	%sil
	testb	$1, %sil
	jne	.LBB44_52	# bb70
.LBB44_39:	# bb57
	movl	$2, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB44_50	# real_catch2
.LBB44_40:	# jump2
	movl	$2, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB44_51	# real_end2
.LBB44_41:	# real_try2
	movl	$2, %edi
	call	llvm_real_try
	testl	%r15d, %r15d
	jg	.LBB44_58	# real_try2.bb69.preheader_crit_edge
.LBB44_42:	# bb58
	movl	$1, %eax
	subl	%ebp, %eax
	imull	%r15d, %eax
.LBB44_43:	# bb69.preheader
	testl	%ebp, %ebp
	jle	.LBB44_51	# real_end2
.LBB44_44:	# bb.nph83
	movl	$1, %ecx
	subl	%r13d, %ecx
	imull	%ebx, %ecx
	xorl	%edx, %edx
	movl	%edx, %esi
	.align	16
.LBB44_45:	# bb61
	movslq	%eax, %rdi
	movsd	32(%rsp), %xmm0
	movq	96(%rsp), %r8
	mulsd	(%r8,%rdi,8), %xmm0
	pxor	%xmm1, %xmm1
	ucomisd	%xmm1, %xmm0
	setnp	%dil
	sete	%r8b
	testb	%dil, %r8b
	jne	.LBB44_49	# bb68
.LBB44_46:	# bb62
	testl	%ebx, %ebx
	movl	$0, %edi
	cmovle	%ecx, %edi
	testl	%r13d, %r13d
	jle	.LBB44_49	# bb68
.LBB44_47:	# bb62.bb66_crit_edge
	xorl	%r8d, %r8d
	.align	16
.LBB44_48:	# bb66
	leal	(%rdx,%r8), %r9d
	movslq	%r9d, %r9
	movapd	%xmm0, %xmm1
	mulsd	(%r12,%r9,8), %xmm1
	movslq	%edi, %r9
	addsd	(%r14,%r9,8), %xmm1
	movsd	%xmm1, (%r14,%r9,8)
	addl	%ebx, %edi
	incl	%r8d
	cmpl	%r13d, %r8d
	jne	.LBB44_48	# bb66
.LBB44_49:	# bb68
	addl	%r15d, %eax
	addl	28(%rsp), %edx
	incl	%esi
	cmpl	%ebp, %esi
	jne	.LBB44_45	# bb61
	jmp	.LBB44_51	# real_end2
.LBB44_50:	# real_catch2
	movl	$2, %edi
	call	llvm_real_catch
.LBB44_51:	# real_end2
	movl	$2, %edi
	jmp	.LBB44_36	# real_end1
.LBB44_52:	# bb70
	xorl	%edi, %edi
	leaq	.str59, %rsi
	leaq	.str160, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB44_53:	# return
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB44_54:	# bb16.bb21.preheader_crit_edge
	xorl	%eax, %eax
	jmp	.LBB44_9	# bb21.preheader
.LBB44_55:	# bb23.bb28.preheader_crit_edge
	xorl	%eax, %eax
	jmp	.LBB44_15	# bb28.preheader
.LBB44_56:	# real_try1.bb48.preheader_crit_edge
	xorl	%eax, %eax
	jmp	.LBB44_28	# bb48.preheader
.LBB44_57:	# bb41.bb47_crit_edge
	pxor	%xmm0, %xmm0
	jmp	.LBB44_33	# bb47
.LBB44_58:	# real_try2.bb69.preheader_crit_edge
	xorl	%eax, %eax
	jmp	.LBB44_43	# bb69.preheader
	.size	cblas_dgemv, .-cblas_dgemv
.Leh_func_end31:


	.section	.rodata.cst8,"aM",@progbits,8
	.align	16
.LCPI45_0:					
	.quad	4607182418800017408	# double value: 1.000000e+00
	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI45_1:					
	.quad	9223372036854775807	# double value: nan
	.quad	9223372036854775807	# double value: nan
	.text
	.align	16
	.globl	cblas_dnrm2
	.type	cblas_dnrm2,@function
cblas_dnrm2:
	testl	%edx, %edx
	jle	.LBB45_12	# bb13
.LBB45_1:	# entry
	testl	%edi, %edi
	jle	.LBB45_12	# bb13
.LBB45_2:	# bb3
	cmpl	$1, %edi
	je	.LBB45_5	# bb4
.LBB45_3:	# bb11.preheader
	testl	%edi, %edi
	jg	.LBB45_6	# bb11.preheader.bb6_crit_edge
.LBB45_4:	# bb11.preheader.bb12_crit_edge
	movsd	.LCPI45_0(%rip), %xmm0
	pxor	%xmm1, %xmm1
	jmp	.LBB45_11	# bb12
.LBB45_5:	# bb4
	movsd	(%rsi), %xmm0
	andpd	.LCPI45_1(%rip), %xmm0
	ret
.LBB45_6:	# bb11.preheader.bb6_crit_edge
	movsd	.LCPI45_0(%rip), %xmm0
	pxor	%xmm1, %xmm1
	xorl	%eax, %eax
	movl	%eax, %ecx
	.align	16
.LBB45_7:	# bb6
	movslq	%eax, %r8
	movsd	(%rsi,%r8,8), %xmm2
	pxor	%xmm3, %xmm3
	ucomisd	%xmm3, %xmm2
	jne	.LBB45_8	# bb7
	jp	.LBB45_8	# bb7
	jmp	.LBB45_10	# bb10
.LBB45_8:	# bb7
	andpd	.LCPI45_1(%rip), %xmm2
	ucomisd	%xmm1, %xmm2
	ja	.LBB45_13	# bb8
.LBB45_9:	# bb9
	divsd	%xmm1, %xmm2
	mulsd	%xmm2, %xmm2
	addsd	%xmm2, %xmm0
.LBB45_10:	# bb10
	addl	%edx, %eax
	incl	%ecx
	cmpl	%edi, %ecx
	jne	.LBB45_7	# bb6
.LBB45_11:	# bb12
	sqrtsd	%xmm0, %xmm0
	mulsd	%xmm1, %xmm0
	ret
.LBB45_12:	# bb13
	pxor	%xmm0, %xmm0
	ret
.LBB45_13:	# bb8
	divsd	%xmm2, %xmm1
	mulsd	%xmm1, %xmm0
	mulsd	%xmm1, %xmm0
	addsd	.LCPI45_0(%rip), %xmm0
	movapd	%xmm2, %xmm1
	jmp	.LBB45_10	# bb10
	.size	cblas_dnrm2, .-cblas_dnrm2


	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI46_0:					
	.quad	9223372036854775808	# double value: -0.000000e+00
	.quad	9223372036854775808	# double value: -0.000000e+00
	.text
	.align	16
	.globl	cblas_drot
	.type	cblas_drot,@function
cblas_drot:
.Leh_func_begin32:
.Llabel32:
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$16, %rsp
	movsd	%xmm1, 8(%rsp)
	movsd	%xmm0, (%rsp)
	movl	%r8d, %ebx
	movq	%rcx, %r14
	movl	%edx, %r15d
	movq	%rsi, %r12
	movl	%edi, %r13d
	xorl	%edi, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB46_9	# real_catch0
.LBB46_1:	# jump0
	xorl	%edi, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB46_10	# real_end0
.LBB46_2:	# real_try0
	xorl	%edi, %edi
	call	llvm_real_try
	testl	%r15d, %r15d
	jg	.LBB46_11	# real_try0.bb2_crit_edge
.LBB46_3:	# bb
	movl	$1, %eax
	subl	%r13d, %eax
	imull	%r15d, %eax
.LBB46_4:	# bb2
	testl	%ebx, %ebx
	jg	.LBB46_12	# bb2.bb7.preheader_crit_edge
.LBB46_5:	# bb3
	movl	$1, %ecx
	subl	%r13d, %ecx
	imull	%ebx, %ecx
.LBB46_6:	# bb7.preheader
	testl	%r13d, %r13d
	jle	.LBB46_10	# real_end0
.LBB46_7:	# bb.nph
	movsd	8(%rsp), %xmm0
	xorpd	.LCPI46_0(%rip), %xmm0
	xorl	%edx, %edx
	.align	16
.LBB46_8:	# bb6
	movslq	%ecx, %rsi
	movsd	(%r14,%rsi,8), %xmm1
	movapd	%xmm1, %xmm2
	mulsd	8(%rsp), %xmm2
	movslq	%eax, %rdi
	movsd	(%r12,%rdi,8), %xmm3
	movapd	%xmm3, %xmm4
	movsd	(%rsp), %xmm5
	mulsd	%xmm5, %xmm4
	addsd	%xmm2, %xmm4
	movsd	%xmm4, (%r12,%rdi,8)
	mulsd	%xmm5, %xmm1
	mulsd	%xmm0, %xmm3
	addsd	%xmm1, %xmm3
	movsd	%xmm3, (%r14,%rsi,8)
	addl	%ebx, %ecx
	addl	%r15d, %eax
	incl	%edx
	cmpl	%r13d, %edx
	jne	.LBB46_8	# bb6
	jmp	.LBB46_10	# real_end0
.LBB46_9:	# real_catch0
	xorl	%edi, %edi
	call	llvm_real_catch
.LBB46_10:	# real_end0
	xorl	%edi, %edi
	call	llvm_real_end
	addq	$16, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	ret
.LBB46_11:	# real_try0.bb2_crit_edge
	xorl	%eax, %eax
	jmp	.LBB46_4	# bb2
.LBB46_12:	# bb2.bb7.preheader_crit_edge
	xorl	%ecx, %ecx
	jmp	.LBB46_6	# bb7.preheader
	.size	cblas_drot, .-cblas_drot
.Leh_func_end32:


	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI47_0:					
	.quad	9223372036854775807	# double value: nan
	.quad	9223372036854775807	# double value: nan
	.section	.rodata.cst8,"aM",@progbits,8
	.align	16
.LCPI47_1:					
	.quad	4607182418800017408	# double value: 1.000000e+00
.LCPI47_2:					
	.quad	13830554455654793216	# double value: -1.000000e+00
	.text
	.align	16
	.globl	cblas_drotg
	.type	cblas_drotg,@function
cblas_drotg:
	movsd	(%rsi), %xmm0
	movsd	.LCPI47_0(%rip), %xmm1
	movapd	%xmm0, %xmm2
	andpd	%xmm1, %xmm2
	movsd	(%rdi), %xmm3
	andpd	%xmm3, %xmm1
	movapd	%xmm1, %xmm4
	addsd	%xmm2, %xmm4
	pxor	%xmm5, %xmm5
	ucomisd	%xmm5, %xmm4
	setnp	%al
	sete	%r8b
	testb	%al, %r8b
	jne	.LBB47_8	# bb11
.LBB47_1:	# bb3
	divsd	%xmm4, %xmm0
	mulsd	%xmm0, %xmm0
	movapd	%xmm3, %xmm5
	divsd	%xmm4, %xmm5
	mulsd	%xmm5, %xmm5
	addsd	%xmm0, %xmm5
	sqrtsd	%xmm5, %xmm0
	mulsd	%xmm4, %xmm0
	ucomisd	%xmm2, %xmm1
	movq	%rsi, %rax
	cmova	%rdi, %rax
	movsd	(%rax), %xmm1
	pxor	%xmm2, %xmm2
	ucomisd	%xmm2, %xmm1
	movsd	.LCPI47_1(%rip), %xmm1
	movsd	.LCPI47_2(%rip), %xmm2
	jb	.LBB47_3	# bb3
.LBB47_2:	# bb3
	movapd	%xmm1, %xmm2
.LBB47_3:	# bb3
	mulsd	%xmm0, %xmm2
	divsd	%xmm2, %xmm3
	movsd	%xmm3, (%rdx)
	movsd	(%rsi), %xmm0
	divsd	%xmm2, %xmm0
	movsd	%xmm0, (%rcx)
	movsd	(%rsi), %xmm3
	movsd	.LCPI47_0(%rip), %xmm4
	andpd	%xmm4, %xmm3
	movsd	(%rdi), %xmm5
	andpd	%xmm4, %xmm5
	ucomisd	%xmm3, %xmm5
	ja	.LBB47_5	# bb3
.LBB47_4:	# bb3
	movapd	%xmm1, %xmm0
.LBB47_5:	# bb3
	ucomisd	%xmm5, %xmm3
	jb	.LBB47_9	# bb12
.LBB47_6:	# bb9
	movsd	(%rdx), %xmm1
	pxor	%xmm3, %xmm3
	ucomisd	%xmm3, %xmm1
	setnp	%al
	sete	%cl
	testb	%al, %cl
	jne	.LBB47_9	# bb12
.LBB47_7:	# bb10
	movsd	.LCPI47_1(%rip), %xmm0
	divsd	%xmm1, %xmm0
	jmp	.LBB47_9	# bb12
.LBB47_8:	# bb11
	movabsq	$4607182418800017408, %rax
	movq	%rax, (%rdx)
	movq	$0, (%rcx)
	pxor	%xmm0, %xmm0
	movapd	%xmm0, %xmm2
.LBB47_9:	# bb12
	movsd	%xmm2, (%rdi)
	movsd	%xmm0, (%rsi)
	ret
	.size	cblas_drotg, .-cblas_drotg


	.section	.rodata.cst8,"aM",@progbits,8
	.align	8
.LCPI48_0:					
	.quad	13830554455654793216	# double value: -1.000000e+00
.LCPI48_1:					
	.quad	4607182418800017408	# double value: 1.000000e+00
.LCPI48_2:					
	.quad	13835058055282163712	# double value: -2.000000e+00
	.text
	.align	16
	.globl	cblas_drotm
	.type	cblas_drotm,@function
cblas_drotm:
.Leh_func_begin33:
.Llabel33:
	pushq	%rbx
	testl	%edx, %edx
	jg	.LBB48_16	# entry.bb2_crit_edge
.LBB48_1:	# bb
	movl	$1, %eax
	subl	%edi, %eax
	imull	%edx, %eax
.LBB48_2:	# bb2
	testl	%r8d, %r8d
	jg	.LBB48_17	# bb2.bb5_crit_edge
.LBB48_3:	# bb3
	movl	$1, %r10d
	subl	%edi, %r10d
	imull	%r8d, %r10d
.LBB48_4:	# bb5
	movsd	(%r9), %xmm0
	ucomisd	.LCPI48_0(%rip), %xmm0
	jne	.LBB48_9	# bb7
	jp	.LBB48_9	# bb7
.LBB48_5:	# bb6
	movsd	32(%r9), %xmm0
	movsd	24(%r9), %xmm1
	movsd	16(%r9), %xmm2
	movsd	8(%r9), %xmm3
.LBB48_6:	# bb15.preheader
	testl	%edi, %edi
	jle	.LBB48_15	# return
.LBB48_7:	# bb15.preheader.bb14_crit_edge
	xorl	%r9d, %r9d
	.align	16
.LBB48_8:	# bb14
	movslq	%r10d, %r11
	movsd	(%rcx,%r11,8), %xmm4
	movapd	%xmm1, %xmm5
	mulsd	%xmm4, %xmm5
	movslq	%eax, %rbx
	movsd	(%rsi,%rbx,8), %xmm6
	movapd	%xmm3, %xmm7
	mulsd	%xmm6, %xmm7
	addsd	%xmm5, %xmm7
	movsd	%xmm7, (%rsi,%rbx,8)
	mulsd	%xmm0, %xmm4
	mulsd	%xmm2, %xmm6
	addsd	%xmm4, %xmm6
	movsd	%xmm6, (%rcx,%r11,8)
	addl	%r8d, %r10d
	addl	%edx, %eax
	incl	%r9d
	cmpl	%edi, %r9d
	jne	.LBB48_8	# bb14
	jmp	.LBB48_15	# return
.LBB48_9:	# bb7
	pxor	%xmm1, %xmm1
	ucomisd	%xmm1, %xmm0
	jne	.LBB48_11	# bb9
	jp	.LBB48_11	# bb9
.LBB48_10:	# bb8
	movsd	24(%r9), %xmm1
	movsd	16(%r9), %xmm2
	movsd	.LCPI48_1(%rip), %xmm3
	movapd	%xmm3, %xmm0
	jmp	.LBB48_6	# bb15.preheader
.LBB48_11:	# bb9
	ucomisd	.LCPI48_1(%rip), %xmm0
	jne	.LBB48_13	# bb11
	jp	.LBB48_13	# bb11
.LBB48_12:	# bb10
	movsd	32(%r9), %xmm0
	movsd	8(%r9), %xmm3
	movsd	.LCPI48_1(%rip), %xmm1
	movsd	.LCPI48_0(%rip), %xmm2
	jmp	.LBB48_6	# bb15.preheader
.LBB48_13:	# bb11
	ucomisd	.LCPI48_2(%rip), %xmm0
	setnp	%al
	sete	%cl
	testb	%al, %cl
	jne	.LBB48_15	# return
.LBB48_14:	# bb12
	xorl	%edi, %edi
	leaq	.str61, %rsi
	leaq	.str162, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB48_15:	# return
	popq	%rbx
	ret
.LBB48_16:	# entry.bb2_crit_edge
	xorl	%eax, %eax
	jmp	.LBB48_2	# bb2
.LBB48_17:	# bb2.bb5_crit_edge
	xorl	%r10d, %r10d
	jmp	.LBB48_4	# bb5
	.size	cblas_drotm, .-cblas_drotm
.Leh_func_end33:


	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI49_0:					
	.quad	9223372036854775807	# double value: nan
	.quad	9223372036854775807	# double value: nan
.LCPI49_1:					
	.quad	9223372036854775808	# double value: -0.000000e+00
	.quad	9223372036854775808	# double value: -0.000000e+00
	.section	.rodata.cst8,"aM",@progbits,8
	.align	16
.LCPI49_2:					
	.quad	4607182418800017408	# double value: 1.000000e+00
.LCPI49_3:					
	.quad	13830554455654793216	# double value: -1.000000e+00
.LCPI49_4:					
	.quad	4499096027743125504	# double value: 5.960464e-08
.LCPI49_5:					
	.quad	4715268809856909312	# double value: 1.677722e+07
.LCPI49_6:					
	.quad	4553139223271571456	# double value: 2.441406e-04
.LCPI49_7:					
	.quad	4661225614328463360	# double value: 4.096000e+03
	.text
	.align	16
	.globl	cblas_drotmg
	.type	cblas_drotmg,@function
cblas_drotmg:
	movsd	(%rdi), %xmm1
	pxor	%xmm2, %xmm2
	ucomisd	%xmm1, %xmm2
	movsd	(%rdx), %xmm2
	movsd	(%rsi), %xmm3
	ja	.LBB49_31	# bb
.LBB49_1:	# bb1
	movapd	%xmm3, %xmm4
	mulsd	%xmm0, %xmm4
	pxor	%xmm5, %xmm5
	ucomisd	%xmm5, %xmm4
	jne	.LBB49_3	# bb3
	jp	.LBB49_3	# bb3
.LBB49_2:	# bb2
	movabsq	$-4611686018427387904, %rdx
	movq	%rdx, (%rcx)
	ret
.LBB49_3:	# bb3
	movapd	%xmm4, %xmm5
	mulsd	%xmm0, %xmm5
	movsd	.LCPI49_0(%rip), %xmm6
	movapd	%xmm5, %xmm7
	andpd	%xmm6, %xmm7
	movapd	%xmm1, %xmm8
	mulsd	%xmm2, %xmm8
	movapd	%xmm8, %xmm9
	mulsd	%xmm2, %xmm9
	andpd	%xmm6, %xmm9
	ucomisd	%xmm7, %xmm9
	jbe	.LBB49_6	# bb7
.LBB49_4:	# bb4
	movq	$0, (%rcx)
	xorpd	.LCPI49_1(%rip), %xmm0
	divsd	%xmm2, %xmm0
	divsd	%xmm8, %xmm4
	movapd	%xmm0, %xmm5
	mulsd	%xmm4, %xmm5
	movsd	.LCPI49_2(%rip), %xmm6
	subsd	%xmm5, %xmm6
	pxor	%xmm5, %xmm5
	ucomisd	%xmm6, %xmm5
	jae	.LBB49_31	# bb
.LBB49_5:	# bb4.bb11.preheader_crit_edge
	movsd	.LCPI49_2(%rip), %xmm5
	movapd	%xmm0, %xmm7
	movapd	%xmm5, %xmm8
	movapd	%xmm3, %xmm0
	movapd	%xmm6, %xmm9
	movapd	%xmm6, %xmm3
	jmp	.LBB49_8	# bb11.preheader
.LBB49_6:	# bb7
	pxor	%xmm6, %xmm6
	ucomisd	%xmm5, %xmm6
	ja	.LBB49_31	# bb
.LBB49_7:	# bb9
	movabsq	$4607182418800017408, %rax
	movq	%rax, (%rcx)
	divsd	%xmm0, %xmm2
	divsd	%xmm4, %xmm8
	movapd	%xmm8, %xmm6
	mulsd	%xmm2, %xmm6
	movsd	.LCPI49_2(%rip), %xmm4
	addsd	%xmm4, %xmm6
	movsd	.LCPI49_3(%rip), %xmm7
	movapd	%xmm2, %xmm5
	movapd	%xmm0, %xmm2
	movapd	%xmm1, %xmm0
	movapd	%xmm6, %xmm9
	movapd	%xmm3, %xmm1
	movapd	%xmm6, %xmm3
.LBB49_8:	# bb11.preheader
	divsd	%xmm3, %xmm1
	pxor	%xmm3, %xmm3
	ucomisd	%xmm3, %xmm1
	setnp	%al
	sete	%r8b
	testb	%al, %r8b
	mulsd	%xmm2, %xmm6
	divsd	%xmm9, %xmm0
	jne	.LBB49_13	# bb14.loopexit
.LBB49_9:	# bb11.preheader
	movsd	.LCPI49_4(%rip), %xmm2
	ucomisd	%xmm1, %xmm2
	jb	.LBB49_13	# bb14.loopexit
	.align	16
.LBB49_10:	# bb10
	mulsd	.LCPI49_5(%rip), %xmm1
	pxor	%xmm2, %xmm2
	ucomisd	%xmm2, %xmm1
	setnp	%al
	sete	%r8b
	testb	%al, %r8b
	movsd	.LCPI49_6(%rip), %xmm2
	mulsd	%xmm2, %xmm6
	mulsd	%xmm2, %xmm4
	mulsd	%xmm2, %xmm8
	jne	.LBB49_12	# bb11.bb14.loopexit_crit_edge
.LBB49_11:	# bb10
	movsd	.LCPI49_4(%rip), %xmm2
	ucomisd	%xmm1, %xmm2
	jae	.LBB49_10	# bb10
.LBB49_12:	# bb11.bb14.loopexit_crit_edge
	movabsq	$-4616189618054758400, %rax
	movq	%rax, (%rcx)
.LBB49_13:	# bb14.loopexit
	ucomisd	.LCPI49_5(%rip), %xmm1
	jb	.LBB49_16	# bb16.loopexit
	.align	16
.LBB49_14:	# bb13
	movsd	.LCPI49_7(%rip), %xmm2
	mulsd	%xmm2, %xmm4
	mulsd	%xmm2, %xmm8
	mulsd	%xmm2, %xmm6
	mulsd	.LCPI49_4(%rip), %xmm1
	ucomisd	.LCPI49_5(%rip), %xmm1
	jae	.LBB49_14	# bb13
.LBB49_15:	# bb14.bb16.loopexit_crit_edge
	movabsq	$-4616189618054758400, %rax
	movq	%rax, (%rcx)
.LBB49_16:	# bb16.loopexit
	movapd	%xmm0, %xmm2
	andpd	.LCPI49_0(%rip), %xmm2
	movsd	.LCPI49_4(%rip), %xmm3
	ucomisd	%xmm2, %xmm3
	jb	.LBB49_21	# bb19.loopexit
.LBB49_17:	# bb16.loopexit
	pxor	%xmm2, %xmm2
	ucomisd	%xmm2, %xmm0
	jne	.LBB49_18	# bb15
	jp	.LBB49_18	# bb15
	jmp	.LBB49_21	# bb19.loopexit
	.align	16
.LBB49_18:	# bb15
	mulsd	.LCPI49_5(%rip), %xmm0
	movapd	%xmm0, %xmm2
	andpd	.LCPI49_0(%rip), %xmm2
	movsd	.LCPI49_4(%rip), %xmm3
	ucomisd	%xmm2, %xmm3
	movsd	.LCPI49_6(%rip), %xmm2
	mulsd	%xmm2, %xmm5
	mulsd	%xmm2, %xmm7
	jb	.LBB49_20	# bb16.bb19.loopexit_crit_edge
.LBB49_19:	# bb15
	pxor	%xmm2, %xmm2
	ucomisd	%xmm2, %xmm0
	jne	.LBB49_18	# bb15
	jp	.LBB49_18	# bb15
.LBB49_20:	# bb16.bb19.loopexit_crit_edge
	movabsq	$-4616189618054758400, %rax
	movq	%rax, (%rcx)
.LBB49_21:	# bb19.loopexit
	movapd	%xmm0, %xmm2
	andpd	.LCPI49_0(%rip), %xmm2
	ucomisd	.LCPI49_5(%rip), %xmm2
	jb	.LBB49_24	# bb20
	.align	16
.LBB49_22:	# bb18
	movsd	.LCPI49_7(%rip), %xmm2
	mulsd	%xmm2, %xmm5
	mulsd	%xmm2, %xmm7
	mulsd	.LCPI49_4(%rip), %xmm0
	movapd	%xmm0, %xmm2
	andpd	.LCPI49_0(%rip), %xmm2
	ucomisd	.LCPI49_5(%rip), %xmm2
	jae	.LBB49_22	# bb18
.LBB49_23:	# bb19.bb20_crit_edge
	movabsq	$-4616189618054758400, %rax
	movq	%rax, (%rcx)
.LBB49_24:	# bb20
	movsd	%xmm1, (%rdi)
	movsd	%xmm0, (%rsi)
	movsd	%xmm6, (%rdx)
	movsd	(%rcx), %xmm0
	ucomisd	.LCPI49_3(%rip), %xmm0
	jne	.LBB49_26	# bb22
	jp	.LBB49_26	# bb22
.LBB49_25:	# bb21
	movsd	%xmm8, 8(%rcx)
	movsd	%xmm7, 16(%rcx)
	movsd	%xmm4, 24(%rcx)
	movsd	%xmm5, 32(%rcx)
	ret
.LBB49_26:	# bb22
	pxor	%xmm1, %xmm1
	ucomisd	%xmm1, %xmm0
	jne	.LBB49_28	# bb24
	jp	.LBB49_28	# bb24
.LBB49_27:	# bb23
	movsd	%xmm7, 16(%rcx)
	movsd	%xmm4, 24(%rcx)
	ret
.LBB49_28:	# bb24
	ucomisd	.LCPI49_2(%rip), %xmm0
	jne	.LBB49_30	# return
	jp	.LBB49_30	# return
.LBB49_29:	# bb25
	movsd	%xmm8, 8(%rcx)
	movsd	%xmm5, 32(%rcx)
	ret
.LBB49_30:	# return
	ret
.LBB49_31:	# bb
	movabsq	$-4616189618054758400, %rax
	movq	%rax, (%rcx)
	movq	$0, 8(%rcx)
	movq	$0, 16(%rcx)
	movq	$0, 24(%rcx)
	movq	$0, 32(%rcx)
	movq	$0, (%rdi)
	movq	$0, (%rsi)
	movq	$0, (%rdx)
	ret
	.size	cblas_drotmg, .-cblas_drotmg


	.section	.rodata.cst8,"aM",@progbits,8
	.align	8
.LCPI50_0:					
	.quad	4607182418800017408	# double value: 1.000000e+00
	.text
	.align	16
	.globl	cblas_dsbmv
	.type	cblas_dsbmv,@function
cblas_dsbmv:
.Leh_func_begin34:
.Llabel34:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$56, %rsp
	testl	%edx, %edx
	movl	136(%rsp), %eax
	movq	128(%rsp), %r10
	movl	120(%rsp), %r11d
	movl	%r9d, 36(%rsp)
	movl	%ecx, 40(%rsp)
	je	.LBB50_43	# return
.LBB50_1:	# bb
	ucomisd	.LCPI50_0(%rip), %xmm1
	jne	.LBB50_3	# bb13
	jp	.LBB50_3	# bb13
.LBB50_2:	# bb
	pxor	%xmm2, %xmm2
	ucomisd	%xmm2, %xmm0
	setnp	%cl
	sete	%r9b
	testb	%cl, %r9b
	jne	.LBB50_43	# return
.LBB50_3:	# bb13
	pxor	%xmm2, %xmm2
	ucomisd	%xmm2, %xmm1
	jne	.LBB50_9	# bb20
	jp	.LBB50_9	# bb20
.LBB50_4:	# bb14
	testl	%eax, %eax
	jg	.LBB50_44	# bb14.bb19.preheader_crit_edge
.LBB50_5:	# bb15
	movl	$1, %ecx
	subl	%edx, %ecx
	imull	%eax, %ecx
.LBB50_6:	# bb19.preheader
	testl	%edx, %edx
	jle	.LBB50_15	# bb27
.LBB50_7:	# bb19.preheader.bb18_crit_edge
	xorl	%r9d, %r9d
	.align	16
.LBB50_8:	# bb18
	movslq	%ecx, %rbx
	movq	$0, (%r10,%rbx,8)
	addl	%eax, %ecx
	incl	%r9d
	cmpl	%edx, %r9d
	jne	.LBB50_8	# bb18
	jmp	.LBB50_15	# bb27
.LBB50_9:	# bb20
	ucomisd	.LCPI50_0(%rip), %xmm1
	setnp	%cl
	sete	%r9b
	testb	%cl, %r9b
	jne	.LBB50_15	# bb27
.LBB50_10:	# bb21
	testl	%eax, %eax
	jg	.LBB50_45	# bb21.bb26.preheader_crit_edge
.LBB50_11:	# bb22
	movl	$1, %ecx
	subl	%edx, %ecx
	imull	%eax, %ecx
.LBB50_12:	# bb26.preheader
	testl	%edx, %edx
	jle	.LBB50_15	# bb27
.LBB50_13:	# bb26.preheader.bb25_crit_edge
	xorl	%r9d, %r9d
	.align	16
.LBB50_14:	# bb25
	movslq	%ecx, %rbx
	movapd	%xmm1, %xmm2
	mulsd	(%r10,%rbx,8), %xmm2
	movsd	%xmm2, (%r10,%rbx,8)
	addl	%eax, %ecx
	incl	%r9d
	cmpl	%edx, %r9d
	jne	.LBB50_14	# bb25
.LBB50_15:	# bb27
	pxor	%xmm1, %xmm1
	ucomisd	%xmm1, %xmm0
	setnp	%cl
	sete	%r9b
	testb	%cl, %r9b
	jne	.LBB50_43	# return
.LBB50_16:	# bb28
	cmpl	$121, %esi
	jne	.LBB50_18	# bb31
.LBB50_17:	# bb28
	cmpl	$101, %edi
	je	.LBB50_20	# bb35
.LBB50_18:	# bb31
	cmpl	$122, %esi
	jne	.LBB50_30	# bb53
.LBB50_19:	# bb31
	cmpl	$102, %edi
	jne	.LBB50_30	# bb53
.LBB50_20:	# bb35
	testl	%r11d, %r11d
	jg	.LBB50_46	# bb35.bb38_crit_edge
.LBB50_21:	# bb36
	movl	$1, %ecx
	subl	%edx, %ecx
	imull	%r11d, %ecx
	movl	%ecx, 16(%rsp)
.LBB50_22:	# bb38
	testl	%eax, %eax
	jg	.LBB50_47	# bb38.bb52.preheader_crit_edge
.LBB50_23:	# bb39
	movl	$1, %ecx
	subl	%edx, %ecx
	imull	%eax, %ecx
	movl	%ecx, 12(%rsp)
.LBB50_24:	# bb52.preheader
	testl	%edx, %edx
	jle	.LBB50_43	# return
.LBB50_25:	# bb.nph122
	movl	$1, %ecx
	subl	%edx, %ecx
	movl	%ecx, %esi
	imull	%r11d, %esi
	movl	%esi, 20(%rsp)
	imull	%eax, %ecx
	movl	%ecx, 8(%rsp)
	movl	40(%rsp), %ecx
	leal	1(%rcx), %ecx
	movl	%ecx, 24(%rsp)
	movl	$4294967294, 28(%rsp)
	xorl	%ecx, %ecx
	movl	%ecx, 44(%rsp)
	movl	%ecx, 48(%rsp)
	movl	%ecx, 52(%rsp)
	movl	%ecx, 32(%rsp)
	.align	16
.LBB50_26:	# bb42
	movl	48(%rsp), %esi
	movl	12(%rsp), %edi
	leal	(%rdi,%rsi), %esi
	movslq	%esi, %rsi
	movl	44(%rsp), %edi
	movl	16(%rsp), %r9d
	leal	(%r9,%rdi), %edi
	movslq	%edi, %rdi
	movapd	%xmm0, %xmm1
	movq	112(%rsp), %r9
	mulsd	(%r9,%rdi,8), %xmm1
	movslq	52(%rsp), %rdi
	movapd	%xmm1, %xmm2
	mulsd	(%r8,%rdi,8), %xmm2
	addsd	(%r10,%rsi,8), %xmm2
	movsd	%xmm2, (%r10,%rsi,8)
	xorl	%edi, %edi
	testl	%eax, %eax
	movl	8(%rsp), %r9d
	cmovg	%edi, %r9d
	testl	%r11d, %r11d
	cmovle	20(%rsp), %edi
	movl	24(%rsp), %ebx
	leal	(%rbx,%rcx), %ebx
	cmpl	%edx, %ebx
	cmovg	%edx, %ebx
	leal	1(%rcx), %r14d
	cmpl	%ebx, %r14d
	jge	.LBB50_48	# bb42.bb51_crit_edge
.LBB50_27:	# bb.nph117
	movl	$4294967295, %ebx
	subl	%edx, %ebx
	movl	32(%rsp), %r14d
	negl	%r14d
	subl	40(%rsp), %r14d
	addl	$4294967294, %r14d
	cmpl	%r14d, %ebx
	cmovg	%ebx, %r14d
	movl	28(%rsp), %ebx
	subl	%r14d, %ebx
	movl	44(%rsp), %r14d
	leal	(%r11,%r14), %r14d
	movl	48(%rsp), %r15d
	leal	(%rax,%r15), %r15d
	movl	52(%rsp), %r12d
	leal	1(%r12), %r12d
	pxor	%xmm2, %xmm2
	xorl	%r13d, %r13d
	.align	16
.LBB50_28:	# bb49
	addl	%r9d, %r15d
	movslq	%r15d, %r9
	leal	(%r12,%r13), %ebp
	movslq	%ebp, %rbp
	movsd	(%r8,%rbp,8), %xmm3
	movapd	%xmm1, %xmm4
	mulsd	%xmm3, %xmm4
	addsd	(%r10,%r9,8), %xmm4
	movsd	%xmm4, (%r10,%r9,8)
	addl	%edi, %r14d
	movslq	%r14d, %rdi
	movq	112(%rsp), %r9
	mulsd	(%r9,%rdi,8), %xmm3
	addsd	%xmm3, %xmm2
	incl	%r13d
	cmpl	%ebx, %r13d
	movl	%eax, %r9d
	movl	%r11d, %edi
	jne	.LBB50_28	# bb49
.LBB50_29:	# bb51
	mulsd	%xmm0, %xmm2
	addsd	(%r10,%rsi,8), %xmm2
	movsd	%xmm2, (%r10,%rsi,8)
	addl	%r11d, 44(%rsp)
	addl	%eax, 48(%rsp)
	movl	52(%rsp), %esi
	addl	36(%rsp), %esi
	movl	%esi, 52(%rsp)
	incl	32(%rsp)
	decl	28(%rsp)
	incl	%ecx
	cmpl	%edx, %ecx
	jne	.LBB50_26	# bb42
	jmp	.LBB50_43	# return
.LBB50_30:	# bb53
	cmpl	$102, %edi
	sete	%cl
	cmpl	$121, %esi
	sete	%r9b
	andb	%cl, %r9b
	cmpl	$101, %edi
	sete	%cl
	cmpl	$122, %esi
	sete	%sil
	testb	%cl, %sil
	jne	.LBB50_32	# bb61
.LBB50_31:	# bb53
	notb	%r9b
	testb	$1, %r9b
	jne	.LBB50_42	# bb82
.LBB50_32:	# bb61
	testl	%r11d, %r11d
	jg	.LBB50_49	# bb61.bb64_crit_edge
.LBB50_33:	# bb62
	movl	$1, %ecx
	subl	%edx, %ecx
	imull	%r11d, %ecx
	movl	%ecx, 48(%rsp)
.LBB50_34:	# bb64
	testl	%eax, %eax
	jg	.LBB50_50	# bb64.bb81.preheader_crit_edge
.LBB50_35:	# bb65
	movl	$1, %ecx
	subl	%edx, %ecx
	imull	%eax, %ecx
	movl	%ecx, 44(%rsp)
.LBB50_36:	# bb81.preheader
	testl	%edx, %edx
	jle	.LBB50_43	# return
.LBB50_37:	# bb.nph106
	movl	$1, %ecx
	subl	%edx, %ecx
	movl	%ecx, %esi
	imull	%r11d, %esi
	movl	%esi, 28(%rsp)
	imull	%eax, %ecx
	movl	%ecx, 20(%rsp)
	movl	36(%rsp), %ecx
	leal	-1(%rcx), %ecx
	movl	%ecx, 32(%rsp)
	movl	40(%rsp), %ecx
	movl	%ecx, %esi
	negl	%esi
	movl	%esi, 24(%rsp)
	xorl	%esi, %esi
	movl	%ecx, 52(%rsp)
	.align	16
.LBB50_38:	# bb68
	xorl	%edi, %edi
	testl	%eax, %eax
	movl	20(%rsp), %r9d
	cmovg	%edi, %r9d
	testl	%r11d, %r11d
	movl	28(%rsp), %ebx
	cmovg	%edi, %ebx
	movl	24(%rsp), %r14d
	leal	(%r14,%rsi), %r14d
	cmpl	40(%rsp), %esi
	cmovle	%edi, %r14d
	movl	%r14d, %edi
	imull	%eax, %edi
	movl	%r14d, %r15d
	imull	%r11d, %r15d
	cmpl	%esi, %r14d
	movslq	48(%rsp), %r12
	movapd	%xmm0, %xmm1
	movq	112(%rsp), %r13
	mulsd	(%r13,%r12,8), %xmm1
	jge	.LBB50_51	# bb68.bb80_crit_edge
.LBB50_39:	# bb.nph101
	movl	%esi, %r12d
	subl	%r14d, %r12d
	addl	%ecx, %r14d
	pxor	%xmm2, %xmm2
	xorl	%r13d, %r13d
	.align	16
.LBB50_40:	# bb78
	addl	%r9d, %edi
	movslq	%edi, %r9
	leal	(%r14,%r13), %ebp
	movslq	%ebp, %rbp
	movsd	(%r8,%rbp,8), %xmm3
	movapd	%xmm1, %xmm4
	mulsd	%xmm3, %xmm4
	addsd	(%r10,%r9,8), %xmm4
	movsd	%xmm4, (%r10,%r9,8)
	addl	%ebx, %r15d
	movslq	%r15d, %r9
	movq	112(%rsp), %rbx
	mulsd	(%rbx,%r9,8), %xmm3
	addsd	%xmm3, %xmm2
	incl	%r13d
	cmpl	%r12d, %r13d
	movl	%eax, %r9d
	movl	%r11d, %ebx
	jne	.LBB50_40	# bb78
.LBB50_41:	# bb80
	movl	52(%rsp), %edi
	movslq	%edi, %r9
	mulsd	(%r8,%r9,8), %xmm1
	mulsd	%xmm0, %xmm2
	addsd	%xmm1, %xmm2
	movl	44(%rsp), %r9d
	movslq	%r9d, %rbx
	addsd	(%r10,%rbx,8), %xmm2
	movsd	%xmm2, (%r10,%rbx,8)
	addl	%r11d, 48(%rsp)
	addl	%eax, %r9d
	movl	%r9d, 44(%rsp)
	addl	36(%rsp), %edi
	movl	%edi, 52(%rsp)
	addl	32(%rsp), %ecx
	incl	%esi
	cmpl	%edx, %esi
	jne	.LBB50_38	# bb68
	jmp	.LBB50_43	# return
.LBB50_42:	# bb82
	xorl	%edi, %edi
	leaq	.str63, %rsi
	leaq	.str164, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB50_43:	# return
	addq	$56, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB50_44:	# bb14.bb19.preheader_crit_edge
	xorl	%ecx, %ecx
	jmp	.LBB50_6	# bb19.preheader
.LBB50_45:	# bb21.bb26.preheader_crit_edge
	xorl	%ecx, %ecx
	jmp	.LBB50_12	# bb26.preheader
.LBB50_46:	# bb35.bb38_crit_edge
	movl	$0, 16(%rsp)
	jmp	.LBB50_22	# bb38
.LBB50_47:	# bb38.bb52.preheader_crit_edge
	movl	$0, 12(%rsp)
	jmp	.LBB50_24	# bb52.preheader
.LBB50_48:	# bb42.bb51_crit_edge
	pxor	%xmm2, %xmm2
	jmp	.LBB50_29	# bb51
.LBB50_49:	# bb61.bb64_crit_edge
	movl	$0, 48(%rsp)
	jmp	.LBB50_34	# bb64
.LBB50_50:	# bb64.bb81.preheader_crit_edge
	movl	$0, 44(%rsp)
	jmp	.LBB50_36	# bb81.preheader
.LBB50_51:	# bb68.bb80_crit_edge
	pxor	%xmm2, %xmm2
	jmp	.LBB50_41	# bb80
	.size	cblas_dsbmv, .-cblas_dsbmv
.Leh_func_end34:


	.align	16
	.globl	cblas_dscal
	.type	cblas_dscal,@function
cblas_dscal:
	testl	%edx, %edx
	jle	.LBB51_4	# return
.LBB51_1:	# entry
	testl	%edi, %edi
	jle	.LBB51_4	# return
.LBB51_2:	# entry.bb1_crit_edge
	xorl	%eax, %eax
	movl	%eax, %ecx
	.align	16
.LBB51_3:	# bb1
	movslq	%eax, %r8
	movapd	%xmm0, %xmm1
	mulsd	(%rsi,%r8,8), %xmm1
	movsd	%xmm1, (%rsi,%r8,8)
	addl	%edx, %eax
	incl	%ecx
	cmpl	%edi, %ecx
	jne	.LBB51_3	# bb1
.LBB51_4:	# return
	ret
	.size	cblas_dscal, .-cblas_dscal


	.align	16
	.globl	cblas_dsdot
	.type	cblas_dsdot,@function
cblas_dsdot:
.Leh_func_begin35:
.Llabel35:
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	movl	%r8d, %ebx
	movq	%rcx, %r14
	movl	%edx, %r15d
	movq	%rsi, %r12
	movl	%edi, %r13d
	xorl	%edi, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB52_9	# real_catch0
.LBB52_1:	# jump0
	xorl	%edi, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB52_10	# real_end0
.LBB52_2:	# real_try0
	xorl	%edi, %edi
	call	llvm_real_try
	testl	%r15d, %r15d
	jg	.LBB52_12	# real_try0.bb2_crit_edge
.LBB52_3:	# bb
	movl	$1, %eax
	subl	%r13d, %eax
	imull	%r15d, %eax
.LBB52_4:	# bb2
	testl	%ebx, %ebx
	jg	.LBB52_13	# bb2.bb7.preheader_crit_edge
.LBB52_5:	# bb3
	movl	$1, %ecx
	subl	%r13d, %ecx
	imull	%ebx, %ecx
.LBB52_6:	# bb7.preheader
	testl	%r13d, %r13d
	jle	.LBB52_14	# bb7.preheader.return_crit_edge
.LBB52_7:	# bb7.preheader.bb6_crit_edge
	pxor	%xmm0, %xmm0
	xorl	%edx, %edx
	.align	16
.LBB52_8:	# bb6
	leal	(%rbx,%rcx), %esi
	leal	(%r15,%rax), %edi
	incl	%edx
	cmpl	%r13d, %edx
	movslq	%eax, %rax
	movss	(%r12,%rax,4), %xmm1
	movslq	%ecx, %rax
	mulss	(%r14,%rax,4), %xmm1
	cvtss2sd	%xmm1, %xmm1
	addsd	%xmm1, %xmm0
	movl	%esi, %ecx
	movl	%edi, %eax
	jne	.LBB52_8	# bb6
	jmp	.LBB52_11	# return
.LBB52_9:	# real_catch0
	xorl	%edi, %edi
	call	llvm_real_catch
.LBB52_10:	# real_end0
	xorl	%edi, %edi
	call	llvm_real_end
	# implicit-def: xmm0
.LBB52_11:	# return
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	ret
.LBB52_12:	# real_try0.bb2_crit_edge
	xorl	%eax, %eax
	jmp	.LBB52_4	# bb2
.LBB52_13:	# bb2.bb7.preheader_crit_edge
	xorl	%ecx, %ecx
	jmp	.LBB52_6	# bb7.preheader
.LBB52_14:	# bb7.preheader.return_crit_edge
	pxor	%xmm0, %xmm0
	jmp	.LBB52_11	# return
	.size	cblas_dsdot, .-cblas_dsdot
.Leh_func_end35:


	.section	.rodata.cst8,"aM",@progbits,8
	.align	8
.LCPI53_0:					
	.quad	4607182418800017408	# double value: 1.000000e+00
	.text
	.align	16
	.globl	cblas_dspmv
	.type	cblas_dspmv,@function
cblas_dspmv:
.Leh_func_begin36:
.Llabel36:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	ucomisd	.LCPI53_0(%rip), %xmm1
	movq	96(%rsp), %rax
	jne	.LBB53_2	# bb12
	jp	.LBB53_2	# bb12
.LBB53_1:	# entry
	pxor	%xmm2, %xmm2
	ucomisd	%xmm2, %xmm0
	setnp	%r10b
	sete	%r11b
	testb	%r10b, %r11b
	jne	.LBB53_42	# return
.LBB53_2:	# bb12
	pxor	%xmm2, %xmm2
	ucomisd	%xmm2, %xmm1
	jne	.LBB53_8	# bb19
	jp	.LBB53_8	# bb19
.LBB53_3:	# bb13
	cmpl	$0, 104(%rsp)
	jg	.LBB53_43	# bb13.bb18.preheader_crit_edge
.LBB53_4:	# bb14
	movl	$1, %r10d
	subl	%edx, %r10d
	imull	104(%rsp), %r10d
.LBB53_5:	# bb18.preheader
	testl	%edx, %edx
	jle	.LBB53_14	# bb26
.LBB53_6:	# bb18.preheader.bb17_crit_edge
	xorl	%r11d, %r11d
	.align	16
.LBB53_7:	# bb17
	movslq	%r10d, %rbx
	movq	$0, (%rax,%rbx,8)
	addl	104(%rsp), %r10d
	incl	%r11d
	cmpl	%edx, %r11d
	jne	.LBB53_7	# bb17
	jmp	.LBB53_14	# bb26
.LBB53_8:	# bb19
	ucomisd	.LCPI53_0(%rip), %xmm1
	setnp	%r10b
	sete	%r11b
	testb	%r10b, %r11b
	jne	.LBB53_14	# bb26
.LBB53_9:	# bb20
	cmpl	$0, 104(%rsp)
	jg	.LBB53_44	# bb20.bb25.preheader_crit_edge
.LBB53_10:	# bb21
	movl	$1, %r10d
	subl	%edx, %r10d
	imull	104(%rsp), %r10d
.LBB53_11:	# bb25.preheader
	testl	%edx, %edx
	jle	.LBB53_14	# bb26
.LBB53_12:	# bb25.preheader.bb24_crit_edge
	xorl	%r11d, %r11d
	.align	16
.LBB53_13:	# bb24
	movslq	%r10d, %rbx
	movapd	%xmm1, %xmm2
	mulsd	(%rax,%rbx,8), %xmm2
	movsd	%xmm2, (%rax,%rbx,8)
	addl	104(%rsp), %r10d
	incl	%r11d
	cmpl	%edx, %r11d
	jne	.LBB53_13	# bb24
.LBB53_14:	# bb26
	pxor	%xmm1, %xmm1
	ucomisd	%xmm1, %xmm0
	setnp	%r10b
	sete	%r11b
	testb	%r10b, %r11b
	jne	.LBB53_42	# return
.LBB53_15:	# bb27
	cmpl	$121, %esi
	jne	.LBB53_17	# bb30
.LBB53_16:	# bb27
	cmpl	$101, %edi
	je	.LBB53_19	# bb34
.LBB53_17:	# bb30
	cmpl	$122, %esi
	jne	.LBB53_29	# bb52
.LBB53_18:	# bb30
	cmpl	$102, %edi
	jne	.LBB53_29	# bb52
.LBB53_19:	# bb34
	testl	%r9d, %r9d
	jg	.LBB53_45	# bb34.bb37_crit_edge
.LBB53_20:	# bb35
	movl	$1, %esi
	subl	%edx, %esi
	imull	%r9d, %esi
	movl	%esi, 12(%rsp)
.LBB53_21:	# bb37
	cmpl	$0, 104(%rsp)
	jg	.LBB53_46	# bb37.bb51.preheader_crit_edge
.LBB53_22:	# bb38
	movl	$1, %esi
	subl	%edx, %esi
	imull	104(%rsp), %esi
	movl	%esi, 8(%rsp)
.LBB53_23:	# bb51.preheader
	testl	%edx, %edx
	jle	.LBB53_42	# return
.LBB53_24:	# bb.nph112
	movl	$1, %esi
	subl	%edx, %esi
	movl	%esi, %edi
	imull	%r9d, %edi
	movl	%edi, 16(%rsp)
	imull	104(%rsp), %esi
	movl	%esi, 4(%rsp)
	leal	1(,%rdx,2), %esi
	movl	%esi, 24(%rsp)
	leal	-1(%rdx), %esi
	movl	%esi, 20(%rsp)
	xorl	%esi, %esi
	movl	%esi, 28(%rsp)
	movl	%esi, 36(%rsp)
	movl	%esi, 32(%rsp)
	.align	16
.LBB53_25:	# bb41
	movl	32(%rsp), %r10d
	movl	24(%rsp), %edi
	leal	(%rdi,%r10), %edi
	imull	%esi, %edi
	movl	%edi, %r10d
	shrl	$31, %r10d
	addl	%edi, %r10d
	sarl	%r10d
	movslq	%r10d, %rdi
	movl	28(%rsp), %ebx
	movl	12(%rsp), %r11d
	leal	(%r11,%rbx), %r11d
	movslq	%r11d, %r11
	movapd	%xmm0, %xmm1
	mulsd	(%r8,%r11,8), %xmm1
	movapd	%xmm1, %xmm2
	mulsd	(%rcx,%rdi,8), %xmm2
	movl	36(%rsp), %r11d
	movl	8(%rsp), %edi
	leal	(%rdi,%r11), %edi
	movslq	%edi, %rdi
	addsd	(%rax,%rdi,8), %xmm2
	movsd	%xmm2, (%rax,%rdi,8)
	xorl	%r11d, %r11d
	cmpl	$0, 104(%rsp)
	movl	4(%rsp), %ebx
	cmovg	%r11d, %ebx
	testl	%r9d, %r9d
	cmovle	16(%rsp), %r11d
	leal	1(%rsi), %r14d
	cmpl	%edx, %r14d
	jge	.LBB53_47	# bb41.bb50_crit_edge
.LBB53_26:	# bb.nph107
	movl	28(%rsp), %r14d
	leal	(%r9,%r14), %r14d
	movl	104(%rsp), %r15d
	movl	36(%rsp), %r12d
	leal	(%r15,%r12), %r15d
	movl	32(%rsp), %r12d
	movl	20(%rsp), %r13d
	leal	(%r13,%r12), %r12d
	incl	%r10d
	pxor	%xmm2, %xmm2
	xorl	%r13d, %r13d
	.align	16
.LBB53_27:	# bb48
	addl	%ebx, %r15d
	movslq	%r15d, %rbx
	leal	(%r10,%r13), %ebp
	movslq	%ebp, %rbp
	movsd	(%rcx,%rbp,8), %xmm3
	movapd	%xmm1, %xmm4
	mulsd	%xmm3, %xmm4
	addsd	(%rax,%rbx,8), %xmm4
	movsd	%xmm4, (%rax,%rbx,8)
	addl	%r11d, %r14d
	movslq	%r14d, %r11
	mulsd	(%r8,%r11,8), %xmm3
	addsd	%xmm3, %xmm2
	incl	%r13d
	cmpl	%r12d, %r13d
	movl	104(%rsp), %ebx
	movl	%r9d, %r11d
	jne	.LBB53_27	# bb48
.LBB53_28:	# bb50
	mulsd	%xmm0, %xmm2
	addsd	(%rax,%rdi,8), %xmm2
	movsd	%xmm2, (%rax,%rdi,8)
	addl	%r9d, 28(%rsp)
	movl	104(%rsp), %edi
	addl	%edi, 36(%rsp)
	decl	32(%rsp)
	incl	%esi
	cmpl	%edx, %esi
	jne	.LBB53_25	# bb41
	jmp	.LBB53_42	# return
.LBB53_29:	# bb52
	cmpl	$102, %edi
	sete	%r10b
	cmpl	$121, %esi
	sete	%r11b
	andb	%r10b, %r11b
	cmpl	$101, %edi
	sete	%dil
	cmpl	$122, %esi
	sete	%sil
	testb	%dil, %sil
	jne	.LBB53_31	# bb60
.LBB53_30:	# bb52
	notb	%r11b
	testb	$1, %r11b
	jne	.LBB53_41	# bb78
.LBB53_31:	# bb60
	testl	%r9d, %r9d
	jg	.LBB53_48	# bb60.bb63_crit_edge
.LBB53_32:	# bb61
	movl	$1, %esi
	subl	%edx, %esi
	imull	%r9d, %esi
.LBB53_33:	# bb63
	cmpl	$0, 104(%rsp)
	jg	.LBB53_49	# bb63.bb77.preheader_crit_edge
.LBB53_34:	# bb64
	movl	$1, %edi
	subl	%edx, %edi
	imull	104(%rsp), %edi
.LBB53_35:	# bb77.preheader
	testl	%edx, %edx
	jle	.LBB53_42	# return
.LBB53_36:	# bb.nph96
	movl	$1, %r10d
	subl	%edx, %r10d
	movl	%r10d, %r11d
	imull	%r9d, %r11d
	imull	104(%rsp), %r10d
	xorl	%ebx, %ebx
	.align	16
.LBB53_37:	# bb67
	movslq	%esi, %r14
	movapd	%xmm0, %xmm1
	mulsd	(%r8,%r14,8), %xmm1
	leal	1(%rbx), %r14d
	imull	%ebx, %r14d
	movl	%r14d, %r15d
	shrl	$31, %r15d
	addl	%r14d, %r15d
	sarl	%r15d
	leal	(%r15,%rbx), %r14d
	movslq	%r14d, %r14
	movapd	%xmm1, %xmm2
	mulsd	(%rcx,%r14,8), %xmm2
	movslq	%edi, %rdi
	addsd	(%rax,%rdi,8), %xmm2
	movsd	%xmm2, (%rax,%rdi,8)
	xorl	%r14d, %r14d
	cmpl	$0, 104(%rsp)
	movl	%r10d, %r12d
	cmovg	%r14d, %r12d
	testl	%r9d, %r9d
	cmovle	%r11d, %r14d
	testl	%ebx, %ebx
	jle	.LBB53_50	# bb67.bb76_crit_edge
.LBB53_38:	# bb67.bb74_crit_edge
	pxor	%xmm2, %xmm2
	xorl	%r13d, %r13d
	.align	16
.LBB53_39:	# bb74
	leal	(%r15,%r13), %ebp
	movslq	%ebp, %rbp
	movsd	(%rcx,%rbp,8), %xmm3
	movapd	%xmm1, %xmm4
	mulsd	%xmm3, %xmm4
	movslq	%r12d, %rbp
	addsd	(%rax,%rbp,8), %xmm4
	movsd	%xmm4, (%rax,%rbp,8)
	leal	(%r9,%r14), %ebp
	addl	104(%rsp), %r12d
	incl	%r13d
	cmpl	%ebx, %r13d
	movslq	%r14d, %r14
	mulsd	(%r8,%r14,8), %xmm3
	addsd	%xmm3, %xmm2
	movl	%ebp, %r14d
	jne	.LBB53_39	# bb74
.LBB53_40:	# bb76
	mulsd	%xmm0, %xmm2
	addsd	(%rax,%rdi,8), %xmm2
	movsd	%xmm2, (%rax,%rdi,8)
	addl	104(%rsp), %edi
	addl	%r9d, %esi
	incl	%ebx
	cmpl	%edx, %ebx
	jne	.LBB53_37	# bb67
	jmp	.LBB53_42	# return
.LBB53_41:	# bb78
	xorl	%edi, %edi
	leaq	.str65, %rsi
	leaq	.str166, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB53_42:	# return
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB53_43:	# bb13.bb18.preheader_crit_edge
	xorl	%r10d, %r10d
	jmp	.LBB53_5	# bb18.preheader
.LBB53_44:	# bb20.bb25.preheader_crit_edge
	xorl	%r10d, %r10d
	jmp	.LBB53_11	# bb25.preheader
.LBB53_45:	# bb34.bb37_crit_edge
	movl	$0, 12(%rsp)
	jmp	.LBB53_21	# bb37
.LBB53_46:	# bb37.bb51.preheader_crit_edge
	movl	$0, 8(%rsp)
	jmp	.LBB53_23	# bb51.preheader
.LBB53_47:	# bb41.bb50_crit_edge
	pxor	%xmm2, %xmm2
	jmp	.LBB53_28	# bb50
.LBB53_48:	# bb60.bb63_crit_edge
	xorl	%esi, %esi
	jmp	.LBB53_33	# bb63
.LBB53_49:	# bb63.bb77.preheader_crit_edge
	xorl	%edi, %edi
	jmp	.LBB53_35	# bb77.preheader
.LBB53_50:	# bb67.bb76_crit_edge
	pxor	%xmm2, %xmm2
	jmp	.LBB53_40	# bb76
	.size	cblas_dspmv, .-cblas_dspmv
.Leh_func_end36:


	.align	16
	.globl	cblas_dspr2
	.type	cblas_dspr2,@function
cblas_dspr2:
.Leh_func_begin37:
.Llabel37:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$8, %rsp
	pxor	%xmm1, %xmm1
	ucomisd	%xmm1, %xmm0
	setnp	%al
	sete	%r10b
	testb	%al, %r10b
	movq	72(%rsp), %rax
	movl	64(%rsp), %r10d
	jne	.LBB54_29	# return
.LBB54_1:	# entry
	testl	%edx, %edx
	je	.LBB54_29	# return
.LBB54_2:	# bb7
	cmpl	$121, %esi
	jne	.LBB54_4	# bb10
.LBB54_3:	# bb7
	cmpl	$101, %edi
	je	.LBB54_6	# bb14
.LBB54_4:	# bb10
	cmpl	$122, %esi
	jne	.LBB54_16	# bb26
.LBB54_5:	# bb10
	cmpl	$102, %edi
	jne	.LBB54_16	# bb26
.LBB54_6:	# bb14
	testl	%r8d, %r8d
	jg	.LBB54_30	# bb14.bb17_crit_edge
.LBB54_7:	# bb15
	movl	$1, %esi
	subl	%edx, %esi
	imull	%r8d, %esi
	movl	%esi, 4(%rsp)
.LBB54_8:	# bb17
	testl	%r10d, %r10d
	jg	.LBB54_31	# bb17.bb25.preheader_crit_edge
.LBB54_9:	# bb18
	movl	$1, %esi
	subl	%edx, %esi
	imull	%r10d, %esi
.LBB54_10:	# bb25.preheader
	testl	%edx, %edx
	jle	.LBB54_29	# return
.LBB54_11:	# bb.nph71
	leal	1(,%rdx,2), %edi
	movl	%edi, (%rsp)
	xorl	%edi, %edi
	movl	%edi, %r11d
	.align	16
.LBB54_12:	# bb21
	movslq	%esi, %rbx
	movapd	%xmm0, %xmm1
	mulsd	(%r9,%rbx,8), %xmm1
	movslq	4(%rsp), %rbx
	movapd	%xmm0, %xmm2
	mulsd	(%rcx,%rbx,8), %xmm2
	cmpl	%edx, %r11d
	jge	.LBB54_15	# bb24
.LBB54_13:	# bb.nph67
	movl	(%rsp), %ebx
	leal	(%rbx,%rdi), %ebx
	imull	%r11d, %ebx
	movl	%ebx, %r14d
	shrl	$31, %r14d
	addl	%ebx, %r14d
	sarl	%r14d
	leal	(%rdx,%rdi), %ebx
	xorl	%r15d, %r15d
	movl	%esi, %r12d
	movl	4(%rsp), %r13d
	.align	16
.LBB54_14:	# bb22
	movslq	%r13d, %rbp
	movapd	%xmm1, %xmm3
	mulsd	(%rcx,%rbp,8), %xmm3
	movslq	%r12d, %rbp
	movapd	%xmm2, %xmm4
	mulsd	(%r9,%rbp,8), %xmm4
	addsd	%xmm3, %xmm4
	leal	(%r14,%r15), %ebp
	movslq	%ebp, %rbp
	addsd	(%rax,%rbp,8), %xmm4
	movsd	%xmm4, (%rax,%rbp,8)
	addl	%r10d, %r12d
	addl	%r8d, %r13d
	incl	%r15d
	cmpl	%ebx, %r15d
	jne	.LBB54_14	# bb22
.LBB54_15:	# bb24
	addl	%r8d, 4(%rsp)
	addl	%r10d, %esi
	decl	%edi
	incl	%r11d
	cmpl	%edx, %r11d
	jne	.LBB54_12	# bb21
	jmp	.LBB54_29	# return
.LBB54_16:	# bb26
	cmpl	$102, %edi
	sete	%r11b
	cmpl	$121, %esi
	sete	%bl
	andb	%r11b, %bl
	cmpl	$101, %edi
	sete	%dil
	cmpl	$122, %esi
	sete	%sil
	testb	%dil, %sil
	jne	.LBB54_18	# bb34
.LBB54_17:	# bb26
	notb	%bl
	testb	$1, %bl
	jne	.LBB54_28	# bb52
.LBB54_18:	# bb34
	testl	%r8d, %r8d
	jg	.LBB54_32	# bb34.bb37_crit_edge
.LBB54_19:	# bb35
	movl	$1, %esi
	subl	%edx, %esi
	imull	%r8d, %esi
.LBB54_20:	# bb37
	testl	%r10d, %r10d
	jg	.LBB54_33	# bb37.bb51.preheader_crit_edge
.LBB54_21:	# bb38
	movl	$1, %edi
	subl	%edx, %edi
	imull	%r10d, %edi
.LBB54_22:	# bb51.preheader
	testl	%edx, %edx
	jle	.LBB54_29	# return
.LBB54_23:	# bb.nph63
	movl	$1, %r11d
	subl	%edx, %r11d
	movl	%r11d, %ebx
	imull	%r8d, %ebx
	movl	%ebx, 4(%rsp)
	imull	%r10d, %r11d
	xorl	%ebx, %ebx
	.align	16
.LBB54_24:	# bb41
	xorl	%r14d, %r14d
	testl	%r10d, %r10d
	movl	%r11d, %r15d
	cmovg	%r14d, %r15d
	testl	%r8d, %r8d
	cmovle	4(%rsp), %r14d
	movslq	%edi, %r12
	movapd	%xmm0, %xmm1
	mulsd	(%r9,%r12,8), %xmm1
	movslq	%esi, %r12
	movapd	%xmm0, %xmm2
	mulsd	(%rcx,%r12,8), %xmm2
	testl	%ebx, %ebx
	js	.LBB54_27	# bb50
.LBB54_25:	# bb.nph
	leal	1(%rbx), %r12d
	imull	%ebx, %r12d
	movl	%r12d, %r13d
	shrl	$31, %r13d
	addl	%r12d, %r13d
	sarl	%r13d
	xorl	%r12d, %r12d
	.align	16
.LBB54_26:	# bb48
	movslq	%r14d, %rbp
	movapd	%xmm1, %xmm3
	mulsd	(%rcx,%rbp,8), %xmm3
	movslq	%r15d, %rbp
	movapd	%xmm2, %xmm4
	mulsd	(%r9,%rbp,8), %xmm4
	addsd	%xmm3, %xmm4
	leal	(%r13,%r12), %ebp
	movslq	%ebp, %rbp
	addsd	(%rax,%rbp,8), %xmm4
	movsd	%xmm4, (%rax,%rbp,8)
	addl	%r8d, %r14d
	addl	%r10d, %r15d
	incl	%r12d
	cmpl	%ebx, %r12d
	jle	.LBB54_26	# bb48
.LBB54_27:	# bb50
	addl	%r8d, %esi
	addl	%r10d, %edi
	incl	%ebx
	cmpl	%edx, %ebx
	jne	.LBB54_24	# bb41
	jmp	.LBB54_29	# return
.LBB54_28:	# bb52
	xorl	%edi, %edi
	leaq	.str67, %rsi
	leaq	.str168, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB54_29:	# return
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB54_30:	# bb14.bb17_crit_edge
	movl	$0, 4(%rsp)
	jmp	.LBB54_8	# bb17
.LBB54_31:	# bb17.bb25.preheader_crit_edge
	xorl	%esi, %esi
	jmp	.LBB54_10	# bb25.preheader
.LBB54_32:	# bb34.bb37_crit_edge
	xorl	%esi, %esi
	jmp	.LBB54_20	# bb37
.LBB54_33:	# bb37.bb51.preheader_crit_edge
	xorl	%edi, %edi
	jmp	.LBB54_22	# bb51.preheader
	.size	cblas_dspr2, .-cblas_dspr2
.Leh_func_end37:


	.align	16
	.globl	cblas_dspr
	.type	cblas_dspr,@function
cblas_dspr:
.Leh_func_begin38:
.Llabel38:
	pushq	%r15
	pushq	%r14
	pushq	%r12
	pushq	%rbx
	subq	$8, %rsp
	pxor	%xmm1, %xmm1
	ucomisd	%xmm1, %xmm0
	setnp	%al
	sete	%r10b
	testb	%al, %r10b
	jne	.LBB55_25	# return
.LBB55_1:	# entry
	testl	%edx, %edx
	je	.LBB55_25	# return
.LBB55_2:	# bb4
	cmpl	$121, %esi
	jne	.LBB55_4	# bb7
.LBB55_3:	# bb4
	cmpl	$101, %edi
	je	.LBB55_6	# bb11
.LBB55_4:	# bb7
	cmpl	$122, %esi
	jne	.LBB55_14	# bb20
.LBB55_5:	# bb7
	cmpl	$102, %edi
	jne	.LBB55_14	# bb20
.LBB55_6:	# bb11
	testl	%r8d, %r8d
	jg	.LBB55_26	# bb11.bb19.preheader_crit_edge
.LBB55_7:	# bb12
	movl	$1, %eax
	subl	%edx, %eax
	imull	%r8d, %eax
.LBB55_8:	# bb19.preheader
	testl	%edx, %edx
	jle	.LBB55_25	# return
.LBB55_9:	# bb.nph55
	leal	1(,%rdx,2), %esi
	xorl	%edi, %edi
	movl	%edi, %r10d
	.align	16
.LBB55_10:	# bb15
	movslq	%eax, %r11
	movapd	%xmm0, %xmm1
	mulsd	(%rcx,%r11,8), %xmm1
	cmpl	%edx, %r10d
	jge	.LBB55_13	# bb18
.LBB55_11:	# bb.nph52
	leal	(%rsi,%rdi), %r11d
	imull	%r10d, %r11d
	movl	%r11d, %ebx
	shrl	$31, %ebx
	addl	%r11d, %ebx
	sarl	%ebx
	leal	(%rdx,%rdi), %r11d
	xorl	%r14d, %r14d
	movl	%eax, %r15d
	.align	16
.LBB55_12:	# bb16
	movslq	%r15d, %r12
	movapd	%xmm1, %xmm2
	mulsd	(%rcx,%r12,8), %xmm2
	leal	(%rbx,%r14), %r12d
	movslq	%r12d, %r12
	addsd	(%r9,%r12,8), %xmm2
	movsd	%xmm2, (%r9,%r12,8)
	addl	%r8d, %r15d
	incl	%r14d
	cmpl	%r11d, %r14d
	jne	.LBB55_12	# bb16
.LBB55_13:	# bb18
	addl	%r8d, %eax
	decl	%edi
	incl	%r10d
	cmpl	%edx, %r10d
	jne	.LBB55_10	# bb15
	jmp	.LBB55_25	# return
.LBB55_14:	# bb20
	cmpl	$102, %edi
	sete	%al
	cmpl	$121, %esi
	sete	%r10b
	andb	%al, %r10b
	cmpl	$101, %edi
	sete	%al
	cmpl	$122, %esi
	sete	%sil
	testb	%al, %sil
	jne	.LBB55_16	# bb28
.LBB55_15:	# bb20
	notb	%r10b
	testb	$1, %r10b
	jne	.LBB55_24	# bb40
.LBB55_16:	# bb28
	testl	%r8d, %r8d
	jg	.LBB55_27	# bb28.bb39.preheader_crit_edge
.LBB55_17:	# bb29
	movl	$1, %eax
	subl	%edx, %eax
	imull	%r8d, %eax
.LBB55_18:	# bb39.preheader
	testl	%edx, %edx
	jle	.LBB55_25	# return
.LBB55_19:	# bb.nph49
	movl	$1, %esi
	subl	%edx, %esi
	imull	%r8d, %esi
	xorl	%edi, %edi
	.align	16
.LBB55_20:	# bb32
	testl	%r8d, %r8d
	movl	$0, %r10d
	cmovle	%esi, %r10d
	movslq	%eax, %r11
	movapd	%xmm0, %xmm1
	mulsd	(%rcx,%r11,8), %xmm1
	testl	%edi, %edi
	js	.LBB55_23	# bb38
.LBB55_21:	# bb.nph
	leal	1(%rdi), %r11d
	imull	%edi, %r11d
	movl	%r11d, %ebx
	shrl	$31, %ebx
	addl	%r11d, %ebx
	sarl	%ebx
	xorl	%r11d, %r11d
	.align	16
.LBB55_22:	# bb36
	movslq	%r10d, %r14
	movapd	%xmm1, %xmm2
	mulsd	(%rcx,%r14,8), %xmm2
	leal	(%rbx,%r11), %r14d
	movslq	%r14d, %r14
	addsd	(%r9,%r14,8), %xmm2
	movsd	%xmm2, (%r9,%r14,8)
	addl	%r8d, %r10d
	incl	%r11d
	cmpl	%edi, %r11d
	jle	.LBB55_22	# bb36
.LBB55_23:	# bb38
	addl	%r8d, %eax
	incl	%edi
	cmpl	%edx, %edi
	jne	.LBB55_20	# bb32
	jmp	.LBB55_25	# return
.LBB55_24:	# bb40
	xorl	%edi, %edi
	leaq	.str69, %rsi
	leaq	.str170, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB55_25:	# return
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r14
	popq	%r15
	ret
.LBB55_26:	# bb11.bb19.preheader_crit_edge
	xorl	%eax, %eax
	jmp	.LBB55_8	# bb19.preheader
.LBB55_27:	# bb28.bb39.preheader_crit_edge
	xorl	%eax, %eax
	jmp	.LBB55_18	# bb39.preheader
	.size	cblas_dspr, .-cblas_dspr
.Leh_func_end38:


	.align	16
	.globl	cblas_dswap
	.type	cblas_dswap,@function
cblas_dswap:
	pushq	%rbx
	testl	%edx, %edx
	jg	.LBB56_8	# entry.bb2_crit_edge
.LBB56_1:	# bb
	movl	$1, %eax
	subl	%edi, %eax
	imull	%edx, %eax
.LBB56_2:	# bb2
	testl	%r8d, %r8d
	jg	.LBB56_9	# bb2.bb7.preheader_crit_edge
.LBB56_3:	# bb3
	movl	$1, %r9d
	subl	%edi, %r9d
	imull	%r8d, %r9d
.LBB56_4:	# bb7.preheader
	testl	%edi, %edi
	jle	.LBB56_7	# return
.LBB56_5:	# bb7.preheader.bb6_crit_edge
	xorl	%r10d, %r10d
	.align	16
.LBB56_6:	# bb6
	movslq	%r9d, %r11
	movsd	(%rcx,%r11,8), %xmm0
	movslq	%eax, %rbx
	movsd	(%rsi,%rbx,8), %xmm1
	movsd	%xmm0, (%rsi,%rbx,8)
	movsd	%xmm1, (%rcx,%r11,8)
	addl	%r8d, %r9d
	addl	%edx, %eax
	incl	%r10d
	cmpl	%edi, %r10d
	jne	.LBB56_6	# bb6
.LBB56_7:	# return
	popq	%rbx
	ret
.LBB56_8:	# entry.bb2_crit_edge
	xorl	%eax, %eax
	jmp	.LBB56_2	# bb2
.LBB56_9:	# bb2.bb7.preheader_crit_edge
	xorl	%r9d, %r9d
	jmp	.LBB56_4	# bb7.preheader
	.size	cblas_dswap, .-cblas_dswap


	.section	.rodata.cst8,"aM",@progbits,8
	.align	8
.LCPI57_0:					
	.quad	4607182418800017408	# double value: 1.000000e+00
	.text
	.align	16
	.globl	cblas_dsymm
	.type	cblas_dsymm,@function
cblas_dsymm:
.Leh_func_begin39:
.Llabel39:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	ucomisd	.LCPI57_0(%rip), %xmm1
	movl	128(%rsp), %eax
	movq	120(%rsp), %r10
	movq	104(%rsp), %r11
	jne	.LBB57_2	# bb9
	jp	.LBB57_2	# bb9
.LBB57_1:	# entry
	pxor	%xmm2, %xmm2
	ucomisd	%xmm2, %xmm0
	setnp	%bl
	sete	%r14b
	testb	%bl, %r14b
	jne	.LBB57_64	# return
.LBB57_2:	# bb9
	cmpl	$101, %edi
	je	.LBB57_65	# bb9.bb18_crit_edge
.LBB57_3:	# bb11
	cmpl	$141, %esi
	movl	$142, %edi
	movl	$141, %esi
	cmove	%edi, %esi
	cmpl	$121, %edx
	movl	$122, %edi
	movl	$121, %edx
	cmove	%edi, %edx
	movl	%ecx, 36(%rsp)
	movl	%r8d, %ecx
.LBB57_4:	# bb18
	pxor	%xmm2, %xmm2
	ucomisd	%xmm2, %xmm1
	jne	.LBB57_11	# bb25
	jp	.LBB57_11	# bb25
.LBB57_5:	# bb24.preheader
	testl	%ecx, %ecx
	jle	.LBB57_18	# bb32
.LBB57_6:	# bb24.preheader
	cmpl	$0, 36(%rsp)
	jle	.LBB57_18	# bb32
.LBB57_7:	# bb24.preheader.bb22.preheader_crit_edge
	xorl	%edi, %edi
	movl	%edi, %r8d
	jmp	.LBB57_10	# bb22.preheader
	.align	16
.LBB57_8:	# bb21
	leal	(%rdi,%rbx), %r14d
	movslq	%r14d, %r14
	movq	$0, (%r10,%r14,8)
	incl	%ebx
	cmpl	36(%rsp), %ebx
	jne	.LBB57_8	# bb21
.LBB57_9:	# bb23
	addl	%eax, %edi
	incl	%r8d
	cmpl	%ecx, %r8d
	je	.LBB57_18	# bb32
.LBB57_10:	# bb22.preheader
	xorl	%ebx, %ebx
	jmp	.LBB57_8	# bb21
.LBB57_11:	# bb25
	ucomisd	.LCPI57_0(%rip), %xmm1
	setnp	%dil
	sete	%r8b
	testb	%dil, %r8b
	jne	.LBB57_18	# bb32
.LBB57_12:	# bb25
	testl	%ecx, %ecx
	jle	.LBB57_18	# bb32
.LBB57_13:	# bb25
	cmpl	$0, 36(%rsp)
	jle	.LBB57_18	# bb32
.LBB57_14:	# bb25.bb29.preheader_crit_edge
	xorl	%edi, %edi
	movl	%edi, %r8d
	.align	16
.LBB57_15:	# bb29.preheader
	xorl	%ebx, %ebx
	.align	16
.LBB57_16:	# bb28
	leal	(%rdi,%rbx), %r14d
	movslq	%r14d, %r14
	movapd	%xmm1, %xmm2
	mulsd	(%r10,%r14,8), %xmm2
	movsd	%xmm2, (%r10,%r14,8)
	incl	%ebx
	cmpl	36(%rsp), %ebx
	jne	.LBB57_16	# bb28
.LBB57_17:	# bb30
	addl	%eax, %edi
	incl	%r8d
	cmpl	%ecx, %r8d
	jne	.LBB57_15	# bb29.preheader
.LBB57_18:	# bb32
	pxor	%xmm1, %xmm1
	ucomisd	%xmm1, %xmm0
	setnp	%dil
	sete	%r8b
	testb	%dil, %r8b
	jne	.LBB57_64	# return
.LBB57_19:	# bb33
	cmpl	$121, %edx
	jne	.LBB57_30	# bb45
.LBB57_20:	# bb33
	cmpl	$141, %esi
	jne	.LBB57_30	# bb45
.LBB57_21:	# bb44.preheader
	testl	%ecx, %ecx
	jle	.LBB57_64	# return
.LBB57_22:	# bb.nph128
	cmpl	$0, 36(%rsp)
	jle	.LBB57_64	# return
.LBB57_23:	# bb42.preheader.preheader
	movl	96(%rsp), %esi
	incl	%esi
	movl	%esi, 8(%rsp)
	leal	-1(%rcx), %esi
	xorl	%edi, %edi
	movl	%edi, 32(%rsp)
	movl	%edi, 28(%rsp)
	movl	%edi, 12(%rsp)
	jmp	.LBB57_29	# bb42.preheader
	.align	16
.LBB57_24:	# bb38
	movl	28(%rsp), %ebx
	leal	(%rbx,%r8), %ebx
	movslq	%ebx, %rbx
	movl	32(%rsp), %r14d
	leal	(%r14,%r8), %r14d
	movslq	%r14d, %r14
	movapd	%xmm0, %xmm1
	mulsd	(%r11,%r14,8), %xmm1
	movapd	%xmm1, %xmm2
	mulsd	(%r9,%rdx,8), %xmm2
	addsd	(%r10,%rbx,8), %xmm2
	movsd	%xmm2, (%r10,%rbx,8)
	cmpl	%ecx, 16(%rsp)
	jge	.LBB57_66	# bb38.bb41_crit_edge
.LBB57_25:	# bb.nph123
	movl	24(%rsp), %r14d
	leal	(%r14,%r8), %r14d
	movl	20(%rsp), %r15d
	leal	(%r15,%r8), %r15d
	pxor	%xmm2, %xmm2
	xorl	%r12d, %r12d
	.align	16
.LBB57_26:	# bb39
	leal	(%rdi,%r12), %r13d
	movslq	%r13d, %r13
	movsd	(%r9,%r13,8), %xmm3
	movapd	%xmm3, %xmm4
	mulsd	%xmm1, %xmm4
	movslq	%r15d, %r13
	addsd	(%r10,%r13,8), %xmm4
	movsd	%xmm4, (%r10,%r13,8)
	addl	%eax, %r15d
	movl	112(%rsp), %r13d
	leal	(%r13,%r14), %r13d
	incl	%r12d
	cmpl	%esi, %r12d
	movslq	%r14d, %r14
	mulsd	(%r11,%r14,8), %xmm3
	addsd	%xmm3, %xmm2
	movl	%r13d, %r14d
	jne	.LBB57_26	# bb39
.LBB57_27:	# bb41
	mulsd	%xmm0, %xmm2
	addsd	(%r10,%rbx,8), %xmm2
	movsd	%xmm2, (%r10,%rbx,8)
	incl	%r8d
	cmpl	36(%rsp), %r8d
	jne	.LBB57_24	# bb38
.LBB57_28:	# bb43
	movl	%edx, %edi
	addl	8(%rsp), %edi
	movl	112(%rsp), %r8d
	addl	%r8d, 32(%rsp)
	addl	%eax, 28(%rsp)
	decl	%esi
	movl	12(%rsp), %r8d
	incl	%r8d
	movl	%r8d, 12(%rsp)
	cmpl	%ecx, %r8d
	je	.LBB57_64	# return
.LBB57_29:	# bb42.preheader
	movl	112(%rsp), %edx
	movl	32(%rsp), %r8d
	leal	(%rdx,%r8), %r8d
	movl	%r8d, 24(%rsp)
	movl	28(%rsp), %r8d
	leal	(%rax,%r8), %r8d
	movl	%r8d, 20(%rsp)
	movslq	%edi, %rdx
	incl	%edi
	movl	12(%rsp), %r8d
	leal	1(%r8), %r8d
	movl	%r8d, 16(%rsp)
	xorl	%r8d, %r8d
	jmp	.LBB57_24	# bb38
.LBB57_30:	# bb45
	cmpl	$122, %edx
	jne	.LBB57_41	# bb58
.LBB57_31:	# bb45
	cmpl	$141, %esi
	jne	.LBB57_41	# bb58
.LBB57_32:	# bb57.preheader
	testl	%ecx, %ecx
	jle	.LBB57_64	# return
.LBB57_33:	# bb.nph118
	cmpl	$0, 36(%rsp)
	jle	.LBB57_64	# return
.LBB57_34:	# bb55.preheader.preheader
	movl	96(%rsp), %edi
	leal	1(%rdi), %edi
	movl	%edi, 28(%rsp)
	xorl	%edi, %edi
	movl	%edi, %edx
	movl	%edi, 32(%rsp)
	movl	%edi, %r8d
	movl	%edi, %ebx
	jmp	.LBB57_40	# bb55.preheader
	.align	16
.LBB57_35:	# bb51
	movl	32(%rsp), %r14d
	leal	(%r14,%rdi), %r14d
	movslq	%r14d, %r14
	movapd	%xmm0, %xmm1
	mulsd	(%r11,%r14,8), %xmm1
	testl	%ebx, %ebx
	jle	.LBB57_67	# bb51.bb54_crit_edge
.LBB57_36:	# bb51.bb52_crit_edge
	pxor	%xmm2, %xmm2
	xorl	%r14d, %r14d
	movl	%edi, %r15d
	movl	%edi, %r12d
	.align	16
.LBB57_37:	# bb52
	leal	(%rdx,%r14), %r13d
	movslq	%r13d, %r13
	movsd	(%r9,%r13,8), %xmm3
	movapd	%xmm3, %xmm4
	mulsd	%xmm1, %xmm4
	movslq	%r15d, %r13
	addsd	(%r10,%r13,8), %xmm4
	movsd	%xmm4, (%r10,%r13,8)
	addl	%eax, %r15d
	movl	112(%rsp), %r13d
	leal	(%r13,%r12), %r13d
	incl	%r14d
	cmpl	%ebx, %r14d
	movslq	%r12d, %r12
	mulsd	(%r11,%r12,8), %xmm3
	addsd	%xmm3, %xmm2
	movl	%r13d, %r12d
	jne	.LBB57_37	# bb52
.LBB57_38:	# bb54
	mulsd	(%r9,%rsi,8), %xmm1
	mulsd	%xmm0, %xmm2
	addsd	%xmm1, %xmm2
	leal	(%r8,%rdi), %r14d
	movslq	%r14d, %r14
	addsd	(%r10,%r14,8), %xmm2
	movsd	%xmm2, (%r10,%r14,8)
	incl	%edi
	cmpl	36(%rsp), %edi
	jne	.LBB57_35	# bb51
.LBB57_39:	# bb56
	movl	%esi, %edi
	addl	28(%rsp), %edi
	addl	96(%rsp), %edx
	movl	112(%rsp), %esi
	addl	%esi, 32(%rsp)
	addl	%eax, %r8d
	incl	%ebx
	cmpl	%ecx, %ebx
	je	.LBB57_64	# return
.LBB57_40:	# bb55.preheader
	movslq	%edi, %rsi
	xorl	%edi, %edi
	jmp	.LBB57_35	# bb51
.LBB57_41:	# bb58
	cmpl	$121, %edx
	jne	.LBB57_52	# bb71
.LBB57_42:	# bb58
	cmpl	$142, %esi
	jne	.LBB57_52	# bb71
.LBB57_43:	# bb70.preheader
	testl	%ecx, %ecx
	jle	.LBB57_64	# return
.LBB57_44:	# bb.nph110
	cmpl	$0, 36(%rsp)
	jle	.LBB57_64	# return
.LBB57_45:	# bb.nph110.bb68.preheader_crit_edge
	xorl	%edx, %edx
	movl	%edx, 28(%rsp)
	movl	%edx, 32(%rsp)
	movl	%edx, 12(%rsp)
	jmp	.LBB57_51	# bb68.preheader
	.align	16
.LBB57_46:	# bb64
	movl	32(%rsp), %r8d
	leal	(%r8,%rdi), %r8d
	movslq	%r8d, %r8
	movl	28(%rsp), %ebx
	leal	(%rbx,%rdi), %ebx
	movslq	%ebx, %rbx
	movapd	%xmm0, %xmm1
	mulsd	(%r11,%rbx,8), %xmm1
	movslq	%edx, %rbx
	movapd	%xmm1, %xmm2
	mulsd	(%r9,%rbx,8), %xmm2
	addsd	(%r10,%r8,8), %xmm2
	movsd	%xmm2, (%r10,%r8,8)
	leal	1(%rdi), %ebx
	cmpl	36(%rsp), %ebx
	jge	.LBB57_68	# bb64.bb67_crit_edge
.LBB57_47:	# bb.nph105
	movl	16(%rsp), %ebx
	leal	(%rbx,%rdi), %ebx
	movl	20(%rsp), %r14d
	leal	(%r14,%rdi), %r14d
	leal	1(%rdx), %r15d
	pxor	%xmm2, %xmm2
	xorl	%r12d, %r12d
	.align	16
.LBB57_48:	# bb65
	leal	(%r14,%r12), %r13d
	movslq	%r13d, %r13
	leal	(%r15,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r9,%rbp,8), %xmm3
	movapd	%xmm1, %xmm4
	mulsd	%xmm3, %xmm4
	addsd	(%r10,%r13,8), %xmm4
	movsd	%xmm4, (%r10,%r13,8)
	leal	(%rbx,%r12), %r13d
	movslq	%r13d, %r13
	mulsd	(%r11,%r13,8), %xmm3
	addsd	%xmm3, %xmm2
	incl	%r12d
	cmpl	%esi, %r12d
	jne	.LBB57_48	# bb65
.LBB57_49:	# bb67
	mulsd	%xmm0, %xmm2
	addsd	(%r10,%r8,8), %xmm2
	movsd	%xmm2, (%r10,%r8,8)
	addl	24(%rsp), %edx
	decl	%esi
	incl	%edi
	cmpl	36(%rsp), %edi
	jne	.LBB57_46	# bb64
.LBB57_50:	# bb69
	movl	112(%rsp), %edx
	addl	%edx, 28(%rsp)
	addl	%eax, 32(%rsp)
	movl	12(%rsp), %edx
	incl	%edx
	movl	%edx, 12(%rsp)
	cmpl	%ecx, %edx
	je	.LBB57_64	# return
.LBB57_51:	# bb68.preheader
	movl	96(%rsp), %edx
	leal	1(%rdx), %edx
	movl	%edx, 24(%rsp)
	movl	36(%rsp), %edx
	leal	-1(%rdx), %esi
	movl	32(%rsp), %edx
	leal	1(%rdx), %edx
	movl	%edx, 20(%rsp)
	movl	28(%rsp), %edx
	leal	1(%rdx), %edx
	movl	%edx, 16(%rsp)
	xorl	%edx, %edx
	movl	%edx, %edi
	jmp	.LBB57_46	# bb64
.LBB57_52:	# bb71
	cmpl	$122, %edx
	jne	.LBB57_63	# bb84
.LBB57_53:	# bb71
	cmpl	$142, %esi
	jne	.LBB57_63	# bb84
.LBB57_54:	# bb83.preheader
	testl	%ecx, %ecx
	jle	.LBB57_64	# return
.LBB57_55:	# bb.nph100
	cmpl	$0, 36(%rsp)
	jle	.LBB57_64	# return
.LBB57_56:	# bb.nph100.bb81.preheader_crit_edge
	xorl	%edi, %edi
	movl	%edi, %r14d
	movl	%edi, %esi
	jmp	.LBB57_62	# bb81.preheader
	.align	16
.LBB57_57:	# bb77
	leal	(%rdi,%r8), %r12d
	movslq	%r12d, %r12
	movapd	%xmm0, %xmm1
	mulsd	(%r11,%r12,8), %xmm1
	testl	%r8d, %r8d
	jle	.LBB57_69	# bb77.bb80_crit_edge
.LBB57_58:	# bb77.bb78_crit_edge
	pxor	%xmm2, %xmm2
	xorl	%r12d, %r12d
	.align	16
.LBB57_59:	# bb78
	leal	(%r14,%r12), %r13d
	movslq	%r13d, %r13
	leal	(%r15,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r9,%rbp,8), %xmm3
	movapd	%xmm1, %xmm4
	mulsd	%xmm3, %xmm4
	addsd	(%r10,%r13,8), %xmm4
	movsd	%xmm4, (%r10,%r13,8)
	leal	(%rdi,%r12), %r13d
	movslq	%r13d, %r13
	mulsd	(%r11,%r13,8), %xmm3
	addsd	%xmm3, %xmm2
	incl	%r12d
	cmpl	%r8d, %r12d
	jne	.LBB57_59	# bb78
.LBB57_60:	# bb80
	movslq	%edx, %r12
	mulsd	(%r9,%r12,8), %xmm1
	mulsd	%xmm0, %xmm2
	addsd	%xmm1, %xmm2
	leal	(%r14,%r8), %r12d
	movslq	%r12d, %r12
	addsd	(%r10,%r12,8), %xmm2
	movsd	%xmm2, (%r10,%r12,8)
	addl	%ebx, %edx
	addl	96(%rsp), %r15d
	incl	%r8d
	cmpl	36(%rsp), %r8d
	jne	.LBB57_57	# bb77
.LBB57_61:	# bb82
	addl	112(%rsp), %edi
	addl	%eax, %r14d
	incl	%esi
	cmpl	%ecx, %esi
	je	.LBB57_64	# return
.LBB57_62:	# bb81.preheader
	movl	96(%rsp), %edx
	leal	1(%rdx), %ebx
	xorl	%edx, %edx
	movl	%edx, %r15d
	movl	%edx, %r8d
	jmp	.LBB57_57	# bb77
.LBB57_63:	# bb84
	xorl	%edi, %edi
	leaq	.str71, %rsi
	leaq	.str172, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB57_64:	# return
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB57_65:	# bb9.bb18_crit_edge
	movl	%r8d, 36(%rsp)
	jmp	.LBB57_4	# bb18
.LBB57_66:	# bb38.bb41_crit_edge
	pxor	%xmm2, %xmm2
	jmp	.LBB57_27	# bb41
.LBB57_67:	# bb51.bb54_crit_edge
	pxor	%xmm2, %xmm2
	jmp	.LBB57_38	# bb54
.LBB57_68:	# bb64.bb67_crit_edge
	pxor	%xmm2, %xmm2
	jmp	.LBB57_49	# bb67
.LBB57_69:	# bb77.bb80_crit_edge
	pxor	%xmm2, %xmm2
	jmp	.LBB57_60	# bb80
	.size	cblas_dsymm, .-cblas_dsymm
.Leh_func_end39:


	.section	.rodata.cst8,"aM",@progbits,8
	.align	8
.LCPI58_0:					
	.quad	4607182418800017408	# double value: 1.000000e+00
	.text
	.align	16
	.globl	cblas_dsymv
	.type	cblas_dsymv,@function
cblas_dsymv:
.Leh_func_begin40:
.Llabel40:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	ucomisd	.LCPI58_0(%rip), %xmm1
	movl	112(%rsp), %eax
	movq	104(%rsp), %r10
	movl	%r8d, 24(%rsp)
	jne	.LBB58_2	# bb11
	jp	.LBB58_2	# bb11
.LBB58_1:	# entry
	pxor	%xmm2, %xmm2
	ucomisd	%xmm2, %xmm0
	setnp	%r8b
	sete	%r11b
	testb	%r8b, %r11b
	jne	.LBB58_42	# bb80.thread
.LBB58_2:	# bb11
	pxor	%xmm2, %xmm2
	ucomisd	%xmm2, %xmm1
	jne	.LBB58_8	# bb18
	jp	.LBB58_8	# bb18
.LBB58_3:	# bb12
	testl	%eax, %eax
	jg	.LBB58_44	# bb12.bb17.preheader_crit_edge
.LBB58_4:	# bb13
	movl	$1, %r8d
	subl	%edx, %r8d
	imull	%eax, %r8d
.LBB58_5:	# bb17.preheader
	testl	%edx, %edx
	jle	.LBB58_14	# bb25
.LBB58_6:	# bb17.preheader.bb16_crit_edge
	xorl	%r11d, %r11d
	.align	16
.LBB58_7:	# bb16
	movslq	%r8d, %rbx
	movq	$0, (%r10,%rbx,8)
	addl	%eax, %r8d
	incl	%r11d
	cmpl	%edx, %r11d
	jne	.LBB58_7	# bb16
	jmp	.LBB58_14	# bb25
.LBB58_8:	# bb18
	ucomisd	.LCPI58_0(%rip), %xmm1
	setnp	%r8b
	sete	%r11b
	testb	%r8b, %r11b
	jne	.LBB58_14	# bb25
.LBB58_9:	# bb19
	testl	%eax, %eax
	jg	.LBB58_45	# bb19.bb24.preheader_crit_edge
.LBB58_10:	# bb20
	movl	$1, %r8d
	subl	%edx, %r8d
	imull	%eax, %r8d
.LBB58_11:	# bb24.preheader
	testl	%edx, %edx
	jle	.LBB58_14	# bb25
.LBB58_12:	# bb24.preheader.bb23_crit_edge
	xorl	%r11d, %r11d
	.align	16
.LBB58_13:	# bb23
	movslq	%r8d, %rbx
	movapd	%xmm1, %xmm2
	mulsd	(%r10,%rbx,8), %xmm2
	movsd	%xmm2, (%r10,%rbx,8)
	addl	%eax, %r8d
	incl	%r11d
	cmpl	%edx, %r11d
	jne	.LBB58_13	# bb23
.LBB58_14:	# bb25
	pxor	%xmm1, %xmm1
	ucomisd	%xmm1, %xmm0
	setnp	%r8b
	sete	%r11b
	testb	%r8b, %r11b
	jne	.LBB58_42	# bb80.thread
.LBB58_15:	# bb26
	cmpl	$121, %esi
	jne	.LBB58_17	# bb29
.LBB58_16:	# bb26
	cmpl	$101, %edi
	je	.LBB58_19	# bb33
.LBB58_17:	# bb29
	cmpl	$122, %esi
	jne	.LBB58_29	# bb51
.LBB58_18:	# bb29
	cmpl	$102, %edi
	jne	.LBB58_29	# bb51
.LBB58_19:	# bb33
	cmpl	$0, 96(%rsp)
	jg	.LBB58_46	# bb33.bb36_crit_edge
.LBB58_20:	# bb34
	movl	$1, %esi
	subl	%edx, %esi
	imull	96(%rsp), %esi
	movl	%esi, 16(%rsp)
.LBB58_21:	# bb36
	testl	%eax, %eax
	jg	.LBB58_47	# bb36.bb50.preheader_crit_edge
.LBB58_22:	# bb37
	movl	$1, %esi
	subl	%edx, %esi
	imull	%eax, %esi
	movl	%esi, 12(%rsp)
.LBB58_23:	# bb50.preheader
	testl	%edx, %edx
	jle	.LBB58_42	# bb80.thread
.LBB58_24:	# bb.nph113
	movl	$1, %esi
	subl	%edx, %esi
	movl	%esi, %edi
	imull	96(%rsp), %edi
	movl	%edi, 20(%rsp)
	imull	%eax, %esi
	movl	%esi, 8(%rsp)
	leal	-1(%rdx), %esi
	incl	24(%rsp)
	xorl	%edi, %edi
	movl	%edi, 32(%rsp)
	movl	%edi, 36(%rsp)
	movl	%edi, 28(%rsp)
	.align	16
.LBB58_25:	# bb40
	movl	32(%rsp), %r8d
	movl	12(%rsp), %r11d
	leal	(%r11,%r8), %r8d
	movslq	%r8d, %r8
	movl	16(%rsp), %r11d
	leal	(%r11,%rdi), %r11d
	movslq	%r11d, %r11
	movapd	%xmm0, %xmm1
	mulsd	(%r9,%r11,8), %xmm1
	movslq	36(%rsp), %r11
	movapd	%xmm1, %xmm2
	mulsd	(%rcx,%r11,8), %xmm2
	addsd	(%r10,%r8,8), %xmm2
	movsd	%xmm2, (%r10,%r8,8)
	xorl	%r11d, %r11d
	testl	%eax, %eax
	movl	8(%rsp), %ebx
	cmovg	%r11d, %ebx
	cmpl	$0, 96(%rsp)
	cmovle	20(%rsp), %r11d
	movl	28(%rsp), %r14d
	leal	1(%r14), %r14d
	cmpl	%edx, %r14d
	jge	.LBB58_48	# bb40.bb49_crit_edge
.LBB58_26:	# bb.nph108
	movl	96(%rsp), %r14d
	leal	(%r14,%rdi), %r14d
	movl	32(%rsp), %r15d
	leal	(%rax,%r15), %r15d
	movl	36(%rsp), %r12d
	leal	1(%r12), %r12d
	pxor	%xmm2, %xmm2
	xorl	%r13d, %r13d
	.align	16
.LBB58_27:	# bb47
	addl	%ebx, %r15d
	movslq	%r15d, %rbx
	leal	(%r12,%r13), %ebp
	movslq	%ebp, %rbp
	movapd	%xmm1, %xmm3
	mulsd	(%rcx,%rbp,8), %xmm3
	addsd	(%r10,%rbx,8), %xmm3
	movsd	%xmm3, (%r10,%rbx,8)
	addl	%r11d, %r14d
	movslq	%r14d, %r11
	movsd	(%r9,%r11,8), %xmm3
	mulsd	(%rcx,%rbp,8), %xmm3
	addsd	%xmm3, %xmm2
	incl	%r13d
	cmpl	%esi, %r13d
	movl	%eax, %ebx
	movl	96(%rsp), %r11d
	jne	.LBB58_27	# bb47
.LBB58_28:	# bb49
	mulsd	%xmm0, %xmm2
	addsd	(%r10,%r8,8), %xmm2
	movsd	%xmm2, (%r10,%r8,8)
	addl	96(%rsp), %edi
	addl	%eax, 32(%rsp)
	movl	36(%rsp), %r8d
	addl	24(%rsp), %r8d
	movl	%r8d, 36(%rsp)
	decl	%esi
	movl	28(%rsp), %r8d
	incl	%r8d
	movl	%r8d, 28(%rsp)
	cmpl	%edx, %r8d
	jne	.LBB58_25	# bb40
	jmp	.LBB58_42	# bb80.thread
.LBB58_29:	# bb51
	cmpl	$102, %edi
	sete	%r8b
	cmpl	$121, %esi
	sete	%r11b
	andb	%r8b, %r11b
	cmpl	$101, %edi
	sete	%dil
	cmpl	$122, %esi
	sete	%sil
	testb	%dil, %sil
	jne	.LBB58_31	# bb59
.LBB58_30:	# bb51
	notb	%r11b
	testb	$1, %r11b
	jne	.LBB58_43	# bb82
.LBB58_31:	# bb59
	cmpl	$0, 96(%rsp)
	jg	.LBB58_49	# bb59.bb62_crit_edge
.LBB58_32:	# bb60
	movl	$1, %esi
	subl	%edx, %esi
	imull	96(%rsp), %esi
	movl	%esi, 36(%rsp)
.LBB58_33:	# bb62
	testl	%eax, %eax
	jg	.LBB58_50	# bb62.bb65_crit_edge
.LBB58_34:	# bb63
	movl	$1, %esi
	subl	%edx, %esi
	imull	%eax, %esi
.LBB58_35:	# bb65
	movl	$1, %edi
	subl	%edx, %edi
	movl	%edi, %r8d
	movl	96(%rsp), %r11d
	imull	%r11d, %r8d
	movl	%r8d, 32(%rsp)
	leal	-1(%rdx), %r8d
	imull	%r8d, %r11d
	imull	%eax, %edi
	movl	%edi, 20(%rsp)
	movl	%eax, %edi
	imull	%r8d, %edi
	movl	$4294967295, %r14d
	movl	24(%rsp), %ebx
	subl	%ebx, %r14d
	movl	%r14d, 28(%rsp)
	movl	%ebx, %r14d
	imull	%r8d, %r14d
	leal	1(%rbx), %ebx
	imull	%r8d, %ebx
	addl	%esi, %edi
	addl	%r11d, 36(%rsp)
	jmp	.LBB58_40	# bb76
.LBB58_36:	# bb66
	movslq	36(%rsp), %r8
	movapd	%xmm0, %xmm1
	mulsd	(%r9,%r8,8), %xmm1
	movslq	%ebx, %r8
	movapd	%xmm1, %xmm2
	mulsd	(%rcx,%r8,8), %xmm2
	movslq	%edi, %r8
	addsd	(%r10,%r8,8), %xmm2
	movsd	%xmm2, (%r10,%r8,8)
	xorl	%r11d, %r11d
	testl	%eax, %eax
	movl	20(%rsp), %r15d
	cmovg	%r11d, %r15d
	cmpl	$0, 96(%rsp)
	cmovle	32(%rsp), %r11d
	testl	%esi, %esi
	jle	.LBB58_51	# bb66.bb75_crit_edge
.LBB58_37:	# bb.nph96
	leal	-1(%rdx), %esi
	pxor	%xmm2, %xmm2
	xorl	%r12d, %r12d
	.align	16
.LBB58_38:	# bb73
	leal	(%r14,%r12), %r13d
	movslq	%r13d, %r13
	movapd	%xmm1, %xmm3
	mulsd	(%rcx,%r13,8), %xmm3
	movslq	%r15d, %rbp
	addsd	(%r10,%rbp,8), %xmm3
	movsd	%xmm3, (%r10,%rbp,8)
	movslq	%r11d, %rbp
	movsd	(%r9,%rbp,8), %xmm3
	mulsd	(%rcx,%r13,8), %xmm3
	addsd	%xmm3, %xmm2
	addl	96(%rsp), %r11d
	addl	%eax, %r15d
	incl	%r12d
	cmpl	%esi, %r12d
	jne	.LBB58_38	# bb73
.LBB58_39:	# bb75
	mulsd	%xmm0, %xmm2
	addsd	(%r10,%r8,8), %xmm2
	movsd	%xmm2, (%r10,%r8,8)
	addl	28(%rsp), %ebx
	subl	%eax, %edi
	movl	36(%rsp), %esi
	subl	96(%rsp), %esi
	movl	%esi, 36(%rsp)
	subl	24(%rsp), %r14d
	decl	%edx
.LBB58_40:	# bb76
	testl	%edx, %edx
	jle	.LBB58_42	# bb80.thread
.LBB58_41:	# bb77
	leal	-1(%rdx), %esi
	testl	%edx, %edx
	jne	.LBB58_36	# bb66
.LBB58_42:	# bb80.thread
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB58_43:	# bb82
	xorl	%edi, %edi
	leaq	.str73, %rsi
	leaq	.str174, %rdx
	xorb	%al, %al
	call	cblas_xerbla
	jmp	.LBB58_42	# bb80.thread
.LBB58_44:	# bb12.bb17.preheader_crit_edge
	xorl	%r8d, %r8d
	jmp	.LBB58_5	# bb17.preheader
.LBB58_45:	# bb19.bb24.preheader_crit_edge
	xorl	%r8d, %r8d
	jmp	.LBB58_11	# bb24.preheader
.LBB58_46:	# bb33.bb36_crit_edge
	movl	$0, 16(%rsp)
	jmp	.LBB58_21	# bb36
.LBB58_47:	# bb36.bb50.preheader_crit_edge
	movl	$0, 12(%rsp)
	jmp	.LBB58_23	# bb50.preheader
.LBB58_48:	# bb40.bb49_crit_edge
	pxor	%xmm2, %xmm2
	jmp	.LBB58_28	# bb49
.LBB58_49:	# bb59.bb62_crit_edge
	movl	$0, 36(%rsp)
	jmp	.LBB58_33	# bb62
.LBB58_50:	# bb62.bb65_crit_edge
	xorl	%esi, %esi
	jmp	.LBB58_35	# bb65
.LBB58_51:	# bb66.bb75_crit_edge
	pxor	%xmm2, %xmm2
	jmp	.LBB58_39	# bb75
	.size	cblas_dsymv, .-cblas_dsymv
.Leh_func_end40:


	.align	16
	.globl	cblas_dsyr2
	.type	cblas_dsyr2,@function
cblas_dsyr2:
.Leh_func_begin41:
.Llabel41:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$8, %rsp
	pxor	%xmm1, %xmm1
	ucomisd	%xmm1, %xmm0
	setnp	%al
	sete	%r10b
	testb	%al, %r10b
	movq	72(%rsp), %rax
	movl	64(%rsp), %r10d
	jne	.LBB59_29	# return
.LBB59_1:	# entry
	testl	%edx, %edx
	je	.LBB59_29	# return
.LBB59_2:	# bb7
	cmpl	$121, %esi
	jne	.LBB59_4	# bb10
.LBB59_3:	# bb7
	cmpl	$101, %edi
	je	.LBB59_6	# bb14
.LBB59_4:	# bb10
	cmpl	$122, %esi
	jne	.LBB59_16	# bb26
.LBB59_5:	# bb10
	cmpl	$102, %edi
	jne	.LBB59_16	# bb26
.LBB59_6:	# bb14
	testl	%r8d, %r8d
	jg	.LBB59_30	# bb14.bb17_crit_edge
.LBB59_7:	# bb15
	movl	$1, %esi
	subl	%edx, %esi
	imull	%r8d, %esi
.LBB59_8:	# bb17
	testl	%r10d, %r10d
	jg	.LBB59_31	# bb17.bb25.preheader_crit_edge
.LBB59_9:	# bb18
	movl	$1, %edi
	subl	%edx, %edi
	imull	%r10d, %edi
.LBB59_10:	# bb25.preheader
	testl	%edx, %edx
	jle	.LBB59_29	# return
.LBB59_11:	# bb21.preheader
	movl	80(%rsp), %r11d
	incl	%r11d
	movl	%r11d, (%rsp)
	xorl	%r11d, %r11d
	movl	%edx, %ebx
	movl	%r11d, %r14d
	.align	16
.LBB59_12:	# bb21
	movslq	%edi, %r15
	movapd	%xmm0, %xmm1
	mulsd	(%r9,%r15,8), %xmm1
	movslq	%esi, %r15
	movapd	%xmm0, %xmm2
	mulsd	(%rcx,%r15,8), %xmm2
	cmpl	%edx, %r14d
	jge	.LBB59_15	# bb24
.LBB59_13:	# bb21.bb22_crit_edge
	xorl	%r15d, %r15d
	movl	%edi, %r12d
	movl	%esi, %r13d
	.align	16
.LBB59_14:	# bb22
	movslq	%r13d, %rbp
	movapd	%xmm1, %xmm3
	mulsd	(%rcx,%rbp,8), %xmm3
	movslq	%r12d, %rbp
	movapd	%xmm2, %xmm4
	mulsd	(%r9,%rbp,8), %xmm4
	addsd	%xmm3, %xmm4
	leal	(%r11,%r15), %ebp
	movslq	%ebp, %rbp
	addsd	(%rax,%rbp,8), %xmm4
	movsd	%xmm4, (%rax,%rbp,8)
	addl	%r10d, %r12d
	addl	%r8d, %r13d
	incl	%r15d
	cmpl	%ebx, %r15d
	jne	.LBB59_14	# bb22
.LBB59_15:	# bb24
	addl	(%rsp), %r11d
	addl	%r8d, %esi
	addl	%r10d, %edi
	decl	%ebx
	incl	%r14d
	cmpl	%edx, %r14d
	jne	.LBB59_12	# bb21
	jmp	.LBB59_29	# return
.LBB59_16:	# bb26
	cmpl	$102, %edi
	sete	%r11b
	cmpl	$121, %esi
	sete	%bl
	andb	%r11b, %bl
	cmpl	$101, %edi
	sete	%dil
	cmpl	$122, %esi
	sete	%sil
	testb	%dil, %sil
	jne	.LBB59_18	# bb34
.LBB59_17:	# bb26
	notb	%bl
	testb	$1, %bl
	jne	.LBB59_28	# bb52
.LBB59_18:	# bb34
	testl	%r8d, %r8d
	jg	.LBB59_32	# bb34.bb37_crit_edge
.LBB59_19:	# bb35
	movl	$1, %esi
	subl	%edx, %esi
	imull	%r8d, %esi
.LBB59_20:	# bb37
	testl	%r10d, %r10d
	jg	.LBB59_33	# bb37.bb51.preheader_crit_edge
.LBB59_21:	# bb38
	movl	$1, %edi
	subl	%edx, %edi
	imull	%r10d, %edi
.LBB59_22:	# bb51.preheader
	testl	%edx, %edx
	jle	.LBB59_29	# return
.LBB59_23:	# bb.nph61
	movl	$1, %r11d
	subl	%edx, %r11d
	movl	%r11d, %ebx
	imull	%r8d, %ebx
	movl	%ebx, 4(%rsp)
	imull	%r10d, %r11d
	xorl	%ebx, %ebx
	movl	%ebx, %r14d
	.align	16
.LBB59_24:	# bb41
	xorl	%r15d, %r15d
	testl	%r10d, %r10d
	movl	%r11d, %r12d
	cmovg	%r15d, %r12d
	testl	%r8d, %r8d
	cmovle	4(%rsp), %r15d
	movslq	%edi, %r13
	movapd	%xmm0, %xmm1
	mulsd	(%r9,%r13,8), %xmm1
	movslq	%esi, %r13
	movapd	%xmm0, %xmm2
	mulsd	(%rcx,%r13,8), %xmm2
	testl	%r14d, %r14d
	js	.LBB59_27	# bb50
.LBB59_25:	# bb41.bb48_crit_edge
	xorl	%r13d, %r13d
	.align	16
.LBB59_26:	# bb48
	movslq	%r15d, %rbp
	movapd	%xmm1, %xmm3
	mulsd	(%rcx,%rbp,8), %xmm3
	movslq	%r12d, %rbp
	movapd	%xmm2, %xmm4
	mulsd	(%r9,%rbp,8), %xmm4
	addsd	%xmm3, %xmm4
	leal	(%rbx,%r13), %ebp
	movslq	%ebp, %rbp
	addsd	(%rax,%rbp,8), %xmm4
	movsd	%xmm4, (%rax,%rbp,8)
	addl	%r8d, %r15d
	addl	%r10d, %r12d
	incl	%r13d
	cmpl	%r14d, %r13d
	jle	.LBB59_26	# bb48
.LBB59_27:	# bb50
	addl	%r8d, %esi
	addl	%r10d, %edi
	addl	80(%rsp), %ebx
	incl	%r14d
	cmpl	%edx, %r14d
	jne	.LBB59_24	# bb41
	jmp	.LBB59_29	# return
.LBB59_28:	# bb52
	xorl	%edi, %edi
	leaq	.str75, %rsi
	leaq	.str176, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB59_29:	# return
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB59_30:	# bb14.bb17_crit_edge
	xorl	%esi, %esi
	jmp	.LBB59_8	# bb17
.LBB59_31:	# bb17.bb25.preheader_crit_edge
	xorl	%edi, %edi
	jmp	.LBB59_10	# bb25.preheader
.LBB59_32:	# bb34.bb37_crit_edge
	xorl	%esi, %esi
	jmp	.LBB59_20	# bb37
.LBB59_33:	# bb37.bb51.preheader_crit_edge
	xorl	%edi, %edi
	jmp	.LBB59_22	# bb51.preheader
	.size	cblas_dsyr2, .-cblas_dsyr2
.Leh_func_end41:


	.section	.rodata.cst8,"aM",@progbits,8
	.align	8
.LCPI60_0:					
	.quad	4607182418800017408	# double value: 1.000000e+00
	.text
	.align	16
	.globl	cblas_dsyr2k
	.type	cblas_dsyr2k,@function
cblas_dsyr2k:
.Leh_func_begin42:
.Llabel42:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$8, %rsp
	ucomisd	.LCPI60_0(%rip), %xmm1
	movq	88(%rsp), %rax
	movq	72(%rsp), %r10
	jne	.LBB60_2	# bb4
	jp	.LBB60_2	# bb4
.LBB60_1:	# entry
	pxor	%xmm2, %xmm2
	ucomisd	%xmm2, %xmm0
	setnp	%r11b
	sete	%bl
	testb	%r11b, %bl
	jne	.LBB60_81	# return
.LBB60_2:	# bb4
	cmpl	$101, %edi
	je	.LBB60_82	# bb5
.LBB60_3:	# bb9
	cmpl	$121, %esi
	movl	$122, %edi
	movl	$121, %esi
	cmove	%edi, %esi
	addl	$4294967184, %edx
	cmpl	$1, %edx
	jbe	.LBB60_83	# bb9.bb15_crit_edge
.LBB60_4:	# bb14
	movl	$112, 4(%rsp)
.LBB60_5:	# bb15
	pxor	%xmm2, %xmm2
	ucomisd	%xmm2, %xmm1
	jne	.LBB60_19	# bb29
	jp	.LBB60_19	# bb29
.LBB60_6:	# bb16
	cmpl	$121, %esi
	je	.LBB60_9	# bb22.preheader
.LBB60_7:	# bb28.preheader
	testl	%ecx, %ecx
	jle	.LBB60_33	# bb43
.LBB60_8:	# bb28.preheader.bb26.preheader_crit_edge
	xorl	%edx, %edx
	movl	%edx, %edi
	jmp	.LBB60_17	# bb26.preheader
.LBB60_9:	# bb22.preheader
	testl	%ecx, %ecx
	jle	.LBB60_33	# bb43
.LBB60_10:	# bb20.preheader.preheader
	movl	96(%rsp), %edx
	leal	1(%rdx), %edx
	xorl	%edi, %edi
	movl	%ecx, %r11d
	movl	%edi, %ebx
	jmp	.LBB60_13	# bb20.preheader
	.align	16
.LBB60_11:	# bb19
	leal	(%rdi,%r14), %r15d
	movslq	%r15d, %r15
	movq	$0, (%rax,%r15,8)
	incl	%r14d
	cmpl	%r11d, %r14d
	jne	.LBB60_11	# bb19
.LBB60_12:	# bb21
	addl	%edx, %edi
	decl	%r11d
	incl	%ebx
	cmpl	%ecx, %ebx
	je	.LBB60_33	# bb43
.LBB60_13:	# bb20.preheader
	cmpl	%ecx, %ebx
	jge	.LBB60_12	# bb21
.LBB60_14:	# bb20.preheader.bb19_crit_edge
	xorl	%r14d, %r14d
	jmp	.LBB60_11	# bb19
	.align	16
.LBB60_15:	# bb25
	leal	(%rdx,%r11), %ebx
	movslq	%ebx, %rbx
	movq	$0, (%rax,%rbx,8)
	incl	%r11d
	cmpl	%edi, %r11d
	jle	.LBB60_15	# bb25
.LBB60_16:	# bb27
	addl	96(%rsp), %edx
	incl	%edi
	cmpl	%ecx, %edi
	je	.LBB60_33	# bb43
.LBB60_17:	# bb26.preheader
	testl	%edi, %edi
	js	.LBB60_16	# bb27
.LBB60_18:	# bb26.preheader.bb25_crit_edge
	xorl	%r11d, %r11d
	jmp	.LBB60_15	# bb25
.LBB60_19:	# bb29
	ucomisd	.LCPI60_0(%rip), %xmm1
	setnp	%dl
	sete	%dil
	testb	%dl, %dil
	jne	.LBB60_33	# bb43
.LBB60_20:	# bb30
	cmpl	$121, %esi
	je	.LBB60_23	# bb36.preheader
.LBB60_21:	# bb42.preheader
	testl	%ecx, %ecx
	jle	.LBB60_33	# bb43
.LBB60_22:	# bb42.preheader.bb40.preheader_crit_edge
	xorl	%edx, %edx
	movl	%edx, %edi
	jmp	.LBB60_31	# bb40.preheader
.LBB60_23:	# bb36.preheader
	testl	%ecx, %ecx
	jle	.LBB60_33	# bb43
.LBB60_24:	# bb34.preheader.preheader
	movl	96(%rsp), %edx
	leal	1(%rdx), %edx
	xorl	%edi, %edi
	movl	%ecx, %r11d
	movl	%edi, %ebx
	jmp	.LBB60_27	# bb34.preheader
	.align	16
.LBB60_25:	# bb33
	leal	(%rdi,%r14), %r15d
	movslq	%r15d, %r15
	movapd	%xmm1, %xmm2
	mulsd	(%rax,%r15,8), %xmm2
	movsd	%xmm2, (%rax,%r15,8)
	incl	%r14d
	cmpl	%r11d, %r14d
	jne	.LBB60_25	# bb33
.LBB60_26:	# bb35
	addl	%edx, %edi
	decl	%r11d
	incl	%ebx
	cmpl	%ecx, %ebx
	je	.LBB60_33	# bb43
.LBB60_27:	# bb34.preheader
	cmpl	%ecx, %ebx
	jge	.LBB60_26	# bb35
.LBB60_28:	# bb34.preheader.bb33_crit_edge
	xorl	%r14d, %r14d
	jmp	.LBB60_25	# bb33
	.align	16
.LBB60_29:	# bb39
	leal	(%rdx,%r11), %ebx
	movslq	%ebx, %rbx
	movapd	%xmm1, %xmm2
	mulsd	(%rax,%rbx,8), %xmm2
	movsd	%xmm2, (%rax,%rbx,8)
	incl	%r11d
	cmpl	%edi, %r11d
	jle	.LBB60_29	# bb39
.LBB60_30:	# bb41
	addl	96(%rsp), %edx
	incl	%edi
	cmpl	%ecx, %edi
	je	.LBB60_33	# bb43
.LBB60_31:	# bb40.preheader
	testl	%edi, %edi
	js	.LBB60_30	# bb41
.LBB60_32:	# bb40.preheader.bb39_crit_edge
	xorl	%r11d, %r11d
	jmp	.LBB60_29	# bb39
.LBB60_33:	# bb43
	pxor	%xmm1, %xmm1
	ucomisd	%xmm1, %xmm0
	setnp	%dl
	sete	%dil
	testb	%dl, %dil
	jne	.LBB60_81	# return
.LBB60_34:	# bb44
	cmpl	$121, %esi
	jne	.LBB60_46	# bb56
.LBB60_35:	# bb44
	cmpl	$111, 4(%rsp)
	jne	.LBB60_46	# bb56
.LBB60_36:	# bb55.preheader
	testl	%ecx, %ecx
	jle	.LBB60_81	# return
.LBB60_37:	# bb.nph141
	movl	96(%rsp), %edx
	incl	%edx
	movl	%edx, (%rsp)
	xorl	%edi, %edi
	movl	%edi, %r15d
	movl	%edi, %edx
	movl	%ecx, %ebx
	movl	%edi, 4(%rsp)
	jmp	.LBB60_44	# bb53.preheader
.LBB60_38:	# bb51.preheader.bb50_crit_edge
	pxor	%xmm1, %xmm1
	xorl	%r14d, %r14d
	.align	16
.LBB60_39:	# bb50
	leal	(%r12,%r14), %r13d
	movslq	%r13d, %r13
	leal	(%rdx,%r14), %ebp
	movslq	%ebp, %rbp
	movsd	(%r10,%rbp,8), %xmm2
	mulsd	(%r9,%r13,8), %xmm2
	leal	(%rsi,%r14), %r13d
	movslq	%r13d, %r13
	leal	(%r15,%r14), %ebp
	movslq	%ebp, %rbp
	movsd	(%r9,%rbp,8), %xmm3
	mulsd	(%r10,%r13,8), %xmm3
	addsd	%xmm2, %xmm3
	addsd	%xmm3, %xmm1
	incl	%r14d
	cmpl	%r8d, %r14d
	jne	.LBB60_39	# bb50
.LBB60_40:	# bb52
	leal	(%rdi,%r11), %r14d
	movslq	%r14d, %r14
	mulsd	%xmm0, %xmm1
	addsd	(%rax,%r14,8), %xmm1
	movsd	%xmm1, (%rax,%r14,8)
	addl	80(%rsp), %esi
	addl	64(%rsp), %r12d
	incl	%r11d
	cmpl	%ebx, %r11d
	je	.LBB60_43	# bb54
.LBB60_41:	# bb51.preheader
	testl	%r8d, %r8d
	jg	.LBB60_38	# bb51.preheader.bb50_crit_edge
.LBB60_42:	# bb51.preheader.bb52_crit_edge
	pxor	%xmm1, %xmm1
	jmp	.LBB60_40	# bb52
.LBB60_43:	# bb54
	addl	(%rsp), %edi
	addl	64(%rsp), %r15d
	addl	80(%rsp), %edx
	decl	%ebx
	movl	4(%rsp), %esi
	incl	%esi
	movl	%esi, 4(%rsp)
	cmpl	%ecx, %esi
	je	.LBB60_81	# return
.LBB60_44:	# bb53.preheader
	cmpl	%ecx, 4(%rsp)
	jge	.LBB60_43	# bb54
.LBB60_45:	# bb53.preheader.bb51.preheader_crit_edge
	xorl	%r11d, %r11d
	movl	%edx, %esi
	movl	%r15d, %r12d
	jmp	.LBB60_41	# bb51.preheader
.LBB60_46:	# bb56
	cmpl	$121, %esi
	jne	.LBB60_57	# bb69
.LBB60_47:	# bb56
	cmpl	$112, 4(%rsp)
	jne	.LBB60_57	# bb69
.LBB60_48:	# bb68.preheader
	testl	%r8d, %r8d
	jle	.LBB60_81	# return
.LBB60_49:	# bb68.preheader
	testl	%ecx, %ecx
	jle	.LBB60_81	# return
.LBB60_50:	# bb68.preheader.bb66.preheader_crit_edge
	xorl	%edx, %edx
	movl	%edx, %esi
	movl	%edx, 4(%rsp)
	jmp	.LBB60_56	# bb66.preheader
	.align	16
.LBB60_51:	# bb62
	leal	(%rsi,%r14), %r15d
	leal	(%rdx,%r14), %r12d
	cmpl	%ecx, %r14d
	movslq	%r12d, %r12
	movapd	%xmm0, %xmm1
	mulsd	(%r10,%r12,8), %xmm1
	movslq	%r15d, %r15
	movapd	%xmm0, %xmm2
	mulsd	(%r9,%r15,8), %xmm2
	jge	.LBB60_54	# bb65
.LBB60_52:	# bb.nph129
	leal	(%rsi,%r14), %r15d
	leal	(%rdx,%r14), %r12d
	xorl	%r13d, %r13d
	.align	16
.LBB60_53:	# bb63
	leal	(%r15,%r13), %ebp
	movslq	%ebp, %rbp
	movapd	%xmm1, %xmm3
	mulsd	(%r9,%rbp,8), %xmm3
	leal	(%r12,%r13), %ebp
	movslq	%ebp, %rbp
	movapd	%xmm2, %xmm4
	mulsd	(%r10,%rbp,8), %xmm4
	addsd	%xmm3, %xmm4
	leal	(%r11,%r13), %ebp
	movslq	%ebp, %rbp
	addsd	(%rax,%rbp,8), %xmm4
	movsd	%xmm4, (%rax,%rbp,8)
	incl	%r13d
	cmpl	%ebx, %r13d
	jne	.LBB60_53	# bb63
.LBB60_54:	# bb65
	addl	%edi, %r11d
	decl	%ebx
	incl	%r14d
	cmpl	%ecx, %r14d
	jne	.LBB60_51	# bb62
.LBB60_55:	# bb67
	addl	80(%rsp), %edx
	addl	64(%rsp), %esi
	movl	4(%rsp), %edi
	incl	%edi
	movl	%edi, 4(%rsp)
	cmpl	%r8d, %edi
	je	.LBB60_81	# return
.LBB60_56:	# bb66.preheader
	movl	64(%rsp), %edi
	leal	1(%rdi), %edi
	xorl	%r11d, %r11d
	movl	%ecx, %ebx
	movl	%r11d, %r14d
	jmp	.LBB60_51	# bb62
.LBB60_57:	# bb69
	cmpl	$122, %esi
	jne	.LBB60_69	# bb82
.LBB60_58:	# bb69
	cmpl	$111, 4(%rsp)
	jne	.LBB60_69	# bb82
.LBB60_59:	# bb81.preheader
	testl	%ecx, %ecx
	jle	.LBB60_81	# return
.LBB60_60:	# bb.nph127
	xorl	%r15d, %r15d
	movl	%r15d, %edi
	movl	%r15d, %edx
	movl	%r15d, %esi
	jmp	.LBB60_67	# bb79.preheader
.LBB60_61:	# bb77.preheader.bb76_crit_edge
	pxor	%xmm1, %xmm1
	xorl	%r14d, %r14d
	.align	16
.LBB60_62:	# bb76
	leal	(%r12,%r14), %r13d
	movslq	%r13d, %r13
	leal	(%rdi,%r14), %ebp
	movslq	%ebp, %rbp
	movsd	(%r10,%rbp,8), %xmm2
	mulsd	(%r9,%r13,8), %xmm2
	leal	(%r11,%r14), %r13d
	movslq	%r13d, %r13
	leal	(%r15,%r14), %ebp
	movslq	%ebp, %rbp
	movsd	(%r9,%rbp,8), %xmm3
	mulsd	(%r10,%r13,8), %xmm3
	addsd	%xmm2, %xmm3
	addsd	%xmm3, %xmm1
	incl	%r14d
	cmpl	%r8d, %r14d
	jne	.LBB60_62	# bb76
.LBB60_63:	# bb78
	leal	(%rdx,%rbx), %r14d
	movslq	%r14d, %r14
	mulsd	%xmm0, %xmm1
	addsd	(%rax,%r14,8), %xmm1
	movsd	%xmm1, (%rax,%r14,8)
	addl	80(%rsp), %r11d
	addl	64(%rsp), %r12d
	incl	%ebx
	cmpl	%esi, %ebx
	jg	.LBB60_66	# bb80
.LBB60_64:	# bb77.preheader
	testl	%r8d, %r8d
	jg	.LBB60_61	# bb77.preheader.bb76_crit_edge
.LBB60_65:	# bb77.preheader.bb78_crit_edge
	pxor	%xmm1, %xmm1
	jmp	.LBB60_63	# bb78
.LBB60_66:	# bb80
	addl	64(%rsp), %r15d
	addl	80(%rsp), %edi
	addl	96(%rsp), %edx
	incl	%esi
	cmpl	%ecx, %esi
	je	.LBB60_81	# return
.LBB60_67:	# bb79.preheader
	testl	%esi, %esi
	js	.LBB60_66	# bb80
.LBB60_68:	# bb79.preheader.bb77.preheader_crit_edge
	xorl	%r11d, %r11d
	movl	%r11d, %r12d
	movl	%r11d, %ebx
	jmp	.LBB60_64	# bb77.preheader
.LBB60_69:	# bb82
	cmpl	$122, %esi
	jne	.LBB60_80	# bb95
.LBB60_70:	# bb82
	cmpl	$112, 4(%rsp)
	jne	.LBB60_80	# bb95
.LBB60_71:	# bb94.preheader
	testl	%r8d, %r8d
	jle	.LBB60_81	# return
.LBB60_72:	# bb94.preheader
	testl	%ecx, %ecx
	jle	.LBB60_81	# return
.LBB60_73:	# bb94.preheader.bb92.preheader_crit_edge
	xorl	%edx, %edx
	movl	%edx, %esi
	movl	%edx, %edi
	jmp	.LBB60_79	# bb92.preheader
	.align	16
.LBB60_74:	# bb88
	leal	(%rsi,%r11), %r14d
	movslq	%r14d, %r14
	movapd	%xmm0, %xmm1
	mulsd	(%r10,%r14,8), %xmm1
	leal	(%rdx,%r11), %r14d
	movslq	%r14d, %r14
	movapd	%xmm0, %xmm2
	mulsd	(%r9,%r14,8), %xmm2
	testl	%r11d, %r11d
	js	.LBB60_77	# bb91
.LBB60_75:	# bb88.bb89_crit_edge
	xorl	%r14d, %r14d
	.align	16
.LBB60_76:	# bb89
	leal	(%rdx,%r14), %r15d
	movslq	%r15d, %r15
	movapd	%xmm1, %xmm3
	mulsd	(%r9,%r15,8), %xmm3
	leal	(%rsi,%r14), %r15d
	movslq	%r15d, %r15
	movapd	%xmm2, %xmm4
	mulsd	(%r10,%r15,8), %xmm4
	addsd	%xmm3, %xmm4
	leal	(%rbx,%r14), %r15d
	movslq	%r15d, %r15
	addsd	(%rax,%r15,8), %xmm4
	movsd	%xmm4, (%rax,%r15,8)
	incl	%r14d
	cmpl	%r11d, %r14d
	jle	.LBB60_76	# bb89
.LBB60_77:	# bb91
	addl	64(%rsp), %ebx
	incl	%r11d
	cmpl	%ecx, %r11d
	jne	.LBB60_74	# bb88
.LBB60_78:	# bb93
	addl	64(%rsp), %edx
	addl	80(%rsp), %esi
	incl	%edi
	cmpl	%r8d, %edi
	je	.LBB60_81	# return
.LBB60_79:	# bb92.preheader
	xorl	%ebx, %ebx
	movl	%ebx, %r11d
	jmp	.LBB60_74	# bb88
.LBB60_80:	# bb95
	xorl	%edi, %edi
	leaq	.str77, %rsi
	leaq	.str178, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB60_81:	# return
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB60_82:	# bb5
	cmpl	$113, %edx
	movl	$112, %edi
	cmovne	%edx, %edi
	movl	%edi, 4(%rsp)
	jmp	.LBB60_5	# bb15
.LBB60_83:	# bb9.bb15_crit_edge
	movl	$111, 4(%rsp)
	jmp	.LBB60_5	# bb15
	.size	cblas_dsyr2k, .-cblas_dsyr2k
.Leh_func_end42:


	.align	16
	.globl	cblas_dsyr
	.type	cblas_dsyr,@function
cblas_dsyr:
.Leh_func_begin43:
.Llabel43:
	pushq	%r15
	pushq	%r14
	pushq	%rbx
	pxor	%xmm1, %xmm1
	ucomisd	%xmm1, %xmm0
	setnp	%al
	sete	%r10b
	testb	%al, %r10b
	movl	32(%rsp), %eax
	jne	.LBB61_25	# return
.LBB61_1:	# entry
	testl	%edx, %edx
	je	.LBB61_25	# return
.LBB61_2:	# bb4
	cmpl	$121, %esi
	jne	.LBB61_4	# bb7
.LBB61_3:	# bb4
	cmpl	$101, %edi
	je	.LBB61_6	# bb11
.LBB61_4:	# bb7
	cmpl	$122, %esi
	jne	.LBB61_14	# bb20
.LBB61_5:	# bb7
	cmpl	$102, %edi
	jne	.LBB61_14	# bb20
.LBB61_6:	# bb11
	testl	%r8d, %r8d
	jg	.LBB61_26	# bb11.bb19.preheader_crit_edge
.LBB61_7:	# bb12
	movl	$1, %esi
	subl	%edx, %esi
	imull	%r8d, %esi
.LBB61_8:	# bb19.preheader
	testl	%edx, %edx
	jle	.LBB61_25	# return
.LBB61_9:	# bb15.preheader
	incl	%eax
	xorl	%edi, %edi
	movl	%edx, %r10d
	movl	%edi, %r11d
	.align	16
.LBB61_10:	# bb15
	movslq	%esi, %rbx
	movapd	%xmm0, %xmm1
	mulsd	(%rcx,%rbx,8), %xmm1
	cmpl	%edx, %r11d
	jge	.LBB61_13	# bb18
.LBB61_11:	# bb15.bb16_crit_edge
	xorl	%ebx, %ebx
	movl	%esi, %r14d
	.align	16
.LBB61_12:	# bb16
	movslq	%r14d, %r15
	movapd	%xmm1, %xmm2
	mulsd	(%rcx,%r15,8), %xmm2
	leal	(%rdi,%rbx), %r15d
	movslq	%r15d, %r15
	addsd	(%r9,%r15,8), %xmm2
	movsd	%xmm2, (%r9,%r15,8)
	addl	%r8d, %r14d
	incl	%ebx
	cmpl	%r10d, %ebx
	jne	.LBB61_12	# bb16
.LBB61_13:	# bb18
	addl	%eax, %edi
	addl	%r8d, %esi
	decl	%r10d
	incl	%r11d
	cmpl	%edx, %r11d
	jne	.LBB61_10	# bb15
	jmp	.LBB61_25	# return
.LBB61_14:	# bb20
	cmpl	$102, %edi
	sete	%r10b
	cmpl	$121, %esi
	sete	%r11b
	andb	%r10b, %r11b
	cmpl	$101, %edi
	sete	%dil
	cmpl	$122, %esi
	sete	%sil
	testb	%dil, %sil
	jne	.LBB61_16	# bb28
.LBB61_15:	# bb20
	notb	%r11b
	testb	$1, %r11b
	jne	.LBB61_24	# bb40
.LBB61_16:	# bb28
	testl	%r8d, %r8d
	jg	.LBB61_27	# bb28.bb39.preheader_crit_edge
.LBB61_17:	# bb29
	movl	$1, %esi
	subl	%edx, %esi
	imull	%r8d, %esi
.LBB61_18:	# bb39.preheader
	testl	%edx, %edx
	jle	.LBB61_25	# return
.LBB61_19:	# bb.nph47
	movl	$1, %edi
	subl	%edx, %edi
	imull	%r8d, %edi
	xorl	%r10d, %r10d
	movl	%r10d, %r11d
	.align	16
.LBB61_20:	# bb32
	testl	%r8d, %r8d
	movl	$0, %ebx
	cmovle	%edi, %ebx
	movslq	%esi, %r14
	movapd	%xmm0, %xmm1
	mulsd	(%rcx,%r14,8), %xmm1
	testl	%r11d, %r11d
	js	.LBB61_23	# bb38
.LBB61_21:	# bb32.bb36_crit_edge
	xorl	%r14d, %r14d
	.align	16
.LBB61_22:	# bb36
	movslq	%ebx, %r15
	movapd	%xmm1, %xmm2
	mulsd	(%rcx,%r15,8), %xmm2
	leal	(%r10,%r14), %r15d
	movslq	%r15d, %r15
	addsd	(%r9,%r15,8), %xmm2
	movsd	%xmm2, (%r9,%r15,8)
	addl	%r8d, %ebx
	incl	%r14d
	cmpl	%r11d, %r14d
	jle	.LBB61_22	# bb36
.LBB61_23:	# bb38
	addl	%r8d, %esi
	addl	%eax, %r10d
	incl	%r11d
	cmpl	%edx, %r11d
	jne	.LBB61_20	# bb32
	jmp	.LBB61_25	# return
.LBB61_24:	# bb40
	xorl	%edi, %edi
	leaq	.str79, %rsi
	leaq	.str180, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB61_25:	# return
	popq	%rbx
	popq	%r14
	popq	%r15
	ret
.LBB61_26:	# bb11.bb19.preheader_crit_edge
	xorl	%esi, %esi
	jmp	.LBB61_8	# bb19.preheader
.LBB61_27:	# bb28.bb39.preheader_crit_edge
	xorl	%esi, %esi
	jmp	.LBB61_18	# bb39.preheader
	.size	cblas_dsyr, .-cblas_dsyr
.Leh_func_end43:


	.section	.rodata.cst8,"aM",@progbits,8
	.align	8
.LCPI62_0:					
	.quad	4607182418800017408	# double value: 1.000000e+00
	.text
	.align	16
	.globl	cblas_dsyrk
	.type	cblas_dsyrk,@function
cblas_dsyrk:
.Leh_func_begin44:
.Llabel44:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$24, %rsp
	ucomisd	.LCPI62_0(%rip), %xmm1
	movl	96(%rsp), %ebx
	movq	88(%rsp), %r14
	movl	80(%rsp), %r15d
	movsd	%xmm1, 8(%rsp)
	movq	%r9, %r12
	movsd	%xmm0, 16(%rsp)
	movl	%r8d, %r13d
	movl	%ecx, %ebp
	jne	.LBB62_2	# bb4
	jp	.LBB62_2	# bb4
.LBB62_1:	# entry
	pxor	%xmm0, %xmm0
	movsd	16(%rsp), %xmm1
	ucomisd	%xmm0, %xmm1
	setnp	%al
	sete	%cl
	testb	%al, %cl
	jne	.LBB62_108	# return
.LBB62_2:	# bb4
	cmpl	$101, %edi
	je	.LBB62_109	# bb5
.LBB62_3:	# bb9
	cmpl	$121, %esi
	movl	$122, %eax
	movl	$121, %esi
	cmove	%eax, %esi
	addl	$4294967184, %edx
	cmpl	$1, %edx
	jbe	.LBB62_110	# bb9.bb15_crit_edge
.LBB62_4:	# bb14
	movl	$112, (%rsp)
.LBB62_5:	# bb15
	movl	%esi, 4(%rsp)
	pxor	%xmm0, %xmm0
	movsd	8(%rsp), %xmm1
	ucomisd	%xmm0, %xmm1
	jne	.LBB62_23	# bb29
	jp	.LBB62_23	# bb29
.LBB62_6:	# bb16
	xorl	%edi, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB62_21	# real_catch0
.LBB62_7:	# jump0
	xorl	%edi, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB62_22	# real_end0
.LBB62_8:	# real_try0
	xorl	%edi, %edi
	call	llvm_real_try
	cmpl	$121, 4(%rsp)
	je	.LBB62_11	# bb22.preheader
.LBB62_9:	# bb28.preheader
	testl	%ebp, %ebp
	jle	.LBB62_22	# real_end0
.LBB62_10:	# bb28.preheader.bb26.preheader_crit_edge
	xorl	%eax, %eax
	movl	%eax, %ecx
	jmp	.LBB62_19	# bb26.preheader
.LBB62_11:	# bb22.preheader
	testl	%ebp, %ebp
	jle	.LBB62_22	# real_end0
.LBB62_12:	# bb20.preheader.preheader
	leal	1(%rbx), %eax
	xorl	%ecx, %ecx
	movl	%ebp, %edx
	movl	%ecx, %esi
	jmp	.LBB62_15	# bb20.preheader
	.align	16
.LBB62_13:	# bb19
	leal	(%rcx,%rdi), %r8d
	movslq	%r8d, %r8
	movq	$0, (%r14,%r8,8)
	incl	%edi
	cmpl	%edx, %edi
	jne	.LBB62_13	# bb19
.LBB62_14:	# bb21
	addl	%eax, %ecx
	decl	%edx
	incl	%esi
	cmpl	%ebp, %esi
	je	.LBB62_22	# real_end0
.LBB62_15:	# bb20.preheader
	cmpl	%ebp, %esi
	jge	.LBB62_14	# bb21
.LBB62_16:	# bb20.preheader.bb19_crit_edge
	xorl	%edi, %edi
	jmp	.LBB62_13	# bb19
	.align	16
.LBB62_17:	# bb25
	leal	(%rax,%rdx), %esi
	movslq	%esi, %rsi
	movq	$0, (%r14,%rsi,8)
	incl	%edx
	cmpl	%ecx, %edx
	jle	.LBB62_17	# bb25
.LBB62_18:	# bb27
	addl	%ebx, %eax
	incl	%ecx
	cmpl	%ebp, %ecx
	je	.LBB62_22	# real_end0
.LBB62_19:	# bb26.preheader
	testl	%ecx, %ecx
	js	.LBB62_18	# bb27
.LBB62_20:	# bb26.preheader.bb25_crit_edge
	xorl	%edx, %edx
	jmp	.LBB62_17	# bb25
.LBB62_21:	# real_catch0
	xorl	%edi, %edi
	call	llvm_real_catch
.LBB62_22:	# real_end0
	xorl	%edi, %edi
	call	llvm_real_end
	jmp	.LBB62_41	# bb43
.LBB62_23:	# bb29
	movsd	8(%rsp), %xmm0
	ucomisd	.LCPI62_0(%rip), %xmm0
	setnp	%al
	sete	%cl
	testb	%al, %cl
	jne	.LBB62_41	# bb43
.LBB62_24:	# bb30
	movl	$1, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB62_39	# real_catch1
.LBB62_25:	# jump1
	movl	$1, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB62_40	# real_end1
.LBB62_26:	# real_try1
	movl	$1, %edi
	call	llvm_real_try
	cmpl	$121, 4(%rsp)
	je	.LBB62_29	# bb36.preheader
.LBB62_27:	# bb42.preheader
	testl	%ebp, %ebp
	jle	.LBB62_40	# real_end1
.LBB62_28:	# bb42.preheader.bb40.preheader_crit_edge
	xorl	%eax, %eax
	movl	%eax, %ecx
	jmp	.LBB62_37	# bb40.preheader
.LBB62_29:	# bb36.preheader
	testl	%ebp, %ebp
	jle	.LBB62_40	# real_end1
.LBB62_30:	# bb34.preheader.preheader
	leal	1(%rbx), %eax
	xorl	%ecx, %ecx
	movl	%ebp, %edx
	movl	%ecx, %esi
	jmp	.LBB62_33	# bb34.preheader
	.align	16
.LBB62_31:	# bb33
	leal	(%rcx,%rdi), %r8d
	movslq	%r8d, %r8
	movsd	8(%rsp), %xmm0
	mulsd	(%r14,%r8,8), %xmm0
	movsd	%xmm0, (%r14,%r8,8)
	incl	%edi
	cmpl	%edx, %edi
	jne	.LBB62_31	# bb33
.LBB62_32:	# bb35
	addl	%eax, %ecx
	decl	%edx
	incl	%esi
	cmpl	%ebp, %esi
	je	.LBB62_40	# real_end1
.LBB62_33:	# bb34.preheader
	cmpl	%ebp, %esi
	jge	.LBB62_32	# bb35
.LBB62_34:	# bb34.preheader.bb33_crit_edge
	xorl	%edi, %edi
	jmp	.LBB62_31	# bb33
	.align	16
.LBB62_35:	# bb39
	leal	(%rax,%rdx), %esi
	movslq	%esi, %rsi
	movsd	8(%rsp), %xmm0
	mulsd	(%r14,%rsi,8), %xmm0
	movsd	%xmm0, (%r14,%rsi,8)
	incl	%edx
	cmpl	%ecx, %edx
	jle	.LBB62_35	# bb39
.LBB62_36:	# bb41
	addl	%ebx, %eax
	incl	%ecx
	cmpl	%ebp, %ecx
	je	.LBB62_40	# real_end1
.LBB62_37:	# bb40.preheader
	testl	%ecx, %ecx
	js	.LBB62_36	# bb41
.LBB62_38:	# bb40.preheader.bb39_crit_edge
	xorl	%edx, %edx
	jmp	.LBB62_35	# bb39
.LBB62_39:	# real_catch1
	movl	$1, %edi
	call	llvm_real_catch
.LBB62_40:	# real_end1
	movl	$1, %edi
	call	llvm_real_end
.LBB62_41:	# bb43
	pxor	%xmm0, %xmm0
	movsd	16(%rsp), %xmm1
	ucomisd	%xmm0, %xmm1
	setnp	%al
	sete	%cl
	testb	%al, %cl
	jne	.LBB62_108	# return
.LBB62_42:	# bb44
	cmpl	$121, 4(%rsp)
	jne	.LBB62_59	# bb56
.LBB62_43:	# bb44
	cmpl	$111, (%rsp)
	jne	.LBB62_59	# bb56
.LBB62_44:	# bb47
	movl	$2, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB62_56	# real_catch2
.LBB62_45:	# jump2
	movl	$2, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB62_57	# real_end2
.LBB62_46:	# real_try2
	movl	$2, %edi
	call	llvm_real_try
	testl	%ebp, %ebp
	jle	.LBB62_57	# real_end2
.LBB62_47:	# bb.nph145
	incl	%ebx
	xorl	%edx, %edx
	movl	%edx, %ecx
	movl	%ebp, %eax
	movl	%edx, %r9d
	jmp	.LBB62_54	# bb53.preheader
.LBB62_48:	# bb51.preheader.bb50_crit_edge
	pxor	%xmm0, %xmm0
	xorl	%r8d, %r8d
	.align	16
.LBB62_49:	# bb50
	leal	(%rdi,%r8), %r10d
	movslq	%r10d, %r10
	leal	(%rcx,%r8), %r11d
	movslq	%r11d, %r11
	movsd	(%r12,%r11,8), %xmm1
	mulsd	(%r12,%r10,8), %xmm1
	addsd	%xmm1, %xmm0
	incl	%r8d
	cmpl	%r13d, %r8d
	jne	.LBB62_49	# bb50
.LBB62_50:	# bb52
	leal	(%rdx,%rsi), %r8d
	movslq	%r8d, %r8
	mulsd	16(%rsp), %xmm0
	addsd	(%r14,%r8,8), %xmm0
	movsd	%xmm0, (%r14,%r8,8)
	addl	%r15d, %edi
	incl	%esi
	cmpl	%eax, %esi
	je	.LBB62_53	# bb54
.LBB62_51:	# bb51.preheader
	testl	%r13d, %r13d
	jg	.LBB62_48	# bb51.preheader.bb50_crit_edge
.LBB62_52:	# bb51.preheader.bb52_crit_edge
	pxor	%xmm0, %xmm0
	jmp	.LBB62_50	# bb52
.LBB62_53:	# bb54
	addl	%ebx, %edx
	addl	%r15d, %ecx
	decl	%eax
	incl	%r9d
	cmpl	%ebp, %r9d
	je	.LBB62_57	# real_end2
.LBB62_54:	# bb53.preheader
	cmpl	%ebp, %r9d
	jge	.LBB62_53	# bb54
.LBB62_55:	# bb53.preheader.bb51.preheader_crit_edge
	xorl	%esi, %esi
	movl	%ecx, %edi
	jmp	.LBB62_51	# bb51.preheader
.LBB62_56:	# real_catch2
	movl	$2, %edi
	call	llvm_real_catch
.LBB62_57:	# real_end2
	movl	$2, %edi
.LBB62_58:	# real_end2
	call	llvm_real_end
	jmp	.LBB62_108	# return
.LBB62_59:	# bb56
	cmpl	$121, 4(%rsp)
	jne	.LBB62_75	# bb69
.LBB62_60:	# bb56
	cmpl	$112, (%rsp)
	jne	.LBB62_75	# bb69
.LBB62_61:	# bb60
	movl	$3, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB62_73	# real_catch3
.LBB62_62:	# jump3
	movl	$3, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB62_74	# real_end3
.LBB62_63:	# real_try3
	movl	$3, %edi
	call	llvm_real_try
	testl	%ebp, %ebp
	jle	.LBB62_74	# real_end3
.LBB62_64:	# bb.nph137
	incl	%ebx
	xorl	%edx, %edx
	movl	%ebp, %eax
	movl	%edx, %ecx
	jmp	.LBB62_71	# bb66.preheader
.LBB62_65:	# bb64.preheader.bb63_crit_edge
	pxor	%xmm0, %xmm0
	xorl	%edi, %edi
	movl	%ecx, %r8d
	.align	16
.LBB62_66:	# bb63
	leal	(%rsi,%r8), %r9d
	movslq	%r9d, %r9
	movslq	%r8d, %r10
	movsd	(%r12,%r10,8), %xmm1
	mulsd	(%r12,%r9,8), %xmm1
	addsd	%xmm1, %xmm0
	addl	%r15d, %r8d
	incl	%edi
	cmpl	%r13d, %edi
	jne	.LBB62_66	# bb63
.LBB62_67:	# bb65
	leal	(%rdx,%rsi), %edi
	movslq	%edi, %rdi
	mulsd	16(%rsp), %xmm0
	addsd	(%r14,%rdi,8), %xmm0
	movsd	%xmm0, (%r14,%rdi,8)
	incl	%esi
	cmpl	%eax, %esi
	je	.LBB62_70	# bb67
.LBB62_68:	# bb64.preheader
	testl	%r13d, %r13d
	jg	.LBB62_65	# bb64.preheader.bb63_crit_edge
.LBB62_69:	# bb64.preheader.bb65_crit_edge
	pxor	%xmm0, %xmm0
	jmp	.LBB62_67	# bb65
.LBB62_70:	# bb67
	addl	%ebx, %edx
	decl	%eax
	incl	%ecx
	cmpl	%ebp, %ecx
	je	.LBB62_74	# real_end3
.LBB62_71:	# bb66.preheader
	cmpl	%ebp, %ecx
	jge	.LBB62_70	# bb67
.LBB62_72:	# bb66.preheader.bb64.preheader_crit_edge
	xorl	%esi, %esi
	jmp	.LBB62_68	# bb64.preheader
.LBB62_73:	# real_catch3
	movl	$3, %edi
	call	llvm_real_catch
.LBB62_74:	# real_end3
	movl	$3, %edi
	jmp	.LBB62_58	# real_end2
.LBB62_75:	# bb69
	cmpl	$122, 4(%rsp)
	jne	.LBB62_91	# bb82
.LBB62_76:	# bb69
	cmpl	$111, (%rsp)
	jne	.LBB62_91	# bb82
.LBB62_77:	# bb73
	movl	$4, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB62_89	# real_catch4
.LBB62_78:	# jump4
	movl	$4, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB62_90	# real_end4
.LBB62_79:	# real_try4
	movl	$4, %edi
	call	llvm_real_try
	testl	%ebp, %ebp
	jle	.LBB62_90	# real_end4
.LBB62_80:	# bb.nph129
	xorl	%edi, %edi
	movl	%edi, %eax
	movl	%edi, %r8d
	jmp	.LBB62_87	# bb79.preheader
.LBB62_81:	# bb77.preheader.bb76_crit_edge
	pxor	%xmm0, %xmm0
	xorl	%edx, %edx
	.align	16
.LBB62_82:	# bb76
	leal	(%rcx,%rdx), %r9d
	movslq	%r9d, %r9
	leal	(%rdi,%rdx), %r10d
	movslq	%r10d, %r10
	movsd	(%r12,%r10,8), %xmm1
	mulsd	(%r12,%r9,8), %xmm1
	addsd	%xmm1, %xmm0
	incl	%edx
	cmpl	%r13d, %edx
	jne	.LBB62_82	# bb76
.LBB62_83:	# bb78
	leal	(%rax,%rsi), %edx
	movslq	%edx, %rdx
	mulsd	16(%rsp), %xmm0
	addsd	(%r14,%rdx,8), %xmm0
	movsd	%xmm0, (%r14,%rdx,8)
	addl	%r15d, %ecx
	incl	%esi
	cmpl	%r8d, %esi
	jg	.LBB62_86	# bb80
.LBB62_84:	# bb77.preheader
	testl	%r13d, %r13d
	jg	.LBB62_81	# bb77.preheader.bb76_crit_edge
.LBB62_85:	# bb77.preheader.bb78_crit_edge
	pxor	%xmm0, %xmm0
	jmp	.LBB62_83	# bb78
.LBB62_86:	# bb80
	addl	%r15d, %edi
	addl	%ebx, %eax
	incl	%r8d
	cmpl	%ebp, %r8d
	je	.LBB62_90	# real_end4
.LBB62_87:	# bb79.preheader
	testl	%r8d, %r8d
	js	.LBB62_86	# bb80
.LBB62_88:	# bb79.preheader.bb77.preheader_crit_edge
	xorl	%ecx, %ecx
	movl	%ecx, %esi
	jmp	.LBB62_84	# bb77.preheader
.LBB62_89:	# real_catch4
	movl	$4, %edi
	call	llvm_real_catch
.LBB62_90:	# real_end4
	movl	$4, %edi
	jmp	.LBB62_58	# real_end2
.LBB62_91:	# bb82
	cmpl	$122, 4(%rsp)
	jne	.LBB62_107	# bb95
.LBB62_92:	# bb82
	cmpl	$112, (%rsp)
	jne	.LBB62_107	# bb95
.LBB62_93:	# bb86
	movl	$5, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB62_105	# real_catch5
.LBB62_94:	# jump5
	movl	$5, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB62_106	# real_end5
.LBB62_95:	# real_try5
	movl	$5, %edi
	call	llvm_real_try
	testl	%ebp, %ebp
	jle	.LBB62_106	# real_end5
.LBB62_96:	# bb.nph121
	xorl	%edx, %edx
	movl	%edx, %edi
	jmp	.LBB62_103	# bb92.preheader
.LBB62_97:	# bb90.preheader.bb89_crit_edge
	pxor	%xmm0, %xmm0
	xorl	%ecx, %ecx
	movl	%ecx, %esi
	.align	16
.LBB62_98:	# bb89
	leal	(%rax,%rcx), %r8d
	movslq	%r8d, %r8
	leal	(%rdi,%rcx), %r9d
	movslq	%r9d, %r9
	movsd	(%r12,%r9,8), %xmm1
	mulsd	(%r12,%r8,8), %xmm1
	addsd	%xmm1, %xmm0
	addl	%r15d, %ecx
	incl	%esi
	cmpl	%r13d, %esi
	jne	.LBB62_98	# bb89
.LBB62_99:	# bb91
	leal	(%rdx,%rax), %ecx
	movslq	%ecx, %rcx
	mulsd	16(%rsp), %xmm0
	addsd	(%r14,%rcx,8), %xmm0
	movsd	%xmm0, (%r14,%rcx,8)
	incl	%eax
	cmpl	%edi, %eax
	jg	.LBB62_102	# bb93
.LBB62_100:	# bb90.preheader
	testl	%r13d, %r13d
	jg	.LBB62_97	# bb90.preheader.bb89_crit_edge
.LBB62_101:	# bb90.preheader.bb91_crit_edge
	pxor	%xmm0, %xmm0
	jmp	.LBB62_99	# bb91
.LBB62_102:	# bb93
	addl	%ebx, %edx
	incl	%edi
	cmpl	%ebp, %edi
	je	.LBB62_106	# real_end5
.LBB62_103:	# bb92.preheader
	testl	%edi, %edi
	js	.LBB62_102	# bb93
.LBB62_104:	# bb92.preheader.bb90.preheader_crit_edge
	xorl	%eax, %eax
	jmp	.LBB62_100	# bb90.preheader
.LBB62_105:	# real_catch5
	movl	$5, %edi
	call	llvm_real_catch
.LBB62_106:	# real_end5
	movl	$5, %edi
	jmp	.LBB62_58	# real_end2
.LBB62_107:	# bb95
	xorl	%edi, %edi
	leaq	.str81, %rsi
	leaq	.str182, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB62_108:	# return
	addq	$24, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB62_109:	# bb5
	cmpl	$113, %edx
	movl	$112, %eax
	cmovne	%edx, %eax
	movl	%eax, (%rsp)
	jmp	.LBB62_5	# bb15
.LBB62_110:	# bb9.bb15_crit_edge
	movl	$111, (%rsp)
	jmp	.LBB62_5	# bb15
	.size	cblas_dsyrk, .-cblas_dsyrk
.Leh_func_end44:


	.section	.rodata.cst8,"aM",@progbits,8
	.align	8
.LCPI63_0:					
	.quad	4607182418800017408	# double value: 1.000000e+00
	.text
	.align	16
	.globl	cblas_dtbmv
	.type	cblas_dtbmv,@function
cblas_dtbmv:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	cmpl	$113, %edx
	movl	$112, %eax
	cmovne	%edx, %eax
	testl	%r8d, %r8d
	movl	80(%rsp), %edx
	movq	72(%rsp), %r10
	movq	56(%rsp), %r11
	movl	%r9d, -4(%rsp)
	movl	%ecx, -8(%rsp)
	je	.LBB63_30	# bb85.thread
.LBB63_1:	# bb18
	cmpl	$121, %esi
	sete	%cl
	setne	%r9b
	cmpl	$101, %edi
	sete	%bl
	setne	%r14b
	andb	%cl, %bl
	orb	%r9b, %r14b
	testb	%r14b, %r14b
	jne	.LBB63_3	# bb25
.LBB63_2:	# bb18
	cmpl	$111, %eax
	je	.LBB63_5	# bb33
.LBB63_3:	# bb25
	cmpl	$122, %esi
	sete	%cl
	setne	%r9b
	cmpl	$102, %edi
	sete	%r14b
	setne	%r15b
	andb	%cl, %r14b
	orb	%r9b, %r15b
	testb	%r15b, %r15b
	jne	.LBB63_15	# bb48
.LBB63_4:	# bb25
	cmpl	$112, %eax
	jne	.LBB63_15	# bb48
.LBB63_5:	# bb33
	testl	%edx, %edx
	jg	.LBB63_56	# bb33.bb47.preheader_crit_edge
.LBB63_6:	# bb34
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
	movl	%eax, -20(%rsp)
.LBB63_7:	# bb47.preheader
	testl	%r8d, %r8d
	jle	.LBB63_30	# bb85.thread
.LBB63_8:	# bb.nph217
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
	movl	%eax, -24(%rsp)
	movl	-4(%rsp), %eax
	leal	1(%rax), %eax
	movl	%eax, -16(%rsp)
	movl	$4294967294, -12(%rsp)
	xorl	%eax, %eax
	movl	%eax, %ecx
	movl	%eax, %esi
	movl	%eax, %edi
	.align	16
.LBB63_9:	# bb37
	movl	-20(%rsp), %r9d
	leal	(%r9,%rax), %r9d
	cmpl	$131, -8(%rsp)
	jne	.LBB63_57	# bb37.bb40_crit_edge
.LBB63_10:	# bb38
	movslq	%ecx, %rbx
	movsd	(%r11,%rbx,8), %xmm0
.LBB63_11:	# bb40
	testl	%edx, %edx
	movl	$0, %ebx
	cmovle	-24(%rsp), %ebx
	movl	-16(%rsp), %r14d
	leal	(%r14,%rdi), %r14d
	cmpl	%r8d, %r14d
	cmovg	%r8d, %r14d
	leal	1(%rdi), %r15d
	cmpl	%r14d, %r15d
	movslq	%r9d, %r9
	mulsd	(%r10,%r9,8), %xmm0
	jge	.LBB63_14	# bb46
.LBB63_12:	# bb.nph213
	movl	$4294967295, %r14d
	subl	%r8d, %r14d
	movl	%esi, %r15d
	negl	%r15d
	subl	-4(%rsp), %r15d
	addl	$4294967294, %r15d
	cmpl	%r15d, %r14d
	cmovg	%r14d, %r15d
	movl	-12(%rsp), %r14d
	subl	%r15d, %r14d
	leal	(%rdx,%rax), %r15d
	leal	1(%rcx), %r12d
	xorl	%r13d, %r13d
	.align	16
.LBB63_13:	# bb44
	leal	(%r12,%r13), %ebp
	movslq	%ebp, %rbp
	addl	%ebx, %r15d
	movslq	%r15d, %rbx
	movsd	(%r10,%rbx,8), %xmm1
	mulsd	(%r11,%rbp,8), %xmm1
	addsd	%xmm1, %xmm0
	incl	%r13d
	cmpl	%r14d, %r13d
	movl	%edx, %ebx
	jne	.LBB63_13	# bb44
.LBB63_14:	# bb46
	movsd	%xmm0, (%r10,%r9,8)
	addl	%edx, %eax
	addl	64(%rsp), %ecx
	incl	%esi
	decl	-12(%rsp)
	incl	%edi
	cmpl	%r8d, %edi
	jne	.LBB63_9	# bb37
	jmp	.LBB63_30	# bb85.thread
.LBB63_15:	# bb48
	cmpl	$122, %esi
	sete	%cl
	setne	%r9b
	cmpl	$101, %edi
	sete	%r15b
	setne	%r12b
	andb	%cl, %r15b
	orb	%r9b, %r12b
	testb	%r12b, %r12b
	jne	.LBB63_17	# bb56
.LBB63_16:	# bb48
	cmpl	$111, %eax
	je	.LBB63_19	# bb64
.LBB63_17:	# bb56
	cmpl	$121, %esi
	sete	%cl
	setne	%sil
	cmpl	$102, %edi
	sete	%dil
	setne	%r9b
	andb	%cl, %dil
	orb	%sil, %r9b
	testb	%r9b, %r9b
	jne	.LBB63_31	# bb87
.LBB63_18:	# bb56
	cmpl	$112, %eax
	jne	.LBB63_31	# bb87
.LBB63_19:	# bb64
	testl	%edx, %edx
	jg	.LBB63_58	# bb64.bb67_crit_edge
.LBB63_20:	# bb65
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
.LBB63_21:	# bb67
	movl	$1, %ecx
	subl	%r8d, %ecx
	movl	$1, %edi
	movl	64(%rsp), %esi
	subl	%esi, %edi
	movl	%edi, -12(%rsp)
	movl	$4294967295, %r9d
	movl	-4(%rsp), %edi
	subl	%edi, %r9d
	movl	%r9d, -16(%rsp)
	movl	%edi, %r9d
	subl	%r8d, %r9d
	leal	-1(%r8), %ebx
	imull	%ebx, %esi
	leal	1(%r9,%rsi), %r9d
	addl	%edi, %esi
	imull	%edx, %ecx
	imull	%edx, %ebx
	addl	%eax, %ebx
	jmp	.LBB63_28	# bb81
.LBB63_22:	# bb68
	cmpl	$131, -8(%rsp)
	jne	.LBB63_59	# bb68.bb71_crit_edge
.LBB63_23:	# bb69
	movslq	%esi, %rdi
	movsd	(%r11,%rdi,8), %xmm0
.LBB63_24:	# bb71
	xorl	%edi, %edi
	testl	%edx, %edx
	movl	%ecx, %r14d
	cmovg	%edi, %r14d
	movl	-16(%rsp), %r15d
	leal	(%r15,%r8), %r15d
	cmpl	-4(%rsp), %eax
	cmovle	%edi, %r15d
	movl	%r15d, %edi
	imull	%edx, %edi
	cmpl	%eax, %r15d
	movslq	%ebx, %rax
	mulsd	(%r10,%rax,8), %xmm0
	jge	.LBB63_27	# bb80
.LBB63_25:	# bb.nph205
	movl	%r8d, %r12d
	subl	%r15d, %r12d
	decl	%r12d
	addl	%r9d, %r15d
	xorl	%r13d, %r13d
	.align	16
.LBB63_26:	# bb78
	leal	(%r15,%r13), %ebp
	movslq	%ebp, %rbp
	addl	%r14d, %edi
	movslq	%edi, %r14
	movsd	(%r10,%r14,8), %xmm1
	mulsd	(%r11,%rbp,8), %xmm1
	addsd	%xmm1, %xmm0
	incl	%r13d
	cmpl	%r12d, %r13d
	movl	%edx, %r14d
	jne	.LBB63_26	# bb78
.LBB63_27:	# bb80
	movsd	%xmm0, (%r10,%rax,8)
	subl	64(%rsp), %esi
	subl	%edx, %ebx
	addl	-12(%rsp), %r9d
	decl	%r8d
.LBB63_28:	# bb81
	testl	%r8d, %r8d
	jle	.LBB63_30	# bb85.thread
.LBB63_29:	# bb82
	leal	-1(%r8), %eax
	testl	%r8d, %r8d
	jne	.LBB63_22	# bb68
.LBB63_30:	# bb85.thread
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB63_31:	# bb87
	cmpl	$111, %eax
	sete	%cl
	andb	%cl, %r14b
	cmpl	$112, %eax
	sete	%cl
	testb	%cl, %bl
	jne	.LBB63_33	# bb103
.LBB63_32:	# bb87
	notb	%r14b
	testb	$1, %r14b
	jne	.LBB63_44	# bb126
.LBB63_33:	# bb103
	testl	%edx, %edx
	jg	.LBB63_60	# bb103.bb106_crit_edge
.LBB63_34:	# bb104
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
.LBB63_35:	# bb106
	movl	$4294967295, %ecx
	subl	-4(%rsp), %ecx
	movl	$1, %esi
	subl	%r8d, %esi
	leal	-1(%r8), %edi
	movl	64(%rsp), %r9d
	imull	%edi, %r9d
	imull	%edx, %esi
	imull	%edx, %edi
	addl	%eax, %edi
	jmp	.LBB63_42	# bb120
.LBB63_36:	# bb107
	xorl	%ebx, %ebx
	testl	%edx, %edx
	movl	%esi, %r14d
	cmovg	%ebx, %r14d
	leal	(%rcx,%r8), %r15d
	cmpl	-4(%rsp), %eax
	cmovl	%ebx, %r15d
	movl	%r15d, %ebx
	imull	%edx, %ebx
	cmpl	%eax, %r15d
	jge	.LBB63_61	# bb107.bb116_crit_edge
.LBB63_37:	# bb.nph197
	movl	64(%rsp), %eax
	leal	-1(%rax), %eax
	movl	%r15d, %r12d
	imull	%eax, %r12d
	leal	-1(%r8,%r12), %r12d
	leal	-1(%r8), %r13d
	subl	%r15d, %r13d
	pxor	%xmm0, %xmm0
	xorl	%r15d, %r15d
	.align	16
.LBB63_38:	# bb114
	leal	(%rax,%r12), %ebp
	addl	%r14d, %ebx
	incl	%r15d
	cmpl	%r13d, %r15d
	movslq	%ebx, %r14
	movsd	(%r10,%r14,8), %xmm1
	movslq	%r12d, %r14
	mulsd	(%r11,%r14,8), %xmm1
	addsd	%xmm1, %xmm0
	movl	%ebp, %r12d
	movl	%edx, %r14d
	jne	.LBB63_38	# bb114
.LBB63_39:	# bb116
	movslq	%edi, %rax
	movsd	(%r10,%rax,8), %xmm1
	cmpl	$131, -8(%rsp)
	jne	.LBB63_41	# bb119
.LBB63_40:	# bb117
	movslq	%r9d, %rbx
	mulsd	(%r11,%rbx,8), %xmm1
.LBB63_41:	# bb119
	addsd	%xmm0, %xmm1
	movsd	%xmm1, (%r10,%rax,8)
	subl	%edx, %edi
	subl	64(%rsp), %r9d
	decl	%r8d
.LBB63_42:	# bb120
	testl	%r8d, %r8d
	jle	.LBB63_30	# bb85.thread
.LBB63_43:	# bb121
	leal	-1(%r8), %eax
	testl	%r8d, %r8d
	jne	.LBB63_36	# bb107
	jmp	.LBB63_30	# bb85.thread
.LBB63_44:	# bb126
	cmpl	$111, %eax
	sete	%cl
	andb	%cl, %dil
	cmpl	$112, %eax
	sete	%al
	testb	%al, %r15b
	jne	.LBB63_46	# bb142
.LBB63_45:	# bb126
	notb	%dil
	testb	$1, %dil
	jne	.LBB63_30	# bb85.thread
.LBB63_46:	# bb142
	testl	%edx, %edx
	jg	.LBB63_62	# bb142.bb157.preheader_crit_edge
.LBB63_47:	# bb143
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
	movl	%eax, -28(%rsp)
.LBB63_48:	# bb157.preheader
	testl	%r8d, %r8d
	jle	.LBB63_30	# bb85.thread
.LBB63_49:	# bb.nph190
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
	movl	%eax, -32(%rsp)
	movl	64(%rsp), %eax
	leal	-1(%rax), %eax
	movl	%eax, -20(%rsp)
	movl	-4(%rsp), %eax
	leal	1(%rax), %ecx
	movl	%ecx, -24(%rsp)
	movl	$4294967294, -16(%rsp)
	xorl	%ecx, %ecx
	movl	%ecx, -12(%rsp)
	movl	%ecx, %esi
	.align	16
.LBB63_50:	# bb146
	testl	%edx, %edx
	movl	$0, %edi
	cmovle	-32(%rsp), %edi
	movl	-24(%rsp), %r9d
	leal	(%r9,%rsi), %r9d
	cmpl	%r8d, %r9d
	cmovg	%r8d, %r9d
	movl	-28(%rsp), %ebx
	leal	(%rbx,%rcx), %ebx
	leal	1(%rsi), %r14d
	cmpl	%r9d, %r14d
	jge	.LBB63_63	# bb146.bb153_crit_edge
.LBB63_51:	# bb.nph
	movl	$4294967295, %r9d
	subl	%r8d, %r9d
	movl	-12(%rsp), %r14d
	negl	%r14d
	subl	-4(%rsp), %r14d
	addl	$4294967294, %r14d
	cmpl	%r14d, %r9d
	cmovg	%r9d, %r14d
	movl	-16(%rsp), %r9d
	subl	%r14d, %r9d
	leal	(%rdx,%rcx), %r14d
	movl	-20(%rsp), %r15d
	leal	(%r15,%rax), %r15d
	movl	64(%rsp), %r12d
	leal	-1(%r12), %r12d
	pxor	%xmm0, %xmm0
	xorl	%r13d, %r13d
	.align	16
.LBB63_52:	# bb151
	leal	(%r12,%r15), %ebp
	addl	%edi, %r14d
	incl	%r13d
	cmpl	%r9d, %r13d
	movslq	%r14d, %rdi
	movsd	(%r10,%rdi,8), %xmm1
	movslq	%r15d, %rdi
	mulsd	(%r11,%rdi,8), %xmm1
	addsd	%xmm1, %xmm0
	movl	%ebp, %r15d
	movl	%edx, %edi
	jne	.LBB63_52	# bb151
.LBB63_53:	# bb153
	movslq	%ebx, %rdi
	movsd	(%r10,%rdi,8), %xmm1
	cmpl	$131, -8(%rsp)
	jne	.LBB63_55	# bb156
.LBB63_54:	# bb154
	movslq	%eax, %r9
	mulsd	(%r11,%r9,8), %xmm1
.LBB63_55:	# bb156
	addsd	%xmm0, %xmm1
	movsd	%xmm1, (%r10,%rdi,8)
	addl	%edx, %ecx
	addl	64(%rsp), %eax
	incl	-12(%rsp)
	decl	-16(%rsp)
	incl	%esi
	cmpl	%r8d, %esi
	je	.LBB63_30	# bb85.thread
	jmp	.LBB63_50	# bb146
.LBB63_56:	# bb33.bb47.preheader_crit_edge
	movl	$0, -20(%rsp)
	jmp	.LBB63_7	# bb47.preheader
.LBB63_57:	# bb37.bb40_crit_edge
	movsd	.LCPI63_0(%rip), %xmm0
	jmp	.LBB63_11	# bb40
.LBB63_58:	# bb64.bb67_crit_edge
	xorl	%eax, %eax
	jmp	.LBB63_21	# bb67
.LBB63_59:	# bb68.bb71_crit_edge
	movsd	.LCPI63_0(%rip), %xmm0
	jmp	.LBB63_24	# bb71
.LBB63_60:	# bb103.bb106_crit_edge
	xorl	%eax, %eax
	jmp	.LBB63_35	# bb106
.LBB63_61:	# bb107.bb116_crit_edge
	pxor	%xmm0, %xmm0
	jmp	.LBB63_39	# bb116
.LBB63_62:	# bb142.bb157.preheader_crit_edge
	movl	$0, -28(%rsp)
	jmp	.LBB63_48	# bb157.preheader
.LBB63_63:	# bb146.bb153_crit_edge
	pxor	%xmm0, %xmm0
	jmp	.LBB63_53	# bb153
	.size	cblas_dtbmv, .-cblas_dtbmv


	.align	16
	.globl	cblas_dtbsv
	.type	cblas_dtbsv,@function
cblas_dtbsv:
.Leh_func_begin45:
.Llabel45:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	cmpl	$113, %edx
	movl	$112, %eax
	cmovne	%edx, %eax
	testl	%r8d, %r8d
	movl	120(%rsp), %edx
	movq	112(%rsp), %r10
	movq	96(%rsp), %r11
	movl	%r9d, 36(%rsp)
	movl	%ecx, 32(%rsp)
	je	.LBB64_16	# bb53.thread
.LBB64_1:	# bb20
	cmpl	$121, %esi
	sete	%cl
	setne	%r9b
	cmpl	$101, %edi
	sete	%bl
	setne	%r14b
	andb	%cl, %bl
	orb	%r9b, %r14b
	testb	%r14b, %r14b
	jne	.LBB64_3	# bb27
.LBB64_2:	# bb20
	cmpl	$111, %eax
	je	.LBB64_5	# bb35
.LBB64_3:	# bb27
	cmpl	$122, %esi
	sete	%cl
	setne	%r9b
	cmpl	$102, %edi
	sete	%r14b
	setne	%r15b
	andb	%cl, %r14b
	orb	%r9b, %r15b
	testb	%r15b, %r15b
	jne	.LBB64_17	# bb55
.LBB64_4:	# bb27
	cmpl	$112, %eax
	jne	.LBB64_17	# bb55
.LBB64_5:	# bb35
	testl	%edx, %edx
	jg	.LBB64_57	# bb35.bb38_crit_edge
.LBB64_6:	# bb36
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
.LBB64_7:	# bb38
	movl	$1, %ecx
	subl	%r8d, %ecx
	leal	-1(%r8), %esi
	movl	104(%rsp), %edi
	imull	%esi, %edi
	imull	%edx, %ecx
	movl	%ecx, 16(%rsp)
	movl	%r8d, %ecx
	imull	%edx, %ecx
	movl	%ecx, 24(%rsp)
	imull	%edx, %esi
	addl	%eax, %esi
	movl	%esi, 20(%rsp)
	xorl	%eax, %eax
	movl	%r8d, %ecx
	movl	%eax, 28(%rsp)
	movl	%eax, %esi
	jmp	.LBB64_14	# bb49
.LBB64_8:	# bb39
	testl	%edx, %edx
	movl	$0, %ebx
	cmovle	16(%rsp), %ebx
	movl	36(%rsp), %r14d
	leal	(%r14,%rcx), %r14d
	cmpl	%r8d, %r14d
	cmovg	%r8d, %r14d
	cmpl	%r14d, %ecx
	movslq	%r9d, %r9
	movsd	(%r10,%r9,8), %xmm0
	jge	.LBB64_11	# bb45
.LBB64_9:	# bb.nph217
	movl	$4294967295, %r14d
	subl	%r8d, %r14d
	movl	36(%rsp), %r15d
	leal	1(%r15,%r8), %r15d
	movl	28(%rsp), %r12d
	subl	%r15d, %r12d
	cmpl	%r12d, %r14d
	cmovg	%r14d, %r12d
	addl	%r8d, %r12d
	movl	%esi, %r14d
	subl	%r12d, %r14d
	decl	%r14d
	movl	24(%rsp), %r15d
	leal	(%r15,%rax), %r15d
	leal	1(%rdi), %r12d
	xorl	%r13d, %r13d
	.align	16
.LBB64_10:	# bb43
	leal	(%r12,%r13), %ebp
	movslq	%ebp, %rbp
	addl	%ebx, %r15d
	movslq	%r15d, %rbx
	movsd	(%r10,%rbx,8), %xmm1
	mulsd	(%r11,%rbp,8), %xmm1
	subsd	%xmm1, %xmm0
	incl	%r13d
	cmpl	%r14d, %r13d
	movl	%edx, %ebx
	jne	.LBB64_10	# bb43
.LBB64_11:	# bb45
	cmpl	$131, 32(%rsp)
	jne	.LBB64_13	# bb48
.LBB64_12:	# bb46
	movslq	%edi, %rbx
	divsd	(%r11,%rbx,8), %xmm0
.LBB64_13:	# bb48
	movsd	%xmm0, (%r10,%r9,8)
	subl	%edx, %eax
	subl	104(%rsp), %edi
	decl	%ecx
	incl	28(%rsp)
	incl	%esi
.LBB64_14:	# bb49
	movl	20(%rsp), %r9d
	leal	(%r9,%rax), %r9d
	testl	%ecx, %ecx
	jle	.LBB64_16	# bb53.thread
.LBB64_15:	# bb50
	testl	%ecx, %ecx
	jne	.LBB64_8	# bb39
.LBB64_16:	# bb53.thread
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB64_17:	# bb55
	cmpl	$122, %esi
	sete	%cl
	setne	%r9b
	cmpl	$101, %edi
	sete	%r15b
	setne	%r12b
	andb	%cl, %r15b
	orb	%r9b, %r12b
	testb	%r12b, %r12b
	jne	.LBB64_19	# bb63
.LBB64_18:	# bb55
	cmpl	$111, %eax
	je	.LBB64_21	# bb71
.LBB64_19:	# bb63
	cmpl	$121, %esi
	sete	%cl
	setne	%sil
	cmpl	$102, %edi
	sete	%dil
	setne	%r9b
	andb	%cl, %dil
	orb	%sil, %r9b
	testb	%r9b, %r9b
	jne	.LBB64_31	# bb89
.LBB64_20:	# bb63
	cmpl	$112, %eax
	jne	.LBB64_31	# bb89
.LBB64_21:	# bb71
	testl	%edx, %edx
	jg	.LBB64_58	# bb71.bb88.preheader_crit_edge
.LBB64_22:	# bb72
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
.LBB64_23:	# bb88.preheader
	testl	%r8d, %r8d
	jle	.LBB64_16	# bb53.thread
.LBB64_24:	# bb.nph210
	movl	$1, %ecx
	subl	%r8d, %ecx
	imull	%edx, %ecx
	movl	104(%rsp), %esi
	leal	-1(%rsi), %esi
	movl	%esi, 28(%rsp)
	movl	36(%rsp), %esi
	movl	%esi, %edi
	negl	%edi
	movl	%edi, 24(%rsp)
	xorl	%edi, %edi
	movl	%esi, %r9d
	.align	16
.LBB64_25:	# bb75
	xorl	%ebx, %ebx
	testl	%edx, %edx
	movl	%ecx, %r14d
	cmovg	%ebx, %r14d
	movl	24(%rsp), %r15d
	leal	(%r15,%rdi), %r15d
	cmpl	36(%rsp), %edi
	cmovle	%ebx, %r15d
	movl	%r15d, %ebx
	imull	%edx, %ebx
	cmpl	%edi, %r15d
	movslq	%eax, %rax
	movsd	(%r10,%rax,8), %xmm0
	jge	.LBB64_28	# bb84
.LBB64_26:	# bb.nph206
	movl	%edi, %r12d
	subl	%r15d, %r12d
	addl	%esi, %r15d
	xorl	%r13d, %r13d
	.align	16
.LBB64_27:	# bb82
	leal	(%r15,%r13), %ebp
	movslq	%ebp, %rbp
	addl	%r14d, %ebx
	movslq	%ebx, %r14
	movsd	(%r10,%r14,8), %xmm1
	mulsd	(%r11,%rbp,8), %xmm1
	subsd	%xmm1, %xmm0
	incl	%r13d
	cmpl	%r12d, %r13d
	movl	%edx, %r14d
	jne	.LBB64_27	# bb82
.LBB64_28:	# bb84
	cmpl	$131, 32(%rsp)
	jne	.LBB64_30	# bb87
.LBB64_29:	# bb85
	movslq	%r9d, %rbx
	divsd	(%r11,%rbx,8), %xmm0
.LBB64_30:	# bb87
	movsd	%xmm0, (%r10,%rax,8)
	addl	%edx, %eax
	addl	104(%rsp), %r9d
	addl	28(%rsp), %esi
	incl	%edi
	cmpl	%r8d, %edi
	je	.LBB64_16	# bb53.thread
	jmp	.LBB64_25	# bb75
.LBB64_31:	# bb89
	cmpl	$111, %eax
	sete	%cl
	andb	%cl, %r14b
	cmpl	$112, %eax
	sete	%cl
	testb	%cl, %bl
	jne	.LBB64_33	# bb105
.LBB64_32:	# bb89
	notb	%r14b
	testb	$1, %r14b
	jne	.LBB64_43	# bb123
.LBB64_33:	# bb105
	testl	%edx, %edx
	jg	.LBB64_59	# bb105.bb122.preheader_crit_edge
.LBB64_34:	# bb106
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
.LBB64_35:	# bb122.preheader
	testl	%r8d, %r8d
	jle	.LBB64_16	# bb53.thread
.LBB64_36:	# bb.nph199
	movl	$1, %ecx
	subl	%r8d, %ecx
	imull	%edx, %ecx
	movl	36(%rsp), %esi
	negl	%esi
	movl	%esi, 28(%rsp)
	xorl	%esi, %esi
	movl	%esi, %edi
	.align	16
.LBB64_37:	# bb109
	xorl	%r9d, %r9d
	testl	%edx, %edx
	movl	%ecx, %ebx
	cmovg	%r9d, %ebx
	movl	28(%rsp), %r14d
	leal	(%r14,%rdi), %r14d
	cmpl	36(%rsp), %edi
	cmovl	%r9d, %r14d
	movl	%r14d, %r9d
	imull	%edx, %r9d
	cmpl	%edi, %r14d
	movslq	%eax, %rax
	movsd	(%r10,%rax,8), %xmm0
	jge	.LBB64_40	# bb118
.LBB64_38:	# bb.nph195
	movl	104(%rsp), %r15d
	leal	-1(%r15), %r15d
	movl	%r14d, %r12d
	imull	%r15d, %r12d
	addl	%edi, %r12d
	movl	%edi, %r13d
	subl	%r14d, %r13d
	xorl	%r14d, %r14d
	.align	16
.LBB64_39:	# bb116
	leal	(%r15,%r12), %ebp
	addl	%ebx, %r9d
	incl	%r14d
	cmpl	%r13d, %r14d
	movslq	%r9d, %rbx
	movsd	(%r10,%rbx,8), %xmm1
	movslq	%r12d, %rbx
	mulsd	(%r11,%rbx,8), %xmm1
	subsd	%xmm1, %xmm0
	movl	%ebp, %r12d
	movl	%edx, %ebx
	jne	.LBB64_39	# bb116
.LBB64_40:	# bb118
	cmpl	$131, 32(%rsp)
	jne	.LBB64_42	# bb121
.LBB64_41:	# bb119
	movslq	%esi, %r9
	divsd	(%r11,%r9,8), %xmm0
.LBB64_42:	# bb121
	movsd	%xmm0, (%r10,%rax,8)
	addl	%edx, %eax
	addl	104(%rsp), %esi
	incl	%edi
	cmpl	%r8d, %edi
	je	.LBB64_16	# bb53.thread
	jmp	.LBB64_37	# bb109
.LBB64_43:	# bb123
	cmpl	$111, %eax
	sete	%cl
	andb	%cl, %dil
	cmpl	$112, %eax
	sete	%al
	testb	%al, %r15b
	jne	.LBB64_45	# bb139
.LBB64_44:	# bb123
	notb	%dil
	testb	$1, %dil
	jne	.LBB64_56	# bb160
.LBB64_45:	# bb139
	testl	%edx, %edx
	jg	.LBB64_60	# bb139.bb142_crit_edge
.LBB64_46:	# bb140
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
.LBB64_47:	# bb142
	movl	104(%rsp), %ecx
	movl	%ecx, %esi
	imull	%r8d, %esi
	decl	%esi
	movl	%esi, 16(%rsp)
	movl	$1, %esi
	subl	%r8d, %esi
	leal	-1(%r8), %edi
	imull	%edi, %ecx
	movl	%ecx, 20(%rsp)
	imull	%edx, %esi
	movl	%esi, 4(%rsp)
	movl	%r8d, %ecx
	imull	%edx, %ecx
	movl	%ecx, 12(%rsp)
	imull	%edx, %edi
	addl	%eax, %edi
	movl	%edi, 8(%rsp)
	xorl	%eax, %eax
	movl	36(%rsp), %ecx
	movl	%r8d, %esi
	movl	%eax, 28(%rsp)
	movl	%eax, 24(%rsp)
	jmp	.LBB64_54	# bb154
.LBB64_48:	# bb143
	testl	%edx, %edx
	movl	$0, %r9d
	cmovle	4(%rsp), %r9d
	movl	36(%rsp), %ebx
	leal	(%rbx,%rsi), %ebx
	cmpl	%r8d, %ebx
	cmovg	%r8d, %ebx
	cmpl	%ebx, %esi
	movslq	%edi, %rdi
	movsd	(%r10,%rdi,8), %xmm0
	jge	.LBB64_51	# bb150
.LBB64_49:	# bb.nph
	movl	$4294967295, %ebx
	subl	%r8d, %ebx
	movl	36(%rsp), %r14d
	leal	1(%r14,%r8), %r14d
	movl	28(%rsp), %r15d
	subl	%r14d, %r15d
	cmpl	%r15d, %ebx
	cmovg	%ebx, %r15d
	addl	%r8d, %r15d
	movl	24(%rsp), %ebx
	subl	%r15d, %ebx
	decl	%ebx
	movl	12(%rsp), %r14d
	leal	(%r14,%rax), %r14d
	movl	16(%rsp), %r15d
	leal	(%r15,%rcx), %r15d
	movl	104(%rsp), %r12d
	leal	-1(%r12), %r12d
	xorl	%r13d, %r13d
	.align	16
.LBB64_50:	# bb148
	leal	(%r12,%r15), %ebp
	addl	%r9d, %r14d
	incl	%r13d
	cmpl	%ebx, %r13d
	movslq	%r14d, %r9
	movsd	(%r10,%r9,8), %xmm1
	movslq	%r15d, %r9
	mulsd	(%r11,%r9,8), %xmm1
	subsd	%xmm1, %xmm0
	movl	%ebp, %r15d
	movl	%edx, %r9d
	jne	.LBB64_50	# bb148
.LBB64_51:	# bb150
	cmpl	$131, 32(%rsp)
	jne	.LBB64_53	# bb153
.LBB64_52:	# bb151
	movl	20(%rsp), %r9d
	leal	(%r9,%rcx), %r9d
	movslq	%r9d, %r9
	divsd	(%r11,%r9,8), %xmm0
.LBB64_53:	# bb153
	movsd	%xmm0, (%r10,%rdi,8)
	subl	%edx, %eax
	subl	104(%rsp), %ecx
	decl	%esi
	incl	28(%rsp)
	incl	24(%rsp)
.LBB64_54:	# bb154
	movl	8(%rsp), %edi
	leal	(%rdi,%rax), %edi
	testl	%esi, %esi
	jle	.LBB64_16	# bb53.thread
.LBB64_55:	# bb155
	testl	%esi, %esi
	jne	.LBB64_48	# bb143
	jmp	.LBB64_16	# bb53.thread
.LBB64_56:	# bb160
	xorl	%edi, %edi
	leaq	.str83, %rsi
	leaq	.str184, %rdx
	xorb	%al, %al
	call	cblas_xerbla
	jmp	.LBB64_16	# bb53.thread
.LBB64_57:	# bb35.bb38_crit_edge
	xorl	%eax, %eax
	jmp	.LBB64_7	# bb38
.LBB64_58:	# bb71.bb88.preheader_crit_edge
	xorl	%eax, %eax
	jmp	.LBB64_23	# bb88.preheader
.LBB64_59:	# bb105.bb122.preheader_crit_edge
	xorl	%eax, %eax
	jmp	.LBB64_35	# bb122.preheader
.LBB64_60:	# bb139.bb142_crit_edge
	xorl	%eax, %eax
	jmp	.LBB64_47	# bb142
	.size	cblas_dtbsv, .-cblas_dtbsv
.Leh_func_end45:


	.align	16
	.globl	cblas_dtpmv
	.type	cblas_dtpmv,@function
cblas_dtpmv:
.Leh_func_begin46:
.Llabel46:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$24, %rsp
	cmpl	$113, %edx
	movl	$112, %eax
	cmovne	%edx, %eax
	testl	%r8d, %r8d
	movl	88(%rsp), %edx
	movq	80(%rsp), %r10
	movl	%ecx, 20(%rsp)
	je	.LBB65_30	# bb79.thread
.LBB65_1:	# bb15
	cmpl	$121, %esi
	sete	%cl
	setne	%r11b
	cmpl	$101, %edi
	sete	%bl
	setne	%r14b
	andb	%cl, %bl
	orb	%r11b, %r14b
	testb	%r14b, %r14b
	jne	.LBB65_3	# bb22
.LBB65_2:	# bb15
	cmpl	$111, %eax
	je	.LBB65_5	# bb30
.LBB65_3:	# bb22
	cmpl	$122, %esi
	sete	%cl
	setne	%r11b
	cmpl	$102, %edi
	sete	%r14b
	setne	%r15b
	andb	%cl, %r14b
	orb	%r11b, %r15b
	testb	%r15b, %r15b
	jne	.LBB65_15	# bb45
.LBB65_4:	# bb22
	cmpl	$112, %eax
	jne	.LBB65_15	# bb45
.LBB65_5:	# bb30
	testl	%edx, %edx
	jg	.LBB65_56	# bb30.bb44.preheader_crit_edge
.LBB65_6:	# bb31
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
	movl	%eax, 12(%rsp)
.LBB65_7:	# bb44.preheader
	testl	%r8d, %r8d
	jle	.LBB65_30	# bb79.thread
.LBB65_8:	# bb.nph206
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
	movl	%eax, 8(%rsp)
	leal	1(,%r8,2), %eax
	leal	-1(%r8), %ecx
	movl	%ecx, 16(%rsp)
	xorl	%ecx, %ecx
	movl	%ecx, %esi
	movl	%ecx, %edi
	.align	16
.LBB65_9:	# bb34
	movl	12(%rsp), %r11d
	leal	(%r11,%rcx), %r11d
	movslq	%r11d, %r11
	movsd	(%r10,%r11,8), %xmm0
	cmpl	$131, 20(%rsp)
	jne	.LBB65_11	# bb37
.LBB65_10:	# bb35
	leal	(%rax,%rsi), %ebx
	imull	%edi, %ebx
	movl	%ebx, %r14d
	shrl	$31, %r14d
	addl	%ebx, %r14d
	sarl	%r14d
	movslq	%r14d, %rbx
	mulsd	(%r9,%rbx,8), %xmm0
.LBB65_11:	# bb37
	testl	%edx, %edx
	movl	$0, %ebx
	cmovle	8(%rsp), %ebx
	leal	1(%rdi), %r14d
	cmpl	%r8d, %r14d
	jge	.LBB65_14	# bb43
.LBB65_12:	# bb.nph202
	leal	(%rax,%rsi), %r14d
	imull	%edi, %r14d
	movl	%r14d, %r15d
	shrl	$31, %r15d
	addl	%r14d, %r15d
	sarl	%r15d
	incl	%r15d
	leal	(%rdx,%rcx), %r14d
	movl	16(%rsp), %r12d
	leal	(%r12,%rsi), %r12d
	xorl	%r13d, %r13d
	.align	16
.LBB65_13:	# bb41
	leal	(%r15,%r13), %ebp
	movslq	%ebp, %rbp
	addl	%ebx, %r14d
	movslq	%r14d, %rbx
	movsd	(%r10,%rbx,8), %xmm1
	mulsd	(%r9,%rbp,8), %xmm1
	addsd	%xmm1, %xmm0
	incl	%r13d
	cmpl	%r12d, %r13d
	movl	%edx, %ebx
	jne	.LBB65_13	# bb41
.LBB65_14:	# bb43
	movsd	%xmm0, (%r10,%r11,8)
	addl	%edx, %ecx
	decl	%esi
	incl	%edi
	cmpl	%r8d, %edi
	jne	.LBB65_9	# bb34
	jmp	.LBB65_30	# bb79.thread
.LBB65_15:	# bb45
	cmpl	$122, %esi
	sete	%cl
	setne	%r11b
	cmpl	$101, %edi
	sete	%r15b
	setne	%r12b
	andb	%cl, %r15b
	orb	%r11b, %r12b
	testb	%r12b, %r12b
	jne	.LBB65_17	# bb53
.LBB65_16:	# bb45
	cmpl	$111, %eax
	je	.LBB65_19	# bb61
.LBB65_17:	# bb53
	cmpl	$121, %esi
	sete	%cl
	setne	%sil
	cmpl	$102, %edi
	sete	%dil
	setne	%r11b
	andb	%cl, %dil
	orb	%sil, %r11b
	testb	%r11b, %r11b
	jne	.LBB65_31	# bb81
.LBB65_18:	# bb53
	cmpl	$112, %eax
	jne	.LBB65_31	# bb81
.LBB65_19:	# bb61
	testl	%edx, %edx
	jg	.LBB65_57	# bb61.bb64_crit_edge
.LBB65_20:	# bb62
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
.LBB65_21:	# bb64
	movl	$1, %ecx
	subl	%r8d, %ecx
	imull	%edx, %ecx
	leal	-1(%r8), %esi
	imull	%edx, %esi
	addl	%eax, %esi
	jmp	.LBB65_28	# bb75
.LBB65_22:	# bb65
	movslq	%esi, %rdi
	movsd	(%r10,%rdi,8), %xmm0
	cmpl	$131, 20(%rsp)
	jne	.LBB65_24	# bb68
.LBB65_23:	# bb66
	movl	%eax, %r11d
	imull	%r8d, %r11d
	movl	%r11d, %ebx
	shrl	$31, %ebx
	addl	%r11d, %ebx
	sarl	%ebx
	leal	-1(%r8,%rbx), %r11d
	movslq	%r11d, %r11
	mulsd	(%r9,%r11,8), %xmm0
.LBB65_24:	# bb68
	testl	%edx, %edx
	movl	$0, %r11d
	cmovle	%ecx, %r11d
	testl	%eax, %eax
	jle	.LBB65_27	# bb74
.LBB65_25:	# bb.nph192
	imull	%r8d, %eax
	movl	%eax, %ebx
	shrl	$31, %ebx
	addl	%eax, %ebx
	sarl	%ebx
	leal	-1(%r8), %eax
	xorl	%r14d, %r14d
	.align	16
.LBB65_26:	# bb72
	leal	(%rbx,%r14), %r15d
	movslq	%r15d, %r15
	movslq	%r11d, %r12
	movsd	(%r10,%r12,8), %xmm1
	mulsd	(%r9,%r15,8), %xmm1
	addsd	%xmm1, %xmm0
	addl	%edx, %r11d
	incl	%r14d
	cmpl	%eax, %r14d
	jne	.LBB65_26	# bb72
.LBB65_27:	# bb74
	movsd	%xmm0, (%r10,%rdi,8)
	subl	%edx, %esi
	decl	%r8d
.LBB65_28:	# bb75
	testl	%r8d, %r8d
	jle	.LBB65_30	# bb79.thread
.LBB65_29:	# bb76
	leal	-1(%r8), %eax
	testl	%r8d, %r8d
	jne	.LBB65_22	# bb65
.LBB65_30:	# bb79.thread
	addq	$24, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB65_31:	# bb81
	cmpl	$111, %eax
	sete	%cl
	andb	%cl, %r14b
	cmpl	$112, %eax
	sete	%cl
	testb	%cl, %bl
	jne	.LBB65_33	# bb97
.LBB65_32:	# bb81
	notb	%r14b
	testb	$1, %r14b
	jne	.LBB65_44	# bb117
.LBB65_33:	# bb97
	testl	%edx, %edx
	jg	.LBB65_58	# bb97.bb100_crit_edge
.LBB65_34:	# bb98
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
.LBB65_35:	# bb100
	movl	$1, %ecx
	subl	%r8d, %ecx
	imull	%edx, %ecx
	movl	%ecx, 16(%rsp)
	leal	-1(%r8), %ecx
	imull	%edx, %ecx
	addl	%eax, %ecx
	leal	2(%r8), %eax
	movl	%r8d, %esi
	jmp	.LBB65_42	# bb111
.LBB65_36:	# bb101
	movslq	%ecx, %r11
	movsd	(%r10,%r11,8), %xmm0
	cmpl	$131, 20(%rsp)
	jne	.LBB65_38	# bb104
.LBB65_37:	# bb102
	movl	%eax, %ebx
	imull	%edi, %ebx
	movl	%ebx, %r14d
	shrl	$31, %r14d
	addl	%ebx, %r14d
	sarl	%r14d
	movslq	%r14d, %rbx
	mulsd	(%r9,%rbx,8), %xmm0
.LBB65_38:	# bb104
	testl	%edx, %edx
	movl	$0, %ebx
	cmovle	16(%rsp), %ebx
	testl	%edi, %edi
	jle	.LBB65_41	# bb110
.LBB65_39:	# bb.nph187
	leal	1(,%r8,2), %edi
	leal	-1(%rsi), %r14d
	xorl	%r15d, %r15d
	movl	%r15d, %r12d
	.align	16
.LBB65_40:	# bb108
	leal	(%rdi,%r15), %r13d
	imull	%r12d, %r13d
	movl	%r13d, %ebp
	shrl	$31, %ebp
	addl	%r13d, %ebp
	sarl	%ebp
	leal	(%r14,%r15), %r13d
	addl	%ebp, %r13d
	movslq	%r13d, %r13
	movslq	%ebx, %rbp
	movsd	(%r10,%rbp,8), %xmm1
	mulsd	(%r9,%r13,8), %xmm1
	addsd	%xmm1, %xmm0
	addl	%edx, %ebx
	decl	%r15d
	incl	%r12d
	cmpl	%r14d, %r12d
	jne	.LBB65_40	# bb108
.LBB65_41:	# bb110
	movsd	%xmm0, (%r10,%r11,8)
	subl	%edx, %ecx
	decl	%esi
	incl	%eax
.LBB65_42:	# bb111
	testl	%esi, %esi
	jle	.LBB65_30	# bb79.thread
.LBB65_43:	# bb112
	leal	-1(%rsi), %edi
	testl	%esi, %esi
	jne	.LBB65_36	# bb101
	jmp	.LBB65_30	# bb79.thread
.LBB65_44:	# bb117
	cmpl	$111, %eax
	sete	%cl
	andb	%cl, %dil
	cmpl	$112, %eax
	sete	%al
	testb	%al, %r15b
	jne	.LBB65_46	# bb133
.LBB65_45:	# bb117
	notb	%dil
	testb	$1, %dil
	jne	.LBB65_60	# bb148
.LBB65_46:	# bb133
	testl	%edx, %edx
	jg	.LBB65_59	# bb133.bb147.preheader_crit_edge
.LBB65_47:	# bb134
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
	movl	%eax, 16(%rsp)
.LBB65_48:	# bb147.preheader
	testl	%r8d, %r8d
	jle	.LBB65_30	# bb79.thread
.LBB65_49:	# bb.nph183
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
	movl	%eax, 12(%rsp)
	leal	-1(%r8), %eax
	xorl	%ecx, %ecx
	movl	%ecx, %esi
	.align	16
.LBB65_50:	# bb137
	movl	16(%rsp), %edi
	leal	(%rdi,%rcx), %edi
	movslq	%edi, %rdi
	movsd	(%r10,%rdi,8), %xmm0
	cmpl	$131, 20(%rsp)
	jne	.LBB65_52	# bb140
.LBB65_51:	# bb138
	leal	1(%rsi), %r11d
	imull	%esi, %r11d
	movl	%r11d, %ebx
	shrl	$31, %ebx
	addl	%r11d, %ebx
	sarl	%ebx
	addl	%esi, %ebx
	movslq	%ebx, %r11
	mulsd	(%r9,%r11,8), %xmm0
.LBB65_52:	# bb140
	testl	%edx, %edx
	movl	$0, %r11d
	cmovle	12(%rsp), %r11d
	leal	1(%rsi), %ebx
	cmpl	%r8d, %ebx
	jge	.LBB65_55	# bb146
.LBB65_53:	# bb.nph
	leal	(%rdx,%rcx), %ebx
	leal	1(%rsi), %r14d
	leal	2(%rsi), %r15d
	xorl	%r12d, %r12d
	.align	16
.LBB65_54:	# bb144
	leal	(%r14,%r12), %r13d
	leal	(%r15,%r12), %ebp
	imull	%r13d, %ebp
	movl	%ebp, %r13d
	shrl	$31, %r13d
	addl	%ebp, %r13d
	sarl	%r13d
	addl	%esi, %r13d
	movslq	%r13d, %r13
	addl	%r11d, %ebx
	movslq	%ebx, %r11
	movsd	(%r10,%r11,8), %xmm1
	mulsd	(%r9,%r13,8), %xmm1
	addsd	%xmm1, %xmm0
	incl	%r12d
	cmpl	%eax, %r12d
	movl	%edx, %r11d
	jne	.LBB65_54	# bb144
.LBB65_55:	# bb146
	movsd	%xmm0, (%r10,%rdi,8)
	addl	%edx, %ecx
	decl	%eax
	incl	%esi
	cmpl	%r8d, %esi
	je	.LBB65_30	# bb79.thread
	jmp	.LBB65_50	# bb137
.LBB65_56:	# bb30.bb44.preheader_crit_edge
	movl	$0, 12(%rsp)
	jmp	.LBB65_7	# bb44.preheader
.LBB65_57:	# bb61.bb64_crit_edge
	xorl	%eax, %eax
	jmp	.LBB65_21	# bb64
.LBB65_58:	# bb97.bb100_crit_edge
	xorl	%eax, %eax
	jmp	.LBB65_35	# bb100
.LBB65_59:	# bb133.bb147.preheader_crit_edge
	movl	$0, 16(%rsp)
	jmp	.LBB65_48	# bb147.preheader
.LBB65_60:	# bb148
	xorl	%edi, %edi
	leaq	.str85, %rsi
	leaq	.str186, %rdx
	xorb	%al, %al
	call	cblas_xerbla
	jmp	.LBB65_30	# bb79.thread
	.size	cblas_dtpmv, .-cblas_dtpmv
.Leh_func_end46:


	.align	16
	.globl	cblas_dtpsv
	.type	cblas_dtpsv,@function
cblas_dtpsv:
.Leh_func_begin47:
.Llabel47:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$24, %rsp
	cmpl	$113, %edx
	movl	$112, %eax
	cmovne	%edx, %eax
	testl	%r8d, %r8d
	movl	88(%rsp), %edx
	movq	80(%rsp), %r10
	movl	%ecx, 20(%rsp)
	je	.LBB66_18	# bb46.thread
.LBB66_1:	# bb14
	cmpl	$121, %esi
	sete	%cl
	setne	%r11b
	cmpl	$101, %edi
	sete	%bl
	setne	%r14b
	andb	%cl, %bl
	orb	%r11b, %r14b
	testb	%r14b, %r14b
	jne	.LBB66_3	# bb21
.LBB66_2:	# bb14
	cmpl	$111, %eax
	je	.LBB66_5	# bb29
.LBB66_3:	# bb21
	cmpl	$122, %esi
	sete	%cl
	setne	%r11b
	cmpl	$102, %edi
	sete	%r14b
	setne	%r15b
	andb	%cl, %r14b
	orb	%r11b, %r15b
	testb	%r15b, %r15b
	jne	.LBB66_19	# bb48
.LBB66_4:	# bb21
	cmpl	$112, %eax
	jne	.LBB66_19	# bb48
.LBB66_5:	# bb29
	testl	%edx, %edx
	jg	.LBB66_66	# bb29.bb32_crit_edge
.LBB66_6:	# bb30
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
.LBB66_7:	# bb32
	leal	-1(%r8), %ecx
	movl	%ecx, %esi
	imull	%edx, %esi
	addl	%eax, %esi
	cmpl	$131, 20(%rsp)
	jne	.LBB66_9	# bb34
.LBB66_8:	# bb33
	leal	(%r8,%r8), %edi
	leal	-2(%r8), %r11d
	subl	%r11d, %edi
	imull	%ecx, %edi
	movl	%edi, %ecx
	shrl	$31, %ecx
	addl	%edi, %ecx
	sarl	%ecx
	movslq	%ecx, %rcx
	movslq	%esi, %rsi
	movsd	(%r10,%rsi,8), %xmm0
	divsd	(%r9,%rcx,8), %xmm0
	movsd	%xmm0, (%r10,%rsi,8)
.LBB66_9:	# bb34
	leal	-1(%r8), %ecx
	imull	%edx, %ecx
	movl	%ecx, 12(%rsp)
	leal	-2(%r8), %ecx
	imull	%edx, %ecx
	movl	%ecx, 8(%rsp)
	leal	3(%r8), %ecx
	movl	%ecx, 16(%rsp)
	xorl	%ecx, %ecx
	movl	%r8d, %esi
	jmp	.LBB66_16	# bb42
.LBB66_10:	# bb35
	cmpl	%r8d, %ebx
	movslq	%r11d, %r11
	movsd	(%r10,%r11,8), %xmm0
	jge	.LBB66_13	# bb38
.LBB66_11:	# bb.nph212
	movl	16(%rsp), %ebx
	leal	(%rbx,%rcx), %ebx
	imull	%edi, %ebx
	movl	%ebx, %r14d
	shrl	$31, %r14d
	addl	%ebx, %r14d
	sarl	%r14d
	incl	%r14d
	movl	12(%rsp), %ebx
	leal	(%rbx,%rax), %ebx
	leal	1(%rcx), %r15d
	xorl	%r12d, %r12d
	.align	16
.LBB66_12:	# bb36
	leal	(%r14,%r12), %r13d
	movslq	%r13d, %r13
	movslq	%ebx, %rbp
	movsd	(%r10,%rbp,8), %xmm1
	mulsd	(%r9,%r13,8), %xmm1
	subsd	%xmm1, %xmm0
	addl	%edx, %ebx
	incl	%r12d
	cmpl	%r15d, %r12d
	jne	.LBB66_12	# bb36
.LBB66_13:	# bb38
	cmpl	$131, 20(%rsp)
	jne	.LBB66_15	# bb42.backedge
.LBB66_14:	# bb39
	movl	16(%rsp), %ebx
	leal	(%rbx,%rcx), %ebx
	imull	%edi, %ebx
	movl	%ebx, %edi
	shrl	$31, %edi
	addl	%ebx, %edi
	sarl	%edi
	movslq	%edi, %rdi
	divsd	(%r9,%rdi,8), %xmm0
.LBB66_15:	# bb42.backedge
	movsd	%xmm0, (%r10,%r11,8)
	subl	%edx, %eax
	decl	%esi
	incl	%ecx
.LBB66_16:	# bb42
	movl	8(%rsp), %edi
	leal	(%rdi,%rax), %r11d
	leal	-1(%rsi), %ebx
	testl	%ebx, %ebx
	jle	.LBB66_18	# bb46.thread
.LBB66_17:	# bb43
	leal	-2(%rsi), %edi
	cmpl	$4294967295, %edi
	jne	.LBB66_10	# bb35
.LBB66_18:	# bb46.thread
	addq	$24, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB66_19:	# bb48
	cmpl	$122, %esi
	sete	%cl
	setne	%r11b
	cmpl	$101, %edi
	sete	%r15b
	setne	%r12b
	andb	%cl, %r15b
	orb	%r11b, %r12b
	testb	%r12b, %r12b
	jne	.LBB66_21	# bb56
.LBB66_20:	# bb48
	cmpl	$111, %eax
	je	.LBB66_23	# bb64
.LBB66_21:	# bb56
	cmpl	$121, %esi
	sete	%cl
	setne	%sil
	cmpl	$102, %edi
	sete	%dil
	setne	%r11b
	andb	%cl, %dil
	orb	%sil, %r11b
	testb	%r11b, %r11b
	jne	.LBB66_36	# bb81
.LBB66_22:	# bb56
	cmpl	$112, %eax
	jne	.LBB66_36	# bb81
.LBB66_23:	# bb64
	testl	%edx, %edx
	jg	.LBB66_67	# bb64.bb67_crit_edge
.LBB66_24:	# bb65
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
.LBB66_25:	# bb67
	cmpl	$131, 20(%rsp)
	jne	.LBB66_27	# bb80.preheader
.LBB66_26:	# bb68
	movslq	%eax, %rcx
	movsd	(%r10,%rcx,8), %xmm0
	divsd	(%r9), %xmm0
	movsd	%xmm0, (%r10,%rcx,8)
.LBB66_27:	# bb80.preheader
	cmpl	$2, %r8d
	jl	.LBB66_18	# bb46.thread
.LBB66_28:	# bb.nph204
	movl	$1, %ecx
	subl	%r8d, %ecx
	imull	%edx, %ecx
	addl	%edx, %eax
	decl	%r8d
	xorl	%esi, %esi
	.align	16
.LBB66_29:	# bb70
	testl	%edx, %edx
	movl	$0, %edi
	cmovle	%ecx, %edi
	movslq	%eax, %rax
	movsd	(%r10,%rax,8), %xmm0
	leal	1(%rsi), %r11d
	testl	%r11d, %r11d
	jle	.LBB66_68	# bb70.bb76_crit_edge
.LBB66_30:	# bb.nph197
	leal	2(%rsi), %ebx
	imull	%r11d, %ebx
	movl	%ebx, %r14d
	shrl	$31, %r14d
	addl	%ebx, %r14d
	sarl	%r14d
	xorl	%ebx, %ebx
	.align	16
.LBB66_31:	# bb74
	leal	(%r14,%rbx), %r15d
	movslq	%r15d, %r15
	movslq	%edi, %r12
	movsd	(%r10,%r12,8), %xmm1
	mulsd	(%r9,%r15,8), %xmm1
	subsd	%xmm1, %xmm0
	addl	%edx, %edi
	incl	%ebx
	cmpl	%r11d, %ebx
	jne	.LBB66_31	# bb74
.LBB66_32:	# bb76.loopexit
	leal	1(%rsi), %edi
.LBB66_33:	# bb76
	cmpl	$131, 20(%rsp)
	jne	.LBB66_35	# bb79
.LBB66_34:	# bb77
	leal	2(%rsi), %ebx
	imull	%r11d, %ebx
	movl	%ebx, %r11d
	shrl	$31, %r11d
	addl	%ebx, %r11d
	sarl	%r11d
	addl	%edi, %r11d
	movslq	%r11d, %rdi
	divsd	(%r9,%rdi,8), %xmm0
.LBB66_35:	# bb79
	movsd	%xmm0, (%r10,%rax,8)
	addl	%edx, %eax
	incl	%esi
	cmpl	%r8d, %esi
	je	.LBB66_18	# bb46.thread
	jmp	.LBB66_29	# bb70
.LBB66_36:	# bb81
	cmpl	$111, %eax
	sete	%cl
	andb	%cl, %r14b
	cmpl	$112, %eax
	sete	%cl
	testb	%cl, %bl
	jne	.LBB66_38	# bb97
.LBB66_37:	# bb81
	notb	%r14b
	testb	$1, %r14b
	jne	.LBB66_50	# bb114
.LBB66_38:	# bb97
	testl	%edx, %edx
	jg	.LBB66_69	# bb97.bb100_crit_edge
.LBB66_39:	# bb98
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
.LBB66_40:	# bb100
	cmpl	$131, 20(%rsp)
	jne	.LBB66_42	# bb113.preheader
.LBB66_41:	# bb101
	movslq	%eax, %rcx
	movsd	(%r10,%rcx,8), %xmm0
	divsd	(%r9), %xmm0
	movsd	%xmm0, (%r10,%rcx,8)
.LBB66_42:	# bb113.preheader
	cmpl	$2, %r8d
	jl	.LBB66_18	# bb46.thread
.LBB66_43:	# bb.nph193
	movl	$1, %ecx
	subl	%r8d, %ecx
	imull	%edx, %ecx
	addl	%edx, %eax
	leal	(%r8,%r8), %esi
	leal	-1(%r8), %edi
	movl	%edi, 16(%rsp)
	xorl	%edi, %edi
	.align	16
.LBB66_44:	# bb103
	testl	%edx, %edx
	movl	$0, %r11d
	cmovle	%ecx, %r11d
	movslq	%eax, %rax
	movsd	(%r10,%rax,8), %xmm0
	leal	1(%rdi), %ebx
	testl	%ebx, %ebx
	jle	.LBB66_47	# bb109
.LBB66_45:	# bb107.preheader
	leal	(%r8,%r8), %r14d
	xorl	%r15d, %r15d
	movl	$1, %r12d
	.align	16
.LBB66_46:	# bb107
	leal	(%r14,%r12), %r13d
	imull	%r15d, %r13d
	movl	%r13d, %ebp
	shrl	$31, %ebp
	addl	%r13d, %ebp
	sarl	%ebp
	leal	(%rdi,%r12), %r13d
	addl	%ebp, %r13d
	movslq	%r13d, %r13
	movslq	%r11d, %rbp
	movsd	(%r10,%rbp,8), %xmm1
	mulsd	(%r9,%r13,8), %xmm1
	subsd	%xmm1, %xmm0
	addl	%edx, %r11d
	decl	%r12d
	incl	%r15d
	cmpl	%ebx, %r15d
	jne	.LBB66_46	# bb107
.LBB66_47:	# bb109
	cmpl	$131, 20(%rsp)
	jne	.LBB66_49	# bb112
.LBB66_48:	# bb110
	imull	%esi, %ebx
	movl	%ebx, %r11d
	shrl	$31, %r11d
	addl	%ebx, %r11d
	sarl	%r11d
	movslq	%r11d, %r11
	divsd	(%r9,%r11,8), %xmm0
.LBB66_49:	# bb112
	movsd	%xmm0, (%r10,%rax,8)
	addl	%edx, %eax
	decl	%esi
	incl	%edi
	cmpl	16(%rsp), %edi
	je	.LBB66_18	# bb46.thread
	jmp	.LBB66_44	# bb103
.LBB66_50:	# bb114
	cmpl	$111, %eax
	sete	%cl
	andb	%cl, %dil
	cmpl	$112, %eax
	sete	%al
	testb	%al, %r15b
	jne	.LBB66_52	# bb130
.LBB66_51:	# bb114
	notb	%dil
	testb	$1, %dil
	jne	.LBB66_65	# bb149
.LBB66_52:	# bb130
	testl	%edx, %edx
	jg	.LBB66_70	# bb130.bb133_crit_edge
.LBB66_53:	# bb131
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
	movl	%eax, 16(%rsp)
.LBB66_54:	# bb133
	leal	-1(%r8), %eax
	movl	%eax, %ecx
	imull	%edx, %ecx
	addl	16(%rsp), %ecx
	cmpl	$131, 20(%rsp)
	jne	.LBB66_56	# bb143.preheader
.LBB66_55:	# bb134
	movl	%eax, %esi
	imull	%r8d, %esi
	movl	%esi, %edi
	shrl	$31, %edi
	addl	%esi, %edi
	sarl	%edi
	addl	%eax, %edi
	movslq	%edi, %rax
	movslq	%ecx, %rcx
	movsd	(%r10,%rcx,8), %xmm0
	divsd	(%r9,%rax,8), %xmm0
	movsd	%xmm0, (%r10,%rcx,8)
.LBB66_56:	# bb143.preheader
	leal	-1(%r8), %eax
	imull	%edx, %eax
	movl	%eax, 12(%rsp)
	leal	-2(%r8), %eax
	imull	%edx, %eax
	movl	%eax, 8(%rsp)
	movl	$1, %eax
	movl	%r8d, %ecx
	jmp	.LBB66_63	# bb143
.LBB66_57:	# bb136
	cmpl	%r8d, %esi
	movslq	%edi, %rdi
	movsd	(%r10,%rdi,8), %xmm0
	jge	.LBB66_60	# bb139
.LBB66_58:	# bb.nph
	movl	16(%rsp), %r14d
	movl	12(%rsp), %ebx
	leal	(%rbx,%r14), %ebx
	leal	-2(%rcx), %r14d
	leal	-1(%rcx), %r15d
	xorl	%r12d, %r12d
	.align	16
.LBB66_59:	# bb137
	leal	(%r15,%r12), %r13d
	leal	(%rcx,%r12), %ebp
	imull	%r13d, %ebp
	movl	%ebp, %r13d
	shrl	$31, %r13d
	addl	%ebp, %r13d
	sarl	%r13d
	addl	%r14d, %r13d
	movslq	%r13d, %r13
	movslq	%ebx, %rbp
	movsd	(%r10,%rbp,8), %xmm1
	mulsd	(%r9,%r13,8), %xmm1
	subsd	%xmm1, %xmm0
	addl	%edx, %ebx
	incl	%r12d
	cmpl	%eax, %r12d
	jne	.LBB66_59	# bb137
.LBB66_60:	# bb139
	cmpl	$131, 20(%rsp)
	jne	.LBB66_62	# bb143.backedge
.LBB66_61:	# bb140
	imull	%esi, %r11d
	movl	%r11d, %esi
	shrl	$31, %esi
	addl	%r11d, %esi
	sarl	%esi
	leal	-2(%rcx,%rsi), %esi
	movslq	%esi, %rsi
	divsd	(%r9,%rsi,8), %xmm0
.LBB66_62:	# bb143.backedge
	movsd	%xmm0, (%r10,%rdi,8)
	subl	%edx, 16(%rsp)
	decl	%ecx
	incl	%eax
.LBB66_63:	# bb143
	movl	16(%rsp), %esi
	movl	8(%rsp), %edi
	leal	(%rdi,%rsi), %edi
	leal	-1(%rcx), %esi
	testl	%esi, %esi
	jle	.LBB66_18	# bb46.thread
.LBB66_64:	# bb144
	leal	-2(%rcx), %r11d
	cmpl	$4294967295, %r11d
	jne	.LBB66_57	# bb136
	jmp	.LBB66_18	# bb46.thread
.LBB66_65:	# bb149
	xorl	%edi, %edi
	leaq	.str87, %rsi
	leaq	.str188, %rdx
	xorb	%al, %al
	call	cblas_xerbla
	jmp	.LBB66_18	# bb46.thread
.LBB66_66:	# bb29.bb32_crit_edge
	xorl	%eax, %eax
	jmp	.LBB66_7	# bb32
.LBB66_67:	# bb64.bb67_crit_edge
	xorl	%eax, %eax
	jmp	.LBB66_25	# bb67
.LBB66_68:	# bb70.bb76_crit_edge
	xorl	%edi, %edi
	jmp	.LBB66_33	# bb76
.LBB66_69:	# bb97.bb100_crit_edge
	xorl	%eax, %eax
	jmp	.LBB66_40	# bb100
.LBB66_70:	# bb130.bb133_crit_edge
	movl	$0, 16(%rsp)
	jmp	.LBB66_54	# bb133
	.size	cblas_dtpsv, .-cblas_dtpsv
.Leh_func_end47:


	.align	16
	.globl	cblas_dtrmm
	.type	cblas_dtrmm,@function
cblas_dtrmm:
.Leh_func_begin48:
.Llabel48:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$24, %rsp
	cmpl	$101, %edi
	movl	112(%rsp), %eax
	movq	104(%rsp), %rdi
	movl	96(%rsp), %r10d
	movq	88(%rsp), %r11
	movl	80(%rsp), %ebx
	movl	%r8d, 16(%rsp)
	je	.LBB67_112	# bb
.LBB67_1:	# bb11
	cmpl	$121, %edx
	movl	$122, %r8d
	movl	$121, %edx
	cmove	%r8d, %edx
	cmpl	$141, %esi
	movl	$142, %r8d
	movl	$141, %esi
	cmove	%r8d, %esi
	cmpl	$113, %ecx
	movl	$112, %r8d
	cmovne	%ecx, %r8d
	movl	%r9d, 20(%rsp)
	movl	%ebx, %r9d
.LBB67_2:	# bb21
	cmpl	$121, %edx
	sete	%cl
	setne	%bl
	cmpl	$141, %esi
	sete	%r14b
	setne	%r15b
	andb	%cl, %r14b
	orb	%bl, %r15b
	testb	%r15b, %r15b
	jne	.LBB67_15	# bb40
.LBB67_3:	# bb21
	cmpl	$111, %r8d
	jne	.LBB67_15	# bb40
.LBB67_4:	# bb39.preheader
	testl	%r9d, %r9d
	jle	.LBB67_111	# bb62.thread
.LBB67_5:	# bb.nph301
	cmpl	$0, 20(%rsp)
	jle	.LBB67_111	# bb62.thread
.LBB67_6:	# bb37.preheader.preheader
	incl	%r10d
	leal	-1(%r9), %r8d
	xorl	%esi, %esi
	movl	%esi, %ecx
	movl	%esi, 8(%rsp)
	jmp	.LBB67_14	# bb37.preheader
	.align	16
.LBB67_7:	# bb30
	cmpl	$131, 16(%rsp)
	je	.LBB67_113	# bb31
.LBB67_8:	# bb32
	leal	(%rcx,%rbx), %r15d
	movslq	%r15d, %r15
	movsd	(%rdi,%r15,8), %xmm1
.LBB67_9:	# bb35.preheader
	cmpl	%r9d, %r14d
	jge	.LBB67_12	# bb36
.LBB67_10:	# bb.nph296
	movl	12(%rsp), %r15d
	leal	(%r15,%rbx), %r15d
	xorl	%r12d, %r12d
	.align	16
.LBB67_11:	# bb34
	leal	(%rax,%r15), %r13d
	leal	(%rsi,%r12), %ebp
	incl	%r12d
	cmpl	%r8d, %r12d
	movslq	%ebp, %rbp
	movsd	(%r11,%rbp,8), %xmm2
	movslq	%r15d, %r15
	mulsd	(%rdi,%r15,8), %xmm2
	addsd	%xmm2, %xmm1
	movl	%r13d, %r15d
	jne	.LBB67_11	# bb34
.LBB67_12:	# bb36
	leal	(%rcx,%rbx), %r15d
	movslq	%r15d, %r15
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, (%rdi,%r15,8)
	incl	%ebx
	cmpl	20(%rsp), %ebx
	jne	.LBB67_7	# bb30
.LBB67_13:	# bb38
	movl	%edx, %esi
	addl	%r10d, %esi
	addl	%eax, %ecx
	decl	%r8d
	movl	8(%rsp), %ebx
	incl	%ebx
	movl	%ebx, 8(%rsp)
	cmpl	%r9d, %ebx
	je	.LBB67_111	# bb62.thread
.LBB67_14:	# bb37.preheader
	leal	(%rax,%rcx), %ebx
	movl	%ebx, 12(%rsp)
	movslq	%esi, %rdx
	incl	%esi
	movl	8(%rsp), %ebx
	leal	1(%rbx), %r14d
	xorl	%ebx, %ebx
	jmp	.LBB67_7	# bb30
.LBB67_15:	# bb40
	cmpl	$112, %r8d
	setne	%cl
	notb	%r14b
	orb	%cl, %r14b
	testb	$1, %r14b
	jne	.LBB67_29	# bb64
.LBB67_16:	# bb58.preheader
	leal	-1(%r9), %r8d
	movl	%eax, %ecx
	imull	%r8d, %ecx
	movl	$4294967295, %edx
	subl	%r10d, %edx
	movl	%edx, 8(%rsp)
	leal	1(%r10), %edx
	imull	%r8d, %edx
	movl	%edx, 12(%rsp)
	.align	16
.LBB67_17:	# bb58
	testl	%r9d, %r9d
	jle	.LBB67_111	# bb62.thread
.LBB67_18:	# bb59
	leal	-1(%r9), %r8d
	testl	%r9d, %r9d
	je	.LBB67_111	# bb62.thread
.LBB67_19:	# bb57.preheader
	cmpl	$0, 20(%rsp)
	jle	.LBB67_27	# bb58.loopexit
.LBB67_20:	# bb.nph291
	leal	-1(%r9), %esi
	movslq	12(%rsp), %r15
	xorl	%edx, %edx
	.align	16
.LBB67_21:	# bb52.preheader
	testl	%r8d, %r8d
	jle	.LBB67_28	# bb52.preheader.bb53_crit_edge
.LBB67_22:	# bb52.preheader.bb51_crit_edge
	pxor	%xmm1, %xmm1
	xorl	%r12d, %r12d
	movl	%esi, %ebx
	movl	%edx, %r14d
	.align	16
.LBB67_23:	# bb51
	leal	(%r10,%rbx), %r13d
	leal	(%rax,%r14), %ebp
	incl	%r12d
	cmpl	%esi, %r12d
	movslq	%ebx, %rbx
	movsd	(%r11,%rbx,8), %xmm2
	movslq	%r14d, %rbx
	mulsd	(%rdi,%rbx,8), %xmm2
	addsd	%xmm2, %xmm1
	movl	%r13d, %ebx
	movl	%ebp, %r14d
	jne	.LBB67_23	# bb51
.LBB67_24:	# bb53
	cmpl	$131, 16(%rsp)
	je	.LBB67_114	# bb54
.LBB67_25:	# bb55
	leal	(%rcx,%rdx), %ebx
	movslq	%ebx, %rbx
	movsd	(%rdi,%rbx,8), %xmm2
.LBB67_26:	# bb56
	addsd	%xmm1, %xmm2
	mulsd	%xmm0, %xmm2
	leal	(%rcx,%rdx), %ebx
	movslq	%ebx, %rbx
	movsd	%xmm2, (%rdi,%rbx,8)
	incl	%edx
	cmpl	20(%rsp), %edx
	jne	.LBB67_21	# bb52.preheader
.LBB67_27:	# bb58.loopexit
	movl	12(%rsp), %r8d
	addl	8(%rsp), %r8d
	movl	%r8d, 12(%rsp)
	subl	%eax, %ecx
	decl	%r9d
	jmp	.LBB67_17	# bb58
.LBB67_28:	# bb52.preheader.bb53_crit_edge
	pxor	%xmm1, %xmm1
	jmp	.LBB67_24	# bb53
.LBB67_29:	# bb64
	cmpl	$122, %edx
	sete	%cl
	setne	%bl
	cmpl	$141, %esi
	sete	%r14b
	setne	%r15b
	andb	%cl, %r14b
	orb	%bl, %r15b
	testb	%r15b, %r15b
	jne	.LBB67_44	# bb88
.LBB67_30:	# bb64
	cmpl	$111, %r8d
	jne	.LBB67_44	# bb88
.LBB67_31:	# bb82.preheader
	leal	-1(%r9), %r8d
	movl	%eax, %ecx
	imull	%r8d, %ecx
	movl	$4294967295, %edx
	subl	%r10d, %edx
	movl	%edx, 8(%rsp)
	movl	%r10d, %edx
	imull	%r8d, %edx
	leal	1(%r10), %esi
	imull	%r8d, %esi
	movl	%esi, 12(%rsp)
	.align	16
.LBB67_32:	# bb82
	testl	%r9d, %r9d
	jle	.LBB67_111	# bb62.thread
.LBB67_33:	# bb83
	leal	-1(%r9), %r12d
	testl	%r9d, %r9d
	je	.LBB67_111	# bb62.thread
.LBB67_34:	# bb81.preheader
	cmpl	$0, 20(%rsp)
	jle	.LBB67_42	# bb82.loopexit
.LBB67_35:	# bb.nph285
	leal	-1(%r9), %r8d
	movslq	12(%rsp), %rsi
	xorl	%ebx, %ebx
	.align	16
.LBB67_36:	# bb76.preheader
	testl	%r12d, %r12d
	jle	.LBB67_43	# bb76.preheader.bb77_crit_edge
.LBB67_37:	# bb76.preheader.bb75_crit_edge
	pxor	%xmm1, %xmm1
	xorl	%r14d, %r14d
	movl	%ebx, %r15d
	.align	16
.LBB67_38:	# bb75
	leal	(%rax,%r15), %r13d
	leal	(%rdx,%r14), %ebp
	incl	%r14d
	cmpl	%r8d, %r14d
	movslq	%ebp, %rbp
	movsd	(%r11,%rbp,8), %xmm2
	movslq	%r15d, %r15
	mulsd	(%rdi,%r15,8), %xmm2
	addsd	%xmm2, %xmm1
	movl	%r13d, %r15d
	jne	.LBB67_38	# bb75
.LBB67_39:	# bb77
	cmpl	$131, 16(%rsp)
	je	.LBB67_115	# bb78
.LBB67_40:	# bb79
	leal	(%rcx,%rbx), %r14d
	movslq	%r14d, %r14
	movsd	(%rdi,%r14,8), %xmm2
.LBB67_41:	# bb80
	addsd	%xmm1, %xmm2
	mulsd	%xmm0, %xmm2
	leal	(%rcx,%rbx), %r14d
	movslq	%r14d, %r14
	movsd	%xmm2, (%rdi,%r14,8)
	incl	%ebx
	cmpl	20(%rsp), %ebx
	jne	.LBB67_36	# bb76.preheader
.LBB67_42:	# bb82.loopexit
	movl	12(%rsp), %r8d
	addl	8(%rsp), %r8d
	movl	%r8d, 12(%rsp)
	subl	%r10d, %edx
	subl	%eax, %ecx
	decl	%r9d
	jmp	.LBB67_32	# bb82
.LBB67_43:	# bb76.preheader.bb77_crit_edge
	pxor	%xmm1, %xmm1
	jmp	.LBB67_39	# bb77
.LBB67_44:	# bb88
	cmpl	$112, %r8d
	setne	%cl
	notb	%r14b
	orb	%cl, %r14b
	testb	$1, %r14b
	jne	.LBB67_56	# bb108
.LBB67_45:	# bb107.preheader
	testl	%r9d, %r9d
	jle	.LBB67_111	# bb62.thread
.LBB67_46:	# bb.nph279
	cmpl	$0, 20(%rsp)
	jle	.LBB67_111	# bb62.thread
.LBB67_47:	# bb105.preheader.preheader
	leal	1(%r10), %r8d
	movl	%r8d, (%rsp)
	leal	-1(%r9), %ecx
	xorl	%ebx, %ebx
	movl	%ebx, %edx
	movl	%ebx, 4(%rsp)
	jmp	.LBB67_55	# bb105.preheader
	.align	16
.LBB67_48:	# bb98
	cmpl	$131, 16(%rsp)
	je	.LBB67_116	# bb99
.LBB67_49:	# bb100
	leal	(%rdx,%rbx), %r14d
	movslq	%r14d, %r14
	movsd	(%rdi,%r14,8), %xmm1
.LBB67_50:	# bb103.preheader
	cmpl	%r9d, %r8d
	jge	.LBB67_53	# bb104
.LBB67_51:	# bb.nph274
	movl	8(%rsp), %r14d
	leal	(%r14,%rbx), %r14d
	xorl	%r15d, %r15d
	movl	12(%rsp), %r12d
	.align	16
.LBB67_52:	# bb102
	leal	(%r10,%r12), %r13d
	leal	(%rax,%r14), %ebp
	incl	%r15d
	cmpl	%ecx, %r15d
	movslq	%r12d, %r12
	movsd	(%r11,%r12,8), %xmm2
	movslq	%r14d, %r14
	mulsd	(%rdi,%r14,8), %xmm2
	addsd	%xmm2, %xmm1
	movl	%r13d, %r12d
	movl	%ebp, %r14d
	jne	.LBB67_52	# bb102
.LBB67_53:	# bb104
	leal	(%rdx,%rbx), %r14d
	movslq	%r14d, %r14
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, (%rdi,%r14,8)
	incl	%ebx
	cmpl	20(%rsp), %ebx
	jne	.LBB67_48	# bb98
.LBB67_54:	# bb106
	movl	%esi, %ebx
	addl	(%rsp), %ebx
	addl	%eax, %edx
	decl	%ecx
	movl	4(%rsp), %r8d
	incl	%r8d
	movl	%r8d, 4(%rsp)
	cmpl	%r9d, %r8d
	je	.LBB67_111	# bb62.thread
.LBB67_55:	# bb105.preheader
	leal	(%r10,%rbx), %r8d
	movl	%r8d, 12(%rsp)
	leal	(%rax,%rdx), %r8d
	movl	%r8d, 8(%rsp)
	movl	4(%rsp), %r8d
	leal	1(%r8), %r8d
	movslq	%ebx, %rsi
	xorl	%ebx, %ebx
	jmp	.LBB67_48	# bb98
.LBB67_56:	# bb108
	cmpl	$121, %edx
	sete	%cl
	setne	%bl
	cmpl	$142, %esi
	sete	%r14b
	setne	%r15b
	andb	%cl, %r14b
	orb	%bl, %r15b
	testb	%r15b, %r15b
	jne	.LBB67_71	# bb133
.LBB67_57:	# bb108
	cmpl	$111, %r8d
	jne	.LBB67_71	# bb133
.LBB67_58:	# bb132.preheader
	testl	%r9d, %r9d
	jle	.LBB67_111	# bb62.thread
.LBB67_59:	# bb132.preheader.bb125.preheader_crit_edge
	xorl	%r8d, %r8d
	movl	%r8d, 12(%rsp)
	jmp	.LBB67_70	# bb125.preheader
.LBB67_60:	# bb.nph266
	leal	-1(%rcx), %r14d
	pxor	%xmm1, %xmm1
	xorl	%r15d, %r15d
	movl	%r14d, %r12d
	.align	16
.LBB67_61:	# bb119
	leal	(%r8,%r15), %r13d
	movslq	%r13d, %r13
	movslq	%r12d, %rbp
	movsd	(%r11,%rbp,8), %xmm2
	mulsd	(%rdi,%r13,8), %xmm2
	addsd	%xmm2, %xmm1
	addl	%r10d, %r12d
	incl	%r15d
	cmpl	%r14d, %r15d
	jne	.LBB67_61	# bb119
.LBB67_62:	# bb121
	cmpl	$131, 16(%rsp)
	je	.LBB67_117	# bb122
.LBB67_63:	# bb123
	leal	(%rbx,%rcx), %r14d
	movslq	%r14d, %r14
	movsd	(%rdi,%r14,8), %xmm2
.LBB67_64:	# bb124
	addsd	%xmm1, %xmm2
	mulsd	%xmm0, %xmm2
	leal	(%rbx,%rcx), %r14d
	movslq	%r14d, %r14
	movsd	%xmm2, (%rdi,%r14,8)
	addl	%esi, %edx
	decl	%ecx
.LBB67_65:	# bb125
	testl	%ecx, %ecx
	jle	.LBB67_69	# bb131
.LBB67_66:	# bb126
	leal	-1(%rcx), %r14d
	testl	%ecx, %ecx
	je	.LBB67_69	# bb131
.LBB67_67:	# bb120.preheader
	testl	%r14d, %r14d
	jg	.LBB67_60	# bb.nph266
.LBB67_68:	# bb120.preheader.bb121_crit_edge
	pxor	%xmm1, %xmm1
	jmp	.LBB67_62	# bb121
.LBB67_69:	# bb131
	addl	%eax, %r8d
	movl	12(%rsp), %ecx
	incl	%ecx
	movl	%ecx, 12(%rsp)
	cmpl	%r9d, %ecx
	je	.LBB67_111	# bb62.thread
.LBB67_70:	# bb125.preheader
	movl	$4294967295, %esi
	subl	%r10d, %esi
	leal	1(%r10), %ecx
	movl	20(%rsp), %r14d
	leal	-1(%r14), %edx
	imull	%ecx, %edx
	leal	-1(%r8), %ebx
	movl	%r14d, %ecx
	jmp	.LBB67_65	# bb125
.LBB67_71:	# bb133
	cmpl	$112, %r8d
	setne	%cl
	notb	%r14b
	orb	%cl, %r14b
	testb	$1, %r14b
	jne	.LBB67_83	# bb153
.LBB67_72:	# bb152.preheader
	testl	%r9d, %r9d
	jle	.LBB67_111	# bb62.thread
.LBB67_73:	# bb.nph263
	cmpl	$0, 20(%rsp)
	jle	.LBB67_111	# bb62.thread
.LBB67_74:	# bb.nph263.bb150.preheader_crit_edge
	xorl	%esi, %esi
	movl	%esi, 8(%rsp)
	jmp	.LBB67_82	# bb150.preheader
	.align	16
.LBB67_75:	# bb143
	cmpl	$131, 16(%rsp)
	je	.LBB67_118	# bb144
.LBB67_76:	# bb145
	leal	(%rsi,%rdx), %r14d
	movslq	%r14d, %r14
	movsd	(%rdi,%r14,8), %xmm1
.LBB67_77:	# bb148.preheader
	leal	1(%rdx), %r14d
	cmpl	20(%rsp), %r14d
	jge	.LBB67_80	# bb149
.LBB67_78:	# bb.nph258
	leal	(%r8,%rdx), %r14d
	leal	1(%rcx), %r15d
	xorl	%r12d, %r12d
	.align	16
.LBB67_79:	# bb147
	leal	(%r14,%r12), %r13d
	movslq	%r13d, %r13
	leal	(%r15,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r11,%rbp,8), %xmm2
	mulsd	(%rdi,%r13,8), %xmm2
	addsd	%xmm2, %xmm1
	incl	%r12d
	cmpl	%ebx, %r12d
	jne	.LBB67_79	# bb147
.LBB67_80:	# bb149
	leal	(%rsi,%rdx), %r14d
	movslq	%r14d, %r14
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, (%rdi,%r14,8)
	addl	12(%rsp), %ecx
	decl	%ebx
	incl	%edx
	cmpl	20(%rsp), %edx
	jne	.LBB67_75	# bb143
.LBB67_81:	# bb151
	addl	%eax, %esi
	movl	8(%rsp), %r8d
	incl	%r8d
	movl	%r8d, 8(%rsp)
	cmpl	%r9d, %r8d
	je	.LBB67_111	# bb62.thread
.LBB67_82:	# bb150.preheader
	leal	1(%r10), %r8d
	movl	%r8d, 12(%rsp)
	movl	20(%rsp), %r8d
	leal	-1(%r8), %ebx
	leal	1(%rsi), %r8d
	xorl	%ecx, %ecx
	movl	%ecx, %edx
	jmp	.LBB67_75	# bb143
.LBB67_83:	# bb153
	cmpl	$122, %edx
	sete	%cl
	setne	%dl
	cmpl	$142, %esi
	sete	%sil
	setne	%bl
	andb	%cl, %sil
	orb	%dl, %bl
	testb	%bl, %bl
	jne	.LBB67_96	# bb173
.LBB67_84:	# bb153
	cmpl	$111, %r8d
	jne	.LBB67_96	# bb173
.LBB67_85:	# bb172.preheader
	testl	%r9d, %r9d
	jle	.LBB67_111	# bb62.thread
.LBB67_86:	# bb.nph253
	cmpl	$0, 20(%rsp)
	jle	.LBB67_111	# bb62.thread
.LBB67_87:	# bb.nph253.bb170.preheader_crit_edge
	xorl	%ebx, %ebx
	movl	%ebx, 8(%rsp)
	jmp	.LBB67_95	# bb170.preheader
	.align	16
.LBB67_88:	# bb163
	cmpl	$131, 16(%rsp)
	je	.LBB67_119	# bb164
.LBB67_89:	# bb165
	leal	(%rbx,%rcx), %r14d
	movslq	%r14d, %r14
	movsd	(%rdi,%r14,8), %xmm1
.LBB67_90:	# bb168.preheader
	leal	1(%rcx), %r14d
	cmpl	20(%rsp), %r14d
	jge	.LBB67_93	# bb169
.LBB67_91:	# bb.nph248
	leal	(%r10,%rsi), %r14d
	leal	(%r8,%rcx), %r15d
	xorl	%r12d, %r12d
	.align	16
.LBB67_92:	# bb167
	leal	(%r15,%r12), %r13d
	movslq	%r13d, %r13
	movslq	%r14d, %rbp
	movsd	(%r11,%rbp,8), %xmm2
	mulsd	(%rdi,%r13,8), %xmm2
	addsd	%xmm2, %xmm1
	addl	%r10d, %r14d
	incl	%r12d
	cmpl	%edx, %r12d
	jne	.LBB67_92	# bb167
.LBB67_93:	# bb169
	leal	(%rbx,%rcx), %r14d
	movslq	%r14d, %r14
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, (%rdi,%r14,8)
	addl	12(%rsp), %esi
	decl	%edx
	incl	%ecx
	cmpl	20(%rsp), %ecx
	jne	.LBB67_88	# bb163
.LBB67_94:	# bb171
	addl	%eax, %ebx
	movl	8(%rsp), %r8d
	incl	%r8d
	movl	%r8d, 8(%rsp)
	cmpl	%r9d, %r8d
	je	.LBB67_111	# bb62.thread
.LBB67_95:	# bb170.preheader
	leal	1(%r10), %r8d
	movl	%r8d, 12(%rsp)
	movl	20(%rsp), %r8d
	leal	-1(%r8), %edx
	leal	1(%rbx), %r8d
	xorl	%esi, %esi
	movl	%esi, %ecx
	jmp	.LBB67_88	# bb163
.LBB67_96:	# bb173
	cmpl	$112, %r8d
	setne	%cl
	notb	%sil
	orb	%cl, %sil
	testb	$1, %sil
	jne	.LBB67_110	# bb198
.LBB67_97:	# bb197.preheader
	testl	%r9d, %r9d
	jle	.LBB67_111	# bb62.thread
.LBB67_98:	# bb197.preheader.bb190.preheader_crit_edge
	xorl	%ecx, %ecx
	movl	%ecx, 12(%rsp)
	jmp	.LBB67_109	# bb190.preheader
.LBB67_99:	# bb.nph
	leal	-1(%rsi), %r15d
	pxor	%xmm1, %xmm1
	xorl	%r12d, %r12d
	.align	16
.LBB67_100:	# bb184
	leal	(%rcx,%r12), %r13d
	movslq	%r13d, %r13
	leal	(%rbx,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r11,%rbp,8), %xmm2
	mulsd	(%rdi,%r13,8), %xmm2
	addsd	%xmm2, %xmm1
	incl	%r12d
	cmpl	%r15d, %r12d
	jne	.LBB67_100	# bb184
.LBB67_101:	# bb186
	cmpl	$131, 16(%rsp)
	je	.LBB67_120	# bb187
.LBB67_102:	# bb188
	leal	(%r8,%rsi), %r15d
	movslq	%r15d, %r15
	movsd	(%rdi,%r15,8), %xmm2
.LBB67_103:	# bb189
	addsd	%xmm1, %xmm2
	mulsd	%xmm0, %xmm2
	leal	(%r8,%rsi), %r15d
	movslq	%r15d, %r15
	movsd	%xmm2, (%rdi,%r15,8)
	addl	%r14d, %edx
	subl	%r10d, %ebx
	decl	%esi
.LBB67_104:	# bb190
	testl	%esi, %esi
	jle	.LBB67_108	# bb196
.LBB67_105:	# bb191
	leal	-1(%rsi), %r15d
	testl	%esi, %esi
	je	.LBB67_108	# bb196
.LBB67_106:	# bb185.preheader
	testl	%r15d, %r15d
	jg	.LBB67_99	# bb.nph
.LBB67_107:	# bb185.preheader.bb186_crit_edge
	pxor	%xmm1, %xmm1
	jmp	.LBB67_101	# bb186
.LBB67_108:	# bb196
	addl	%eax, %ecx
	movl	12(%rsp), %edx
	incl	%edx
	movl	%edx, 12(%rsp)
	cmpl	%r9d, %edx
	je	.LBB67_111	# bb62.thread
.LBB67_109:	# bb190.preheader
	movl	$4294967295, %r14d
	subl	%r10d, %r14d
	movl	20(%rsp), %esi
	leal	-1(%rsi), %r8d
	movl	%r10d, %ebx
	imull	%r8d, %ebx
	leal	1(%r10), %edx
	imull	%r8d, %edx
	leal	-1(%rcx), %r8d
	jmp	.LBB67_104	# bb190
.LBB67_110:	# bb198
	xorl	%edi, %edi
	leaq	.str89, %rsi
	leaq	.str190, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB67_111:	# bb62.thread
	addq	$24, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB67_112:	# bb
	cmpl	$113, %ecx
	movl	$112, %r8d
	cmovne	%ecx, %r8d
	movl	%ebx, 20(%rsp)
	jmp	.LBB67_2	# bb21
.LBB67_113:	# bb31
	leal	(%rcx,%rbx), %r15d
	movslq	%r15d, %r15
	movsd	(%r11,%rdx,8), %xmm1
	mulsd	(%rdi,%r15,8), %xmm1
	jmp	.LBB67_9	# bb35.preheader
.LBB67_114:	# bb54
	leal	(%rcx,%rdx), %ebx
	movslq	%ebx, %rbx
	movsd	(%r11,%r15,8), %xmm2
	mulsd	(%rdi,%rbx,8), %xmm2
	jmp	.LBB67_26	# bb56
.LBB67_115:	# bb78
	leal	(%rcx,%rbx), %r14d
	movslq	%r14d, %r14
	movsd	(%r11,%rsi,8), %xmm2
	mulsd	(%rdi,%r14,8), %xmm2
	jmp	.LBB67_41	# bb80
.LBB67_116:	# bb99
	leal	(%rdx,%rbx), %r14d
	movslq	%r14d, %r14
	movsd	(%r11,%rsi,8), %xmm1
	mulsd	(%rdi,%r14,8), %xmm1
	jmp	.LBB67_50	# bb103.preheader
.LBB67_117:	# bb122
	leal	(%rbx,%rcx), %r14d
	movslq	%r14d, %r14
	movslq	%edx, %r15
	movsd	(%r11,%r15,8), %xmm2
	mulsd	(%rdi,%r14,8), %xmm2
	jmp	.LBB67_64	# bb124
.LBB67_118:	# bb144
	leal	(%rsi,%rdx), %r14d
	movslq	%r14d, %r14
	movslq	%ecx, %r15
	movsd	(%r11,%r15,8), %xmm1
	mulsd	(%rdi,%r14,8), %xmm1
	jmp	.LBB67_77	# bb148.preheader
.LBB67_119:	# bb164
	leal	(%rbx,%rcx), %r14d
	movslq	%r14d, %r14
	movslq	%esi, %r15
	movsd	(%r11,%r15,8), %xmm1
	mulsd	(%rdi,%r14,8), %xmm1
	jmp	.LBB67_90	# bb168.preheader
.LBB67_120:	# bb187
	leal	(%r8,%rsi), %r15d
	movslq	%r15d, %r15
	movslq	%edx, %r12
	movsd	(%r11,%r12,8), %xmm2
	mulsd	(%rdi,%r15,8), %xmm2
	jmp	.LBB67_103	# bb189
	.size	cblas_dtrmm, .-cblas_dtrmm
.Leh_func_end48:


	.align	16
	.globl	cblas_dtrmv
	.type	cblas_dtrmv,@function
cblas_dtrmv:
.Leh_func_begin49:
.Llabel49:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$24, %rsp
	cmpl	$113, %edx
	movl	$112, %eax
	cmovne	%edx, %eax
	cmpl	$121, %esi
	sete	%dl
	setne	%r10b
	cmpl	$101, %edi
	sete	%r11b
	setne	%bl
	andb	%dl, %r11b
	orb	%r10b, %bl
	testb	%bl, %bl
	movl	96(%rsp), %edx
	movq	88(%rsp), %r10
	movl	80(%rsp), %ebx
	movl	%ecx, 20(%rsp)
	jne	.LBB68_2	# bb24
.LBB68_1:	# entry
	cmpl	$111, %eax
	je	.LBB68_4	# bb32
.LBB68_2:	# bb24
	cmpl	$122, %esi
	sete	%cl
	setne	%r14b
	cmpl	$102, %edi
	sete	%r15b
	setne	%r12b
	andb	%cl, %r15b
	orb	%r14b, %r12b
	testb	%r12b, %r12b
	jne	.LBB68_14	# bb47
.LBB68_3:	# bb24
	cmpl	$112, %eax
	jne	.LBB68_14	# bb47
.LBB68_4:	# bb32
	testl	%edx, %edx
	jg	.LBB68_55	# bb32.bb46.preheader_crit_edge
.LBB68_5:	# bb33
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
	movl	%eax, 16(%rsp)
.LBB68_6:	# bb46.preheader
	testl	%r8d, %r8d
	jle	.LBB68_29	# bb81.thread
.LBB68_7:	# bb.nph199
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
	movl	%eax, 12(%rsp)
	leal	-1(%r8), %eax
	incl	%ebx
	xorl	%ecx, %ecx
	movl	%ecx, %esi
	movl	%ecx, %edi
	.align	16
.LBB68_8:	# bb36
	testl	%edx, %edx
	movl	$0, %r11d
	cmovle	12(%rsp), %r11d
	movl	16(%rsp), %r14d
	leal	(%r14,%rcx), %r14d
	leal	1(%rdi), %r15d
	cmpl	%r8d, %r15d
	jge	.LBB68_56	# bb36.bb42_crit_edge
.LBB68_9:	# bb.nph195
	leal	(%rdx,%rcx), %r15d
	leal	1(%rsi), %r12d
	pxor	%xmm0, %xmm0
	xorl	%r13d, %r13d
	.align	16
.LBB68_10:	# bb40
	leal	(%r12,%r13), %ebp
	movslq	%ebp, %rbp
	addl	%r11d, %r15d
	movslq	%r15d, %r11
	movsd	(%r10,%r11,8), %xmm1
	mulsd	(%r9,%rbp,8), %xmm1
	addsd	%xmm1, %xmm0
	incl	%r13d
	cmpl	%eax, %r13d
	movl	%edx, %r11d
	jne	.LBB68_10	# bb40
.LBB68_11:	# bb42
	movslq	%r14d, %r11
	movsd	(%r10,%r11,8), %xmm1
	cmpl	$131, 20(%rsp)
	jne	.LBB68_13	# bb45
.LBB68_12:	# bb43
	movslq	%esi, %r14
	mulsd	(%r9,%r14,8), %xmm1
.LBB68_13:	# bb45
	addsd	%xmm0, %xmm1
	movsd	%xmm1, (%r10,%r11,8)
	addl	%edx, %ecx
	addl	%ebx, %esi
	decl	%eax
	incl	%edi
	cmpl	%r8d, %edi
	jne	.LBB68_8	# bb36
	jmp	.LBB68_29	# bb81.thread
.LBB68_14:	# bb47
	cmpl	$122, %esi
	sete	%cl
	setne	%r14b
	cmpl	$101, %edi
	sete	%r12b
	setne	%r13b
	andb	%cl, %r12b
	orb	%r14b, %r13b
	testb	%r13b, %r13b
	jne	.LBB68_16	# bb55
.LBB68_15:	# bb47
	cmpl	$111, %eax
	je	.LBB68_18	# bb63
.LBB68_16:	# bb55
	cmpl	$121, %esi
	sete	%cl
	setne	%sil
	cmpl	$102, %edi
	sete	%dil
	setne	%r14b
	andb	%cl, %dil
	orb	%sil, %r14b
	testb	%r14b, %r14b
	jne	.LBB68_30	# bb83
.LBB68_17:	# bb55
	cmpl	$112, %eax
	jne	.LBB68_30	# bb83
.LBB68_18:	# bb63
	testl	%edx, %edx
	jg	.LBB68_57	# bb63.bb66_crit_edge
.LBB68_19:	# bb64
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
.LBB68_20:	# bb66
	movl	$1, %ecx
	subl	%r8d, %ecx
	imull	%edx, %ecx
	leal	-1(%r8), %esi
	movl	%edx, %edi
	imull	%esi, %edi
	movl	$4294967295, %r11d
	subl	%ebx, %r11d
	movl	%ebx, %r14d
	imull	%esi, %r14d
	leal	1(%rbx), %r15d
	imull	%esi, %r15d
	addl	%eax, %edi
	jmp	.LBB68_27	# bb77
.LBB68_21:	# bb67
	testl	%edx, %edx
	movl	$0, %esi
	cmovle	%ecx, %esi
	testl	%eax, %eax
	jle	.LBB68_58	# bb67.bb73_crit_edge
.LBB68_22:	# bb.nph187
	leal	-1(%r8), %eax
	pxor	%xmm0, %xmm0
	xorl	%r12d, %r12d
	.align	16
.LBB68_23:	# bb71
	leal	(%r14,%r12), %r13d
	movslq	%r13d, %r13
	movslq	%esi, %rbp
	movsd	(%r10,%rbp,8), %xmm1
	mulsd	(%r9,%r13,8), %xmm1
	addsd	%xmm1, %xmm0
	addl	%edx, %esi
	incl	%r12d
	cmpl	%eax, %r12d
	jne	.LBB68_23	# bb71
.LBB68_24:	# bb73
	movslq	%edi, %rax
	movsd	(%r10,%rax,8), %xmm1
	cmpl	$131, 20(%rsp)
	jne	.LBB68_26	# bb76
.LBB68_25:	# bb74
	movslq	%r15d, %rsi
	mulsd	(%r9,%rsi,8), %xmm1
.LBB68_26:	# bb76
	addsd	%xmm0, %xmm1
	movsd	%xmm1, (%r10,%rax,8)
	addl	%r11d, %r15d
	subl	%edx, %edi
	subl	%ebx, %r14d
	decl	%r8d
.LBB68_27:	# bb77
	testl	%r8d, %r8d
	jle	.LBB68_29	# bb81.thread
.LBB68_28:	# bb78
	leal	-1(%r8), %eax
	testl	%r8d, %r8d
	jne	.LBB68_21	# bb67
.LBB68_29:	# bb81.thread
	addq	$24, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB68_30:	# bb83
	cmpl	$111, %eax
	sete	%cl
	andb	%cl, %r15b
	cmpl	$112, %eax
	sete	%cl
	testb	%cl, %r11b
	jne	.LBB68_32	# bb99
.LBB68_31:	# bb83
	notb	%r15b
	testb	$1, %r15b
	jne	.LBB68_43	# bb119
.LBB68_32:	# bb99
	testl	%edx, %edx
	jg	.LBB68_59	# bb99.bb102_crit_edge
.LBB68_33:	# bb100
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
.LBB68_34:	# bb102
	movl	$1, %ecx
	subl	%r8d, %ecx
	imull	%edx, %ecx
	leal	-1(%r8), %esi
	movl	%edx, %edi
	imull	%esi, %edi
	movl	$4294967295, %r11d
	subl	%ebx, %r11d
	leal	1(%rbx), %r14d
	imull	%esi, %r14d
	addl	%eax, %edi
	jmp	.LBB68_41	# bb113
.LBB68_35:	# bb103
	testl	%edx, %edx
	movl	$0, %esi
	cmovle	%ecx, %esi
	testl	%eax, %eax
	jle	.LBB68_60	# bb103.bb109_crit_edge
.LBB68_36:	# bb.nph182
	leal	-1(%r8), %eax
	pxor	%xmm0, %xmm0
	xorl	%r15d, %r15d
	movl	%eax, %r12d
	.align	16
.LBB68_37:	# bb107
	leal	(%rdx,%rsi), %r13d
	leal	(%rbx,%r12), %ebp
	incl	%r15d
	cmpl	%eax, %r15d
	movslq	%esi, %rsi
	movsd	(%r10,%rsi,8), %xmm1
	movslq	%r12d, %rsi
	mulsd	(%r9,%rsi,8), %xmm1
	addsd	%xmm1, %xmm0
	movl	%r13d, %esi
	movl	%ebp, %r12d
	jne	.LBB68_37	# bb107
.LBB68_38:	# bb109
	movslq	%edi, %rax
	movsd	(%r10,%rax,8), %xmm1
	cmpl	$131, 20(%rsp)
	jne	.LBB68_40	# bb112
.LBB68_39:	# bb110
	movslq	%r14d, %rsi
	mulsd	(%r9,%rsi,8), %xmm1
.LBB68_40:	# bb112
	addsd	%xmm0, %xmm1
	movsd	%xmm1, (%r10,%rax,8)
	addl	%r11d, %r14d
	subl	%edx, %edi
	decl	%r8d
.LBB68_41:	# bb113
	testl	%r8d, %r8d
	jle	.LBB68_29	# bb81.thread
.LBB68_42:	# bb114
	leal	-1(%r8), %eax
	testl	%r8d, %r8d
	jne	.LBB68_35	# bb103
	jmp	.LBB68_29	# bb81.thread
.LBB68_43:	# bb119
	cmpl	$111, %eax
	sete	%cl
	andb	%cl, %dil
	cmpl	$112, %eax
	sete	%al
	testb	%al, %r12b
	jne	.LBB68_45	# bb135
.LBB68_44:	# bb119
	notb	%dil
	testb	$1, %dil
	jne	.LBB68_63	# bb150
.LBB68_45:	# bb135
	testl	%edx, %edx
	jg	.LBB68_61	# bb135.bb149.preheader_crit_edge
.LBB68_46:	# bb136
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
	movl	%eax, 12(%rsp)
.LBB68_47:	# bb149.preheader
	testl	%r8d, %r8d
	jle	.LBB68_29	# bb81.thread
.LBB68_48:	# bb.nph178
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
	movl	%eax, 8(%rsp)
	leal	-1(%r8), %eax
	leal	1(%rbx), %ecx
	movl	%ecx, 16(%rsp)
	xorl	%ecx, %ecx
	movl	%ecx, %esi
	movl	%ecx, %edi
	.align	16
.LBB68_49:	# bb139
	testl	%edx, %edx
	movl	$0, %r11d
	cmovle	8(%rsp), %r11d
	movl	12(%rsp), %r14d
	leal	(%r14,%rcx), %r14d
	leal	1(%rdi), %r15d
	cmpl	%r8d, %r15d
	jge	.LBB68_62	# bb139.bb145_crit_edge
.LBB68_50:	# bb.nph
	leal	(%rdx,%rcx), %r15d
	leal	(%rbx,%rsi), %r12d
	pxor	%xmm0, %xmm0
	xorl	%r13d, %r13d
	.align	16
.LBB68_51:	# bb143
	leal	(%rbx,%r12), %ebp
	addl	%r11d, %r15d
	incl	%r13d
	cmpl	%eax, %r13d
	movslq	%r15d, %r11
	movsd	(%r10,%r11,8), %xmm1
	movslq	%r12d, %r11
	mulsd	(%r9,%r11,8), %xmm1
	addsd	%xmm1, %xmm0
	movl	%ebp, %r12d
	movl	%edx, %r11d
	jne	.LBB68_51	# bb143
.LBB68_52:	# bb145
	movslq	%r14d, %r11
	movsd	(%r10,%r11,8), %xmm1
	cmpl	$131, 20(%rsp)
	jne	.LBB68_54	# bb148
.LBB68_53:	# bb146
	movslq	%esi, %r14
	mulsd	(%r9,%r14,8), %xmm1
.LBB68_54:	# bb148
	addsd	%xmm0, %xmm1
	movsd	%xmm1, (%r10,%r11,8)
	addl	%edx, %ecx
	addl	16(%rsp), %esi
	decl	%eax
	incl	%edi
	cmpl	%r8d, %edi
	je	.LBB68_29	# bb81.thread
	jmp	.LBB68_49	# bb139
.LBB68_55:	# bb32.bb46.preheader_crit_edge
	movl	$0, 16(%rsp)
	jmp	.LBB68_6	# bb46.preheader
.LBB68_56:	# bb36.bb42_crit_edge
	pxor	%xmm0, %xmm0
	jmp	.LBB68_11	# bb42
.LBB68_57:	# bb63.bb66_crit_edge
	xorl	%eax, %eax
	jmp	.LBB68_20	# bb66
.LBB68_58:	# bb67.bb73_crit_edge
	pxor	%xmm0, %xmm0
	jmp	.LBB68_24	# bb73
.LBB68_59:	# bb99.bb102_crit_edge
	xorl	%eax, %eax
	jmp	.LBB68_34	# bb102
.LBB68_60:	# bb103.bb109_crit_edge
	pxor	%xmm0, %xmm0
	jmp	.LBB68_38	# bb109
.LBB68_61:	# bb135.bb149.preheader_crit_edge
	movl	$0, 12(%rsp)
	jmp	.LBB68_47	# bb149.preheader
.LBB68_62:	# bb139.bb145_crit_edge
	pxor	%xmm0, %xmm0
	jmp	.LBB68_52	# bb145
.LBB68_63:	# bb150
	xorl	%edi, %edi
	leaq	.str91, %rsi
	leaq	.str192, %rdx
	xorb	%al, %al
	call	cblas_xerbla
	jmp	.LBB68_29	# bb81.thread
	.size	cblas_dtrmv, .-cblas_dtrmv
.Leh_func_end49:


	.section	.rodata.cst8,"aM",@progbits,8
	.align	8
.LCPI69_0:					
	.quad	4607182418800017408	# double value: 1.000000e+00
	.text
	.align	16
	.globl	cblas_dtrsm
	.type	cblas_dtrsm,@function
cblas_dtrsm:
.Leh_func_begin50:
.Llabel50:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$8, %rsp
	cmpl	$101, %edi
	movq	88(%rsp), %rax
	movl	80(%rsp), %edi
	movq	72(%rsp), %r10
	movl	64(%rsp), %r11d
	movl	%r8d, 4(%rsp)
	je	.LBB69_170	# bb
.LBB69_1:	# bb15
	cmpl	$121, %edx
	movl	$122, %r8d
	movl	$121, %edx
	cmove	%r8d, %edx
	cmpl	$141, %esi
	movl	$142, %r8d
	movl	$141, %esi
	cmove	%r8d, %esi
	cmpl	$113, %ecx
	movl	$112, %r8d
	cmovne	%ecx, %r8d
	movl	%r9d, %ecx
	movl	%r11d, %r9d
.LBB69_2:	# bb25
	cmpl	$121, %edx
	sete	%r11b
	setne	%bl
	cmpl	$111, %r8d
	sete	%r14b
	setne	%r15b
	andb	%r11b, %r14b
	orb	%bl, %r15b
	testb	%r15b, %r15b
	jne	.LBB69_25	# bb56
.LBB69_3:	# bb25
	cmpl	$141, %esi
	jne	.LBB69_25	# bb56
.LBB69_4:	# bb32
	ucomisd	.LCPI69_0(%rip), %xmm0
	setnp	%r14b
	sete	%dl
	testb	%r14b, %dl
	jne	.LBB69_171	# bb50.preheader
.LBB69_5:	# bb32
	testl	%r9d, %r9d
	jle	.LBB69_171	# bb50.preheader
.LBB69_6:	# bb32
	testl	%ecx, %ecx
	jle	.LBB69_171	# bb50.preheader
.LBB69_7:	# bb32.bb36.preheader_crit_edge
	xorl	%r8d, %r8d
	movl	%r8d, %edx
	jmp	.LBB69_10	# bb36.preheader
	.align	16
.LBB69_8:	# bb35
	leal	(%r8,%rsi), %r11d
	movslq	%r11d, %r11
	movapd	%xmm0, %xmm1
	mulsd	(%rax,%r11,8), %xmm1
	movsd	%xmm1, (%rax,%r11,8)
	incl	%esi
	cmpl	%ecx, %esi
	jne	.LBB69_8	# bb35
.LBB69_9:	# bb37
	addl	96(%rsp), %r8d
	incl	%edx
	cmpl	%r9d, %edx
	je	.LBB69_171	# bb50.preheader
.LBB69_10:	# bb36.preheader
	xorl	%esi, %esi
	jmp	.LBB69_8	# bb35
.LBB69_11:	# bb40
	cmpl	$131, 4(%rsp)
	jne	.LBB69_15	# bb49.preheader
.LBB69_12:	# bb41
	movslq	%edx, %r11
	movsd	(%r10,%r11,8), %xmm0
	testl	%ecx, %ecx
	jle	.LBB69_15	# bb49.preheader
.LBB69_13:	# bb41.bb42_crit_edge
	xorl	%r11d, %r11d
	.align	16
.LBB69_14:	# bb42
	leal	(%r8,%r11), %ebx
	movslq	%ebx, %rbx
	movsd	(%rax,%rbx,8), %xmm1
	divsd	%xmm0, %xmm1
	movsd	%xmm1, (%rax,%rbx,8)
	incl	%r11d
	cmpl	%ecx, %r11d
	jne	.LBB69_14	# bb42
.LBB69_15:	# bb49.preheader
	testl	%esi, %esi
	jle	.LBB69_21	# bb50.loopexit289
.LBB69_16:	# bb.nph378
	testl	%ecx, %ecx
	jle	.LBB69_21	# bb50.loopexit289
.LBB69_17:	# bb.nph378.split
	leal	-1(%r9), %esi
	xorl	%r11d, %r11d
	movl	%esi, %ebx
	movl	%r11d, %r14d
	.align	16
.LBB69_18:	# bb45
	movslq	%ebx, %r15
	movsd	(%r10,%r15,8), %xmm0
	xorl	%r15d, %r15d
	.align	16
.LBB69_19:	# bb46
	leal	(%r8,%r15), %r12d
	movslq	%r12d, %r12
	movapd	%xmm0, %xmm1
	mulsd	(%rax,%r12,8), %xmm1
	leal	(%r11,%r15), %r12d
	movslq	%r12d, %r12
	movsd	(%rax,%r12,8), %xmm2
	subsd	%xmm1, %xmm2
	movsd	%xmm2, (%rax,%r12,8)
	incl	%r15d
	cmpl	%ecx, %r15d
	jne	.LBB69_19	# bb46
.LBB69_20:	# bb48
	addl	%edi, %ebx
	addl	96(%rsp), %r11d
	incl	%r14d
	cmpl	%esi, %r14d
	jne	.LBB69_18	# bb45
.LBB69_21:	# bb50.loopexit289
	addl	(%rsp), %edx
	subl	96(%rsp), %r8d
	decl	%r9d
.LBB69_22:	# bb50
	testl	%r9d, %r9d
	jle	.LBB69_24	# bb54.thread
.LBB69_23:	# bb51
	leal	-1(%r9), %esi
	testl	%r9d, %r9d
	jne	.LBB69_11	# bb40
.LBB69_24:	# bb54.thread
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB69_25:	# bb56
	cmpl	$121, %edx
	sete	%r11b
	setne	%bl
	cmpl	$112, %r8d
	sete	%r15b
	setne	%r12b
	andb	%r11b, %r15b
	orb	%bl, %r12b
	testb	%r12b, %r12b
	jne	.LBB69_47	# bb84
.LBB69_26:	# bb56
	cmpl	$141, %esi
	jne	.LBB69_47	# bb84
.LBB69_27:	# bb64
	ucomisd	.LCPI69_0(%rip), %xmm0
	setnp	%r14b
	sete	%r15b
	testb	%r14b, %r15b
	jne	.LBB69_34	# bb83.preheader
.LBB69_28:	# bb64
	testl	%r9d, %r9d
	jle	.LBB69_34	# bb83.preheader
.LBB69_29:	# bb64
	testl	%ecx, %ecx
	jle	.LBB69_34	# bb83.preheader
.LBB69_30:	# bb64.bb68.preheader_crit_edge
	xorl	%r8d, %r8d
	movl	%r8d, %edx
	.align	16
.LBB69_31:	# bb68.preheader
	xorl	%esi, %esi
	.align	16
.LBB69_32:	# bb67
	leal	(%r8,%rsi), %r11d
	movslq	%r11d, %r11
	movapd	%xmm0, %xmm1
	mulsd	(%rax,%r11,8), %xmm1
	movsd	%xmm1, (%rax,%r11,8)
	incl	%esi
	cmpl	%ecx, %esi
	jne	.LBB69_32	# bb67
.LBB69_33:	# bb69
	addl	96(%rsp), %r8d
	incl	%edx
	cmpl	%r9d, %edx
	jne	.LBB69_31	# bb68.preheader
.LBB69_34:	# bb83.preheader
	testl	%r9d, %r9d
	jle	.LBB69_24	# bb54.thread
.LBB69_35:	# bb.nph368
	leal	-1(%r9), %r8d
	incl	%edi
	xorl	%edx, %edx
	movl	%edx, %esi
	movl	%edx, %r11d
	.align	16
.LBB69_36:	# bb72
	cmpl	$131, 4(%rsp)
	jne	.LBB69_40	# bb81.preheader
.LBB69_37:	# bb73
	movslq	%edx, %rbx
	movsd	(%r10,%rbx,8), %xmm0
	testl	%ecx, %ecx
	jle	.LBB69_40	# bb81.preheader
.LBB69_38:	# bb73.bb74_crit_edge
	xorl	%ebx, %ebx
	.align	16
.LBB69_39:	# bb74
	leal	(%rsi,%rbx), %r14d
	movslq	%r14d, %r14
	movsd	(%rax,%r14,8), %xmm1
	divsd	%xmm0, %xmm1
	movsd	%xmm1, (%rax,%r14,8)
	incl	%ebx
	cmpl	%ecx, %ebx
	jne	.LBB69_39	# bb74
.LBB69_40:	# bb81.preheader
	leal	1(%r11), %ebx
	cmpl	%r9d, %ebx
	jge	.LBB69_46	# bb82
.LBB69_41:	# bb.nph366
	testl	%ecx, %ecx
	jle	.LBB69_46	# bb82
.LBB69_42:	# bb.nph366.split
	movl	96(%rsp), %ebx
	leal	(%rbx,%rsi), %ebx
	leal	1(%rdx), %r14d
	xorl	%r15d, %r15d
	.align	16
.LBB69_43:	# bb77
	leal	(%r14,%r15), %r12d
	movslq	%r12d, %r12
	movsd	(%r10,%r12,8), %xmm0
	xorl	%r12d, %r12d
	.align	16
.LBB69_44:	# bb78
	leal	(%rsi,%r12), %r13d
	movslq	%r13d, %r13
	movapd	%xmm0, %xmm1
	mulsd	(%rax,%r13,8), %xmm1
	leal	(%rbx,%r12), %r13d
	movslq	%r13d, %r13
	movsd	(%rax,%r13,8), %xmm2
	subsd	%xmm1, %xmm2
	movsd	%xmm2, (%rax,%r13,8)
	incl	%r12d
	cmpl	%ecx, %r12d
	jne	.LBB69_44	# bb78
.LBB69_45:	# bb81.loopexit
	addl	96(%rsp), %ebx
	incl	%r15d
	cmpl	%r8d, %r15d
	jne	.LBB69_43	# bb77
.LBB69_46:	# bb82
	addl	%edi, %edx
	addl	96(%rsp), %esi
	decl	%r8d
	incl	%r11d
	cmpl	%r9d, %r11d
	je	.LBB69_24	# bb54.thread
	jmp	.LBB69_36	# bb72
.LBB69_47:	# bb84
	cmpl	$122, %edx
	sete	%r11b
	setne	%bl
	cmpl	$111, %r8d
	sete	%r12b
	setne	%r13b
	andb	%r11b, %r12b
	orb	%bl, %r13b
	testb	%r13b, %r13b
	jne	.LBB69_69	# bb112
.LBB69_48:	# bb84
	cmpl	$141, %esi
	jne	.LBB69_69	# bb112
.LBB69_49:	# bb92
	ucomisd	.LCPI69_0(%rip), %xmm0
	setnp	%r14b
	sete	%r15b
	testb	%r14b, %r15b
	jne	.LBB69_56	# bb111.preheader
.LBB69_50:	# bb92
	testl	%r9d, %r9d
	jle	.LBB69_56	# bb111.preheader
.LBB69_51:	# bb92
	testl	%ecx, %ecx
	jle	.LBB69_56	# bb111.preheader
.LBB69_52:	# bb92.bb96.preheader_crit_edge
	xorl	%r8d, %r8d
	movl	%r8d, %edx
	.align	16
.LBB69_53:	# bb96.preheader
	xorl	%esi, %esi
	.align	16
.LBB69_54:	# bb95
	leal	(%r8,%rsi), %r11d
	movslq	%r11d, %r11
	movapd	%xmm0, %xmm1
	mulsd	(%rax,%r11,8), %xmm1
	movsd	%xmm1, (%rax,%r11,8)
	incl	%esi
	cmpl	%ecx, %esi
	jne	.LBB69_54	# bb95
.LBB69_55:	# bb97
	addl	96(%rsp), %r8d
	incl	%edx
	cmpl	%r9d, %edx
	jne	.LBB69_53	# bb96.preheader
.LBB69_56:	# bb111.preheader
	testl	%r9d, %r9d
	jle	.LBB69_24	# bb54.thread
.LBB69_57:	# bb.nph354
	leal	-1(%r9), %r8d
	leal	1(%rdi), %edx
	movl	%edx, (%rsp)
	xorl	%edx, %edx
	movl	%edx, %esi
	movl	%edx, %r11d
	.align	16
.LBB69_58:	# bb100
	cmpl	$131, 4(%rsp)
	jne	.LBB69_62	# bb109.preheader
.LBB69_59:	# bb101
	movslq	%edx, %rbx
	movsd	(%r10,%rbx,8), %xmm0
	testl	%ecx, %ecx
	jle	.LBB69_62	# bb109.preheader
.LBB69_60:	# bb101.bb102_crit_edge
	xorl	%ebx, %ebx
	.align	16
.LBB69_61:	# bb102
	leal	(%rsi,%rbx), %r14d
	movslq	%r14d, %r14
	movsd	(%rax,%r14,8), %xmm1
	divsd	%xmm0, %xmm1
	movsd	%xmm1, (%rax,%r14,8)
	incl	%ebx
	cmpl	%ecx, %ebx
	jne	.LBB69_61	# bb102
.LBB69_62:	# bb109.preheader
	leal	1(%r11), %ebx
	cmpl	%r9d, %ebx
	jge	.LBB69_68	# bb110
.LBB69_63:	# bb.nph352
	testl	%ecx, %ecx
	jle	.LBB69_68	# bb110
.LBB69_64:	# bb.nph352.split
	leal	(%rdi,%rdx), %ebx
	movl	96(%rsp), %r14d
	leal	(%r14,%rsi), %r14d
	xorl	%r15d, %r15d
	.align	16
.LBB69_65:	# bb105
	movslq	%ebx, %r12
	movsd	(%r10,%r12,8), %xmm0
	xorl	%r12d, %r12d
	.align	16
.LBB69_66:	# bb106
	leal	(%rsi,%r12), %r13d
	movslq	%r13d, %r13
	movapd	%xmm0, %xmm1
	mulsd	(%rax,%r13,8), %xmm1
	leal	(%r14,%r12), %r13d
	movslq	%r13d, %r13
	movsd	(%rax,%r13,8), %xmm2
	subsd	%xmm1, %xmm2
	movsd	%xmm2, (%rax,%r13,8)
	incl	%r12d
	cmpl	%ecx, %r12d
	jne	.LBB69_66	# bb106
.LBB69_67:	# bb109.loopexit
	addl	%edi, %ebx
	addl	96(%rsp), %r14d
	incl	%r15d
	cmpl	%r8d, %r15d
	jne	.LBB69_65	# bb105
.LBB69_68:	# bb110
	addl	(%rsp), %edx
	addl	96(%rsp), %esi
	decl	%r8d
	incl	%r11d
	cmpl	%r9d, %r11d
	je	.LBB69_24	# bb54.thread
	jmp	.LBB69_58	# bb100
.LBB69_69:	# bb112
	cmpl	$122, %edx
	sete	%dl
	setne	%r11b
	cmpl	$112, %r8d
	sete	%r8b
	setne	%bl
	andb	%dl, %r8b
	orb	%r11b, %bl
	testb	%bl, %bl
	jne	.LBB69_91	# bb144
.LBB69_70:	# bb112
	cmpl	$141, %esi
	jne	.LBB69_91	# bb144
.LBB69_71:	# bb120
	ucomisd	.LCPI69_0(%rip), %xmm0
	setnp	%r8b
	sete	%r14b
	testb	%r8b, %r14b
	jne	.LBB69_172	# bb138.preheader
.LBB69_72:	# bb120
	testl	%r9d, %r9d
	jle	.LBB69_172	# bb138.preheader
.LBB69_73:	# bb120
	testl	%ecx, %ecx
	jle	.LBB69_172	# bb138.preheader
.LBB69_74:	# bb120.bb124.preheader_crit_edge
	xorl	%edx, %edx
	movl	%edx, %esi
	jmp	.LBB69_77	# bb124.preheader
	.align	16
.LBB69_75:	# bb123
	leal	(%rdx,%r8), %r11d
	movslq	%r11d, %r11
	movapd	%xmm0, %xmm1
	mulsd	(%rax,%r11,8), %xmm1
	movsd	%xmm1, (%rax,%r11,8)
	incl	%r8d
	cmpl	%ecx, %r8d
	jne	.LBB69_75	# bb123
.LBB69_76:	# bb125
	addl	96(%rsp), %edx
	incl	%esi
	cmpl	%r9d, %esi
	je	.LBB69_172	# bb138.preheader
.LBB69_77:	# bb124.preheader
	xorl	%r8d, %r8d
	jmp	.LBB69_75	# bb123
.LBB69_78:	# bb128
	cmpl	$131, 4(%rsp)
	jne	.LBB69_82	# bb137.preheader
.LBB69_79:	# bb129
	movslq	%edx, %r14
	movsd	(%r10,%r14,8), %xmm0
	testl	%ecx, %ecx
	jle	.LBB69_82	# bb137.preheader
.LBB69_80:	# bb129.bb130_crit_edge
	xorl	%r14d, %r14d
	.align	16
.LBB69_81:	# bb130
	leal	(%r11,%r14), %r15d
	movslq	%r15d, %r15
	movsd	(%rax,%r15,8), %xmm1
	divsd	%xmm0, %xmm1
	movsd	%xmm1, (%rax,%r15,8)
	incl	%r14d
	cmpl	%ecx, %r14d
	jne	.LBB69_81	# bb130
.LBB69_82:	# bb137.preheader
	testl	%esi, %esi
	jle	.LBB69_88	# bb138.loopexit291
.LBB69_83:	# bb.nph340
	testl	%ecx, %ecx
	jle	.LBB69_88	# bb138.loopexit291
.LBB69_84:	# bb.nph340.split
	leal	-1(%r9), %esi
	xorl	%r14d, %r14d
	movl	%r14d, %r15d
	.align	16
.LBB69_85:	# bb133
	leal	(%r8,%r15), %r12d
	movslq	%r12d, %r12
	movsd	(%r10,%r12,8), %xmm0
	xorl	%r12d, %r12d
	.align	16
.LBB69_86:	# bb134
	leal	(%r11,%r12), %r13d
	movslq	%r13d, %r13
	movapd	%xmm0, %xmm1
	mulsd	(%rax,%r13,8), %xmm1
	leal	(%r14,%r12), %r13d
	movslq	%r13d, %r13
	movsd	(%rax,%r13,8), %xmm2
	subsd	%xmm1, %xmm2
	movsd	%xmm2, (%rax,%r13,8)
	incl	%r12d
	cmpl	%ecx, %r12d
	jne	.LBB69_86	# bb134
.LBB69_87:	# bb136
	addl	96(%rsp), %r14d
	incl	%r15d
	cmpl	%esi, %r15d
	jne	.LBB69_85	# bb133
.LBB69_88:	# bb138.loopexit291
	addl	%ebx, %edx
	subl	%edi, %r8d
	subl	96(%rsp), %r11d
	decl	%r9d
.LBB69_89:	# bb138
	testl	%r9d, %r9d
	jle	.LBB69_24	# bb54.thread
.LBB69_90:	# bb139
	leal	-1(%r9), %esi
	testl	%r9d, %r9d
	jne	.LBB69_78	# bb128
	jmp	.LBB69_24	# bb54.thread
.LBB69_91:	# bb144
	cmpl	$142, %esi
	setne	%dl
	notb	%r14b
	orb	%dl, %r14b
	testb	$1, %r14b
	jne	.LBB69_110	# bb170
.LBB69_92:	# bb152
	ucomisd	.LCPI69_0(%rip), %xmm0
	setnp	%r8b
	sete	%r15b
	testb	%r8b, %r15b
	jne	.LBB69_106	# bb169.preheader
.LBB69_93:	# bb152
	testl	%r9d, %r9d
	jle	.LBB69_106	# bb169.preheader
.LBB69_94:	# bb152
	testl	%ecx, %ecx
	jle	.LBB69_106	# bb169.preheader
.LBB69_95:	# bb152.bb156.preheader_crit_edge
	xorl	%edx, %edx
	movl	%edx, %esi
	jmp	.LBB69_98	# bb156.preheader
	.align	16
.LBB69_96:	# bb155
	leal	(%rdx,%r8), %r11d
	movslq	%r11d, %r11
	movapd	%xmm0, %xmm1
	mulsd	(%rax,%r11,8), %xmm1
	movsd	%xmm1, (%rax,%r11,8)
	incl	%r8d
	cmpl	%ecx, %r8d
	jne	.LBB69_96	# bb155
.LBB69_97:	# bb157
	addl	96(%rsp), %edx
	incl	%esi
	cmpl	%r9d, %esi
	je	.LBB69_106	# bb169.preheader
.LBB69_98:	# bb156.preheader
	xorl	%r8d, %r8d
	jmp	.LBB69_96	# bb155
	.align	16
.LBB69_99:	# bb161
	cmpl	$131, 4(%rsp)
	jne	.LBB69_101	# bb163
.LBB69_100:	# bb162
	leal	(%rsi,%r11), %r15d
	movslq	%r15d, %r15
	movsd	(%rax,%r15,8), %xmm0
	movslq	%r8d, %r12
	divsd	(%r10,%r12,8), %xmm0
	movsd	%xmm0, (%rax,%r15,8)
.LBB69_101:	# bb163
	leal	(%rsi,%r11), %r15d
	leal	1(%r11), %r12d
	cmpl	%ecx, %r12d
	movslq	%r15d, %r15
	movsd	(%rax,%r15,8), %xmm0
	jge	.LBB69_104	# bb166
.LBB69_102:	# bb.nph326
	leal	(%r14,%r11), %r15d
	leal	1(%r8), %r12d
	xorl	%r13d, %r13d
	.align	16
.LBB69_103:	# bb164
	leal	(%r12,%r13), %ebp
	movslq	%ebp, %rbp
	movapd	%xmm0, %xmm1
	mulsd	(%r10,%rbp,8), %xmm1
	leal	(%r15,%r13), %ebp
	movslq	%ebp, %rbp
	movsd	(%rax,%rbp,8), %xmm2
	subsd	%xmm1, %xmm2
	movsd	%xmm2, (%rax,%rbp,8)
	incl	%r13d
	cmpl	%ebx, %r13d
	jne	.LBB69_103	# bb164
.LBB69_104:	# bb166
	addl	%edx, %r8d
	decl	%ebx
	incl	%r11d
	cmpl	%ecx, %r11d
	jne	.LBB69_99	# bb161
.LBB69_105:	# bb168
	addl	96(%rsp), %esi
	movl	(%rsp), %edx
	incl	%edx
	movl	%edx, (%rsp)
	cmpl	%r9d, %edx
	je	.LBB69_24	# bb54.thread
	jmp	.LBB69_109	# bb167.preheader
.LBB69_106:	# bb169.preheader
	testl	%r9d, %r9d
	jle	.LBB69_24	# bb54.thread
.LBB69_107:	# bb.nph330
	testl	%ecx, %ecx
	jle	.LBB69_24	# bb54.thread
.LBB69_108:	# bb.nph330.bb167.preheader_crit_edge
	xorl	%esi, %esi
	movl	%esi, (%rsp)
	.align	16
.LBB69_109:	# bb167.preheader
	leal	1(%rdi), %edx
	leal	-1(%rcx), %ebx
	leal	1(%rsi), %r14d
	xorl	%r8d, %r8d
	movl	%r8d, %r11d
	jmp	.LBB69_99	# bb161
.LBB69_110:	# bb170
	cmpl	$142, %esi
	setne	%dl
	notb	%r15b
	orb	%dl, %r15b
	testb	$1, %r15b
	jne	.LBB69_130	# bb200
.LBB69_111:	# bb178
	ucomisd	.LCPI69_0(%rip), %xmm0
	setnp	%r8b
	sete	%r12b
	testb	%r8b, %r12b
	jne	.LBB69_127	# bb199.preheader
.LBB69_112:	# bb178
	testl	%r9d, %r9d
	jle	.LBB69_127	# bb199.preheader
.LBB69_113:	# bb178
	testl	%ecx, %ecx
	jle	.LBB69_127	# bb199.preheader
.LBB69_114:	# bb178.bb182.preheader_crit_edge
	xorl	%edx, %edx
	movl	%edx, %esi
	jmp	.LBB69_117	# bb182.preheader
	.align	16
.LBB69_115:	# bb181
	leal	(%rdx,%r8), %r11d
	movslq	%r11d, %r11
	movapd	%xmm0, %xmm1
	mulsd	(%rax,%r11,8), %xmm1
	movsd	%xmm1, (%rax,%r11,8)
	incl	%r8d
	cmpl	%ecx, %r8d
	jne	.LBB69_115	# bb181
.LBB69_116:	# bb183
	addl	96(%rsp), %edx
	incl	%esi
	cmpl	%r9d, %esi
	je	.LBB69_127	# bb199.preheader
.LBB69_117:	# bb182.preheader
	xorl	%r8d, %r8d
	jmp	.LBB69_115	# bb181
.LBB69_118:	# bb187
	cmpl	$131, 4(%rsp)
	jne	.LBB69_120	# bb189
.LBB69_119:	# bb188
	leal	(%r14,%rsi), %r12d
	movslq	%r12d, %r12
	movsd	(%rax,%r12,8), %xmm0
	movslq	%ebx, %r13
	divsd	(%r10,%r13,8), %xmm0
	movsd	%xmm0, (%rax,%r12,8)
.LBB69_120:	# bb189
	leal	(%r14,%rsi), %r12d
	movslq	%r12d, %r12
	movsd	(%rax,%r12,8), %xmm0
	testl	%r15d, %r15d
	jle	.LBB69_123	# bb192.loopexit
.LBB69_121:	# bb.nph316
	leal	-1(%rsi), %r15d
	xorl	%r12d, %r12d
	movl	%r15d, %r13d
	.align	16
.LBB69_122:	# bb190
	movslq	%r13d, %rbp
	movapd	%xmm0, %xmm1
	mulsd	(%r10,%rbp,8), %xmm1
	leal	(%r11,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%rax,%rbp,8), %xmm2
	subsd	%xmm1, %xmm2
	movsd	%xmm2, (%rax,%rbp,8)
	addl	%edi, %r13d
	incl	%r12d
	cmpl	%r15d, %r12d
	jne	.LBB69_122	# bb190
.LBB69_123:	# bb192.loopexit
	addl	%edx, %ebx
	decl	%esi
.LBB69_124:	# bb192
	testl	%esi, %esi
	jle	.LBB69_126	# bb198
.LBB69_125:	# bb193
	leal	-1(%rsi), %r15d
	testl	%esi, %esi
	jne	.LBB69_118	# bb187
.LBB69_126:	# bb198
	addl	96(%rsp), %r11d
	incl	%r8d
	cmpl	%r9d, %r8d
	je	.LBB69_24	# bb54.thread
	jmp	.LBB69_129	# bb192.preheader
.LBB69_127:	# bb199.preheader
	testl	%r9d, %r9d
	jle	.LBB69_24	# bb54.thread
.LBB69_128:	# bb199.preheader.bb192.preheader_crit_edge
	xorl	%r11d, %r11d
	movl	%r11d, %r8d
	.align	16
.LBB69_129:	# bb192.preheader
	movl	$4294967295, %edx
	subl	%edi, %edx
	leal	1(%rdi), %esi
	leal	-1(%rcx), %ebx
	imull	%esi, %ebx
	leal	-1(%r11), %r14d
	movl	%ecx, %esi
	jmp	.LBB69_124	# bb192
.LBB69_130:	# bb200
	cmpl	$142, %esi
	setne	%dl
	notb	%r12b
	orb	%dl, %r12b
	testb	$1, %r12b
	jne	.LBB69_150	# bb230
.LBB69_131:	# bb208
	ucomisd	.LCPI69_0(%rip), %xmm0
	setnp	%r8b
	sete	%dl
	testb	%r8b, %dl
	jne	.LBB69_147	# bb229.preheader
.LBB69_132:	# bb208
	testl	%r9d, %r9d
	jle	.LBB69_147	# bb229.preheader
.LBB69_133:	# bb208
	testl	%ecx, %ecx
	jle	.LBB69_147	# bb229.preheader
.LBB69_134:	# bb208.bb212.preheader_crit_edge
	xorl	%edx, %edx
	movl	%edx, %esi
	jmp	.LBB69_137	# bb212.preheader
	.align	16
.LBB69_135:	# bb211
	leal	(%rdx,%r8), %r11d
	movslq	%r11d, %r11
	movapd	%xmm0, %xmm1
	mulsd	(%rax,%r11,8), %xmm1
	movsd	%xmm1, (%rax,%r11,8)
	incl	%r8d
	cmpl	%ecx, %r8d
	jne	.LBB69_135	# bb211
.LBB69_136:	# bb213
	addl	96(%rsp), %edx
	incl	%esi
	cmpl	%r9d, %esi
	je	.LBB69_147	# bb229.preheader
.LBB69_137:	# bb212.preheader
	xorl	%r8d, %r8d
	jmp	.LBB69_135	# bb211
.LBB69_138:	# bb217
	cmpl	$131, 4(%rsp)
	jne	.LBB69_140	# bb219
.LBB69_139:	# bb218
	leal	(%rdx,%r11), %r13d
	movslq	%r13d, %r13
	movsd	(%rax,%r13,8), %xmm0
	movslq	%r12d, %rbp
	divsd	(%r10,%rbp,8), %xmm0
	movsd	%xmm0, (%rax,%r13,8)
.LBB69_140:	# bb219
	leal	(%rdx,%r11), %r13d
	movslq	%r13d, %r13
	movsd	(%rax,%r13,8), %xmm0
	testl	%r14d, %r14d
	jle	.LBB69_143	# bb222.loopexit
.LBB69_141:	# bb.nph308
	leal	-1(%r11), %r14d
	xorl	%r13d, %r13d
	.align	16
.LBB69_142:	# bb220
	leal	(%rbx,%r13), %ebp
	movslq	%ebp, %rbp
	movapd	%xmm0, %xmm1
	mulsd	(%r10,%rbp,8), %xmm1
	leal	(%rsi,%r13), %ebp
	movslq	%ebp, %rbp
	movsd	(%rax,%rbp,8), %xmm2
	subsd	%xmm1, %xmm2
	movsd	%xmm2, (%rax,%rbp,8)
	incl	%r13d
	cmpl	%r14d, %r13d
	jne	.LBB69_142	# bb220
.LBB69_143:	# bb222.loopexit
	addl	%r15d, %r12d
	subl	%edi, %ebx
	decl	%r11d
.LBB69_144:	# bb222
	testl	%r11d, %r11d
	jle	.LBB69_146	# bb228
.LBB69_145:	# bb223
	leal	-1(%r11), %r14d
	testl	%r11d, %r11d
	jne	.LBB69_138	# bb217
.LBB69_146:	# bb228
	addl	96(%rsp), %esi
	incl	%r8d
	cmpl	%r9d, %r8d
	je	.LBB69_24	# bb54.thread
	jmp	.LBB69_149	# bb222.preheader
.LBB69_147:	# bb229.preheader
	testl	%r9d, %r9d
	jle	.LBB69_24	# bb54.thread
.LBB69_148:	# bb229.preheader.bb222.preheader_crit_edge
	xorl	%esi, %esi
	movl	%esi, %r8d
	.align	16
.LBB69_149:	# bb222.preheader
	movl	$4294967295, %r15d
	subl	%edi, %r15d
	leal	-1(%rcx), %edx
	movl	%edi, %ebx
	imull	%edx, %ebx
	leal	1(%rdi), %r12d
	imull	%edx, %r12d
	leal	-1(%rsi), %edx
	movl	%ecx, %r11d
	jmp	.LBB69_144	# bb222
.LBB69_150:	# bb230
	cmpl	$142, %esi
	setne	%dl
	notb	%r8b
	orb	%dl, %r8b
	testb	$1, %r8b
	jne	.LBB69_169	# bb256
.LBB69_151:	# bb238
	ucomisd	.LCPI69_0(%rip), %xmm0
	setnp	%dl
	sete	%sil
	testb	%dl, %sil
	jne	.LBB69_165	# bb255.preheader
.LBB69_152:	# bb238
	testl	%r9d, %r9d
	jle	.LBB69_165	# bb255.preheader
.LBB69_153:	# bb238
	testl	%ecx, %ecx
	jle	.LBB69_165	# bb255.preheader
.LBB69_154:	# bb238.bb242.preheader_crit_edge
	xorl	%edx, %edx
	movl	%edx, %esi
	jmp	.LBB69_157	# bb242.preheader
	.align	16
.LBB69_155:	# bb241
	leal	(%rdx,%r8), %r11d
	movslq	%r11d, %r11
	movapd	%xmm0, %xmm1
	mulsd	(%rax,%r11,8), %xmm1
	movsd	%xmm1, (%rax,%r11,8)
	incl	%r8d
	cmpl	%ecx, %r8d
	jne	.LBB69_155	# bb241
.LBB69_156:	# bb243
	addl	96(%rsp), %edx
	incl	%esi
	cmpl	%r9d, %esi
	je	.LBB69_165	# bb255.preheader
.LBB69_157:	# bb242.preheader
	xorl	%r8d, %r8d
	jmp	.LBB69_155	# bb241
	.align	16
.LBB69_158:	# bb247
	cmpl	$131, 4(%rsp)
	jne	.LBB69_160	# bb249
.LBB69_159:	# bb248
	leal	(%rsi,%r11), %r15d
	movslq	%r15d, %r15
	movsd	(%rax,%r15,8), %xmm0
	movslq	%r14d, %r12
	divsd	(%r10,%r12,8), %xmm0
	movsd	%xmm0, (%rax,%r15,8)
.LBB69_160:	# bb249
	leal	(%rsi,%r11), %r15d
	leal	1(%r11), %r12d
	cmpl	%ecx, %r12d
	movslq	%r15d, %r15
	movsd	(%rax,%r15,8), %xmm0
	jge	.LBB69_163	# bb252
.LBB69_161:	# bb.nph
	leal	(%rdi,%r14), %r15d
	leal	(%r8,%r11), %r12d
	xorl	%r13d, %r13d
	.align	16
.LBB69_162:	# bb250
	movslq	%r15d, %rbp
	movapd	%xmm0, %xmm1
	mulsd	(%r10,%rbp,8), %xmm1
	leal	(%r12,%r13), %ebp
	movslq	%ebp, %rbp
	movsd	(%rax,%rbp,8), %xmm2
	subsd	%xmm1, %xmm2
	movsd	%xmm2, (%rax,%rbp,8)
	addl	%edi, %r15d
	incl	%r13d
	cmpl	%ebx, %r13d
	jne	.LBB69_162	# bb250
.LBB69_163:	# bb252
	addl	%edx, %r14d
	decl	%ebx
	incl	%r11d
	cmpl	%ecx, %r11d
	jne	.LBB69_158	# bb247
.LBB69_164:	# bb254
	addl	96(%rsp), %esi
	movl	(%rsp), %edx
	incl	%edx
	movl	%edx, (%rsp)
	cmpl	%r9d, %edx
	je	.LBB69_24	# bb54.thread
	jmp	.LBB69_168	# bb253.preheader
.LBB69_165:	# bb255.preheader
	testl	%r9d, %r9d
	jle	.LBB69_24	# bb54.thread
.LBB69_166:	# bb.nph302
	testl	%ecx, %ecx
	jle	.LBB69_24	# bb54.thread
.LBB69_167:	# bb.nph302.bb253.preheader_crit_edge
	xorl	%esi, %esi
	movl	%esi, (%rsp)
	.align	16
.LBB69_168:	# bb253.preheader
	leal	1(%rdi), %edx
	leal	-1(%rcx), %ebx
	leal	1(%rsi), %r8d
	xorl	%r14d, %r14d
	movl	%r14d, %r11d
	jmp	.LBB69_158	# bb247
.LBB69_169:	# bb256
	xorl	%edi, %edi
	leaq	.str93, %rsi
	leaq	.str194, %rdx
	xorb	%al, %al
	call	cblas_xerbla
	jmp	.LBB69_24	# bb54.thread
.LBB69_170:	# bb
	cmpl	$113, %ecx
	movl	$112, %r8d
	cmovne	%ecx, %r8d
	movl	%r11d, %ecx
	jmp	.LBB69_2	# bb25
.LBB69_171:	# bb50.preheader
	leal	-1(%r9), %esi
	movl	96(%rsp), %r8d
	imull	%esi, %r8d
	movl	$4294967295, %edx
	subl	%edi, %edx
	movl	%edx, (%rsp)
	leal	1(%rdi), %edx
	imull	%esi, %edx
	jmp	.LBB69_22	# bb50
.LBB69_172:	# bb138.preheader
	leal	-1(%r9), %esi
	movl	96(%rsp), %r11d
	imull	%esi, %r11d
	movl	$4294967295, %ebx
	subl	%edi, %ebx
	movl	%edi, %r8d
	imull	%esi, %r8d
	leal	1(%rdi), %edx
	imull	%esi, %edx
	jmp	.LBB69_89	# bb138
	.size	cblas_dtrsm, .-cblas_dtrsm
.Leh_func_end50:


	.align	16
	.globl	cblas_dtrsv
	.type	cblas_dtrsv,@function
cblas_dtrsv:
.Leh_func_begin51:
.Llabel51:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$24, %rsp
	cmpl	$113, %edx
	movl	$112, %eax
	cmovne	%edx, %eax
	testl	%r8d, %r8d
	movl	96(%rsp), %ebx
	movq	88(%rsp), %r14
	movl	80(%rsp), %r15d
	movq	%r9, %r12
	movl	%r8d, 20(%rsp)
	movl	%ecx, 16(%rsp)
	je	.LBB70_81	# return
.LBB70_1:	# bb8
	cmpl	$121, %esi
	sete	%cl
	setne	%dl
	cmpl	$101, %edi
	sete	%r8b
	setne	%r9b
	andb	%cl, %r8b
	orb	%dl, %r9b
	testb	%r9b, %r9b
	jne	.LBB70_3	# bb15
.LBB70_2:	# bb8
	cmpl	$111, %eax
	je	.LBB70_5	# bb23
.LBB70_3:	# bb15
	cmpl	$122, %esi
	sete	%cl
	setne	%dl
	cmpl	$102, %edi
	sete	%r9b
	setne	%r10b
	andb	%cl, %r9b
	orb	%dl, %r10b
	testb	%r10b, %r10b
	jne	.LBB70_23	# bb42
.LBB70_4:	# bb15
	cmpl	$112, %eax
	jne	.LBB70_23	# bb42
.LBB70_5:	# bb23
	xorl	%edi, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB70_20	# real_catch0
.LBB70_6:	# jump0
	xorl	%edi, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB70_21	# real_end0
.LBB70_7:	# real_try0
	xorl	%edi, %edi
	call	llvm_real_try
	testl	%ebx, %ebx
	jg	.LBB70_82	# real_try0.bb26_crit_edge
.LBB70_8:	# bb24
	movl	$1, %eax
	subl	20(%rsp), %eax
	imull	%ebx, %eax
.LBB70_9:	# bb26
	movl	20(%rsp), %ecx
	leal	-1(%rcx), %ecx
	movl	%ecx, %edx
	imull	%ebx, %edx
	addl	%eax, %edx
	cmpl	$131, 16(%rsp)
	jne	.LBB70_11	# bb28
.LBB70_10:	# bb27
	movslq	%edx, %rdx
	movsd	(%r14,%rdx,8), %xmm0
	leal	1(%r15), %esi
	imull	%ecx, %esi
	movslq	%esi, %rcx
	divsd	(%r12,%rcx,8), %xmm0
	movsd	%xmm0, (%r14,%rdx,8)
.LBB70_11:	# bb28
	movl	20(%rsp), %ecx
	leal	-2(%rcx), %edx
	movl	%ebx, %esi
	imull	%edx, %esi
	movl	%esi, 12(%rsp)
	movl	$4294967295, %esi
	subl	%r15d, %esi
	movl	%esi, 4(%rsp)
	movl	%r15d, %esi
	imull	%edx, %esi
	incl	%r15d
	imull	%edx, %r15d
	leal	-1(%rcx,%rsi), %edx
	movl	%edx, 8(%rsp)
	leal	-1(%rcx), %edx
	imull	%ebx, %edx
	movl	$1, %esi
	xorl	%edi, %edi
	jmp	.LBB70_18	# bb36
.LBB70_12:	# bb29
	cmpl	20(%rsp), %r9d
	movslq	%r8d, %r8
	movsd	(%r14,%r8,8), %xmm0
	jge	.LBB70_15	# bb32
.LBB70_13:	# bb.nph196
	leal	(%rdx,%rax), %r9d
	movl	8(%rsp), %r10d
	leal	(%r10,%rdi), %r10d
	xorl	%r11d, %r11d
	.align	16
.LBB70_14:	# bb30
	leal	(%r10,%r11), %r13d
	movslq	%r13d, %r13
	movslq	%r9d, %rbp
	movsd	(%r14,%rbp,8), %xmm1
	mulsd	(%r12,%r13,8), %xmm1
	subsd	%xmm1, %xmm0
	addl	%ebx, %r9d
	incl	%r11d
	cmpl	%esi, %r11d
	jne	.LBB70_14	# bb30
.LBB70_15:	# bb32
	cmpl	$131, 16(%rsp)
	jne	.LBB70_17	# bb36.backedge
.LBB70_16:	# bb33
	leal	(%r15,%rdi), %r9d
	movslq	%r9d, %r9
	divsd	(%r12,%r9,8), %xmm0
.LBB70_17:	# bb36.backedge
	movsd	%xmm0, (%r14,%r8,8)
	addl	4(%rsp), %edi
	subl	%ebx, %eax
	decl	%ecx
	incl	%esi
.LBB70_18:	# bb36
	movl	12(%rsp), %r8d
	leal	(%r8,%rax), %r8d
	leal	-1(%rcx), %r9d
	testl	%r9d, %r9d
	jle	.LBB70_21	# real_end0
.LBB70_19:	# bb37
	cmpl	$1, %ecx
	jne	.LBB70_12	# bb29
	jmp	.LBB70_21	# real_end0
.LBB70_20:	# real_catch0
	xorl	%edi, %edi
	call	llvm_real_catch
.LBB70_21:	# real_end0
	xorl	%edi, %edi
.LBB70_22:	# real_end0
	call	llvm_real_end
	jmp	.LBB70_81	# return
.LBB70_23:	# bb42
	cmpl	$122, %esi
	sete	%cl
	setne	%dl
	cmpl	$101, %edi
	sete	%r10b
	setne	%r11b
	andb	%cl, %r10b
	orb	%dl, %r11b
	testb	%r11b, %r11b
	jne	.LBB70_25	# bb50
.LBB70_24:	# bb42
	cmpl	$111, %eax
	je	.LBB70_27	# bb58
.LBB70_25:	# bb50
	cmpl	$121, %esi
	sete	%cl
	setne	%dl
	cmpl	$102, %edi
	sete	%sil
	setne	%dil
	andb	%cl, %sil
	orb	%dl, %dil
	testb	%dil, %dil
	jne	.LBB70_43	# bb75
.LBB70_26:	# bb50
	cmpl	$112, %eax
	jne	.LBB70_43	# bb75
.LBB70_27:	# bb58
	movl	$1, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB70_41	# real_catch1
.LBB70_28:	# jump1
	movl	$1, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB70_42	# real_end1
.LBB70_29:	# real_try1
	movl	$1, %edi
	call	llvm_real_try
	testl	%ebx, %ebx
	jg	.LBB70_83	# real_try1.bb61_crit_edge
.LBB70_30:	# bb59
	movl	$1, %eax
	subl	20(%rsp), %eax
	imull	%ebx, %eax
.LBB70_31:	# bb61
	cmpl	$131, 16(%rsp)
	jne	.LBB70_33	# bb74.preheader
.LBB70_32:	# bb62
	movslq	%eax, %rcx
	movsd	(%r14,%rcx,8), %xmm0
	divsd	(%r12), %xmm0
	movsd	%xmm0, (%r14,%rcx,8)
.LBB70_33:	# bb74.preheader
	cmpl	$2, 20(%rsp)
	jl	.LBB70_42	# real_end1
.LBB70_34:	# bb.nph188
	movl	$1, %ecx
	movl	20(%rsp), %edx
	subl	%edx, %ecx
	imull	%ebx, %ecx
	addl	%ebx, %eax
	decl	%edx
	movl	%edx, 20(%rsp)
	leal	1(%r15), %edx
	xorl	%esi, %esi
	movl	%edx, %edi
	movl	%r15d, %r8d
	.align	16
.LBB70_35:	# bb64
	testl	%ebx, %ebx
	movl	$0, %r9d
	cmovle	%ecx, %r9d
	movslq	%eax, %rax
	movsd	(%r14,%rax,8), %xmm0
	leal	1(%rsi), %r10d
	testl	%r10d, %r10d
	jle	.LBB70_38	# bb70
.LBB70_36:	# bb64.bb68_crit_edge
	xorl	%r11d, %r11d
	.align	16
.LBB70_37:	# bb68
	leal	(%r8,%r11), %r13d
	movslq	%r13d, %r13
	movslq	%r9d, %rbp
	movsd	(%r14,%rbp,8), %xmm1
	mulsd	(%r12,%r13,8), %xmm1
	subsd	%xmm1, %xmm0
	addl	%ebx, %r9d
	incl	%r11d
	cmpl	%r10d, %r11d
	jne	.LBB70_37	# bb68
.LBB70_38:	# bb70
	cmpl	$131, 16(%rsp)
	jne	.LBB70_40	# bb73
.LBB70_39:	# bb71
	movslq	%edi, %r9
	divsd	(%r12,%r9,8), %xmm0
.LBB70_40:	# bb73
	movsd	%xmm0, (%r14,%rax,8)
	addl	%ebx, %eax
	addl	%edx, %edi
	addl	%r15d, %r8d
	incl	%esi
	cmpl	20(%rsp), %esi
	jne	.LBB70_35	# bb64
	jmp	.LBB70_42	# real_end1
.LBB70_41:	# real_catch1
	movl	$1, %edi
	call	llvm_real_catch
.LBB70_42:	# real_end1
	movl	$1, %edi
	jmp	.LBB70_22	# real_end0
.LBB70_43:	# bb75
	cmpl	$111, %eax
	sete	%cl
	andb	%cl, %r9b
	cmpl	$112, %eax
	sete	%cl
	testb	%cl, %r8b
	jne	.LBB70_45	# bb91
.LBB70_44:	# bb75
	notb	%r9b
	testb	$1, %r9b
	jne	.LBB70_61	# bb108
.LBB70_45:	# bb91
	movl	$2, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB70_59	# real_catch2
.LBB70_46:	# jump2
	movl	$2, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB70_60	# real_end2
.LBB70_47:	# real_try2
	movl	$2, %edi
	call	llvm_real_try
	testl	%ebx, %ebx
	jg	.LBB70_84	# real_try2.bb94_crit_edge
.LBB70_48:	# bb92
	movl	$1, %eax
	subl	20(%rsp), %eax
	imull	%ebx, %eax
.LBB70_49:	# bb94
	cmpl	$131, 16(%rsp)
	jne	.LBB70_51	# bb107.preheader
.LBB70_50:	# bb95
	movslq	%eax, %rcx
	movsd	(%r14,%rcx,8), %xmm0
	divsd	(%r12), %xmm0
	movsd	%xmm0, (%r14,%rcx,8)
.LBB70_51:	# bb107.preheader
	cmpl	$2, 20(%rsp)
	jl	.LBB70_60	# real_end2
.LBB70_52:	# bb.nph178
	movl	$1, %ecx
	movl	20(%rsp), %edx
	subl	%edx, %ecx
	imull	%ebx, %ecx
	addl	%ebx, %eax
	decl	%edx
	movl	%edx, 20(%rsp)
	leal	1(%r15), %edx
	xorl	%esi, %esi
	movl	%edx, %edi
	.align	16
.LBB70_53:	# bb97
	testl	%ebx, %ebx
	movl	$0, %r8d
	cmovle	%ecx, %r8d
	movslq	%eax, %rax
	movsd	(%r14,%rax,8), %xmm0
	leal	1(%rsi), %r9d
	testl	%r9d, %r9d
	jle	.LBB70_56	# bb103
.LBB70_54:	# bb101.preheader
	leal	1(%rsi), %r10d
	xorl	%r11d, %r11d
	.align	16
.LBB70_55:	# bb101
	leal	(%rbx,%r8), %r13d
	leal	(%r15,%r10), %ebp
	incl	%r11d
	cmpl	%r9d, %r11d
	movslq	%r8d, %r8
	movsd	(%r14,%r8,8), %xmm1
	movslq	%r10d, %r8
	mulsd	(%r12,%r8,8), %xmm1
	subsd	%xmm1, %xmm0
	movl	%r13d, %r8d
	movl	%ebp, %r10d
	jne	.LBB70_55	# bb101
.LBB70_56:	# bb103
	cmpl	$131, 16(%rsp)
	jne	.LBB70_58	# bb106
.LBB70_57:	# bb104
	movslq	%edi, %r8
	divsd	(%r12,%r8,8), %xmm0
.LBB70_58:	# bb106
	movsd	%xmm0, (%r14,%rax,8)
	addl	%ebx, %eax
	addl	%edx, %edi
	incl	%esi
	cmpl	20(%rsp), %esi
	jne	.LBB70_53	# bb97
	jmp	.LBB70_60	# real_end2
.LBB70_59:	# real_catch2
	movl	$2, %edi
	call	llvm_real_catch
.LBB70_60:	# real_end2
	movl	$2, %edi
	jmp	.LBB70_22	# real_end0
.LBB70_61:	# bb108
	cmpl	$111, %eax
	sete	%cl
	andb	%cl, %sil
	cmpl	$112, %eax
	sete	%al
	testb	%al, %r10b
	jne	.LBB70_63	# bb124
.LBB70_62:	# bb108
	notb	%sil
	testb	$1, %sil
	jne	.LBB70_80	# bb143
.LBB70_63:	# bb124
	movl	$3, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB70_78	# real_catch3
.LBB70_64:	# jump3
	movl	$3, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB70_79	# real_end3
.LBB70_65:	# real_try3
	movl	$3, %edi
	call	llvm_real_try
	testl	%ebx, %ebx
	jg	.LBB70_85	# real_try3.bb127_crit_edge
.LBB70_66:	# bb125
	movl	$1, %eax
	subl	20(%rsp), %eax
	imull	%ebx, %eax
.LBB70_67:	# bb127
	movl	20(%rsp), %ecx
	leal	-1(%rcx), %ecx
	movl	%ecx, %edx
	imull	%ebx, %edx
	addl	%eax, %edx
	cmpl	$131, 16(%rsp)
	jne	.LBB70_69	# bb129
.LBB70_68:	# bb128
	movslq	%edx, %rdx
	movsd	(%r14,%rdx,8), %xmm0
	leal	1(%r15), %esi
	imull	%ecx, %esi
	movslq	%esi, %rcx
	divsd	(%r12,%rcx,8), %xmm0
	movsd	%xmm0, (%r14,%rdx,8)
.LBB70_69:	# bb129
	movl	$4294967295, %ecx
	subl	%r15d, %ecx
	movl	%ecx, (%rsp)
	movl	20(%rsp), %ecx
	leal	-1(%rcx), %edx
	movl	%r15d, %esi
	imull	%edx, %esi
	leal	-2(%rcx,%rsi), %esi
	movl	%esi, 12(%rsp)
	leal	-2(%rcx), %esi
	movl	%ebx, %edi
	imull	%esi, %edi
	movl	%edi, 8(%rsp)
	leal	1(%r15), %edi
	imull	%esi, %edi
	movl	%edi, 4(%rsp)
	imull	%ebx, %edx
	movl	$1, %esi
	xorl	%edi, %edi
	jmp	.LBB70_76	# bb137
.LBB70_70:	# bb130
	cmpl	20(%rsp), %r9d
	movslq	%r8d, %r8
	movsd	(%r14,%r8,8), %xmm0
	jge	.LBB70_73	# bb133
.LBB70_71:	# bb.nph
	leal	(%rdx,%rax), %r9d
	movl	12(%rsp), %r10d
	leal	(%r10,%rdi), %r10d
	xorl	%r11d, %r11d
	.align	16
.LBB70_72:	# bb131
	leal	(%r15,%r10), %r13d
	leal	(%rbx,%r9), %ebp
	incl	%r11d
	cmpl	%esi, %r11d
	movslq	%r9d, %r9
	movsd	(%r14,%r9,8), %xmm1
	movslq	%r10d, %r9
	mulsd	(%r12,%r9,8), %xmm1
	subsd	%xmm1, %xmm0
	movl	%r13d, %r10d
	movl	%ebp, %r9d
	jne	.LBB70_72	# bb131
.LBB70_73:	# bb133
	cmpl	$131, 16(%rsp)
	jne	.LBB70_75	# bb137.backedge
.LBB70_74:	# bb134
	movl	4(%rsp), %r9d
	leal	(%r9,%rdi), %r9d
	movslq	%r9d, %r9
	divsd	(%r12,%r9,8), %xmm0
.LBB70_75:	# bb137.backedge
	movsd	%xmm0, (%r14,%r8,8)
	subl	%ebx, %eax
	addl	(%rsp), %edi
	decl	%ecx
	incl	%esi
.LBB70_76:	# bb137
	movl	8(%rsp), %r8d
	leal	(%r8,%rax), %r8d
	leal	-1(%rcx), %r9d
	testl	%r9d, %r9d
	jle	.LBB70_79	# real_end3
.LBB70_77:	# bb138
	cmpl	$1, %ecx
	jne	.LBB70_70	# bb130
	jmp	.LBB70_79	# real_end3
.LBB70_78:	# real_catch3
	movl	$3, %edi
	call	llvm_real_catch
.LBB70_79:	# real_end3
	movl	$3, %edi
	jmp	.LBB70_22	# real_end0
.LBB70_80:	# bb143
	xorl	%edi, %edi
	leaq	.str95, %rsi
	leaq	.str196, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB70_81:	# return
	addq	$24, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB70_82:	# real_try0.bb26_crit_edge
	xorl	%eax, %eax
	jmp	.LBB70_9	# bb26
.LBB70_83:	# real_try1.bb61_crit_edge
	xorl	%eax, %eax
	jmp	.LBB70_31	# bb61
.LBB70_84:	# real_try2.bb94_crit_edge
	xorl	%eax, %eax
	jmp	.LBB70_49	# bb94
.LBB70_85:	# real_try3.bb127_crit_edge
	xorl	%eax, %eax
	jmp	.LBB70_67	# bb127
	.size	cblas_dtrsv, .-cblas_dtrsv
.Leh_func_end51:


	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI71_0:					
	.quad	9223372036854775807	# double value: nan
	.quad	9223372036854775807	# double value: nan
	.text
	.align	16
	.globl	cblas_dzasum
	.type	cblas_dzasum,@function
cblas_dzasum:
	testl	%edx, %edx
	jle	.LBB71_5	# entry.bb5_crit_edge
.LBB71_1:	# entry
	testl	%edi, %edi
	jle	.LBB71_5	# entry.bb5_crit_edge
.LBB71_2:	# bb.nph
	addl	%edx, %edx
	pxor	%xmm0, %xmm0
	xorl	%eax, %eax
	movl	%eax, %ecx
	.align	16
.LBB71_3:	# bb2
	movslq	%eax, %r8
	movsd	(%rsi,%r8,8), %xmm1
	movsd	.LCPI71_0(%rip), %xmm2
	andpd	%xmm2, %xmm1
	leal	1(%rax), %r8d
	movslq	%r8d, %r8
	movsd	(%rsi,%r8,8), %xmm3
	andpd	%xmm2, %xmm3
	addsd	%xmm1, %xmm3
	addsd	%xmm3, %xmm0
	addl	%edx, %eax
	incl	%ecx
	cmpl	%edi, %ecx
	jne	.LBB71_3	# bb2
.LBB71_4:	# bb5
	ret
.LBB71_5:	# entry.bb5_crit_edge
	pxor	%xmm0, %xmm0
	jmp	.LBB71_4	# bb5
	.size	cblas_dzasum, .-cblas_dzasum


	.section	.rodata.cst8,"aM",@progbits,8
	.align	16
.LCPI72_0:					
	.quad	4607182418800017408	# double value: 1.000000e+00
	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI72_1:					
	.quad	9223372036854775807	# double value: nan
	.quad	9223372036854775807	# double value: nan
	.text
	.align	16
	.globl	cblas_dznrm2
	.type	cblas_dznrm2,@function
cblas_dznrm2:
	testl	%edx, %edx
	jle	.LBB72_12	# bb15
.LBB72_1:	# entry
	testl	%edi, %edi
	je	.LBB72_12	# bb15
.LBB72_2:	# bb13.preheader
	testl	%edi, %edi
	jle	.LBB72_13	# bb13.preheader.bb14_crit_edge
.LBB72_3:	# bb.nph
	addl	%edx, %edx
	movsd	.LCPI72_0(%rip), %xmm0
	pxor	%xmm1, %xmm1
	xorl	%eax, %eax
	movl	%eax, %ecx
	.align	16
.LBB72_4:	# bb4
	movslq	%eax, %r8
	movsd	(%rsi,%r8,8), %xmm2
	leal	1(%rax), %r8d
	pxor	%xmm3, %xmm3
	ucomisd	%xmm3, %xmm2
	movslq	%r8d, %r8
	movsd	(%rsi,%r8,8), %xmm3
	jne	.LBB72_5	# bb5
	jp	.LBB72_5	# bb5
	jmp	.LBB72_7	# bb8
.LBB72_5:	# bb5
	andpd	.LCPI72_1(%rip), %xmm2
	ucomisd	%xmm1, %xmm2
	ja	.LBB72_14	# bb6
.LBB72_6:	# bb7
	divsd	%xmm1, %xmm2
	mulsd	%xmm2, %xmm2
	addsd	%xmm2, %xmm0
.LBB72_7:	# bb8
	pxor	%xmm2, %xmm2
	ucomisd	%xmm2, %xmm3
	jne	.LBB72_8	# bb9
	jp	.LBB72_8	# bb9
	jmp	.LBB72_10	# bb12
.LBB72_8:	# bb9
	andpd	.LCPI72_1(%rip), %xmm3
	ucomisd	%xmm1, %xmm3
	ja	.LBB72_15	# bb10
.LBB72_9:	# bb11
	divsd	%xmm1, %xmm3
	mulsd	%xmm3, %xmm3
	addsd	%xmm3, %xmm0
.LBB72_10:	# bb12
	addl	%edx, %eax
	incl	%ecx
	cmpl	%edi, %ecx
	jne	.LBB72_4	# bb4
.LBB72_11:	# bb14
	sqrtsd	%xmm0, %xmm0
	mulsd	%xmm1, %xmm0
	ret
.LBB72_12:	# bb15
	pxor	%xmm0, %xmm0
	ret
.LBB72_13:	# bb13.preheader.bb14_crit_edge
	movsd	.LCPI72_0(%rip), %xmm0
	pxor	%xmm1, %xmm1
	jmp	.LBB72_11	# bb14
.LBB72_14:	# bb6
	divsd	%xmm2, %xmm1
	mulsd	%xmm1, %xmm0
	mulsd	%xmm1, %xmm0
	addsd	.LCPI72_0(%rip), %xmm0
	movapd	%xmm2, %xmm1
	jmp	.LBB72_7	# bb8
.LBB72_15:	# bb10
	divsd	%xmm3, %xmm1
	mulsd	%xmm1, %xmm0
	mulsd	%xmm1, %xmm0
	addsd	.LCPI72_0(%rip), %xmm0
	movapd	%xmm3, %xmm1
	jmp	.LBB72_10	# bb12
	.size	cblas_dznrm2, .-cblas_dznrm2


	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI73_0:					
	.long	2147483647	# float nan
	.long	2147483647	# float nan
	.long	2147483647	# float nan
	.long	2147483647	# float nan
	.text
	.align	16
	.globl	cblas_icamax
	.type	cblas_icamax,@function
cblas_icamax:
	testl	%edx, %edx
	jle	.LBB73_7	# entry.bb7_crit_edge
.LBB73_1:	# entry
	testl	%edi, %edi
	jle	.LBB73_7	# entry.bb7_crit_edge
.LBB73_2:	# bb.nph
	addl	%edx, %edx
	xorl	%eax, %eax
	pxor	%xmm0, %xmm0
	xorl	%ecx, %ecx
	movl	%ecx, %r8d
	.align	16
.LBB73_3:	# bb2
	movslq	%ecx, %r9
	movss	(%rsi,%r9,4), %xmm1
	movss	.LCPI73_0(%rip), %xmm2
	andps	%xmm2, %xmm1
	leal	1(%rcx), %r9d
	movslq	%r9d, %r9
	movss	(%rsi,%r9,4), %xmm3
	andps	%xmm2, %xmm3
	addss	%xmm1, %xmm3
	ucomiss	%xmm0, %xmm3
	jbe	.LBB73_5	# bb4
.LBB73_4:	# bb3
	movslq	%r8d, %rax
	movaps	%xmm3, %xmm0
.LBB73_5:	# bb4
	addl	%edx, %ecx
	incl	%r8d
	cmpl	%edi, %r8d
	jne	.LBB73_3	# bb2
.LBB73_6:	# bb7
	ret
.LBB73_7:	# entry.bb7_crit_edge
	xorl	%eax, %eax
	jmp	.LBB73_6	# bb7
	.size	cblas_icamax, .-cblas_icamax


	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI74_0:					
	.quad	9223372036854775807	# double value: nan
	.quad	9223372036854775807	# double value: nan
	.text
	.align	16
	.globl	cblas_idamax
	.type	cblas_idamax,@function
cblas_idamax:
	testl	%edx, %edx
	jle	.LBB74_7	# entry.bb7_crit_edge
.LBB74_1:	# entry
	testl	%edi, %edi
	jle	.LBB74_7	# entry.bb7_crit_edge
.LBB74_2:	# entry.bb2_crit_edge
	xorl	%eax, %eax
	pxor	%xmm0, %xmm0
	xorl	%ecx, %ecx
	movl	%ecx, %r8d
	.align	16
.LBB74_3:	# bb2
	movslq	%ecx, %r9
	movsd	(%rsi,%r9,8), %xmm1
	andpd	.LCPI74_0(%rip), %xmm1
	ucomisd	%xmm0, %xmm1
	jbe	.LBB74_5	# bb4
.LBB74_4:	# bb3
	movslq	%r8d, %rax
	movapd	%xmm1, %xmm0
.LBB74_5:	# bb4
	addl	%edx, %ecx
	incl	%r8d
	cmpl	%edi, %r8d
	jne	.LBB74_3	# bb2
.LBB74_6:	# bb7
	ret
.LBB74_7:	# entry.bb7_crit_edge
	xorl	%eax, %eax
	jmp	.LBB74_6	# bb7
	.size	cblas_idamax, .-cblas_idamax


	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI75_0:					
	.long	2147483647	# float nan
	.long	2147483647	# float nan
	.long	2147483647	# float nan
	.long	2147483647	# float nan
	.text
	.align	16
	.globl	cblas_isamax
	.type	cblas_isamax,@function
cblas_isamax:
	testl	%edx, %edx
	jle	.LBB75_7	# entry.bb7_crit_edge
.LBB75_1:	# entry
	testl	%edi, %edi
	jle	.LBB75_7	# entry.bb7_crit_edge
.LBB75_2:	# entry.bb2_crit_edge
	xorl	%eax, %eax
	pxor	%xmm0, %xmm0
	xorl	%ecx, %ecx
	movl	%ecx, %r8d
	.align	16
.LBB75_3:	# bb2
	movslq	%ecx, %r9
	movss	(%rsi,%r9,4), %xmm1
	andps	.LCPI75_0(%rip), %xmm1
	ucomiss	%xmm0, %xmm1
	jbe	.LBB75_5	# bb4
.LBB75_4:	# bb3
	movslq	%r8d, %rax
	movaps	%xmm1, %xmm0
.LBB75_5:	# bb4
	addl	%edx, %ecx
	incl	%r8d
	cmpl	%edi, %r8d
	jne	.LBB75_3	# bb2
.LBB75_6:	# bb7
	ret
.LBB75_7:	# entry.bb7_crit_edge
	xorl	%eax, %eax
	jmp	.LBB75_6	# bb7
	.size	cblas_isamax, .-cblas_isamax


	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI76_0:					
	.quad	9223372036854775807	# double value: nan
	.quad	9223372036854775807	# double value: nan
	.text
	.align	16
	.globl	cblas_izamax
	.type	cblas_izamax,@function
cblas_izamax:
	testl	%edx, %edx
	jle	.LBB76_7	# entry.bb7_crit_edge
.LBB76_1:	# entry
	testl	%edi, %edi
	jle	.LBB76_7	# entry.bb7_crit_edge
.LBB76_2:	# bb.nph
	addl	%edx, %edx
	xorl	%eax, %eax
	pxor	%xmm0, %xmm0
	xorl	%ecx, %ecx
	movl	%ecx, %r8d
	.align	16
.LBB76_3:	# bb2
	movslq	%ecx, %r9
	movsd	(%rsi,%r9,8), %xmm1
	movsd	.LCPI76_0(%rip), %xmm2
	andpd	%xmm2, %xmm1
	leal	1(%rcx), %r9d
	movslq	%r9d, %r9
	movsd	(%rsi,%r9,8), %xmm3
	andpd	%xmm2, %xmm3
	addsd	%xmm1, %xmm3
	ucomisd	%xmm0, %xmm3
	jbe	.LBB76_5	# bb4
.LBB76_4:	# bb3
	movslq	%r8d, %rax
	movapd	%xmm3, %xmm0
.LBB76_5:	# bb4
	addl	%edx, %ecx
	incl	%r8d
	cmpl	%edi, %r8d
	jne	.LBB76_3	# bb2
.LBB76_6:	# bb7
	ret
.LBB76_7:	# entry.bb7_crit_edge
	xorl	%eax, %eax
	jmp	.LBB76_6	# bb7
	.size	cblas_izamax, .-cblas_izamax


	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI77_0:					
	.long	2147483647	# float nan
	.long	2147483647	# float nan
	.long	2147483647	# float nan
	.long	2147483647	# float nan
	.text
	.align	16
	.globl	cblas_sasum
	.type	cblas_sasum,@function
cblas_sasum:
	testl	%edx, %edx
	jle	.LBB77_5	# entry.bb5_crit_edge
.LBB77_1:	# entry
	testl	%edi, %edi
	jle	.LBB77_5	# entry.bb5_crit_edge
.LBB77_2:	# entry.bb2_crit_edge
	pxor	%xmm0, %xmm0
	xorl	%eax, %eax
	movl	%eax, %ecx
	.align	16
.LBB77_3:	# bb2
	leal	(%rdx,%rax), %r8d
	incl	%ecx
	cmpl	%edi, %ecx
	movslq	%eax, %rax
	movss	(%rsi,%rax,4), %xmm1
	andps	.LCPI77_0(%rip), %xmm1
	addss	%xmm1, %xmm0
	movl	%r8d, %eax
	jne	.LBB77_3	# bb2
.LBB77_4:	# bb5
	ret
.LBB77_5:	# entry.bb5_crit_edge
	pxor	%xmm0, %xmm0
	jmp	.LBB77_4	# bb5
	.size	cblas_sasum, .-cblas_sasum


	.align	16
	.globl	cblas_saxpy
	.type	cblas_saxpy,@function
cblas_saxpy:
.Leh_func_begin52:
.Llabel52:
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$16, %rsp
	movl	%r8d, %ebx
	movq	%rcx, %r14
	movl	%edx, %r15d
	movq	%rsi, %r12
	movss	%xmm0, 12(%rsp)
	movl	%edi, %r13d
	xorl	%edi, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB78_18	# real_catch0
.LBB78_1:	# jump0
	xorl	%edi, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB78_19	# real_end0
.LBB78_2:	# real_try0
	xorl	%edi, %edi
	call	llvm_real_try
	pxor	%xmm0, %xmm0
	movss	12(%rsp), %xmm1
	ucomiss	%xmm0, %xmm1
	setnp	%al
	sete	%cl
	testb	%al, %cl
	jne	.LBB78_20	# return
.LBB78_3:	# bb
	cmpl	$1, %ebx
	jne	.LBB78_11	# bb9
.LBB78_4:	# bb
	cmpl	$1, %r15d
	jne	.LBB78_11	# bb9
.LBB78_5:	# bb3
	movl	%r13d, %ebx
	sarl	$31, %ebx
	shrl	$30, %ebx
	addl	%r13d, %ebx
	andl	$4294967292, %ebx
	movl	%r13d, %r15d
	subl	%ebx, %r15d
	testl	%r15d, %r15d
	jle	.LBB78_8	# bb8.loopexit
.LBB78_6:	# bb3.bb4_crit_edge
	xorl	%ebx, %ebx
	.align	16
.LBB78_7:	# bb4
	movslq	%ebx, %rax
	movss	12(%rsp), %xmm0
	mulss	(%r12,%rax,4), %xmm0
	addss	(%r14,%rax,4), %xmm0
	movss	%xmm0, (%r14,%rax,4)
	incl	%ebx
	cmpl	%r15d, %ebx
	jne	.LBB78_7	# bb4
.LBB78_8:	# bb8.loopexit
	leal	3(%r15), %ebx
	.align	16
.LBB78_9:	# bb8.loopexit
	cmpl	%r13d, %ebx
	jge	.LBB78_19	# real_end0
.LBB78_10:	# bb7
	movslq	%r15d, %rax
	movss	12(%rsp), %xmm0
	movaps	%xmm0, %xmm1
	mulss	(%r12,%rax,4), %xmm1
	addss	(%r14,%rax,4), %xmm1
	movss	%xmm1, (%r14,%rax,4)
	leal	1(%r15), %ebx
	movslq	%ebx, %rax
	movaps	%xmm0, %xmm1
	mulss	(%r12,%rax,4), %xmm1
	addss	(%r14,%rax,4), %xmm1
	movss	%xmm1, (%r14,%rax,4)
	leal	2(%r15), %ebx
	movslq	%ebx, %rax
	movaps	%xmm0, %xmm1
	mulss	(%r12,%rax,4), %xmm1
	addss	(%r14,%rax,4), %xmm1
	movss	%xmm1, (%r14,%rax,4)
	leal	3(%r15), %ebx
	movslq	%ebx, %rax
	mulss	(%r12,%rax,4), %xmm0
	addss	(%r14,%rax,4), %xmm0
	movss	%xmm0, (%r14,%rax,4)
	leal	7(%r15), %ebx
	leal	4(%r15), %r15d
	jmp	.LBB78_9	# bb8.loopexit
.LBB78_11:	# bb9
	testl	%r15d, %r15d
	jg	.LBB78_21	# bb9.bb12_crit_edge
.LBB78_12:	# bb10
	movl	$1, %eax
	subl	%r13d, %eax
	imull	%r15d, %eax
.LBB78_13:	# bb12
	testl	%ebx, %ebx
	jg	.LBB78_22	# bb12.bb17.preheader_crit_edge
.LBB78_14:	# bb13
	movl	$1, %ecx
	subl	%r13d, %ecx
	imull	%ebx, %ecx
.LBB78_15:	# bb17.preheader
	testl	%r13d, %r13d
	jle	.LBB78_19	# real_end0
.LBB78_16:	# bb17.preheader.bb16_crit_edge
	xorl	%edx, %edx
	.align	16
.LBB78_17:	# bb16
	movslq	%eax, %rsi
	movss	12(%rsp), %xmm0
	mulss	(%r12,%rsi,4), %xmm0
	movslq	%ecx, %rsi
	addss	(%r14,%rsi,4), %xmm0
	movss	%xmm0, (%r14,%rsi,4)
	addl	%ebx, %ecx
	addl	%r15d, %eax
	incl	%edx
	cmpl	%r13d, %edx
	jne	.LBB78_17	# bb16
	jmp	.LBB78_19	# real_end0
.LBB78_18:	# real_catch0
	xorl	%edi, %edi
	call	llvm_real_catch
.LBB78_19:	# real_end0
	xorl	%edi, %edi
	call	llvm_real_end
.LBB78_20:	# return
	addq	$16, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	ret
.LBB78_21:	# bb9.bb12_crit_edge
	xorl	%eax, %eax
	jmp	.LBB78_13	# bb12
.LBB78_22:	# bb12.bb17.preheader_crit_edge
	xorl	%ecx, %ecx
	jmp	.LBB78_15	# bb17.preheader
	.size	cblas_saxpy, .-cblas_saxpy
.Leh_func_end52:


	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI79_0:					
	.long	2147483647	# float nan
	.long	2147483647	# float nan
	.long	2147483647	# float nan
	.long	2147483647	# float nan
	.text
	.align	16
	.globl	cblas_scasum
	.type	cblas_scasum,@function
cblas_scasum:
	testl	%edx, %edx
	jle	.LBB79_5	# entry.bb5_crit_edge
.LBB79_1:	# entry
	testl	%edi, %edi
	jle	.LBB79_5	# entry.bb5_crit_edge
.LBB79_2:	# bb.nph
	addl	%edx, %edx
	pxor	%xmm0, %xmm0
	xorl	%eax, %eax
	movl	%eax, %ecx
	.align	16
.LBB79_3:	# bb2
	movslq	%eax, %r8
	movss	(%rsi,%r8,4), %xmm1
	movss	.LCPI79_0(%rip), %xmm2
	andps	%xmm2, %xmm1
	cvtss2sd	%xmm1, %xmm1
	leal	1(%rax), %r8d
	movslq	%r8d, %r8
	movss	(%rsi,%r8,4), %xmm3
	andps	%xmm2, %xmm3
	cvtss2sd	%xmm3, %xmm2
	addsd	%xmm1, %xmm2
	cvtss2sd	%xmm0, %xmm0
	addsd	%xmm2, %xmm0
	cvtsd2ss	%xmm0, %xmm0
	addl	%edx, %eax
	incl	%ecx
	cmpl	%edi, %ecx
	jne	.LBB79_3	# bb2
.LBB79_4:	# bb5
	ret
.LBB79_5:	# entry.bb5_crit_edge
	pxor	%xmm0, %xmm0
	jmp	.LBB79_4	# bb5
	.size	cblas_scasum, .-cblas_scasum


	.section	.rodata.cst4,"aM",@progbits,4
	.align	16
.LCPI80_0:					
	.long	1065353216	# float 1.000000e+00
	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI80_1:					
	.long	2147483647	# float nan
	.long	2147483647	# float nan
	.long	2147483647	# float nan
	.long	2147483647	# float nan
	.text
	.align	16
	.globl	cblas_scnrm2
	.type	cblas_scnrm2,@function
cblas_scnrm2:
	testl	%edx, %edx
	jle	.LBB80_12	# bb15
.LBB80_1:	# entry
	testl	%edi, %edi
	je	.LBB80_12	# bb15
.LBB80_2:	# bb13.preheader
	testl	%edi, %edi
	jle	.LBB80_13	# bb13.preheader.bb14_crit_edge
.LBB80_3:	# bb.nph
	addl	%edx, %edx
	movss	.LCPI80_0(%rip), %xmm0
	pxor	%xmm1, %xmm1
	xorl	%eax, %eax
	movl	%eax, %ecx
	.align	16
.LBB80_4:	# bb4
	movslq	%eax, %r8
	movss	(%rsi,%r8,4), %xmm2
	leal	1(%rax), %r8d
	pxor	%xmm3, %xmm3
	ucomiss	%xmm3, %xmm2
	movslq	%r8d, %r8
	movss	(%rsi,%r8,4), %xmm3
	jne	.LBB80_5	# bb5
	jp	.LBB80_5	# bb5
	jmp	.LBB80_7	# bb8
.LBB80_5:	# bb5
	andps	.LCPI80_1(%rip), %xmm2
	ucomiss	%xmm1, %xmm2
	ja	.LBB80_14	# bb6
.LBB80_6:	# bb7
	divss	%xmm1, %xmm2
	mulss	%xmm2, %xmm2
	addss	%xmm2, %xmm0
.LBB80_7:	# bb8
	pxor	%xmm2, %xmm2
	ucomiss	%xmm2, %xmm3
	jne	.LBB80_8	# bb9
	jp	.LBB80_8	# bb9
	jmp	.LBB80_10	# bb12
.LBB80_8:	# bb9
	andps	.LCPI80_1(%rip), %xmm3
	ucomiss	%xmm1, %xmm3
	ja	.LBB80_15	# bb10
.LBB80_9:	# bb11
	divss	%xmm1, %xmm3
	mulss	%xmm3, %xmm3
	addss	%xmm3, %xmm0
.LBB80_10:	# bb12
	addl	%edx, %eax
	incl	%ecx
	cmpl	%edi, %ecx
	jne	.LBB80_4	# bb4
.LBB80_11:	# bb14
	cvtss2sd	%xmm0, %xmm0
	sqrtsd	%xmm0, %xmm0
	cvtss2sd	%xmm1, %xmm1
	mulsd	%xmm0, %xmm1
	cvtsd2ss	%xmm1, %xmm0
	ret
.LBB80_12:	# bb15
	pxor	%xmm0, %xmm0
	ret
.LBB80_13:	# bb13.preheader.bb14_crit_edge
	movss	.LCPI80_0(%rip), %xmm0
	pxor	%xmm1, %xmm1
	jmp	.LBB80_11	# bb14
.LBB80_14:	# bb6
	divss	%xmm2, %xmm1
	mulss	%xmm1, %xmm0
	mulss	%xmm1, %xmm0
	addss	.LCPI80_0(%rip), %xmm0
	movaps	%xmm2, %xmm1
	jmp	.LBB80_7	# bb8
.LBB80_15:	# bb10
	divss	%xmm3, %xmm1
	mulss	%xmm1, %xmm0
	mulss	%xmm1, %xmm0
	addss	.LCPI80_0(%rip), %xmm0
	movaps	%xmm3, %xmm1
	jmp	.LBB80_10	# bb12
	.size	cblas_scnrm2, .-cblas_scnrm2


	.align	16
	.globl	cblas_scopy
	.type	cblas_scopy,@function
cblas_scopy:
	testl	%edx, %edx
	jg	.LBB81_8	# entry.bb2_crit_edge
.LBB81_1:	# bb
	movl	$1, %eax
	subl	%edi, %eax
	imull	%edx, %eax
.LBB81_2:	# bb2
	testl	%r8d, %r8d
	jg	.LBB81_9	# bb2.bb7.preheader_crit_edge
.LBB81_3:	# bb3
	movl	$1, %r9d
	subl	%edi, %r9d
	imull	%r8d, %r9d
.LBB81_4:	# bb7.preheader
	testl	%edi, %edi
	jle	.LBB81_7	# return
.LBB81_5:	# bb7.preheader.bb6_crit_edge
	xorl	%r10d, %r10d
	.align	16
.LBB81_6:	# bb6
	movslq	%eax, %r11
	movss	(%rsi,%r11,4), %xmm0
	movslq	%r9d, %r11
	movss	%xmm0, (%rcx,%r11,4)
	addl	%r8d, %r9d
	addl	%edx, %eax
	incl	%r10d
	cmpl	%edi, %r10d
	jne	.LBB81_6	# bb6
.LBB81_7:	# return
	ret
.LBB81_8:	# entry.bb2_crit_edge
	xorl	%eax, %eax
	jmp	.LBB81_2	# bb2
.LBB81_9:	# bb2.bb7.preheader_crit_edge
	xorl	%r9d, %r9d
	jmp	.LBB81_4	# bb7.preheader
	.size	cblas_scopy, .-cblas_scopy


	.align	16
	.globl	cblas_sdot
	.type	cblas_sdot,@function
cblas_sdot:
.Leh_func_begin53:
.Llabel53:
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	movl	%r8d, %ebx
	movq	%rcx, %r14
	movl	%edx, %r15d
	movq	%rsi, %r12
	movl	%edi, %r13d
	xorl	%edi, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB82_9	# real_catch0
.LBB82_1:	# jump0
	xorl	%edi, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB82_10	# real_end0
.LBB82_2:	# real_try0
	xorl	%edi, %edi
	call	llvm_real_try
	testl	%r15d, %r15d
	jg	.LBB82_12	# real_try0.bb2_crit_edge
.LBB82_3:	# bb
	movl	$1, %eax
	subl	%r13d, %eax
	imull	%r15d, %eax
.LBB82_4:	# bb2
	testl	%ebx, %ebx
	jg	.LBB82_13	# bb2.bb7.preheader_crit_edge
.LBB82_5:	# bb3
	movl	$1, %ecx
	subl	%r13d, %ecx
	imull	%ebx, %ecx
.LBB82_6:	# bb7.preheader
	testl	%r13d, %r13d
	jle	.LBB82_14	# bb7.preheader.return_crit_edge
.LBB82_7:	# bb7.preheader.bb6_crit_edge
	pxor	%xmm0, %xmm0
	xorl	%edx, %edx
	.align	16
.LBB82_8:	# bb6
	leal	(%rbx,%rcx), %esi
	leal	(%r15,%rax), %edi
	incl	%edx
	cmpl	%r13d, %edx
	movslq	%eax, %rax
	movss	(%r12,%rax,4), %xmm1
	movslq	%ecx, %rax
	mulss	(%r14,%rax,4), %xmm1
	addss	%xmm1, %xmm0
	movl	%esi, %ecx
	movl	%edi, %eax
	jne	.LBB82_8	# bb6
	jmp	.LBB82_11	# return
.LBB82_9:	# real_catch0
	xorl	%edi, %edi
	call	llvm_real_catch
.LBB82_10:	# real_end0
	xorl	%edi, %edi
	call	llvm_real_end
	# implicit-def: xmm0
.LBB82_11:	# return
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	ret
.LBB82_12:	# real_try0.bb2_crit_edge
	xorl	%eax, %eax
	jmp	.LBB82_4	# bb2
.LBB82_13:	# bb2.bb7.preheader_crit_edge
	xorl	%ecx, %ecx
	jmp	.LBB82_6	# bb7.preheader
.LBB82_14:	# bb7.preheader.return_crit_edge
	pxor	%xmm0, %xmm0
	jmp	.LBB82_11	# return
	.size	cblas_sdot, .-cblas_sdot
.Leh_func_end53:


	.align	16
	.globl	cblas_sdsdot
	.type	cblas_sdsdot,@function
cblas_sdsdot:
.Leh_func_begin54:
.Llabel54:
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$16, %rsp
	movl	%r8d, %ebx
	movq	%rcx, %r14
	movl	%edx, %r15d
	movq	%rsi, %r12
	movss	%xmm0, 12(%rsp)
	movl	%edi, %r13d
	xorl	%edi, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB83_11	# real_catch0
.LBB83_1:	# jump0
	xorl	%edi, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB83_12	# real_end0
.LBB83_2:	# real_try0
	xorl	%edi, %edi
	call	llvm_real_try
	testl	%r15d, %r15d
	cvtss2sd	12(%rsp), %xmm0
	jg	.LBB83_13	# real_try0.bb2_crit_edge
.LBB83_3:	# bb
	movl	$1, %eax
	subl	%r13d, %eax
	imull	%r15d, %eax
.LBB83_4:	# bb2
	testl	%ebx, %ebx
	jg	.LBB83_14	# bb2.bb7.preheader_crit_edge
.LBB83_5:	# bb3
	movl	$1, %ecx
	subl	%r13d, %ecx
	imull	%ebx, %ecx
.LBB83_6:	# bb7.preheader
	testl	%r13d, %r13d
	jle	.LBB83_9	# bb8
.LBB83_7:	# bb7.preheader.bb6_crit_edge
	xorl	%edx, %edx
	.align	16
.LBB83_8:	# bb6
	leal	(%rbx,%rcx), %esi
	leal	(%r15,%rax), %edi
	incl	%edx
	cmpl	%r13d, %edx
	movslq	%eax, %rax
	movss	(%r12,%rax,4), %xmm1
	movslq	%ecx, %rax
	mulss	(%r14,%rax,4), %xmm1
	cvtss2sd	%xmm1, %xmm1
	addsd	%xmm1, %xmm0
	movl	%esi, %ecx
	movl	%edi, %eax
	jne	.LBB83_8	# bb6
.LBB83_9:	# bb8
	cvtsd2ss	%xmm0, %xmm0
.LBB83_10:	# bb8
	addq	$16, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	ret
.LBB83_11:	# real_catch0
	xorl	%edi, %edi
	call	llvm_real_catch
.LBB83_12:	# real_end0
	xorl	%edi, %edi
	call	llvm_real_end
	# implicit-def: xmm0
	jmp	.LBB83_10	# bb8
.LBB83_13:	# real_try0.bb2_crit_edge
	xorl	%eax, %eax
	jmp	.LBB83_4	# bb2
.LBB83_14:	# bb2.bb7.preheader_crit_edge
	xorl	%ecx, %ecx
	jmp	.LBB83_6	# bb7.preheader
	.size	cblas_sdsdot, .-cblas_sdsdot
.Leh_func_end54:


	.section	.rodata.cst4,"aM",@progbits,4
	.align	4
.LCPI84_0:					
	.long	1065353216	# float 1.000000e+00
	.text
	.align	16
	.globl	cblas_sgbmv
	.type	cblas_sgbmv,@function
cblas_sgbmv:
.Leh_func_begin55:
.Llabel55:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$24, %rsp
	cmpl	$113, %esi
	movl	$112, %eax
	cmovne	%esi, %eax
	testl	%ecx, %ecx
	movl	120(%rsp), %esi
	movq	112(%rsp), %r10
	movq	80(%rsp), %r11
	je	.LBB84_43	# return
.LBB84_1:	# entry
	testl	%edx, %edx
	je	.LBB84_43	# return
.LBB84_2:	# bb10
	ucomiss	.LCPI84_0(%rip), %xmm1
	jne	.LBB84_4	# bb12
	jp	.LBB84_4	# bb12
.LBB84_3:	# bb10
	pxor	%xmm2, %xmm2
	ucomiss	%xmm2, %xmm0
	setnp	%bl
	sete	%r14b
	testb	%bl, %r14b
	jne	.LBB84_43	# return
.LBB84_4:	# bb12
	cmpl	$111, %eax
	je	.LBB84_44	# bb12.bb15_crit_edge
.LBB84_5:	# bb14
	movl	%r8d, 16(%rsp)
	movl	%r9d, %r8d
	movl	%ecx, %r9d
	movl	%edx, %ecx
.LBB84_6:	# bb15
	movl	%r8d, 12(%rsp)
	movl	%ecx, 20(%rsp)
	pxor	%xmm2, %xmm2
	ucomiss	%xmm2, %xmm1
	jne	.LBB84_12	# bb22
	jp	.LBB84_12	# bb22
.LBB84_7:	# bb16
	testl	%esi, %esi
	jg	.LBB84_45	# bb16.bb21.preheader_crit_edge
.LBB84_8:	# bb17
	movl	$1, %ecx
	subl	%r9d, %ecx
	imull	%esi, %ecx
.LBB84_9:	# bb21.preheader
	testl	%r9d, %r9d
	jle	.LBB84_18	# bb29
.LBB84_10:	# bb21.preheader.bb20_crit_edge
	xorl	%edx, %edx
	.align	16
.LBB84_11:	# bb20
	movslq	%ecx, %r8
	movl	$0, (%r10,%r8,4)
	addl	%esi, %ecx
	incl	%edx
	cmpl	%r9d, %edx
	jne	.LBB84_11	# bb20
	jmp	.LBB84_18	# bb29
.LBB84_12:	# bb22
	ucomiss	.LCPI84_0(%rip), %xmm1
	setnp	%cl
	sete	%dl
	testb	%cl, %dl
	jne	.LBB84_18	# bb29
.LBB84_13:	# bb23
	testl	%esi, %esi
	jg	.LBB84_46	# bb23.bb28.preheader_crit_edge
.LBB84_14:	# bb24
	movl	$1, %ecx
	subl	%r9d, %ecx
	imull	%esi, %ecx
.LBB84_15:	# bb28.preheader
	testl	%r9d, %r9d
	jle	.LBB84_18	# bb29
.LBB84_16:	# bb28.preheader.bb27_crit_edge
	xorl	%edx, %edx
	.align	16
.LBB84_17:	# bb27
	movslq	%ecx, %r8
	movaps	%xmm1, %xmm2
	mulss	(%r10,%r8,4), %xmm2
	movss	%xmm2, (%r10,%r8,4)
	addl	%esi, %ecx
	incl	%edx
	cmpl	%r9d, %edx
	jne	.LBB84_17	# bb27
.LBB84_18:	# bb29
	pxor	%xmm1, %xmm1
	ucomiss	%xmm1, %xmm0
	setnp	%cl
	sete	%dl
	testb	%cl, %dl
	jne	.LBB84_43	# return
.LBB84_19:	# bb30
	cmpl	$111, %eax
	jne	.LBB84_21	# bb34
.LBB84_20:	# bb30
	cmpl	$101, %edi
	je	.LBB84_23	# bb38
.LBB84_21:	# bb34
	cmpl	$112, %eax
	jne	.LBB84_31	# bb53
.LBB84_22:	# bb34
	cmpl	$102, %edi
	jne	.LBB84_31	# bb53
.LBB84_23:	# bb38
	testl	%esi, %esi
	jg	.LBB84_47	# bb38.bb52.preheader_crit_edge
.LBB84_24:	# bb39
	movl	$1, %eax
	subl	%r9d, %eax
	imull	%esi, %eax
.LBB84_25:	# bb52.preheader
	testl	%r9d, %r9d
	jle	.LBB84_43	# return
.LBB84_26:	# bb.nph104
	movl	$1, %edi
	subl	20(%rsp), %edi
	imull	104(%rsp), %edi
	movl	%edi, (%rsp)
	movl	$4294967294, %edi
	movl	16(%rsp), %ecx
	subl	%ecx, %edi
	movl	88(%rsp), %edx
	decl	%edx
	movl	%edx, 8(%rsp)
	movl	12(%rsp), %edx
	movl	%edx, %r8d
	negl	%r8d
	movl	%r8d, 4(%rsp)
	incl	%ecx
	movl	%ecx, 16(%rsp)
	xorl	%ecx, %ecx
	.align	16
.LBB84_27:	# bb42
	xorl	%r8d, %r8d
	movl	104(%rsp), %ebx
	testl	%ebx, %ebx
	movl	(%rsp), %r14d
	cmovg	%r8d, %r14d
	movl	4(%rsp), %r15d
	leal	(%r15,%rcx), %r15d
	cmpl	12(%rsp), %ecx
	cmovle	%r8d, %r15d
	movl	%r15d, %r8d
	imull	%ebx, %r8d
	movl	16(%rsp), %ebx
	leal	(%rbx,%rcx), %ebx
	movl	20(%rsp), %r12d
	cmpl	%r12d, %ebx
	cmovg	%r12d, %ebx
	cmpl	%ebx, %r15d
	jge	.LBB84_48	# bb42.bb51_crit_edge
.LBB84_28:	# bb.nph100
	movl	20(%rsp), %ebx
	notl	%ebx
	cmpl	%ebx, %edi
	cmovge	%edi, %ebx
	addl	%r15d, %ebx
	notl	%ebx
	addl	%edx, %r15d
	pxor	%xmm1, %xmm1
	xorl	%r12d, %r12d
	.align	16
.LBB84_29:	# bb49
	leal	(%r15,%r12), %r13d
	movslq	%r13d, %r13
	addl	%r14d, %r8d
	movslq	%r8d, %r14
	movq	96(%rsp), %rbp
	movss	(%rbp,%r14,4), %xmm2
	mulss	(%r11,%r13,4), %xmm2
	addss	%xmm2, %xmm1
	incl	%r12d
	cmpl	%ebx, %r12d
	movl	104(%rsp), %r14d
	jne	.LBB84_29	# bb49
.LBB84_30:	# bb51
	mulss	%xmm0, %xmm1
	movslq	%eax, %r8
	addss	(%r10,%r8,4), %xmm1
	movss	%xmm1, (%r10,%r8,4)
	addl	%esi, %eax
	addl	8(%rsp), %edx
	decl	%edi
	incl	%ecx
	cmpl	%r9d, %ecx
	jne	.LBB84_27	# bb42
	jmp	.LBB84_43	# return
.LBB84_31:	# bb53
	cmpl	$102, %edi
	sete	%cl
	cmpl	$111, %eax
	sete	%dl
	andb	%cl, %dl
	cmpl	$101, %edi
	sete	%cl
	cmpl	$112, %eax
	sete	%al
	testb	%cl, %al
	jne	.LBB84_33	# bb61
.LBB84_32:	# bb53
	notb	%dl
	testb	$1, %dl
	jne	.LBB84_42	# bb78
.LBB84_33:	# bb61
	cmpl	$0, 104(%rsp)
	jg	.LBB84_49	# bb61.bb77.preheader_crit_edge
.LBB84_34:	# bb62
	movl	$1, %eax
	subl	20(%rsp), %eax
	imull	104(%rsp), %eax
.LBB84_35:	# bb77.preheader
	cmpl	$0, 20(%rsp)
	jle	.LBB84_43	# return
.LBB84_36:	# bb.nph93
	movl	$1, %ecx
	subl	%r9d, %ecx
	imull	%esi, %ecx
	movl	%ecx, 4(%rsp)
	movl	$4294967294, %ecx
	movl	12(%rsp), %edx
	subl	%edx, %ecx
	movl	88(%rsp), %edi
	decl	%edi
	movl	%edi, 8(%rsp)
	movl	16(%rsp), %edi
	movl	%edi, %r8d
	negl	%r8d
	incl	%edx
	movl	%edx, 12(%rsp)
	xorl	%edx, %edx
	.align	16
.LBB84_37:	# bb65
	movslq	%eax, %rbx
	movaps	%xmm0, %xmm1
	movq	96(%rsp), %r14
	mulss	(%r14,%rbx,4), %xmm1
	pxor	%xmm2, %xmm2
	ucomiss	%xmm2, %xmm1
	setnp	%bl
	sete	%r14b
	testb	%bl, %r14b
	jne	.LBB84_41	# bb76
.LBB84_38:	# bb66
	xorl	%ebx, %ebx
	testl	%esi, %esi
	movl	4(%rsp), %r14d
	cmovg	%ebx, %r14d
	leal	(%r8,%rdx), %r15d
	cmpl	16(%rsp), %edx
	cmovle	%ebx, %r15d
	movl	%r15d, %ebx
	imull	%esi, %ebx
	movl	12(%rsp), %r12d
	leal	(%r12,%rdx), %r12d
	cmpl	%r9d, %r12d
	cmovg	%r9d, %r12d
	cmpl	%r12d, %r15d
	jge	.LBB84_41	# bb76
.LBB84_39:	# bb.nph90
	movl	%r9d, %r12d
	notl	%r12d
	cmpl	%r12d, %ecx
	cmovge	%ecx, %r12d
	addl	%r15d, %r12d
	notl	%r12d
	addl	%edi, %r15d
	xorl	%r13d, %r13d
	.align	16
.LBB84_40:	# bb74
	addl	%r14d, %ebx
	movslq	%ebx, %r14
	leal	(%r15,%r13), %ebp
	movslq	%ebp, %rbp
	movaps	%xmm1, %xmm2
	mulss	(%r11,%rbp,4), %xmm2
	addss	(%r10,%r14,4), %xmm2
	movss	%xmm2, (%r10,%r14,4)
	incl	%r13d
	cmpl	%r12d, %r13d
	movl	%esi, %r14d
	jne	.LBB84_40	# bb74
.LBB84_41:	# bb76
	addl	104(%rsp), %eax
	addl	8(%rsp), %edi
	decl	%ecx
	incl	%edx
	cmpl	20(%rsp), %edx
	jne	.LBB84_37	# bb65
	jmp	.LBB84_43	# return
.LBB84_42:	# bb78
	xorl	%edi, %edi
	leaq	.str97, %rsi
	leaq	.str198, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB84_43:	# return
	addq	$24, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB84_44:	# bb12.bb15_crit_edge
	movl	%r9d, 16(%rsp)
	movl	%edx, %r9d
	jmp	.LBB84_6	# bb15
.LBB84_45:	# bb16.bb21.preheader_crit_edge
	xorl	%ecx, %ecx
	jmp	.LBB84_9	# bb21.preheader
.LBB84_46:	# bb23.bb28.preheader_crit_edge
	xorl	%ecx, %ecx
	jmp	.LBB84_15	# bb28.preheader
.LBB84_47:	# bb38.bb52.preheader_crit_edge
	xorl	%eax, %eax
	jmp	.LBB84_25	# bb52.preheader
.LBB84_48:	# bb42.bb51_crit_edge
	pxor	%xmm1, %xmm1
	jmp	.LBB84_30	# bb51
.LBB84_49:	# bb61.bb77.preheader_crit_edge
	xorl	%eax, %eax
	jmp	.LBB84_35	# bb77.preheader
	.size	cblas_sgbmv, .-cblas_sgbmv
.Leh_func_end55:


	.section	.rodata.cst4,"aM",@progbits,4
	.align	4
.LCPI85_0:					
	.long	1065353216	# float 1.000000e+00
	.text
	.align	16
	.globl	cblas_sgemm
	.type	cblas_sgemm,@function
cblas_sgemm:
.Leh_func_begin56:
.Llabel56:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	ucomiss	.LCPI85_0(%rip), %xmm1
	movq	128(%rsp), %rbx
	movl	104(%rsp), %r14d
	movq	96(%rsp), %r15
	movss	%xmm1, 24(%rsp)
	movss	%xmm0, 36(%rsp)
	movl	%r9d, %r12d
	movl	%r8d, 12(%rsp)
	movl	%ecx, 32(%rsp)
	movl	%edx, 20(%rsp)
	movl	%esi, 16(%rsp)
	jne	.LBB85_2	# bb4
	jp	.LBB85_2	# bb4
.LBB85_1:	# entry
	pxor	%xmm0, %xmm0
	movss	36(%rsp), %xmm1
	ucomiss	%xmm0, %xmm1
	setnp	%al
	sete	%cl
	testb	%al, %cl
	jne	.LBB85_97	# return
.LBB85_2:	# bb4
	cmpl	$101, %edi
	jne	.LBB85_8	# bb12
.LBB85_3:	# bb5
	movl	$1, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB85_6	# real_catch1
.LBB85_4:	# jump1
	movl	$1, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB85_7	# real_end1
.LBB85_5:	# real_try1
	movl	$1, %edi
	call	llvm_real_try
	movl	20(%rsp), %eax
	cmpl	$113, %eax
	movl	$112, %ecx
	cmove	%ecx, %eax
	movl	%eax, 20(%rsp)
	movl	16(%rsp), %eax
	cmpl	$113, %eax
	cmove	%ecx, %eax
	movl	%eax, 16(%rsp)
	jmp	.LBB85_7	# real_end1
.LBB85_6:	# real_catch1
	movl	$1, %edi
	call	llvm_real_catch
.LBB85_7:	# real_end1
	movl	$1, %edi
	call	llvm_real_end
	movq	112(%rsp), %r13
	movl	120(%rsp), %eax
	movl	%eax, 28(%rsp)
	movl	12(%rsp), %ebp
	jmp	.LBB85_13	# bb19
.LBB85_8:	# bb12
	xorl	%edi, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB85_11	# real_catch0
.LBB85_9:	# jump0
	xorl	%edi, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB85_12	# real_end0
.LBB85_10:	# real_try0
	xorl	%edi, %edi
	call	llvm_real_try
	movl	16(%rsp), %ebp
	cmpl	$113, %ebp
	movl	$112, %eax
	cmove	%eax, %ebp
	movl	%ebp, 16(%rsp)
	movl	20(%rsp), %ebp
	cmpl	$113, %ebp
	cmove	%eax, %ebp
	movl	%ebp, 20(%rsp)
	jmp	.LBB85_12	# real_end0
.LBB85_11:	# real_catch0
	xorl	%edi, %edi
	call	llvm_real_catch
.LBB85_12:	# real_end0
	movl	20(%rsp), %ebp
	xorl	%edi, %edi
	call	llvm_real_end
	movq	%r15, %r13
	movq	112(%rsp), %r15
	movl	16(%rsp), %eax
	movl	%eax, 20(%rsp)
	movl	%ebp, 16(%rsp)
	movl	%r14d, 28(%rsp)
	movl	120(%rsp), %r14d
	movl	32(%rsp), %ebp
	movl	12(%rsp), %eax
	movl	%eax, 32(%rsp)
.LBB85_13:	# bb19
	movl	$2, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB85_29	# real_catch2
.LBB85_14:	# jump2
	movl	$2, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB85_30	# real_end2
.LBB85_15:	# real_try2
	movl	$2, %edi
	call	llvm_real_try
	pxor	%xmm0, %xmm0
	movss	24(%rsp), %xmm1
	ucomiss	%xmm0, %xmm1
	jne	.LBB85_22	# bb26
	jp	.LBB85_22	# bb26
.LBB85_16:	# bb25.preheader
	cmpl	$0, 32(%rsp)
	jle	.LBB85_30	# real_end2
.LBB85_17:	# bb25.preheader
	testl	%ebp, %ebp
	jle	.LBB85_30	# real_end2
.LBB85_18:	# bb25.preheader.bb23.preheader_crit_edge
	xorl	%eax, %eax
	movl	%eax, %ecx
	jmp	.LBB85_21	# bb23.preheader
	.align	16
.LBB85_19:	# bb22
	leal	(%rax,%rdx), %esi
	movslq	%esi, %rsi
	movl	$0, (%rbx,%rsi,4)
	incl	%edx
	cmpl	%ebp, %edx
	jne	.LBB85_19	# bb22
.LBB85_20:	# bb24
	addl	136(%rsp), %eax
	incl	%ecx
	cmpl	32(%rsp), %ecx
	je	.LBB85_30	# real_end2
.LBB85_21:	# bb23.preheader
	xorl	%edx, %edx
	jmp	.LBB85_19	# bb22
.LBB85_22:	# bb26
	movss	24(%rsp), %xmm0
	ucomiss	.LCPI85_0(%rip), %xmm0
	setnp	%al
	sete	%cl
	testb	%al, %cl
	jne	.LBB85_30	# real_end2
.LBB85_23:	# bb26
	cmpl	$0, 32(%rsp)
	jle	.LBB85_30	# real_end2
.LBB85_24:	# bb26
	testl	%ebp, %ebp
	jle	.LBB85_30	# real_end2
.LBB85_25:	# bb26.bb30.preheader_crit_edge
	xorl	%eax, %eax
	movl	%eax, %ecx
	jmp	.LBB85_28	# bb30.preheader
	.align	16
.LBB85_26:	# bb29
	leal	(%rax,%rdx), %esi
	movslq	%esi, %rsi
	movss	24(%rsp), %xmm0
	mulss	(%rbx,%rsi,4), %xmm0
	movss	%xmm0, (%rbx,%rsi,4)
	incl	%edx
	cmpl	%ebp, %edx
	jne	.LBB85_26	# bb29
.LBB85_27:	# bb31
	addl	136(%rsp), %eax
	incl	%ecx
	cmpl	32(%rsp), %ecx
	je	.LBB85_30	# real_end2
.LBB85_28:	# bb30.preheader
	xorl	%edx, %edx
	jmp	.LBB85_26	# bb29
.LBB85_29:	# real_catch2
	movl	$2, %edi
	call	llvm_real_catch
.LBB85_30:	# real_end2
	movl	$2, %edi
	call	llvm_real_end
	pxor	%xmm0, %xmm0
	movss	36(%rsp), %xmm1
	ucomiss	%xmm0, %xmm1
	setnp	%al
	sete	%cl
	testb	%al, %cl
	jne	.LBB85_97	# return
.LBB85_31:	# bb33
	cmpl	$111, 16(%rsp)
	jne	.LBB85_48	# bb46
.LBB85_32:	# bb33
	cmpl	$111, 20(%rsp)
	jne	.LBB85_48	# bb46
.LBB85_33:	# bb36
	movl	$3, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB85_45	# real_catch3
.LBB85_34:	# jump3
	movl	$3, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB85_46	# real_end3
.LBB85_35:	# real_try3
	movl	$3, %edi
	call	llvm_real_try
	testl	%r12d, %r12d
	jle	.LBB85_46	# real_end3
.LBB85_36:	# bb.nph123
	cmpl	$0, 32(%rsp)
	jle	.LBB85_46	# real_end3
.LBB85_37:	# bb.nph123.bb43.preheader_crit_edge
	xorl	%eax, %eax
	movl	%eax, %ecx
	jmp	.LBB85_44	# bb43.preheader
	.align	16
.LBB85_38:	# bb38
	movslq	%edi, %r8
	movss	36(%rsp), %xmm0
	mulss	(%r15,%r8,4), %xmm0
	pxor	%xmm1, %xmm1
	ucomiss	%xmm1, %xmm0
	setnp	%r8b
	sete	%r9b
	testl	%ebp, %ebp
	setle	%r10b
	testb	%r8b, %r9b
	jne	.LBB85_42	# bb42
.LBB85_39:	# bb38
	testb	$1, %r10b
	jne	.LBB85_42	# bb42
.LBB85_40:	# bb38.bb40_crit_edge
	xorl	%r8d, %r8d
	.align	16
.LBB85_41:	# bb40
	leal	(%rsi,%r8), %r9d
	movslq	%r9d, %r9
	leal	(%rax,%r8), %r10d
	movslq	%r10d, %r10
	movaps	%xmm0, %xmm1
	mulss	(%r13,%r10,4), %xmm1
	addss	(%rbx,%r9,4), %xmm1
	movss	%xmm1, (%rbx,%r9,4)
	incl	%r8d
	cmpl	%ebp, %r8d
	jne	.LBB85_41	# bb40
.LBB85_42:	# bb42
	addl	%r14d, %edi
	addl	136(%rsp), %esi
	incl	%edx
	cmpl	32(%rsp), %edx
	jne	.LBB85_38	# bb38
.LBB85_43:	# bb44
	addl	28(%rsp), %eax
	incl	%ecx
	cmpl	%r12d, %ecx
	je	.LBB85_46	# real_end3
.LBB85_44:	# bb43.preheader
	xorl	%esi, %esi
	movl	%ecx, %edi
	movl	%esi, %edx
	jmp	.LBB85_38	# bb38
.LBB85_45:	# real_catch3
	movl	$3, %edi
	call	llvm_real_catch
.LBB85_46:	# real_end3
	movl	$3, %edi
.LBB85_47:	# real_end3
	call	llvm_real_end
	jmp	.LBB85_97	# return
.LBB85_48:	# bb46
	cmpl	$111, 16(%rsp)
	jne	.LBB85_64	# bb59
.LBB85_49:	# bb46
	cmpl	$112, 20(%rsp)
	jne	.LBB85_64	# bb59
.LBB85_50:	# bb50
	movl	$4, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB85_62	# real_catch4
.LBB85_51:	# jump4
	movl	$4, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB85_63	# real_end4
.LBB85_52:	# real_try4
	movl	$4, %edi
	call	llvm_real_try
	cmpl	$0, 32(%rsp)
	jle	.LBB85_63	# real_end4
.LBB85_53:	# bb.nph117
	testl	%ebp, %ebp
	jle	.LBB85_63	# real_end4
.LBB85_54:	# bb.nph117.bb56.preheader_crit_edge
	xorl	%edx, %edx
	movl	%edx, %ecx
	movl	%edx, %edi
	jmp	.LBB85_61	# bb56.preheader
.LBB85_55:	# bb54.preheader.bb53_crit_edge
	pxor	%xmm0, %xmm0
	xorl	%eax, %eax
	.align	16
.LBB85_56:	# bb53
	leal	(%r8,%rax), %r9d
	movslq	%r9d, %r9
	leal	(%rdx,%rax), %r10d
	movslq	%r10d, %r10
	movss	(%r15,%r10,4), %xmm1
	mulss	(%r13,%r9,4), %xmm1
	addss	%xmm1, %xmm0
	incl	%eax
	cmpl	%r12d, %eax
	jne	.LBB85_56	# bb53
.LBB85_57:	# bb55
	leal	(%rcx,%rsi), %eax
	movslq	%eax, %rax
	mulss	36(%rsp), %xmm0
	addss	(%rbx,%rax,4), %xmm0
	movss	%xmm0, (%rbx,%rax,4)
	addl	28(%rsp), %r8d
	incl	%esi
	cmpl	%ebp, %esi
	je	.LBB85_60	# bb57
.LBB85_58:	# bb54.preheader
	testl	%r12d, %r12d
	jg	.LBB85_55	# bb54.preheader.bb53_crit_edge
.LBB85_59:	# bb54.preheader.bb55_crit_edge
	pxor	%xmm0, %xmm0
	jmp	.LBB85_57	# bb55
.LBB85_60:	# bb57
	addl	%r14d, %edx
	addl	136(%rsp), %ecx
	incl	%edi
	cmpl	32(%rsp), %edi
	je	.LBB85_63	# real_end4
.LBB85_61:	# bb56.preheader
	xorl	%r8d, %r8d
	movl	%r8d, %esi
	jmp	.LBB85_58	# bb54.preheader
.LBB85_62:	# real_catch4
	movl	$4, %edi
	call	llvm_real_catch
.LBB85_63:	# real_end4
	movl	$4, %edi
	jmp	.LBB85_47	# real_end3
.LBB85_64:	# bb59
	cmpl	$112, 16(%rsp)
	jne	.LBB85_80	# bb73
.LBB85_65:	# bb59
	cmpl	$111, 20(%rsp)
	jne	.LBB85_80	# bb73
.LBB85_66:	# bb63
	movl	$5, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB85_78	# real_catch5
.LBB85_67:	# jump5
	movl	$5, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB85_79	# real_end5
.LBB85_68:	# real_try5
	movl	$5, %edi
	call	llvm_real_try
	testl	%r12d, %r12d
	jle	.LBB85_79	# real_end5
.LBB85_69:	# bb.nph109
	cmpl	$0, 32(%rsp)
	jle	.LBB85_79	# real_end5
.LBB85_70:	# bb.nph109.bb70.preheader_crit_edge
	xorl	%edi, %edi
	movl	%edi, %eax
	movl	%edi, %edx
	jmp	.LBB85_77	# bb70.preheader
	.align	16
.LBB85_71:	# bb65
	leal	(%rax,%rcx), %r8d
	movslq	%r8d, %r8
	movss	36(%rsp), %xmm0
	mulss	(%r15,%r8,4), %xmm0
	pxor	%xmm1, %xmm1
	ucomiss	%xmm1, %xmm0
	setnp	%r8b
	sete	%r9b
	testl	%ebp, %ebp
	setle	%r10b
	testb	%r8b, %r9b
	jne	.LBB85_75	# bb69
.LBB85_72:	# bb65
	testb	$1, %r10b
	jne	.LBB85_75	# bb69
.LBB85_73:	# bb65.bb67_crit_edge
	xorl	%r8d, %r8d
	.align	16
.LBB85_74:	# bb67
	leal	(%rsi,%r8), %r9d
	movslq	%r9d, %r9
	leal	(%rdi,%r8), %r10d
	movslq	%r10d, %r10
	movaps	%xmm0, %xmm1
	mulss	(%r13,%r10,4), %xmm1
	addss	(%rbx,%r9,4), %xmm1
	movss	%xmm1, (%rbx,%r9,4)
	incl	%r8d
	cmpl	%ebp, %r8d
	jne	.LBB85_74	# bb67
.LBB85_75:	# bb69
	addl	136(%rsp), %esi
	incl	%ecx
	cmpl	32(%rsp), %ecx
	jne	.LBB85_71	# bb65
.LBB85_76:	# bb71
	addl	28(%rsp), %edi
	addl	%r14d, %eax
	incl	%edx
	cmpl	%r12d, %edx
	je	.LBB85_79	# real_end5
.LBB85_77:	# bb70.preheader
	xorl	%esi, %esi
	movl	%esi, %ecx
	jmp	.LBB85_71	# bb65
.LBB85_78:	# real_catch5
	movl	$5, %edi
	call	llvm_real_catch
.LBB85_79:	# real_end5
	movl	$5, %edi
	jmp	.LBB85_47	# real_end3
.LBB85_80:	# bb73
	cmpl	$112, 16(%rsp)
	jne	.LBB85_96	# bb86
.LBB85_81:	# bb73
	cmpl	$112, 20(%rsp)
	jne	.LBB85_96	# bb86
.LBB85_82:	# bb77
	movl	$6, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB85_94	# real_catch6
.LBB85_83:	# jump6
	movl	$6, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB85_95	# real_end6
.LBB85_84:	# real_try6
	movl	$6, %edi
	call	llvm_real_try
	cmpl	$0, 32(%rsp)
	jle	.LBB85_95	# real_end6
.LBB85_85:	# bb.nph103
	testl	%ebp, %ebp
	jle	.LBB85_95	# real_end6
.LBB85_86:	# bb.nph103.bb83.preheader_crit_edge
	xorl	%edi, %edi
	movl	%edi, %edx
	jmp	.LBB85_93	# bb83.preheader
.LBB85_87:	# bb81.preheader.bb80_crit_edge
	pxor	%xmm0, %xmm0
	xorl	%ecx, %ecx
	movl	%edx, %eax
	.align	16
.LBB85_88:	# bb80
	leal	(%rsi,%rcx), %r9d
	movslq	%r9d, %r9
	movslq	%eax, %r10
	movss	(%r15,%r10,4), %xmm1
	mulss	(%r13,%r9,4), %xmm1
	addss	%xmm1, %xmm0
	addl	%r14d, %eax
	incl	%ecx
	cmpl	%r12d, %ecx
	jne	.LBB85_88	# bb80
.LBB85_89:	# bb82
	leal	(%rdi,%r8), %eax
	movslq	%eax, %rax
	mulss	36(%rsp), %xmm0
	addss	(%rbx,%rax,4), %xmm0
	movss	%xmm0, (%rbx,%rax,4)
	addl	28(%rsp), %esi
	incl	%r8d
	cmpl	%ebp, %r8d
	je	.LBB85_92	# bb84
.LBB85_90:	# bb81.preheader
	testl	%r12d, %r12d
	jg	.LBB85_87	# bb81.preheader.bb80_crit_edge
.LBB85_91:	# bb81.preheader.bb82_crit_edge
	pxor	%xmm0, %xmm0
	jmp	.LBB85_89	# bb82
.LBB85_92:	# bb84
	addl	136(%rsp), %edi
	incl	%edx
	cmpl	32(%rsp), %edx
	je	.LBB85_95	# real_end6
.LBB85_93:	# bb83.preheader
	xorl	%esi, %esi
	movl	%esi, %r8d
	jmp	.LBB85_90	# bb81.preheader
.LBB85_94:	# real_catch6
	movl	$6, %edi
	call	llvm_real_catch
.LBB85_95:	# real_end6
	movl	$6, %edi
	jmp	.LBB85_47	# real_end3
.LBB85_96:	# bb86
	xorl	%edi, %edi
	leaq	.str99, %rsi
	leaq	.str1100, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB85_97:	# return
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
	.size	cblas_sgemm, .-cblas_sgemm
.Leh_func_end56:


	.section	.rodata.cst4,"aM",@progbits,4
	.align	4
.LCPI86_0:					
	.long	1065353216	# float 1.000000e+00
	.text
	.align	16
	.globl	cblas_sgemv
	.type	cblas_sgemv,@function
cblas_sgemv:
.Leh_func_begin57:
.Llabel57:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$24, %rsp
	cmpl	$113, %esi
	movl	$112, %eax
	cmovne	%esi, %eax
	movl	%eax, 8(%rsp)
	testl	%ecx, %ecx
	movl	104(%rsp), %ebx
	movq	96(%rsp), %r14
	movl	88(%rsp), %r15d
	movss	%xmm1, 12(%rsp)
	movl	%r9d, 16(%rsp)
	movq	%r8, %r12
	movss	%xmm0, 20(%rsp)
	movl	%ecx, %r13d
	movl	%edx, (%rsp)
	movl	%edi, 4(%rsp)
	je	.LBB86_53	# return
.LBB86_1:	# entry
	cmpl	$0, (%rsp)
	je	.LBB86_53	# return
.LBB86_2:	# bb10
	movss	12(%rsp), %xmm0
	ucomiss	.LCPI86_0(%rip), %xmm0
	jne	.LBB86_4	# bb12
	jp	.LBB86_4	# bb12
.LBB86_3:	# bb10
	pxor	%xmm0, %xmm0
	movss	20(%rsp), %xmm1
	ucomiss	%xmm0, %xmm1
	setnp	%al
	sete	%cl
	testb	%al, %cl
	jne	.LBB86_53	# return
.LBB86_4:	# bb12
	xorl	%edi, %edi
	call	llvm_real_split
	cmpl	$111, 8(%rsp)
	movl	(%rsp), %eax
	movl	%eax, %ebp
	cmove	%r13d, %ebp
	cmove	%eax, %r13d
	cmpl	$0, dummy
	je	.LBB86_18	# real_catch0
.LBB86_5:	# jump0
	xorl	%edi, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB86_19	# real_end0
.LBB86_6:	# real_try0
	xorl	%edi, %edi
	call	llvm_real_try
	pxor	%xmm0, %xmm0
	movss	12(%rsp), %xmm1
	ucomiss	%xmm0, %xmm1
	jne	.LBB86_12	# bb22
	jp	.LBB86_12	# bb22
.LBB86_7:	# bb16
	testl	%ebx, %ebx
	jg	.LBB86_54	# bb16.bb21.preheader_crit_edge
.LBB86_8:	# bb17
	movl	$1, %eax
	subl	%r13d, %eax
	imull	%ebx, %eax
.LBB86_9:	# bb21.preheader
	testl	%r13d, %r13d
	jle	.LBB86_19	# real_end0
.LBB86_10:	# bb21.preheader.bb20_crit_edge
	xorl	%ecx, %ecx
	.align	16
.LBB86_11:	# bb20
	movslq	%eax, %rdx
	movl	$0, (%r14,%rdx,4)
	addl	%ebx, %eax
	incl	%ecx
	cmpl	%r13d, %ecx
	jne	.LBB86_11	# bb20
	jmp	.LBB86_19	# real_end0
.LBB86_12:	# bb22
	movss	12(%rsp), %xmm0
	ucomiss	.LCPI86_0(%rip), %xmm0
	setnp	%al
	sete	%cl
	testb	%al, %cl
	jne	.LBB86_19	# real_end0
.LBB86_13:	# bb23
	testl	%ebx, %ebx
	jg	.LBB86_55	# bb23.bb28.preheader_crit_edge
.LBB86_14:	# bb24
	movl	$1, %eax
	subl	%r13d, %eax
	imull	%ebx, %eax
.LBB86_15:	# bb28.preheader
	testl	%r13d, %r13d
	jle	.LBB86_19	# real_end0
.LBB86_16:	# bb28.preheader.bb27_crit_edge
	xorl	%ecx, %ecx
	.align	16
.LBB86_17:	# bb27
	movslq	%eax, %rdx
	movss	12(%rsp), %xmm0
	mulss	(%r14,%rdx,4), %xmm0
	movss	%xmm0, (%r14,%rdx,4)
	addl	%ebx, %eax
	incl	%ecx
	cmpl	%r13d, %ecx
	jne	.LBB86_17	# bb27
	jmp	.LBB86_19	# real_end0
.LBB86_18:	# real_catch0
	xorl	%edi, %edi
	call	llvm_real_catch
.LBB86_19:	# real_end0
	xorl	%edi, %edi
	call	llvm_real_end
	pxor	%xmm0, %xmm0
	movss	20(%rsp), %xmm1
	ucomiss	%xmm0, %xmm1
	setnp	%al
	sete	%cl
	testb	%al, %cl
	jne	.LBB86_53	# return
.LBB86_20:	# bb29
	cmpl	$111, 8(%rsp)
	jne	.LBB86_22	# bb33
.LBB86_21:	# bb29
	cmpl	$101, 4(%rsp)
	je	.LBB86_24	# bb37
.LBB86_22:	# bb33
	cmpl	$112, 8(%rsp)
	jne	.LBB86_37	# bb49
.LBB86_23:	# bb33
	cmpl	$102, 4(%rsp)
	jne	.LBB86_37	# bb49
.LBB86_24:	# bb37
	movl	$1, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB86_34	# real_catch1
.LBB86_25:	# jump1
	movl	$1, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB86_35	# real_end1
.LBB86_26:	# real_try1
	movl	$1, %edi
	call	llvm_real_try
	testl	%ebx, %ebx
	jg	.LBB86_56	# real_try1.bb48.preheader_crit_edge
.LBB86_27:	# bb38
	movl	$1, %eax
	subl	%r13d, %eax
	imull	%ebx, %eax
.LBB86_28:	# bb48.preheader
	testl	%r13d, %r13d
	jle	.LBB86_35	# real_end1
.LBB86_29:	# bb.nph91
	movl	$1, %ecx
	subl	%ebp, %ecx
	imull	%r15d, %ecx
	xorl	%edx, %edx
	movl	%edx, %esi
	.align	16
.LBB86_30:	# bb41
	testl	%r15d, %r15d
	movl	$0, %edi
	cmovle	%ecx, %edi
	testl	%ebp, %ebp
	jle	.LBB86_57	# bb41.bb47_crit_edge
.LBB86_31:	# bb41.bb45_crit_edge
	pxor	%xmm0, %xmm0
	xorl	%r8d, %r8d
	.align	16
.LBB86_32:	# bb45
	leal	(%rdx,%r8), %r9d
	movslq	%r9d, %r9
	movslq	%edi, %r10
	movq	80(%rsp), %r11
	movss	(%r11,%r10,4), %xmm1
	mulss	(%r12,%r9,4), %xmm1
	addss	%xmm1, %xmm0
	addl	%r15d, %edi
	incl	%r8d
	cmpl	%ebp, %r8d
	jne	.LBB86_32	# bb45
.LBB86_33:	# bb47
	mulss	20(%rsp), %xmm0
	movslq	%eax, %rdi
	addss	(%r14,%rdi,4), %xmm0
	movss	%xmm0, (%r14,%rdi,4)
	addl	%ebx, %eax
	addl	16(%rsp), %edx
	incl	%esi
	cmpl	%r13d, %esi
	jne	.LBB86_30	# bb41
	jmp	.LBB86_35	# real_end1
.LBB86_34:	# real_catch1
	movl	$1, %edi
	call	llvm_real_catch
.LBB86_35:	# real_end1
	movl	$1, %edi
.LBB86_36:	# real_end1
	call	llvm_real_end
	jmp	.LBB86_53	# return
.LBB86_37:	# bb49
	movl	4(%rsp), %eax
	cmpl	$102, %eax
	sete	%cl
	movl	8(%rsp), %edx
	cmpl	$111, %edx
	sete	%sil
	andb	%cl, %sil
	cmpl	$101, %eax
	sete	%al
	cmpl	$112, %edx
	sete	%cl
	testb	%al, %cl
	jne	.LBB86_39	# bb57
.LBB86_38:	# bb49
	notb	%sil
	testb	$1, %sil
	jne	.LBB86_52	# bb70
.LBB86_39:	# bb57
	movl	$2, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB86_50	# real_catch2
.LBB86_40:	# jump2
	movl	$2, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB86_51	# real_end2
.LBB86_41:	# real_try2
	movl	$2, %edi
	call	llvm_real_try
	testl	%r15d, %r15d
	jg	.LBB86_58	# real_try2.bb69.preheader_crit_edge
.LBB86_42:	# bb58
	movl	$1, %eax
	subl	%ebp, %eax
	imull	%r15d, %eax
.LBB86_43:	# bb69.preheader
	testl	%ebp, %ebp
	jle	.LBB86_51	# real_end2
.LBB86_44:	# bb.nph83
	movl	$1, %ecx
	subl	%r13d, %ecx
	imull	%ebx, %ecx
	xorl	%edx, %edx
	movl	%edx, %esi
	.align	16
.LBB86_45:	# bb61
	movslq	%eax, %rdi
	movss	20(%rsp), %xmm0
	movq	80(%rsp), %r8
	mulss	(%r8,%rdi,4), %xmm0
	pxor	%xmm1, %xmm1
	ucomiss	%xmm1, %xmm0
	setnp	%dil
	sete	%r8b
	testb	%dil, %r8b
	jne	.LBB86_49	# bb68
.LBB86_46:	# bb62
	testl	%ebx, %ebx
	movl	$0, %edi
	cmovle	%ecx, %edi
	testl	%r13d, %r13d
	jle	.LBB86_49	# bb68
.LBB86_47:	# bb62.bb66_crit_edge
	xorl	%r8d, %r8d
	.align	16
.LBB86_48:	# bb66
	leal	(%rdx,%r8), %r9d
	movslq	%r9d, %r9
	movaps	%xmm0, %xmm1
	mulss	(%r12,%r9,4), %xmm1
	movslq	%edi, %r9
	addss	(%r14,%r9,4), %xmm1
	movss	%xmm1, (%r14,%r9,4)
	addl	%ebx, %edi
	incl	%r8d
	cmpl	%r13d, %r8d
	jne	.LBB86_48	# bb66
.LBB86_49:	# bb68
	addl	%r15d, %eax
	addl	16(%rsp), %edx
	incl	%esi
	cmpl	%ebp, %esi
	jne	.LBB86_45	# bb61
	jmp	.LBB86_51	# real_end2
.LBB86_50:	# real_catch2
	movl	$2, %edi
	call	llvm_real_catch
.LBB86_51:	# real_end2
	movl	$2, %edi
	jmp	.LBB86_36	# real_end1
.LBB86_52:	# bb70
	xorl	%edi, %edi
	leaq	.str101, %rsi
	leaq	.str1102, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB86_53:	# return
	addq	$24, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB86_54:	# bb16.bb21.preheader_crit_edge
	xorl	%eax, %eax
	jmp	.LBB86_9	# bb21.preheader
.LBB86_55:	# bb23.bb28.preheader_crit_edge
	xorl	%eax, %eax
	jmp	.LBB86_15	# bb28.preheader
.LBB86_56:	# real_try1.bb48.preheader_crit_edge
	xorl	%eax, %eax
	jmp	.LBB86_28	# bb48.preheader
.LBB86_57:	# bb41.bb47_crit_edge
	pxor	%xmm0, %xmm0
	jmp	.LBB86_33	# bb47
.LBB86_58:	# real_try2.bb69.preheader_crit_edge
	xorl	%eax, %eax
	jmp	.LBB86_43	# bb69.preheader
	.size	cblas_sgemv, .-cblas_sgemv
.Leh_func_end57:


	.align	16
	.globl	cblas_sger
	.type	cblas_sger,@function
cblas_sger:
.Leh_func_begin58:
.Llabel58:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$8, %rsp
	cmpl	$102, %edi
	movq	72(%rsp), %rbx
	movq	%r9, %r14
	movl	%r8d, %r15d
	movq	%rcx, %r12
	movss	%xmm0, 4(%rsp)
	movl	%edx, %r13d
	movl	%esi, %ebp
	je	.LBB87_16	# bb16
.LBB87_1:	# entry
	cmpl	$101, %edi
	jne	.LBB87_28	# bb28
.LBB87_2:	# bb
	xorl	%edi, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB87_12	# real_catch0
.LBB87_3:	# jump0
	xorl	%edi, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB87_13	# real_end0
.LBB87_4:	# real_try0
	xorl	%edi, %edi
	call	llvm_real_try
	testl	%r15d, %r15d
	jg	.LBB87_29	# real_try0.bb14.preheader_crit_edge
.LBB87_5:	# bb4
	movl	$1, %eax
	subl	%ebp, %eax
	imull	%r15d, %eax
.LBB87_6:	# bb14.preheader
	testl	%ebp, %ebp
	jle	.LBB87_13	# real_end0
.LBB87_7:	# bb.nph40
	movl	$1, %ecx
	subl	%r13d, %ecx
	imull	64(%rsp), %ecx
	xorl	%edx, %edx
	movl	%edx, %esi
	.align	16
.LBB87_8:	# bb7
	cmpl	$0, 64(%rsp)
	movl	$0, %edi
	cmovle	%ecx, %edi
	movslq	%eax, %r8
	movss	4(%rsp), %xmm0
	mulss	(%r12,%r8,4), %xmm0
	testl	%r13d, %r13d
	jle	.LBB87_11	# bb13
.LBB87_9:	# bb7.bb11_crit_edge
	xorl	%r8d, %r8d
	.align	16
.LBB87_10:	# bb11
	movslq	%edi, %r9
	movaps	%xmm0, %xmm1
	mulss	(%r14,%r9,4), %xmm1
	leal	(%rdx,%r8), %r9d
	movslq	%r9d, %r9
	addss	(%rbx,%r9,4), %xmm1
	movss	%xmm1, (%rbx,%r9,4)
	addl	64(%rsp), %edi
	incl	%r8d
	cmpl	%r13d, %r8d
	jne	.LBB87_10	# bb11
.LBB87_11:	# bb13
	addl	%r15d, %eax
	addl	80(%rsp), %edx
	incl	%esi
	cmpl	%ebp, %esi
	jne	.LBB87_8	# bb7
	jmp	.LBB87_13	# real_end0
.LBB87_12:	# real_catch0
	xorl	%edi, %edi
	call	llvm_real_catch
.LBB87_13:	# real_end0
	xorl	%edi, %edi
.LBB87_14:	# real_end0
	call	llvm_real_end
.LBB87_15:	# real_end0
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB87_16:	# bb16
	movl	$1, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB87_26	# real_catch1
.LBB87_17:	# jump1
	movl	$1, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB87_27	# real_end1
.LBB87_18:	# real_try1
	movl	$1, %edi
	call	llvm_real_try
	cmpl	$0, 64(%rsp)
	jg	.LBB87_30	# real_try1.bb27.preheader_crit_edge
.LBB87_19:	# bb17
	movl	$1, %eax
	subl	%r13d, %eax
	imull	64(%rsp), %eax
.LBB87_20:	# bb27.preheader
	testl	%r13d, %r13d
	jle	.LBB87_27	# real_end1
.LBB87_21:	# bb.nph34
	movl	$1, %ecx
	subl	%ebp, %ecx
	imull	%r15d, %ecx
	xorl	%edx, %edx
	movl	%edx, %esi
	.align	16
.LBB87_22:	# bb20
	testl	%r15d, %r15d
	movl	$0, %edi
	cmovle	%ecx, %edi
	movslq	%eax, %r8
	movss	4(%rsp), %xmm0
	mulss	(%r14,%r8,4), %xmm0
	testl	%ebp, %ebp
	jle	.LBB87_25	# bb26
.LBB87_23:	# bb20.bb24_crit_edge
	xorl	%r8d, %r8d
	.align	16
.LBB87_24:	# bb24
	movslq	%edi, %r9
	movaps	%xmm0, %xmm1
	mulss	(%r12,%r9,4), %xmm1
	leal	(%rdx,%r8), %r9d
	movslq	%r9d, %r9
	addss	(%rbx,%r9,4), %xmm1
	movss	%xmm1, (%rbx,%r9,4)
	addl	%r15d, %edi
	incl	%r8d
	cmpl	%ebp, %r8d
	jne	.LBB87_24	# bb24
.LBB87_25:	# bb26
	addl	64(%rsp), %eax
	addl	80(%rsp), %edx
	incl	%esi
	cmpl	%r13d, %esi
	jne	.LBB87_22	# bb20
	jmp	.LBB87_27	# real_end1
.LBB87_26:	# real_catch1
	movl	$1, %edi
	call	llvm_real_catch
.LBB87_27:	# real_end1
	movl	$1, %edi
	jmp	.LBB87_14	# real_end0
.LBB87_28:	# bb28
	xorl	%edi, %edi
	leaq	.str103, %rsi
	leaq	.str1104, %rdx
	xorb	%al, %al
	call	cblas_xerbla
	jmp	.LBB87_15	# real_end0
.LBB87_29:	# real_try0.bb14.preheader_crit_edge
	xorl	%eax, %eax
	jmp	.LBB87_6	# bb14.preheader
.LBB87_30:	# real_try1.bb27.preheader_crit_edge
	xorl	%eax, %eax
	jmp	.LBB87_20	# bb27.preheader
	.size	cblas_sger, .-cblas_sger
.Leh_func_end58:


	.section	.rodata.cst4,"aM",@progbits,4
	.align	16
.LCPI88_0:					
	.long	1065353216	# float 1.000000e+00
	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI88_1:					
	.long	2147483647	# float nan
	.long	2147483647	# float nan
	.long	2147483647	# float nan
	.long	2147483647	# float nan
	.text
	.align	16
	.globl	cblas_snrm2
	.type	cblas_snrm2,@function
cblas_snrm2:
	testl	%edx, %edx
	jle	.LBB88_12	# bb13
.LBB88_1:	# entry
	testl	%edi, %edi
	jle	.LBB88_12	# bb13
.LBB88_2:	# bb3
	cmpl	$1, %edi
	je	.LBB88_5	# bb4
.LBB88_3:	# bb11.preheader
	testl	%edi, %edi
	jg	.LBB88_6	# bb11.preheader.bb6_crit_edge
.LBB88_4:	# bb11.preheader.bb12_crit_edge
	movss	.LCPI88_0(%rip), %xmm0
	pxor	%xmm1, %xmm1
	jmp	.LBB88_11	# bb12
.LBB88_5:	# bb4
	movss	(%rsi), %xmm0
	andps	.LCPI88_1(%rip), %xmm0
	ret
.LBB88_6:	# bb11.preheader.bb6_crit_edge
	movss	.LCPI88_0(%rip), %xmm0
	pxor	%xmm1, %xmm1
	xorl	%eax, %eax
	movl	%eax, %ecx
	.align	16
.LBB88_7:	# bb6
	movslq	%eax, %r8
	movss	(%rsi,%r8,4), %xmm2
	pxor	%xmm3, %xmm3
	ucomiss	%xmm3, %xmm2
	jne	.LBB88_8	# bb7
	jp	.LBB88_8	# bb7
	jmp	.LBB88_10	# bb10
.LBB88_8:	# bb7
	andps	.LCPI88_1(%rip), %xmm2
	ucomiss	%xmm1, %xmm2
	ja	.LBB88_13	# bb8
.LBB88_9:	# bb9
	divss	%xmm1, %xmm2
	mulss	%xmm2, %xmm2
	addss	%xmm2, %xmm0
.LBB88_10:	# bb10
	addl	%edx, %eax
	incl	%ecx
	cmpl	%edi, %ecx
	jne	.LBB88_7	# bb6
.LBB88_11:	# bb12
	cvtss2sd	%xmm0, %xmm0
	sqrtsd	%xmm0, %xmm0
	cvtss2sd	%xmm1, %xmm1
	mulsd	%xmm0, %xmm1
	cvtsd2ss	%xmm1, %xmm0
	ret
.LBB88_12:	# bb13
	pxor	%xmm0, %xmm0
	ret
.LBB88_13:	# bb8
	divss	%xmm2, %xmm1
	mulss	%xmm1, %xmm0
	mulss	%xmm1, %xmm0
	addss	.LCPI88_0(%rip), %xmm0
	movaps	%xmm2, %xmm1
	jmp	.LBB88_10	# bb10
	.size	cblas_snrm2, .-cblas_snrm2


	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI89_0:					
	.long	2147483648	# float -0.000000e+00
	.long	2147483648	# float -0.000000e+00
	.long	2147483648	# float -0.000000e+00
	.long	2147483648	# float -0.000000e+00
	.text
	.align	16
	.globl	cblas_srot
	.type	cblas_srot,@function
cblas_srot:
.Leh_func_begin59:
.Llabel59:
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$16, %rsp
	movss	%xmm1, 12(%rsp)
	movss	%xmm0, 8(%rsp)
	movl	%r8d, %ebx
	movq	%rcx, %r14
	movl	%edx, %r15d
	movq	%rsi, %r12
	movl	%edi, %r13d
	xorl	%edi, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB89_9	# real_catch0
.LBB89_1:	# jump0
	xorl	%edi, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB89_10	# real_end0
.LBB89_2:	# real_try0
	xorl	%edi, %edi
	call	llvm_real_try
	testl	%r15d, %r15d
	jg	.LBB89_11	# real_try0.bb2_crit_edge
.LBB89_3:	# bb
	movl	$1, %eax
	subl	%r13d, %eax
	imull	%r15d, %eax
.LBB89_4:	# bb2
	testl	%ebx, %ebx
	jg	.LBB89_12	# bb2.bb7.preheader_crit_edge
.LBB89_5:	# bb3
	movl	$1, %ecx
	subl	%r13d, %ecx
	imull	%ebx, %ecx
.LBB89_6:	# bb7.preheader
	testl	%r13d, %r13d
	jle	.LBB89_10	# real_end0
.LBB89_7:	# bb.nph
	movss	12(%rsp), %xmm0
	xorps	.LCPI89_0(%rip), %xmm0
	xorl	%edx, %edx
	.align	16
.LBB89_8:	# bb6
	movslq	%ecx, %rsi
	movss	(%r14,%rsi,4), %xmm1
	movaps	%xmm1, %xmm2
	mulss	12(%rsp), %xmm2
	movslq	%eax, %rdi
	movss	(%r12,%rdi,4), %xmm3
	movaps	%xmm3, %xmm4
	movss	8(%rsp), %xmm5
	mulss	%xmm5, %xmm4
	addss	%xmm2, %xmm4
	movss	%xmm4, (%r12,%rdi,4)
	mulss	%xmm5, %xmm1
	mulss	%xmm0, %xmm3
	addss	%xmm1, %xmm3
	movss	%xmm3, (%r14,%rsi,4)
	addl	%ebx, %ecx
	addl	%r15d, %eax
	incl	%edx
	cmpl	%r13d, %edx
	jne	.LBB89_8	# bb6
	jmp	.LBB89_10	# real_end0
.LBB89_9:	# real_catch0
	xorl	%edi, %edi
	call	llvm_real_catch
.LBB89_10:	# real_end0
	xorl	%edi, %edi
	call	llvm_real_end
	addq	$16, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	ret
.LBB89_11:	# real_try0.bb2_crit_edge
	xorl	%eax, %eax
	jmp	.LBB89_4	# bb2
.LBB89_12:	# bb2.bb7.preheader_crit_edge
	xorl	%ecx, %ecx
	jmp	.LBB89_6	# bb7.preheader
	.size	cblas_srot, .-cblas_srot
.Leh_func_end59:


	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI90_0:					
	.long	2147483647	# float nan
	.long	2147483647	# float nan
	.long	2147483647	# float nan
	.long	2147483647	# float nan
	.section	.rodata.cst4,"aM",@progbits,4
	.align	16
.LCPI90_1:					
	.long	1065353216	# float 1.000000e+00
.LCPI90_2:					
	.long	3212836864	# float -1.000000e+00
	.text
	.align	16
	.globl	cblas_srotg
	.type	cblas_srotg,@function
cblas_srotg:
	movss	(%rsi), %xmm0
	movss	.LCPI90_0(%rip), %xmm1
	movaps	%xmm0, %xmm2
	andps	%xmm1, %xmm2
	movss	(%rdi), %xmm3
	andps	%xmm3, %xmm1
	movaps	%xmm1, %xmm4
	addss	%xmm2, %xmm4
	pxor	%xmm5, %xmm5
	ucomiss	%xmm5, %xmm4
	setnp	%al
	sete	%r8b
	testb	%al, %r8b
	jne	.LBB90_8	# bb11
.LBB90_1:	# bb3
	divss	%xmm4, %xmm0
	mulss	%xmm0, %xmm0
	movaps	%xmm3, %xmm5
	divss	%xmm4, %xmm5
	mulss	%xmm5, %xmm5
	addss	%xmm0, %xmm5
	cvtss2sd	%xmm5, %xmm0
	sqrtsd	%xmm0, %xmm0
	cvtss2sd	%xmm4, %xmm4
	mulsd	%xmm0, %xmm4
	cvtsd2ss	%xmm4, %xmm0
	ucomiss	%xmm2, %xmm1
	movq	%rsi, %rax
	cmova	%rdi, %rax
	movss	(%rax), %xmm1
	pxor	%xmm2, %xmm2
	ucomiss	%xmm2, %xmm1
	movss	.LCPI90_1(%rip), %xmm1
	movss	.LCPI90_2(%rip), %xmm2
	jb	.LBB90_3	# bb3
.LBB90_2:	# bb3
	movaps	%xmm1, %xmm2
.LBB90_3:	# bb3
	mulss	%xmm0, %xmm2
	divss	%xmm2, %xmm3
	movss	%xmm3, (%rdx)
	movss	(%rsi), %xmm0
	divss	%xmm2, %xmm0
	movss	%xmm0, (%rcx)
	movss	(%rsi), %xmm3
	movss	.LCPI90_0(%rip), %xmm4
	andps	%xmm4, %xmm3
	movss	(%rdi), %xmm5
	andps	%xmm4, %xmm5
	ucomiss	%xmm3, %xmm5
	ja	.LBB90_5	# bb3
.LBB90_4:	# bb3
	movaps	%xmm1, %xmm0
.LBB90_5:	# bb3
	ucomiss	%xmm5, %xmm3
	jb	.LBB90_9	# bb12
.LBB90_6:	# bb9
	movss	(%rdx), %xmm1
	pxor	%xmm3, %xmm3
	ucomiss	%xmm3, %xmm1
	setnp	%al
	sete	%cl
	testb	%al, %cl
	jne	.LBB90_9	# bb12
.LBB90_7:	# bb10
	movss	.LCPI90_1(%rip), %xmm0
	divss	%xmm1, %xmm0
	jmp	.LBB90_9	# bb12
.LBB90_8:	# bb11
	movl	$1065353216, (%rdx)
	movl	$0, (%rcx)
	pxor	%xmm0, %xmm0
	movaps	%xmm0, %xmm2
.LBB90_9:	# bb12
	movss	%xmm2, (%rdi)
	movss	%xmm0, (%rsi)
	ret
	.size	cblas_srotg, .-cblas_srotg


	.section	.rodata.cst4,"aM",@progbits,4
	.align	4
.LCPI91_0:					
	.long	3212836864	# float -1.000000e+00
.LCPI91_1:					
	.long	1065353216	# float 1.000000e+00
.LCPI91_2:					
	.long	3221225472	# float -2.000000e+00
	.text
	.align	16
	.globl	cblas_srotm
	.type	cblas_srotm,@function
cblas_srotm:
.Leh_func_begin60:
.Llabel60:
	pushq	%rbx
	testl	%edx, %edx
	jg	.LBB91_16	# entry.bb2_crit_edge
.LBB91_1:	# bb
	movl	$1, %eax
	subl	%edi, %eax
	imull	%edx, %eax
.LBB91_2:	# bb2
	testl	%r8d, %r8d
	jg	.LBB91_17	# bb2.bb5_crit_edge
.LBB91_3:	# bb3
	movl	$1, %r10d
	subl	%edi, %r10d
	imull	%r8d, %r10d
.LBB91_4:	# bb5
	movss	(%r9), %xmm0
	ucomiss	.LCPI91_0(%rip), %xmm0
	jne	.LBB91_9	# bb7
	jp	.LBB91_9	# bb7
.LBB91_5:	# bb6
	movss	16(%r9), %xmm0
	movss	12(%r9), %xmm1
	movss	8(%r9), %xmm2
	movss	4(%r9), %xmm3
.LBB91_6:	# bb15.preheader
	testl	%edi, %edi
	jle	.LBB91_15	# return
.LBB91_7:	# bb15.preheader.bb14_crit_edge
	xorl	%r9d, %r9d
	.align	16
.LBB91_8:	# bb14
	movslq	%r10d, %r11
	movss	(%rcx,%r11,4), %xmm4
	movaps	%xmm1, %xmm5
	mulss	%xmm4, %xmm5
	movslq	%eax, %rbx
	movss	(%rsi,%rbx,4), %xmm6
	movaps	%xmm3, %xmm7
	mulss	%xmm6, %xmm7
	addss	%xmm5, %xmm7
	movss	%xmm7, (%rsi,%rbx,4)
	mulss	%xmm0, %xmm4
	mulss	%xmm2, %xmm6
	addss	%xmm4, %xmm6
	movss	%xmm6, (%rcx,%r11,4)
	addl	%r8d, %r10d
	addl	%edx, %eax
	incl	%r9d
	cmpl	%edi, %r9d
	jne	.LBB91_8	# bb14
	jmp	.LBB91_15	# return
.LBB91_9:	# bb7
	pxor	%xmm1, %xmm1
	ucomiss	%xmm1, %xmm0
	jne	.LBB91_11	# bb9
	jp	.LBB91_11	# bb9
.LBB91_10:	# bb8
	movss	12(%r9), %xmm1
	movss	8(%r9), %xmm2
	movss	.LCPI91_1(%rip), %xmm3
	movaps	%xmm3, %xmm0
	jmp	.LBB91_6	# bb15.preheader
.LBB91_11:	# bb9
	ucomiss	.LCPI91_1(%rip), %xmm0
	jne	.LBB91_13	# bb11
	jp	.LBB91_13	# bb11
.LBB91_12:	# bb10
	movss	16(%r9), %xmm0
	movss	4(%r9), %xmm3
	movss	.LCPI91_1(%rip), %xmm1
	movss	.LCPI91_0(%rip), %xmm2
	jmp	.LBB91_6	# bb15.preheader
.LBB91_13:	# bb11
	ucomiss	.LCPI91_2(%rip), %xmm0
	setnp	%al
	sete	%cl
	testb	%al, %cl
	jne	.LBB91_15	# return
.LBB91_14:	# bb12
	xorl	%edi, %edi
	leaq	.str105, %rsi
	leaq	.str1106, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB91_15:	# return
	popq	%rbx
	ret
.LBB91_16:	# entry.bb2_crit_edge
	xorl	%eax, %eax
	jmp	.LBB91_2	# bb2
.LBB91_17:	# bb2.bb5_crit_edge
	xorl	%r10d, %r10d
	jmp	.LBB91_4	# bb5
	.size	cblas_srotm, .-cblas_srotm
.Leh_func_end60:


	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI92_0:					
	.long	2147483647	# float nan
	.long	2147483647	# float nan
	.long	2147483647	# float nan
	.long	2147483647	# float nan
.LCPI92_1:					
	.long	2147483648	# float -0.000000e+00
	.long	2147483648	# float -0.000000e+00
	.long	2147483648	# float -0.000000e+00
	.long	2147483648	# float -0.000000e+00
	.section	.rodata.cst4,"aM",@progbits,4
	.align	16
.LCPI92_2:					
	.long	1065353216	# float 1.000000e+00
.LCPI92_3:					
	.long	3212836864	# float -1.000000e+00
.LCPI92_4:					
	.long	864026624	# float 5.960464e-08
.LCPI92_5:					
	.long	1266679808	# float 1.677722e+07
.LCPI92_6:					
	.long	964689920	# float 2.441406e-04
.LCPI92_7:					
	.long	1166016512	# float 4.096000e+03
	.text
	.align	16
	.globl	cblas_srotmg
	.type	cblas_srotmg,@function
cblas_srotmg:
	movss	(%rdi), %xmm1
	pxor	%xmm2, %xmm2
	ucomiss	%xmm1, %xmm2
	movss	(%rdx), %xmm2
	movss	(%rsi), %xmm3
	ja	.LBB92_31	# bb
.LBB92_1:	# bb1
	movaps	%xmm3, %xmm4
	mulss	%xmm0, %xmm4
	pxor	%xmm5, %xmm5
	ucomiss	%xmm5, %xmm4
	jne	.LBB92_3	# bb3
	jp	.LBB92_3	# bb3
.LBB92_2:	# bb2
	movl	$3221225472, (%rcx)
	ret
.LBB92_3:	# bb3
	movaps	%xmm4, %xmm5
	mulss	%xmm0, %xmm5
	movss	.LCPI92_0(%rip), %xmm6
	movaps	%xmm5, %xmm7
	andps	%xmm6, %xmm7
	movaps	%xmm1, %xmm8
	mulss	%xmm2, %xmm8
	movaps	%xmm8, %xmm9
	mulss	%xmm2, %xmm9
	andps	%xmm6, %xmm9
	ucomiss	%xmm7, %xmm9
	jbe	.LBB92_6	# bb7
.LBB92_4:	# bb4
	movl	$0, (%rcx)
	xorps	.LCPI92_1(%rip), %xmm0
	divss	%xmm2, %xmm0
	divss	%xmm8, %xmm4
	movaps	%xmm0, %xmm5
	mulss	%xmm4, %xmm5
	movss	.LCPI92_2(%rip), %xmm6
	subss	%xmm5, %xmm6
	pxor	%xmm5, %xmm5
	ucomiss	%xmm6, %xmm5
	jae	.LBB92_31	# bb
.LBB92_5:	# bb4.bb11.preheader_crit_edge
	movss	.LCPI92_2(%rip), %xmm5
	movaps	%xmm0, %xmm7
	movaps	%xmm5, %xmm8
	movaps	%xmm3, %xmm0
	movaps	%xmm6, %xmm9
	movaps	%xmm6, %xmm3
	jmp	.LBB92_8	# bb11.preheader
.LBB92_6:	# bb7
	pxor	%xmm6, %xmm6
	ucomiss	%xmm5, %xmm6
	ja	.LBB92_31	# bb
.LBB92_7:	# bb9
	movl	$1065353216, (%rcx)
	divss	%xmm0, %xmm2
	divss	%xmm4, %xmm8
	movaps	%xmm8, %xmm6
	mulss	%xmm2, %xmm6
	movss	.LCPI92_2(%rip), %xmm4
	addss	%xmm4, %xmm6
	movss	.LCPI92_3(%rip), %xmm7
	movaps	%xmm2, %xmm5
	movaps	%xmm0, %xmm2
	movaps	%xmm1, %xmm0
	movaps	%xmm6, %xmm9
	movaps	%xmm3, %xmm1
	movaps	%xmm6, %xmm3
.LBB92_8:	# bb11.preheader
	divss	%xmm3, %xmm1
	pxor	%xmm3, %xmm3
	ucomiss	%xmm3, %xmm1
	setnp	%al
	sete	%r8b
	testb	%al, %r8b
	mulss	%xmm2, %xmm6
	divss	%xmm9, %xmm0
	jne	.LBB92_13	# bb14.loopexit
.LBB92_9:	# bb11.preheader
	movss	.LCPI92_4(%rip), %xmm2
	ucomiss	%xmm1, %xmm2
	jb	.LBB92_13	# bb14.loopexit
	.align	16
.LBB92_10:	# bb10
	mulss	.LCPI92_5(%rip), %xmm1
	pxor	%xmm2, %xmm2
	ucomiss	%xmm2, %xmm1
	setnp	%al
	sete	%r8b
	testb	%al, %r8b
	movss	.LCPI92_6(%rip), %xmm2
	mulss	%xmm2, %xmm6
	mulss	%xmm2, %xmm4
	mulss	%xmm2, %xmm8
	jne	.LBB92_12	# bb11.bb14.loopexit_crit_edge
.LBB92_11:	# bb10
	movss	.LCPI92_4(%rip), %xmm2
	ucomiss	%xmm1, %xmm2
	jae	.LBB92_10	# bb10
.LBB92_12:	# bb11.bb14.loopexit_crit_edge
	movl	$3212836864, (%rcx)
.LBB92_13:	# bb14.loopexit
	ucomiss	.LCPI92_5(%rip), %xmm1
	jb	.LBB92_16	# bb16.loopexit
	.align	16
.LBB92_14:	# bb13
	movss	.LCPI92_7(%rip), %xmm2
	mulss	%xmm2, %xmm4
	mulss	%xmm2, %xmm8
	mulss	%xmm2, %xmm6
	mulss	.LCPI92_4(%rip), %xmm1
	ucomiss	.LCPI92_5(%rip), %xmm1
	jae	.LBB92_14	# bb13
.LBB92_15:	# bb14.bb16.loopexit_crit_edge
	movl	$3212836864, (%rcx)
.LBB92_16:	# bb16.loopexit
	movaps	%xmm0, %xmm2
	andps	.LCPI92_0(%rip), %xmm2
	movss	.LCPI92_4(%rip), %xmm3
	ucomiss	%xmm2, %xmm3
	jb	.LBB92_21	# bb19.loopexit
.LBB92_17:	# bb16.loopexit
	pxor	%xmm2, %xmm2
	ucomiss	%xmm2, %xmm0
	jne	.LBB92_18	# bb15
	jp	.LBB92_18	# bb15
	jmp	.LBB92_21	# bb19.loopexit
	.align	16
.LBB92_18:	# bb15
	mulss	.LCPI92_5(%rip), %xmm0
	movaps	%xmm0, %xmm2
	andps	.LCPI92_0(%rip), %xmm2
	movss	.LCPI92_4(%rip), %xmm3
	ucomiss	%xmm2, %xmm3
	movss	.LCPI92_6(%rip), %xmm2
	mulss	%xmm2, %xmm5
	mulss	%xmm2, %xmm7
	jb	.LBB92_20	# bb16.bb19.loopexit_crit_edge
.LBB92_19:	# bb15
	pxor	%xmm2, %xmm2
	ucomiss	%xmm2, %xmm0
	jne	.LBB92_18	# bb15
	jp	.LBB92_18	# bb15
.LBB92_20:	# bb16.bb19.loopexit_crit_edge
	movl	$3212836864, (%rcx)
.LBB92_21:	# bb19.loopexit
	movaps	%xmm0, %xmm2
	andps	.LCPI92_0(%rip), %xmm2
	ucomiss	.LCPI92_5(%rip), %xmm2
	jb	.LBB92_24	# bb20
	.align	16
.LBB92_22:	# bb18
	movss	.LCPI92_7(%rip), %xmm2
	mulss	%xmm2, %xmm5
	mulss	%xmm2, %xmm7
	mulss	.LCPI92_4(%rip), %xmm0
	movaps	%xmm0, %xmm2
	andps	.LCPI92_0(%rip), %xmm2
	ucomiss	.LCPI92_5(%rip), %xmm2
	jae	.LBB92_22	# bb18
.LBB92_23:	# bb19.bb20_crit_edge
	movl	$3212836864, (%rcx)
.LBB92_24:	# bb20
	movss	%xmm1, (%rdi)
	movss	%xmm0, (%rsi)
	movss	%xmm6, (%rdx)
	movss	(%rcx), %xmm0
	ucomiss	.LCPI92_3(%rip), %xmm0
	jne	.LBB92_26	# bb22
	jp	.LBB92_26	# bb22
.LBB92_25:	# bb21
	movss	%xmm8, 4(%rcx)
	movss	%xmm7, 8(%rcx)
	movss	%xmm4, 12(%rcx)
	movss	%xmm5, 16(%rcx)
	ret
.LBB92_26:	# bb22
	pxor	%xmm1, %xmm1
	ucomiss	%xmm1, %xmm0
	jne	.LBB92_28	# bb24
	jp	.LBB92_28	# bb24
.LBB92_27:	# bb23
	movss	%xmm7, 8(%rcx)
	movss	%xmm4, 12(%rcx)
	ret
.LBB92_28:	# bb24
	ucomiss	.LCPI92_2(%rip), %xmm0
	jne	.LBB92_30	# return
	jp	.LBB92_30	# return
.LBB92_29:	# bb25
	movss	%xmm8, 4(%rcx)
	movss	%xmm5, 16(%rcx)
	ret
.LBB92_30:	# return
	ret
.LBB92_31:	# bb
	movl	$3212836864, (%rcx)
	movq	$0, 12(%rcx)
	movq	$0, 4(%rcx)
	movl	$0, (%rdi)
	movl	$0, (%rsi)
	movl	$0, (%rdx)
	ret
	.size	cblas_srotmg, .-cblas_srotmg


	.section	.rodata.cst4,"aM",@progbits,4
	.align	4
.LCPI93_0:					
	.long	1065353216	# float 1.000000e+00
	.text
	.align	16
	.globl	cblas_ssbmv
	.type	cblas_ssbmv,@function
cblas_ssbmv:
.Leh_func_begin61:
.Llabel61:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$56, %rsp
	testl	%edx, %edx
	movl	136(%rsp), %eax
	movq	128(%rsp), %r10
	movl	120(%rsp), %r11d
	movl	%r9d, 36(%rsp)
	movl	%ecx, 40(%rsp)
	je	.LBB93_43	# return
.LBB93_1:	# bb
	ucomiss	.LCPI93_0(%rip), %xmm1
	jne	.LBB93_3	# bb13
	jp	.LBB93_3	# bb13
.LBB93_2:	# bb
	pxor	%xmm2, %xmm2
	ucomiss	%xmm2, %xmm0
	setnp	%cl
	sete	%r9b
	testb	%cl, %r9b
	jne	.LBB93_43	# return
.LBB93_3:	# bb13
	pxor	%xmm2, %xmm2
	ucomiss	%xmm2, %xmm1
	jne	.LBB93_9	# bb20
	jp	.LBB93_9	# bb20
.LBB93_4:	# bb14
	testl	%eax, %eax
	jg	.LBB93_44	# bb14.bb19.preheader_crit_edge
.LBB93_5:	# bb15
	movl	$1, %ecx
	subl	%edx, %ecx
	imull	%eax, %ecx
.LBB93_6:	# bb19.preheader
	testl	%edx, %edx
	jle	.LBB93_15	# bb27
.LBB93_7:	# bb19.preheader.bb18_crit_edge
	xorl	%r9d, %r9d
	.align	16
.LBB93_8:	# bb18
	movslq	%ecx, %rbx
	movl	$0, (%r10,%rbx,4)
	addl	%eax, %ecx
	incl	%r9d
	cmpl	%edx, %r9d
	jne	.LBB93_8	# bb18
	jmp	.LBB93_15	# bb27
.LBB93_9:	# bb20
	ucomiss	.LCPI93_0(%rip), %xmm1
	setnp	%cl
	sete	%r9b
	testb	%cl, %r9b
	jne	.LBB93_15	# bb27
.LBB93_10:	# bb21
	testl	%eax, %eax
	jg	.LBB93_45	# bb21.bb26.preheader_crit_edge
.LBB93_11:	# bb22
	movl	$1, %ecx
	subl	%edx, %ecx
	imull	%eax, %ecx
.LBB93_12:	# bb26.preheader
	testl	%edx, %edx
	jle	.LBB93_15	# bb27
.LBB93_13:	# bb26.preheader.bb25_crit_edge
	xorl	%r9d, %r9d
	.align	16
.LBB93_14:	# bb25
	movslq	%ecx, %rbx
	movaps	%xmm1, %xmm2
	mulss	(%r10,%rbx,4), %xmm2
	movss	%xmm2, (%r10,%rbx,4)
	addl	%eax, %ecx
	incl	%r9d
	cmpl	%edx, %r9d
	jne	.LBB93_14	# bb25
.LBB93_15:	# bb27
	pxor	%xmm1, %xmm1
	ucomiss	%xmm1, %xmm0
	setnp	%cl
	sete	%r9b
	testb	%cl, %r9b
	jne	.LBB93_43	# return
.LBB93_16:	# bb28
	cmpl	$121, %esi
	jne	.LBB93_18	# bb31
.LBB93_17:	# bb28
	cmpl	$101, %edi
	je	.LBB93_20	# bb35
.LBB93_18:	# bb31
	cmpl	$122, %esi
	jne	.LBB93_30	# bb53
.LBB93_19:	# bb31
	cmpl	$102, %edi
	jne	.LBB93_30	# bb53
.LBB93_20:	# bb35
	testl	%r11d, %r11d
	jg	.LBB93_46	# bb35.bb38_crit_edge
.LBB93_21:	# bb36
	movl	$1, %ecx
	subl	%edx, %ecx
	imull	%r11d, %ecx
	movl	%ecx, 16(%rsp)
.LBB93_22:	# bb38
	testl	%eax, %eax
	jg	.LBB93_47	# bb38.bb52.preheader_crit_edge
.LBB93_23:	# bb39
	movl	$1, %ecx
	subl	%edx, %ecx
	imull	%eax, %ecx
	movl	%ecx, 12(%rsp)
.LBB93_24:	# bb52.preheader
	testl	%edx, %edx
	jle	.LBB93_43	# return
.LBB93_25:	# bb.nph122
	movl	$1, %ecx
	subl	%edx, %ecx
	movl	%ecx, %esi
	imull	%r11d, %esi
	movl	%esi, 20(%rsp)
	imull	%eax, %ecx
	movl	%ecx, 8(%rsp)
	movl	40(%rsp), %ecx
	leal	1(%rcx), %ecx
	movl	%ecx, 24(%rsp)
	movl	$4294967294, 28(%rsp)
	xorl	%ecx, %ecx
	movl	%ecx, 44(%rsp)
	movl	%ecx, 48(%rsp)
	movl	%ecx, 52(%rsp)
	movl	%ecx, 32(%rsp)
	.align	16
.LBB93_26:	# bb42
	movl	48(%rsp), %esi
	movl	12(%rsp), %edi
	leal	(%rdi,%rsi), %esi
	movslq	%esi, %rsi
	movl	44(%rsp), %edi
	movl	16(%rsp), %r9d
	leal	(%r9,%rdi), %edi
	movslq	%edi, %rdi
	movaps	%xmm0, %xmm1
	movq	112(%rsp), %r9
	mulss	(%r9,%rdi,4), %xmm1
	movslq	52(%rsp), %rdi
	movaps	%xmm1, %xmm2
	mulss	(%r8,%rdi,4), %xmm2
	addss	(%r10,%rsi,4), %xmm2
	movss	%xmm2, (%r10,%rsi,4)
	xorl	%edi, %edi
	testl	%eax, %eax
	movl	8(%rsp), %r9d
	cmovg	%edi, %r9d
	testl	%r11d, %r11d
	cmovle	20(%rsp), %edi
	movl	24(%rsp), %ebx
	leal	(%rbx,%rcx), %ebx
	cmpl	%edx, %ebx
	cmovg	%edx, %ebx
	leal	1(%rcx), %r14d
	cmpl	%ebx, %r14d
	jge	.LBB93_48	# bb42.bb51_crit_edge
.LBB93_27:	# bb.nph117
	movl	$4294967295, %ebx
	subl	%edx, %ebx
	movl	32(%rsp), %r14d
	negl	%r14d
	subl	40(%rsp), %r14d
	addl	$4294967294, %r14d
	cmpl	%r14d, %ebx
	cmovg	%ebx, %r14d
	movl	28(%rsp), %ebx
	subl	%r14d, %ebx
	movl	44(%rsp), %r14d
	leal	(%r11,%r14), %r14d
	movl	48(%rsp), %r15d
	leal	(%rax,%r15), %r15d
	movl	52(%rsp), %r12d
	leal	1(%r12), %r12d
	pxor	%xmm2, %xmm2
	xorl	%r13d, %r13d
	.align	16
.LBB93_28:	# bb49
	addl	%r9d, %r15d
	movslq	%r15d, %r9
	leal	(%r12,%r13), %ebp
	movslq	%ebp, %rbp
	movss	(%r8,%rbp,4), %xmm3
	movaps	%xmm1, %xmm4
	mulss	%xmm3, %xmm4
	addss	(%r10,%r9,4), %xmm4
	movss	%xmm4, (%r10,%r9,4)
	addl	%edi, %r14d
	movslq	%r14d, %rdi
	movq	112(%rsp), %r9
	mulss	(%r9,%rdi,4), %xmm3
	addss	%xmm3, %xmm2
	incl	%r13d
	cmpl	%ebx, %r13d
	movl	%eax, %r9d
	movl	%r11d, %edi
	jne	.LBB93_28	# bb49
.LBB93_29:	# bb51
	mulss	%xmm0, %xmm2
	addss	(%r10,%rsi,4), %xmm2
	movss	%xmm2, (%r10,%rsi,4)
	addl	%r11d, 44(%rsp)
	addl	%eax, 48(%rsp)
	movl	52(%rsp), %esi
	addl	36(%rsp), %esi
	movl	%esi, 52(%rsp)
	incl	32(%rsp)
	decl	28(%rsp)
	incl	%ecx
	cmpl	%edx, %ecx
	jne	.LBB93_26	# bb42
	jmp	.LBB93_43	# return
.LBB93_30:	# bb53
	cmpl	$102, %edi
	sete	%cl
	cmpl	$121, %esi
	sete	%r9b
	andb	%cl, %r9b
	cmpl	$101, %edi
	sete	%cl
	cmpl	$122, %esi
	sete	%sil
	testb	%cl, %sil
	jne	.LBB93_32	# bb61
.LBB93_31:	# bb53
	notb	%r9b
	testb	$1, %r9b
	jne	.LBB93_42	# bb82
.LBB93_32:	# bb61
	testl	%r11d, %r11d
	jg	.LBB93_49	# bb61.bb64_crit_edge
.LBB93_33:	# bb62
	movl	$1, %ecx
	subl	%edx, %ecx
	imull	%r11d, %ecx
	movl	%ecx, 48(%rsp)
.LBB93_34:	# bb64
	testl	%eax, %eax
	jg	.LBB93_50	# bb64.bb81.preheader_crit_edge
.LBB93_35:	# bb65
	movl	$1, %ecx
	subl	%edx, %ecx
	imull	%eax, %ecx
	movl	%ecx, 44(%rsp)
.LBB93_36:	# bb81.preheader
	testl	%edx, %edx
	jle	.LBB93_43	# return
.LBB93_37:	# bb.nph106
	movl	$1, %ecx
	subl	%edx, %ecx
	movl	%ecx, %esi
	imull	%r11d, %esi
	movl	%esi, 28(%rsp)
	imull	%eax, %ecx
	movl	%ecx, 20(%rsp)
	movl	36(%rsp), %ecx
	leal	-1(%rcx), %ecx
	movl	%ecx, 32(%rsp)
	movl	40(%rsp), %ecx
	movl	%ecx, %esi
	negl	%esi
	movl	%esi, 24(%rsp)
	xorl	%esi, %esi
	movl	%ecx, 52(%rsp)
	.align	16
.LBB93_38:	# bb68
	xorl	%edi, %edi
	testl	%eax, %eax
	movl	20(%rsp), %r9d
	cmovg	%edi, %r9d
	testl	%r11d, %r11d
	movl	28(%rsp), %ebx
	cmovg	%edi, %ebx
	movl	24(%rsp), %r14d
	leal	(%r14,%rsi), %r14d
	cmpl	40(%rsp), %esi
	cmovle	%edi, %r14d
	movl	%r14d, %edi
	imull	%eax, %edi
	movl	%r14d, %r15d
	imull	%r11d, %r15d
	cmpl	%esi, %r14d
	movslq	48(%rsp), %r12
	movaps	%xmm0, %xmm1
	movq	112(%rsp), %r13
	mulss	(%r13,%r12,4), %xmm1
	jge	.LBB93_51	# bb68.bb80_crit_edge
.LBB93_39:	# bb.nph101
	movl	%esi, %r12d
	subl	%r14d, %r12d
	addl	%ecx, %r14d
	pxor	%xmm2, %xmm2
	xorl	%r13d, %r13d
	.align	16
.LBB93_40:	# bb78
	addl	%r9d, %edi
	movslq	%edi, %r9
	leal	(%r14,%r13), %ebp
	movslq	%ebp, %rbp
	movss	(%r8,%rbp,4), %xmm3
	movaps	%xmm1, %xmm4
	mulss	%xmm3, %xmm4
	addss	(%r10,%r9,4), %xmm4
	movss	%xmm4, (%r10,%r9,4)
	addl	%ebx, %r15d
	movslq	%r15d, %r9
	movq	112(%rsp), %rbx
	mulss	(%rbx,%r9,4), %xmm3
	addss	%xmm3, %xmm2
	incl	%r13d
	cmpl	%r12d, %r13d
	movl	%eax, %r9d
	movl	%r11d, %ebx
	jne	.LBB93_40	# bb78
.LBB93_41:	# bb80
	movl	52(%rsp), %edi
	movslq	%edi, %r9
	mulss	(%r8,%r9,4), %xmm1
	mulss	%xmm0, %xmm2
	addss	%xmm1, %xmm2
	movl	44(%rsp), %r9d
	movslq	%r9d, %rbx
	addss	(%r10,%rbx,4), %xmm2
	movss	%xmm2, (%r10,%rbx,4)
	addl	%r11d, 48(%rsp)
	addl	%eax, %r9d
	movl	%r9d, 44(%rsp)
	addl	36(%rsp), %edi
	movl	%edi, 52(%rsp)
	addl	32(%rsp), %ecx
	incl	%esi
	cmpl	%edx, %esi
	jne	.LBB93_38	# bb68
	jmp	.LBB93_43	# return
.LBB93_42:	# bb82
	xorl	%edi, %edi
	leaq	.str107, %rsi
	leaq	.str1108, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB93_43:	# return
	addq	$56, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB93_44:	# bb14.bb19.preheader_crit_edge
	xorl	%ecx, %ecx
	jmp	.LBB93_6	# bb19.preheader
.LBB93_45:	# bb21.bb26.preheader_crit_edge
	xorl	%ecx, %ecx
	jmp	.LBB93_12	# bb26.preheader
.LBB93_46:	# bb35.bb38_crit_edge
	movl	$0, 16(%rsp)
	jmp	.LBB93_22	# bb38
.LBB93_47:	# bb38.bb52.preheader_crit_edge
	movl	$0, 12(%rsp)
	jmp	.LBB93_24	# bb52.preheader
.LBB93_48:	# bb42.bb51_crit_edge
	pxor	%xmm2, %xmm2
	jmp	.LBB93_29	# bb51
.LBB93_49:	# bb61.bb64_crit_edge
	movl	$0, 48(%rsp)
	jmp	.LBB93_34	# bb64
.LBB93_50:	# bb64.bb81.preheader_crit_edge
	movl	$0, 44(%rsp)
	jmp	.LBB93_36	# bb81.preheader
.LBB93_51:	# bb68.bb80_crit_edge
	pxor	%xmm2, %xmm2
	jmp	.LBB93_41	# bb80
	.size	cblas_ssbmv, .-cblas_ssbmv
.Leh_func_end61:


	.align	16
	.globl	cblas_sscal
	.type	cblas_sscal,@function
cblas_sscal:
	testl	%edx, %edx
	jle	.LBB94_4	# return
.LBB94_1:	# entry
	testl	%edi, %edi
	jle	.LBB94_4	# return
.LBB94_2:	# entry.bb1_crit_edge
	xorl	%eax, %eax
	movl	%eax, %ecx
	.align	16
.LBB94_3:	# bb1
	movslq	%eax, %r8
	movaps	%xmm0, %xmm1
	mulss	(%rsi,%r8,4), %xmm1
	movss	%xmm1, (%rsi,%r8,4)
	addl	%edx, %eax
	incl	%ecx
	cmpl	%edi, %ecx
	jne	.LBB94_3	# bb1
.LBB94_4:	# return
	ret
	.size	cblas_sscal, .-cblas_sscal


	.section	.rodata.cst4,"aM",@progbits,4
	.align	4
.LCPI95_0:					
	.long	1065353216	# float 1.000000e+00
	.text
	.align	16
	.globl	cblas_sspmv
	.type	cblas_sspmv,@function
cblas_sspmv:
.Leh_func_begin62:
.Llabel62:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	ucomiss	.LCPI95_0(%rip), %xmm1
	movq	96(%rsp), %rax
	jne	.LBB95_2	# bb12
	jp	.LBB95_2	# bb12
.LBB95_1:	# entry
	pxor	%xmm2, %xmm2
	ucomiss	%xmm2, %xmm0
	setnp	%r10b
	sete	%r11b
	testb	%r10b, %r11b
	jne	.LBB95_42	# return
.LBB95_2:	# bb12
	pxor	%xmm2, %xmm2
	ucomiss	%xmm2, %xmm1
	jne	.LBB95_8	# bb19
	jp	.LBB95_8	# bb19
.LBB95_3:	# bb13
	cmpl	$0, 104(%rsp)
	jg	.LBB95_43	# bb13.bb18.preheader_crit_edge
.LBB95_4:	# bb14
	movl	$1, %r10d
	subl	%edx, %r10d
	imull	104(%rsp), %r10d
.LBB95_5:	# bb18.preheader
	testl	%edx, %edx
	jle	.LBB95_14	# bb26
.LBB95_6:	# bb18.preheader.bb17_crit_edge
	xorl	%r11d, %r11d
	.align	16
.LBB95_7:	# bb17
	movslq	%r10d, %rbx
	movl	$0, (%rax,%rbx,4)
	addl	104(%rsp), %r10d
	incl	%r11d
	cmpl	%edx, %r11d
	jne	.LBB95_7	# bb17
	jmp	.LBB95_14	# bb26
.LBB95_8:	# bb19
	ucomiss	.LCPI95_0(%rip), %xmm1
	setnp	%r10b
	sete	%r11b
	testb	%r10b, %r11b
	jne	.LBB95_14	# bb26
.LBB95_9:	# bb20
	cmpl	$0, 104(%rsp)
	jg	.LBB95_44	# bb20.bb25.preheader_crit_edge
.LBB95_10:	# bb21
	movl	$1, %r10d
	subl	%edx, %r10d
	imull	104(%rsp), %r10d
.LBB95_11:	# bb25.preheader
	testl	%edx, %edx
	jle	.LBB95_14	# bb26
.LBB95_12:	# bb25.preheader.bb24_crit_edge
	xorl	%r11d, %r11d
	.align	16
.LBB95_13:	# bb24
	movslq	%r10d, %rbx
	movaps	%xmm1, %xmm2
	mulss	(%rax,%rbx,4), %xmm2
	movss	%xmm2, (%rax,%rbx,4)
	addl	104(%rsp), %r10d
	incl	%r11d
	cmpl	%edx, %r11d
	jne	.LBB95_13	# bb24
.LBB95_14:	# bb26
	pxor	%xmm1, %xmm1
	ucomiss	%xmm1, %xmm0
	setnp	%r10b
	sete	%r11b
	testb	%r10b, %r11b
	jne	.LBB95_42	# return
.LBB95_15:	# bb27
	cmpl	$121, %esi
	jne	.LBB95_17	# bb30
.LBB95_16:	# bb27
	cmpl	$101, %edi
	je	.LBB95_19	# bb34
.LBB95_17:	# bb30
	cmpl	$122, %esi
	jne	.LBB95_29	# bb52
.LBB95_18:	# bb30
	cmpl	$102, %edi
	jne	.LBB95_29	# bb52
.LBB95_19:	# bb34
	testl	%r9d, %r9d
	jg	.LBB95_45	# bb34.bb37_crit_edge
.LBB95_20:	# bb35
	movl	$1, %esi
	subl	%edx, %esi
	imull	%r9d, %esi
	movl	%esi, 12(%rsp)
.LBB95_21:	# bb37
	cmpl	$0, 104(%rsp)
	jg	.LBB95_46	# bb37.bb51.preheader_crit_edge
.LBB95_22:	# bb38
	movl	$1, %esi
	subl	%edx, %esi
	imull	104(%rsp), %esi
	movl	%esi, 8(%rsp)
.LBB95_23:	# bb51.preheader
	testl	%edx, %edx
	jle	.LBB95_42	# return
.LBB95_24:	# bb.nph112
	movl	$1, %esi
	subl	%edx, %esi
	movl	%esi, %edi
	imull	%r9d, %edi
	movl	%edi, 16(%rsp)
	imull	104(%rsp), %esi
	movl	%esi, 4(%rsp)
	leal	1(,%rdx,2), %esi
	movl	%esi, 24(%rsp)
	leal	-1(%rdx), %esi
	movl	%esi, 20(%rsp)
	xorl	%esi, %esi
	movl	%esi, 28(%rsp)
	movl	%esi, 36(%rsp)
	movl	%esi, 32(%rsp)
	.align	16
.LBB95_25:	# bb41
	movl	32(%rsp), %r10d
	movl	24(%rsp), %edi
	leal	(%rdi,%r10), %edi
	imull	%esi, %edi
	movl	%edi, %r10d
	shrl	$31, %r10d
	addl	%edi, %r10d
	sarl	%r10d
	movslq	%r10d, %rdi
	movl	28(%rsp), %ebx
	movl	12(%rsp), %r11d
	leal	(%r11,%rbx), %r11d
	movslq	%r11d, %r11
	movaps	%xmm0, %xmm1
	mulss	(%r8,%r11,4), %xmm1
	movaps	%xmm1, %xmm2
	mulss	(%rcx,%rdi,4), %xmm2
	movl	36(%rsp), %r11d
	movl	8(%rsp), %edi
	leal	(%rdi,%r11), %edi
	movslq	%edi, %rdi
	addss	(%rax,%rdi,4), %xmm2
	movss	%xmm2, (%rax,%rdi,4)
	xorl	%r11d, %r11d
	cmpl	$0, 104(%rsp)
	movl	4(%rsp), %ebx
	cmovg	%r11d, %ebx
	testl	%r9d, %r9d
	cmovle	16(%rsp), %r11d
	leal	1(%rsi), %r14d
	cmpl	%edx, %r14d
	jge	.LBB95_47	# bb41.bb50_crit_edge
.LBB95_26:	# bb.nph107
	movl	28(%rsp), %r14d
	leal	(%r9,%r14), %r14d
	movl	104(%rsp), %r15d
	movl	36(%rsp), %r12d
	leal	(%r15,%r12), %r15d
	movl	32(%rsp), %r12d
	movl	20(%rsp), %r13d
	leal	(%r13,%r12), %r12d
	incl	%r10d
	pxor	%xmm2, %xmm2
	xorl	%r13d, %r13d
	.align	16
.LBB95_27:	# bb48
	addl	%ebx, %r15d
	movslq	%r15d, %rbx
	leal	(%r10,%r13), %ebp
	movslq	%ebp, %rbp
	movss	(%rcx,%rbp,4), %xmm3
	movaps	%xmm1, %xmm4
	mulss	%xmm3, %xmm4
	addss	(%rax,%rbx,4), %xmm4
	movss	%xmm4, (%rax,%rbx,4)
	addl	%r11d, %r14d
	movslq	%r14d, %r11
	mulss	(%r8,%r11,4), %xmm3
	addss	%xmm3, %xmm2
	incl	%r13d
	cmpl	%r12d, %r13d
	movl	104(%rsp), %ebx
	movl	%r9d, %r11d
	jne	.LBB95_27	# bb48
.LBB95_28:	# bb50
	mulss	%xmm0, %xmm2
	addss	(%rax,%rdi,4), %xmm2
	movss	%xmm2, (%rax,%rdi,4)
	addl	%r9d, 28(%rsp)
	movl	104(%rsp), %edi
	addl	%edi, 36(%rsp)
	decl	32(%rsp)
	incl	%esi
	cmpl	%edx, %esi
	jne	.LBB95_25	# bb41
	jmp	.LBB95_42	# return
.LBB95_29:	# bb52
	cmpl	$102, %edi
	sete	%r10b
	cmpl	$121, %esi
	sete	%r11b
	andb	%r10b, %r11b
	cmpl	$101, %edi
	sete	%dil
	cmpl	$122, %esi
	sete	%sil
	testb	%dil, %sil
	jne	.LBB95_31	# bb60
.LBB95_30:	# bb52
	notb	%r11b
	testb	$1, %r11b
	jne	.LBB95_41	# bb78
.LBB95_31:	# bb60
	testl	%r9d, %r9d
	jg	.LBB95_48	# bb60.bb63_crit_edge
.LBB95_32:	# bb61
	movl	$1, %esi
	subl	%edx, %esi
	imull	%r9d, %esi
.LBB95_33:	# bb63
	cmpl	$0, 104(%rsp)
	jg	.LBB95_49	# bb63.bb77.preheader_crit_edge
.LBB95_34:	# bb64
	movl	$1, %edi
	subl	%edx, %edi
	imull	104(%rsp), %edi
.LBB95_35:	# bb77.preheader
	testl	%edx, %edx
	jle	.LBB95_42	# return
.LBB95_36:	# bb.nph96
	movl	$1, %r10d
	subl	%edx, %r10d
	movl	%r10d, %r11d
	imull	%r9d, %r11d
	imull	104(%rsp), %r10d
	xorl	%ebx, %ebx
	.align	16
.LBB95_37:	# bb67
	movslq	%esi, %r14
	movaps	%xmm0, %xmm1
	mulss	(%r8,%r14,4), %xmm1
	leal	1(%rbx), %r14d
	imull	%ebx, %r14d
	movl	%r14d, %r15d
	shrl	$31, %r15d
	addl	%r14d, %r15d
	sarl	%r15d
	leal	(%r15,%rbx), %r14d
	movslq	%r14d, %r14
	movaps	%xmm1, %xmm2
	mulss	(%rcx,%r14,4), %xmm2
	movslq	%edi, %rdi
	addss	(%rax,%rdi,4), %xmm2
	movss	%xmm2, (%rax,%rdi,4)
	xorl	%r14d, %r14d
	cmpl	$0, 104(%rsp)
	movl	%r10d, %r12d
	cmovg	%r14d, %r12d
	testl	%r9d, %r9d
	cmovle	%r11d, %r14d
	testl	%ebx, %ebx
	jle	.LBB95_50	# bb67.bb76_crit_edge
.LBB95_38:	# bb67.bb74_crit_edge
	pxor	%xmm2, %xmm2
	xorl	%r13d, %r13d
	.align	16
.LBB95_39:	# bb74
	leal	(%r15,%r13), %ebp
	movslq	%ebp, %rbp
	movss	(%rcx,%rbp,4), %xmm3
	movaps	%xmm1, %xmm4
	mulss	%xmm3, %xmm4
	movslq	%r12d, %rbp
	addss	(%rax,%rbp,4), %xmm4
	movss	%xmm4, (%rax,%rbp,4)
	leal	(%r9,%r14), %ebp
	addl	104(%rsp), %r12d
	incl	%r13d
	cmpl	%ebx, %r13d
	movslq	%r14d, %r14
	mulss	(%r8,%r14,4), %xmm3
	addss	%xmm3, %xmm2
	movl	%ebp, %r14d
	jne	.LBB95_39	# bb74
.LBB95_40:	# bb76
	mulss	%xmm0, %xmm2
	addss	(%rax,%rdi,4), %xmm2
	movss	%xmm2, (%rax,%rdi,4)
	addl	104(%rsp), %edi
	addl	%r9d, %esi
	incl	%ebx
	cmpl	%edx, %ebx
	jne	.LBB95_37	# bb67
	jmp	.LBB95_42	# return
.LBB95_41:	# bb78
	xorl	%edi, %edi
	leaq	.str109, %rsi
	leaq	.str1110, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB95_42:	# return
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB95_43:	# bb13.bb18.preheader_crit_edge
	xorl	%r10d, %r10d
	jmp	.LBB95_5	# bb18.preheader
.LBB95_44:	# bb20.bb25.preheader_crit_edge
	xorl	%r10d, %r10d
	jmp	.LBB95_11	# bb25.preheader
.LBB95_45:	# bb34.bb37_crit_edge
	movl	$0, 12(%rsp)
	jmp	.LBB95_21	# bb37
.LBB95_46:	# bb37.bb51.preheader_crit_edge
	movl	$0, 8(%rsp)
	jmp	.LBB95_23	# bb51.preheader
.LBB95_47:	# bb41.bb50_crit_edge
	pxor	%xmm2, %xmm2
	jmp	.LBB95_28	# bb50
.LBB95_48:	# bb60.bb63_crit_edge
	xorl	%esi, %esi
	jmp	.LBB95_33	# bb63
.LBB95_49:	# bb63.bb77.preheader_crit_edge
	xorl	%edi, %edi
	jmp	.LBB95_35	# bb77.preheader
.LBB95_50:	# bb67.bb76_crit_edge
	pxor	%xmm2, %xmm2
	jmp	.LBB95_40	# bb76
	.size	cblas_sspmv, .-cblas_sspmv
.Leh_func_end62:


	.align	16
	.globl	cblas_sspr2
	.type	cblas_sspr2,@function
cblas_sspr2:
.Leh_func_begin63:
.Llabel63:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$8, %rsp
	pxor	%xmm1, %xmm1
	ucomiss	%xmm1, %xmm0
	setnp	%al
	sete	%r10b
	testb	%al, %r10b
	movq	72(%rsp), %rax
	movl	64(%rsp), %r10d
	jne	.LBB96_29	# return
.LBB96_1:	# entry
	testl	%edx, %edx
	je	.LBB96_29	# return
.LBB96_2:	# bb7
	cmpl	$121, %esi
	jne	.LBB96_4	# bb10
.LBB96_3:	# bb7
	cmpl	$101, %edi
	je	.LBB96_6	# bb14
.LBB96_4:	# bb10
	cmpl	$122, %esi
	jne	.LBB96_16	# bb26
.LBB96_5:	# bb10
	cmpl	$102, %edi
	jne	.LBB96_16	# bb26
.LBB96_6:	# bb14
	testl	%r8d, %r8d
	jg	.LBB96_30	# bb14.bb17_crit_edge
.LBB96_7:	# bb15
	movl	$1, %esi
	subl	%edx, %esi
	imull	%r8d, %esi
	movl	%esi, 4(%rsp)
.LBB96_8:	# bb17
	testl	%r10d, %r10d
	jg	.LBB96_31	# bb17.bb25.preheader_crit_edge
.LBB96_9:	# bb18
	movl	$1, %esi
	subl	%edx, %esi
	imull	%r10d, %esi
.LBB96_10:	# bb25.preheader
	testl	%edx, %edx
	jle	.LBB96_29	# return
.LBB96_11:	# bb.nph71
	leal	1(,%rdx,2), %edi
	movl	%edi, (%rsp)
	xorl	%edi, %edi
	movl	%edi, %r11d
	.align	16
.LBB96_12:	# bb21
	movslq	%esi, %rbx
	movaps	%xmm0, %xmm1
	mulss	(%r9,%rbx,4), %xmm1
	cvtss2sd	%xmm1, %xmm1
	movslq	4(%rsp), %rbx
	movaps	%xmm0, %xmm2
	mulss	(%rcx,%rbx,4), %xmm2
	cvtss2sd	%xmm2, %xmm2
	cmpl	%edx, %r11d
	jge	.LBB96_15	# bb24
.LBB96_13:	# bb.nph67
	movl	(%rsp), %ebx
	leal	(%rbx,%rdi), %ebx
	imull	%r11d, %ebx
	movl	%ebx, %r14d
	shrl	$31, %r14d
	addl	%ebx, %r14d
	sarl	%r14d
	leal	(%rdx,%rdi), %ebx
	xorl	%r15d, %r15d
	movl	%esi, %r12d
	movl	4(%rsp), %r13d
	.align	16
.LBB96_14:	# bb22
	movslq	%r13d, %rbp
	cvtss2sd	(%rcx,%rbp,4), %xmm3
	mulsd	%xmm1, %xmm3
	movslq	%r12d, %rbp
	cvtss2sd	(%r9,%rbp,4), %xmm4
	mulsd	%xmm2, %xmm4
	addsd	%xmm3, %xmm4
	leal	(%r14,%r15), %ebp
	movslq	%ebp, %rbp
	cvtss2sd	(%rax,%rbp,4), %xmm3
	addsd	%xmm4, %xmm3
	cvtsd2ss	%xmm3, %xmm3
	movss	%xmm3, (%rax,%rbp,4)
	addl	%r10d, %r12d
	addl	%r8d, %r13d
	incl	%r15d
	cmpl	%ebx, %r15d
	jne	.LBB96_14	# bb22
.LBB96_15:	# bb24
	addl	%r8d, 4(%rsp)
	addl	%r10d, %esi
	decl	%edi
	incl	%r11d
	cmpl	%edx, %r11d
	jne	.LBB96_12	# bb21
	jmp	.LBB96_29	# return
.LBB96_16:	# bb26
	cmpl	$102, %edi
	sete	%r11b
	cmpl	$121, %esi
	sete	%bl
	andb	%r11b, %bl
	cmpl	$101, %edi
	sete	%dil
	cmpl	$122, %esi
	sete	%sil
	testb	%dil, %sil
	jne	.LBB96_18	# bb34
.LBB96_17:	# bb26
	notb	%bl
	testb	$1, %bl
	jne	.LBB96_28	# bb52
.LBB96_18:	# bb34
	testl	%r8d, %r8d
	jg	.LBB96_32	# bb34.bb37_crit_edge
.LBB96_19:	# bb35
	movl	$1, %esi
	subl	%edx, %esi
	imull	%r8d, %esi
.LBB96_20:	# bb37
	testl	%r10d, %r10d
	jg	.LBB96_33	# bb37.bb51.preheader_crit_edge
.LBB96_21:	# bb38
	movl	$1, %edi
	subl	%edx, %edi
	imull	%r10d, %edi
.LBB96_22:	# bb51.preheader
	testl	%edx, %edx
	jle	.LBB96_29	# return
.LBB96_23:	# bb.nph63
	movl	$1, %r11d
	subl	%edx, %r11d
	movl	%r11d, %ebx
	imull	%r8d, %ebx
	movl	%ebx, 4(%rsp)
	imull	%r10d, %r11d
	xorl	%ebx, %ebx
	.align	16
.LBB96_24:	# bb41
	xorl	%r14d, %r14d
	testl	%r10d, %r10d
	movl	%r11d, %r15d
	cmovg	%r14d, %r15d
	testl	%r8d, %r8d
	cmovle	4(%rsp), %r14d
	movslq	%edi, %r12
	movaps	%xmm0, %xmm1
	mulss	(%r9,%r12,4), %xmm1
	cvtss2sd	%xmm1, %xmm1
	movslq	%esi, %r12
	movaps	%xmm0, %xmm2
	mulss	(%rcx,%r12,4), %xmm2
	cvtss2sd	%xmm2, %xmm2
	testl	%ebx, %ebx
	js	.LBB96_27	# bb50
.LBB96_25:	# bb.nph
	leal	1(%rbx), %r12d
	imull	%ebx, %r12d
	movl	%r12d, %r13d
	shrl	$31, %r13d
	addl	%r12d, %r13d
	sarl	%r13d
	xorl	%r12d, %r12d
	.align	16
.LBB96_26:	# bb48
	movslq	%r14d, %rbp
	cvtss2sd	(%rcx,%rbp,4), %xmm3
	mulsd	%xmm1, %xmm3
	movslq	%r15d, %rbp
	cvtss2sd	(%r9,%rbp,4), %xmm4
	mulsd	%xmm2, %xmm4
	addsd	%xmm3, %xmm4
	leal	(%r13,%r12), %ebp
	movslq	%ebp, %rbp
	cvtss2sd	(%rax,%rbp,4), %xmm3
	addsd	%xmm4, %xmm3
	cvtsd2ss	%xmm3, %xmm3
	movss	%xmm3, (%rax,%rbp,4)
	addl	%r8d, %r14d
	addl	%r10d, %r15d
	incl	%r12d
	cmpl	%ebx, %r12d
	jle	.LBB96_26	# bb48
.LBB96_27:	# bb50
	addl	%r8d, %esi
	addl	%r10d, %edi
	incl	%ebx
	cmpl	%edx, %ebx
	jne	.LBB96_24	# bb41
	jmp	.LBB96_29	# return
.LBB96_28:	# bb52
	xorl	%edi, %edi
	leaq	.str112, %rsi
	leaq	.str1113, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB96_29:	# return
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB96_30:	# bb14.bb17_crit_edge
	movl	$0, 4(%rsp)
	jmp	.LBB96_8	# bb17
.LBB96_31:	# bb17.bb25.preheader_crit_edge
	xorl	%esi, %esi
	jmp	.LBB96_10	# bb25.preheader
.LBB96_32:	# bb34.bb37_crit_edge
	xorl	%esi, %esi
	jmp	.LBB96_20	# bb37
.LBB96_33:	# bb37.bb51.preheader_crit_edge
	xorl	%edi, %edi
	jmp	.LBB96_22	# bb51.preheader
	.size	cblas_sspr2, .-cblas_sspr2
.Leh_func_end63:


	.align	16
	.globl	cblas_sspr
	.type	cblas_sspr,@function
cblas_sspr:
.Leh_func_begin64:
.Llabel64:
	pushq	%r15
	pushq	%r14
	pushq	%r12
	pushq	%rbx
	subq	$8, %rsp
	pxor	%xmm1, %xmm1
	ucomiss	%xmm1, %xmm0
	setnp	%al
	sete	%r10b
	testb	%al, %r10b
	jne	.LBB97_25	# return
.LBB97_1:	# entry
	testl	%edx, %edx
	je	.LBB97_25	# return
.LBB97_2:	# bb4
	cmpl	$121, %esi
	jne	.LBB97_4	# bb7
.LBB97_3:	# bb4
	cmpl	$101, %edi
	je	.LBB97_6	# bb11
.LBB97_4:	# bb7
	cmpl	$122, %esi
	jne	.LBB97_14	# bb20
.LBB97_5:	# bb7
	cmpl	$102, %edi
	jne	.LBB97_14	# bb20
.LBB97_6:	# bb11
	testl	%r8d, %r8d
	jg	.LBB97_26	# bb11.bb19.preheader_crit_edge
.LBB97_7:	# bb12
	movl	$1, %eax
	subl	%edx, %eax
	imull	%r8d, %eax
.LBB97_8:	# bb19.preheader
	testl	%edx, %edx
	jle	.LBB97_25	# return
.LBB97_9:	# bb.nph55
	leal	1(,%rdx,2), %esi
	xorl	%edi, %edi
	movl	%edi, %r10d
	.align	16
.LBB97_10:	# bb15
	movslq	%eax, %r11
	movaps	%xmm0, %xmm1
	mulss	(%rcx,%r11,4), %xmm1
	cmpl	%edx, %r10d
	jge	.LBB97_13	# bb18
.LBB97_11:	# bb.nph52
	leal	(%rsi,%rdi), %r11d
	imull	%r10d, %r11d
	movl	%r11d, %ebx
	shrl	$31, %ebx
	addl	%r11d, %ebx
	sarl	%ebx
	leal	(%rdx,%rdi), %r11d
	xorl	%r14d, %r14d
	movl	%eax, %r15d
	.align	16
.LBB97_12:	# bb16
	movslq	%r15d, %r12
	movaps	%xmm1, %xmm2
	mulss	(%rcx,%r12,4), %xmm2
	leal	(%rbx,%r14), %r12d
	movslq	%r12d, %r12
	addss	(%r9,%r12,4), %xmm2
	movss	%xmm2, (%r9,%r12,4)
	addl	%r8d, %r15d
	incl	%r14d
	cmpl	%r11d, %r14d
	jne	.LBB97_12	# bb16
.LBB97_13:	# bb18
	addl	%r8d, %eax
	decl	%edi
	incl	%r10d
	cmpl	%edx, %r10d
	jne	.LBB97_10	# bb15
	jmp	.LBB97_25	# return
.LBB97_14:	# bb20
	cmpl	$102, %edi
	sete	%al
	cmpl	$121, %esi
	sete	%r10b
	andb	%al, %r10b
	cmpl	$101, %edi
	sete	%al
	cmpl	$122, %esi
	sete	%sil
	testb	%al, %sil
	jne	.LBB97_16	# bb28
.LBB97_15:	# bb20
	notb	%r10b
	testb	$1, %r10b
	jne	.LBB97_24	# bb40
.LBB97_16:	# bb28
	testl	%r8d, %r8d
	jg	.LBB97_27	# bb28.bb39.preheader_crit_edge
.LBB97_17:	# bb29
	movl	$1, %eax
	subl	%edx, %eax
	imull	%r8d, %eax
.LBB97_18:	# bb39.preheader
	testl	%edx, %edx
	jle	.LBB97_25	# return
.LBB97_19:	# bb.nph49
	movl	$1, %esi
	subl	%edx, %esi
	imull	%r8d, %esi
	xorl	%edi, %edi
	.align	16
.LBB97_20:	# bb32
	testl	%r8d, %r8d
	movl	$0, %r10d
	cmovle	%esi, %r10d
	movslq	%eax, %r11
	movaps	%xmm0, %xmm1
	mulss	(%rcx,%r11,4), %xmm1
	testl	%edi, %edi
	js	.LBB97_23	# bb38
.LBB97_21:	# bb.nph
	leal	1(%rdi), %r11d
	imull	%edi, %r11d
	movl	%r11d, %ebx
	shrl	$31, %ebx
	addl	%r11d, %ebx
	sarl	%ebx
	xorl	%r11d, %r11d
	.align	16
.LBB97_22:	# bb36
	movslq	%r10d, %r14
	movaps	%xmm1, %xmm2
	mulss	(%rcx,%r14,4), %xmm2
	leal	(%rbx,%r11), %r14d
	movslq	%r14d, %r14
	addss	(%r9,%r14,4), %xmm2
	movss	%xmm2, (%r9,%r14,4)
	addl	%r8d, %r10d
	incl	%r11d
	cmpl	%edi, %r11d
	jle	.LBB97_22	# bb36
.LBB97_23:	# bb38
	addl	%r8d, %eax
	incl	%edi
	cmpl	%edx, %edi
	jne	.LBB97_20	# bb32
	jmp	.LBB97_25	# return
.LBB97_24:	# bb40
	xorl	%edi, %edi
	leaq	.str114, %rsi
	leaq	.str1115, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB97_25:	# return
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r14
	popq	%r15
	ret
.LBB97_26:	# bb11.bb19.preheader_crit_edge
	xorl	%eax, %eax
	jmp	.LBB97_8	# bb19.preheader
.LBB97_27:	# bb28.bb39.preheader_crit_edge
	xorl	%eax, %eax
	jmp	.LBB97_18	# bb39.preheader
	.size	cblas_sspr, .-cblas_sspr
.Leh_func_end64:


	.align	16
	.globl	cblas_sswap
	.type	cblas_sswap,@function
cblas_sswap:
	pushq	%rbx
	testl	%edx, %edx
	jg	.LBB98_8	# entry.bb2_crit_edge
.LBB98_1:	# bb
	movl	$1, %eax
	subl	%edi, %eax
	imull	%edx, %eax
.LBB98_2:	# bb2
	testl	%r8d, %r8d
	jg	.LBB98_9	# bb2.bb7.preheader_crit_edge
.LBB98_3:	# bb3
	movl	$1, %r9d
	subl	%edi, %r9d
	imull	%r8d, %r9d
.LBB98_4:	# bb7.preheader
	testl	%edi, %edi
	jle	.LBB98_7	# return
.LBB98_5:	# bb7.preheader.bb6_crit_edge
	xorl	%r10d, %r10d
	.align	16
.LBB98_6:	# bb6
	movslq	%r9d, %r11
	movss	(%rcx,%r11,4), %xmm0
	movslq	%eax, %rbx
	movss	(%rsi,%rbx,4), %xmm1
	movss	%xmm0, (%rsi,%rbx,4)
	movss	%xmm1, (%rcx,%r11,4)
	addl	%r8d, %r9d
	addl	%edx, %eax
	incl	%r10d
	cmpl	%edi, %r10d
	jne	.LBB98_6	# bb6
.LBB98_7:	# return
	popq	%rbx
	ret
.LBB98_8:	# entry.bb2_crit_edge
	xorl	%eax, %eax
	jmp	.LBB98_2	# bb2
.LBB98_9:	# bb2.bb7.preheader_crit_edge
	xorl	%r9d, %r9d
	jmp	.LBB98_4	# bb7.preheader
	.size	cblas_sswap, .-cblas_sswap


	.section	.rodata.cst4,"aM",@progbits,4
	.align	4
.LCPI99_0:					
	.long	1065353216	# float 1.000000e+00
	.text
	.align	16
	.globl	cblas_ssymm
	.type	cblas_ssymm,@function
cblas_ssymm:
.Leh_func_begin65:
.Llabel65:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	ucomiss	.LCPI99_0(%rip), %xmm1
	movl	128(%rsp), %eax
	movq	120(%rsp), %r10
	movq	104(%rsp), %r11
	jne	.LBB99_2	# bb9
	jp	.LBB99_2	# bb9
.LBB99_1:	# entry
	pxor	%xmm2, %xmm2
	ucomiss	%xmm2, %xmm0
	setnp	%bl
	sete	%r14b
	testb	%bl, %r14b
	jne	.LBB99_64	# return
.LBB99_2:	# bb9
	cmpl	$101, %edi
	je	.LBB99_65	# bb9.bb18_crit_edge
.LBB99_3:	# bb11
	cmpl	$141, %esi
	movl	$142, %edi
	movl	$141, %esi
	cmove	%edi, %esi
	cmpl	$121, %edx
	movl	$122, %edi
	movl	$121, %edx
	cmove	%edi, %edx
	movl	%ecx, 36(%rsp)
	movl	%r8d, %ecx
.LBB99_4:	# bb18
	pxor	%xmm2, %xmm2
	ucomiss	%xmm2, %xmm1
	jne	.LBB99_11	# bb25
	jp	.LBB99_11	# bb25
.LBB99_5:	# bb24.preheader
	testl	%ecx, %ecx
	jle	.LBB99_18	# bb32
.LBB99_6:	# bb24.preheader
	cmpl	$0, 36(%rsp)
	jle	.LBB99_18	# bb32
.LBB99_7:	# bb24.preheader.bb22.preheader_crit_edge
	xorl	%edi, %edi
	movl	%edi, %r8d
	jmp	.LBB99_10	# bb22.preheader
	.align	16
.LBB99_8:	# bb21
	leal	(%rdi,%rbx), %r14d
	movslq	%r14d, %r14
	movl	$0, (%r10,%r14,4)
	incl	%ebx
	cmpl	36(%rsp), %ebx
	jne	.LBB99_8	# bb21
.LBB99_9:	# bb23
	addl	%eax, %edi
	incl	%r8d
	cmpl	%ecx, %r8d
	je	.LBB99_18	# bb32
.LBB99_10:	# bb22.preheader
	xorl	%ebx, %ebx
	jmp	.LBB99_8	# bb21
.LBB99_11:	# bb25
	ucomiss	.LCPI99_0(%rip), %xmm1
	setnp	%dil
	sete	%r8b
	testb	%dil, %r8b
	jne	.LBB99_18	# bb32
.LBB99_12:	# bb25
	testl	%ecx, %ecx
	jle	.LBB99_18	# bb32
.LBB99_13:	# bb25
	cmpl	$0, 36(%rsp)
	jle	.LBB99_18	# bb32
.LBB99_14:	# bb25.bb29.preheader_crit_edge
	xorl	%edi, %edi
	movl	%edi, %r8d
	.align	16
.LBB99_15:	# bb29.preheader
	xorl	%ebx, %ebx
	.align	16
.LBB99_16:	# bb28
	leal	(%rdi,%rbx), %r14d
	movslq	%r14d, %r14
	movaps	%xmm1, %xmm2
	mulss	(%r10,%r14,4), %xmm2
	movss	%xmm2, (%r10,%r14,4)
	incl	%ebx
	cmpl	36(%rsp), %ebx
	jne	.LBB99_16	# bb28
.LBB99_17:	# bb30
	addl	%eax, %edi
	incl	%r8d
	cmpl	%ecx, %r8d
	jne	.LBB99_15	# bb29.preheader
.LBB99_18:	# bb32
	pxor	%xmm1, %xmm1
	ucomiss	%xmm1, %xmm0
	setnp	%dil
	sete	%r8b
	testb	%dil, %r8b
	jne	.LBB99_64	# return
.LBB99_19:	# bb33
	cmpl	$121, %edx
	jne	.LBB99_30	# bb45
.LBB99_20:	# bb33
	cmpl	$141, %esi
	jne	.LBB99_30	# bb45
.LBB99_21:	# bb44.preheader
	testl	%ecx, %ecx
	jle	.LBB99_64	# return
.LBB99_22:	# bb.nph128
	cmpl	$0, 36(%rsp)
	jle	.LBB99_64	# return
.LBB99_23:	# bb42.preheader.preheader
	movl	96(%rsp), %esi
	incl	%esi
	movl	%esi, 8(%rsp)
	leal	-1(%rcx), %esi
	xorl	%edi, %edi
	movl	%edi, 32(%rsp)
	movl	%edi, 28(%rsp)
	movl	%edi, 12(%rsp)
	jmp	.LBB99_29	# bb42.preheader
	.align	16
.LBB99_24:	# bb38
	movl	28(%rsp), %ebx
	leal	(%rbx,%r8), %ebx
	movslq	%ebx, %rbx
	movl	32(%rsp), %r14d
	leal	(%r14,%r8), %r14d
	movslq	%r14d, %r14
	movaps	%xmm0, %xmm1
	mulss	(%r11,%r14,4), %xmm1
	movaps	%xmm1, %xmm2
	mulss	(%r9,%rdx,4), %xmm2
	addss	(%r10,%rbx,4), %xmm2
	movss	%xmm2, (%r10,%rbx,4)
	cmpl	%ecx, 16(%rsp)
	jge	.LBB99_66	# bb38.bb41_crit_edge
.LBB99_25:	# bb.nph123
	movl	24(%rsp), %r14d
	leal	(%r14,%r8), %r14d
	movl	20(%rsp), %r15d
	leal	(%r15,%r8), %r15d
	pxor	%xmm2, %xmm2
	xorl	%r12d, %r12d
	.align	16
.LBB99_26:	# bb39
	leal	(%rdi,%r12), %r13d
	movslq	%r13d, %r13
	movss	(%r9,%r13,4), %xmm3
	movaps	%xmm3, %xmm4
	mulss	%xmm1, %xmm4
	movslq	%r15d, %r13
	addss	(%r10,%r13,4), %xmm4
	movss	%xmm4, (%r10,%r13,4)
	addl	%eax, %r15d
	movl	112(%rsp), %r13d
	leal	(%r13,%r14), %r13d
	incl	%r12d
	cmpl	%esi, %r12d
	movslq	%r14d, %r14
	mulss	(%r11,%r14,4), %xmm3
	addss	%xmm3, %xmm2
	movl	%r13d, %r14d
	jne	.LBB99_26	# bb39
.LBB99_27:	# bb41
	mulss	%xmm0, %xmm2
	addss	(%r10,%rbx,4), %xmm2
	movss	%xmm2, (%r10,%rbx,4)
	incl	%r8d
	cmpl	36(%rsp), %r8d
	jne	.LBB99_24	# bb38
.LBB99_28:	# bb43
	movl	%edx, %edi
	addl	8(%rsp), %edi
	movl	112(%rsp), %r8d
	addl	%r8d, 32(%rsp)
	addl	%eax, 28(%rsp)
	decl	%esi
	movl	12(%rsp), %r8d
	incl	%r8d
	movl	%r8d, 12(%rsp)
	cmpl	%ecx, %r8d
	je	.LBB99_64	# return
.LBB99_29:	# bb42.preheader
	movl	112(%rsp), %edx
	movl	32(%rsp), %r8d
	leal	(%rdx,%r8), %r8d
	movl	%r8d, 24(%rsp)
	movl	28(%rsp), %r8d
	leal	(%rax,%r8), %r8d
	movl	%r8d, 20(%rsp)
	movslq	%edi, %rdx
	incl	%edi
	movl	12(%rsp), %r8d
	leal	1(%r8), %r8d
	movl	%r8d, 16(%rsp)
	xorl	%r8d, %r8d
	jmp	.LBB99_24	# bb38
.LBB99_30:	# bb45
	cmpl	$122, %edx
	jne	.LBB99_41	# bb58
.LBB99_31:	# bb45
	cmpl	$141, %esi
	jne	.LBB99_41	# bb58
.LBB99_32:	# bb57.preheader
	testl	%ecx, %ecx
	jle	.LBB99_64	# return
.LBB99_33:	# bb.nph118
	cmpl	$0, 36(%rsp)
	jle	.LBB99_64	# return
.LBB99_34:	# bb55.preheader.preheader
	movl	96(%rsp), %edi
	leal	1(%rdi), %edi
	movl	%edi, 28(%rsp)
	xorl	%edi, %edi
	movl	%edi, %edx
	movl	%edi, 32(%rsp)
	movl	%edi, %r8d
	movl	%edi, %ebx
	jmp	.LBB99_40	# bb55.preheader
	.align	16
.LBB99_35:	# bb51
	movl	32(%rsp), %r14d
	leal	(%r14,%rdi), %r14d
	movslq	%r14d, %r14
	movaps	%xmm0, %xmm1
	mulss	(%r11,%r14,4), %xmm1
	testl	%ebx, %ebx
	jle	.LBB99_67	# bb51.bb54_crit_edge
.LBB99_36:	# bb51.bb52_crit_edge
	pxor	%xmm2, %xmm2
	xorl	%r14d, %r14d
	movl	%edi, %r15d
	movl	%edi, %r12d
	.align	16
.LBB99_37:	# bb52
	leal	(%rdx,%r14), %r13d
	movslq	%r13d, %r13
	movss	(%r9,%r13,4), %xmm3
	movaps	%xmm3, %xmm4
	mulss	%xmm1, %xmm4
	movslq	%r15d, %r13
	addss	(%r10,%r13,4), %xmm4
	movss	%xmm4, (%r10,%r13,4)
	addl	%eax, %r15d
	movl	112(%rsp), %r13d
	leal	(%r13,%r12), %r13d
	incl	%r14d
	cmpl	%ebx, %r14d
	movslq	%r12d, %r12
	mulss	(%r11,%r12,4), %xmm3
	addss	%xmm3, %xmm2
	movl	%r13d, %r12d
	jne	.LBB99_37	# bb52
.LBB99_38:	# bb54
	mulss	(%r9,%rsi,4), %xmm1
	mulss	%xmm0, %xmm2
	addss	%xmm1, %xmm2
	leal	(%r8,%rdi), %r14d
	movslq	%r14d, %r14
	addss	(%r10,%r14,4), %xmm2
	movss	%xmm2, (%r10,%r14,4)
	incl	%edi
	cmpl	36(%rsp), %edi
	jne	.LBB99_35	# bb51
.LBB99_39:	# bb56
	movl	%esi, %edi
	addl	28(%rsp), %edi
	addl	96(%rsp), %edx
	movl	112(%rsp), %esi
	addl	%esi, 32(%rsp)
	addl	%eax, %r8d
	incl	%ebx
	cmpl	%ecx, %ebx
	je	.LBB99_64	# return
.LBB99_40:	# bb55.preheader
	movslq	%edi, %rsi
	xorl	%edi, %edi
	jmp	.LBB99_35	# bb51
.LBB99_41:	# bb58
	cmpl	$121, %edx
	jne	.LBB99_52	# bb71
.LBB99_42:	# bb58
	cmpl	$142, %esi
	jne	.LBB99_52	# bb71
.LBB99_43:	# bb70.preheader
	testl	%ecx, %ecx
	jle	.LBB99_64	# return
.LBB99_44:	# bb.nph110
	cmpl	$0, 36(%rsp)
	jle	.LBB99_64	# return
.LBB99_45:	# bb.nph110.bb68.preheader_crit_edge
	xorl	%edx, %edx
	movl	%edx, 28(%rsp)
	movl	%edx, 32(%rsp)
	movl	%edx, 12(%rsp)
	jmp	.LBB99_51	# bb68.preheader
	.align	16
.LBB99_46:	# bb64
	movl	32(%rsp), %r8d
	leal	(%r8,%rdi), %r8d
	movslq	%r8d, %r8
	movl	28(%rsp), %ebx
	leal	(%rbx,%rdi), %ebx
	movslq	%ebx, %rbx
	movaps	%xmm0, %xmm1
	mulss	(%r11,%rbx,4), %xmm1
	movslq	%edx, %rbx
	movaps	%xmm1, %xmm2
	mulss	(%r9,%rbx,4), %xmm2
	addss	(%r10,%r8,4), %xmm2
	movss	%xmm2, (%r10,%r8,4)
	leal	1(%rdi), %ebx
	cmpl	36(%rsp), %ebx
	jge	.LBB99_68	# bb64.bb67_crit_edge
.LBB99_47:	# bb.nph105
	movl	16(%rsp), %ebx
	leal	(%rbx,%rdi), %ebx
	movl	20(%rsp), %r14d
	leal	(%r14,%rdi), %r14d
	leal	1(%rdx), %r15d
	pxor	%xmm2, %xmm2
	xorl	%r12d, %r12d
	.align	16
.LBB99_48:	# bb65
	leal	(%r14,%r12), %r13d
	movslq	%r13d, %r13
	leal	(%r15,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r9,%rbp,4), %xmm3
	movaps	%xmm1, %xmm4
	mulss	%xmm3, %xmm4
	addss	(%r10,%r13,4), %xmm4
	movss	%xmm4, (%r10,%r13,4)
	leal	(%rbx,%r12), %r13d
	movslq	%r13d, %r13
	mulss	(%r11,%r13,4), %xmm3
	addss	%xmm3, %xmm2
	incl	%r12d
	cmpl	%esi, %r12d
	jne	.LBB99_48	# bb65
.LBB99_49:	# bb67
	mulss	%xmm0, %xmm2
	addss	(%r10,%r8,4), %xmm2
	movss	%xmm2, (%r10,%r8,4)
	addl	24(%rsp), %edx
	decl	%esi
	incl	%edi
	cmpl	36(%rsp), %edi
	jne	.LBB99_46	# bb64
.LBB99_50:	# bb69
	movl	112(%rsp), %edx
	addl	%edx, 28(%rsp)
	addl	%eax, 32(%rsp)
	movl	12(%rsp), %edx
	incl	%edx
	movl	%edx, 12(%rsp)
	cmpl	%ecx, %edx
	je	.LBB99_64	# return
.LBB99_51:	# bb68.preheader
	movl	96(%rsp), %edx
	leal	1(%rdx), %edx
	movl	%edx, 24(%rsp)
	movl	36(%rsp), %edx
	leal	-1(%rdx), %esi
	movl	32(%rsp), %edx
	leal	1(%rdx), %edx
	movl	%edx, 20(%rsp)
	movl	28(%rsp), %edx
	leal	1(%rdx), %edx
	movl	%edx, 16(%rsp)
	xorl	%edx, %edx
	movl	%edx, %edi
	jmp	.LBB99_46	# bb64
.LBB99_52:	# bb71
	cmpl	$122, %edx
	jne	.LBB99_63	# bb84
.LBB99_53:	# bb71
	cmpl	$142, %esi
	jne	.LBB99_63	# bb84
.LBB99_54:	# bb83.preheader
	testl	%ecx, %ecx
	jle	.LBB99_64	# return
.LBB99_55:	# bb.nph100
	cmpl	$0, 36(%rsp)
	jle	.LBB99_64	# return
.LBB99_56:	# bb.nph100.bb81.preheader_crit_edge
	xorl	%edi, %edi
	movl	%edi, %r14d
	movl	%edi, %esi
	jmp	.LBB99_62	# bb81.preheader
	.align	16
.LBB99_57:	# bb77
	leal	(%rdi,%r8), %r12d
	movslq	%r12d, %r12
	movaps	%xmm0, %xmm1
	mulss	(%r11,%r12,4), %xmm1
	testl	%r8d, %r8d
	jle	.LBB99_69	# bb77.bb80_crit_edge
.LBB99_58:	# bb77.bb78_crit_edge
	pxor	%xmm2, %xmm2
	xorl	%r12d, %r12d
	.align	16
.LBB99_59:	# bb78
	leal	(%r14,%r12), %r13d
	movslq	%r13d, %r13
	leal	(%r15,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r9,%rbp,4), %xmm3
	movaps	%xmm1, %xmm4
	mulss	%xmm3, %xmm4
	addss	(%r10,%r13,4), %xmm4
	movss	%xmm4, (%r10,%r13,4)
	leal	(%rdi,%r12), %r13d
	movslq	%r13d, %r13
	mulss	(%r11,%r13,4), %xmm3
	addss	%xmm3, %xmm2
	incl	%r12d
	cmpl	%r8d, %r12d
	jne	.LBB99_59	# bb78
.LBB99_60:	# bb80
	movslq	%edx, %r12
	mulss	(%r9,%r12,4), %xmm1
	mulss	%xmm0, %xmm2
	addss	%xmm1, %xmm2
	leal	(%r14,%r8), %r12d
	movslq	%r12d, %r12
	addss	(%r10,%r12,4), %xmm2
	movss	%xmm2, (%r10,%r12,4)
	addl	%ebx, %edx
	addl	96(%rsp), %r15d
	incl	%r8d
	cmpl	36(%rsp), %r8d
	jne	.LBB99_57	# bb77
.LBB99_61:	# bb82
	addl	112(%rsp), %edi
	addl	%eax, %r14d
	incl	%esi
	cmpl	%ecx, %esi
	je	.LBB99_64	# return
.LBB99_62:	# bb81.preheader
	movl	96(%rsp), %edx
	leal	1(%rdx), %ebx
	xorl	%edx, %edx
	movl	%edx, %r15d
	movl	%edx, %r8d
	jmp	.LBB99_57	# bb77
.LBB99_63:	# bb84
	xorl	%edi, %edi
	leaq	.str116, %rsi
	leaq	.str1117, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB99_64:	# return
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB99_65:	# bb9.bb18_crit_edge
	movl	%r8d, 36(%rsp)
	jmp	.LBB99_4	# bb18
.LBB99_66:	# bb38.bb41_crit_edge
	pxor	%xmm2, %xmm2
	jmp	.LBB99_27	# bb41
.LBB99_67:	# bb51.bb54_crit_edge
	pxor	%xmm2, %xmm2
	jmp	.LBB99_38	# bb54
.LBB99_68:	# bb64.bb67_crit_edge
	pxor	%xmm2, %xmm2
	jmp	.LBB99_49	# bb67
.LBB99_69:	# bb77.bb80_crit_edge
	pxor	%xmm2, %xmm2
	jmp	.LBB99_60	# bb80
	.size	cblas_ssymm, .-cblas_ssymm
.Leh_func_end65:


	.section	.rodata.cst4,"aM",@progbits,4
	.align	4
.LCPI100_0:					
	.long	1065353216	# float 1.000000e+00
	.text
	.align	16
	.globl	cblas_ssymv
	.type	cblas_ssymv,@function
cblas_ssymv:
.Leh_func_begin66:
.Llabel66:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	ucomiss	.LCPI100_0(%rip), %xmm1
	movl	112(%rsp), %eax
	movq	104(%rsp), %r10
	movl	%r8d, 24(%rsp)
	jne	.LBB100_2	# bb11
	jp	.LBB100_2	# bb11
.LBB100_1:	# entry
	pxor	%xmm2, %xmm2
	ucomiss	%xmm2, %xmm0
	setnp	%r8b
	sete	%r11b
	testb	%r8b, %r11b
	jne	.LBB100_42	# bb80.thread
.LBB100_2:	# bb11
	pxor	%xmm2, %xmm2
	ucomiss	%xmm2, %xmm1
	jne	.LBB100_8	# bb18
	jp	.LBB100_8	# bb18
.LBB100_3:	# bb12
	testl	%eax, %eax
	jg	.LBB100_44	# bb12.bb17.preheader_crit_edge
.LBB100_4:	# bb13
	movl	$1, %r8d
	subl	%edx, %r8d
	imull	%eax, %r8d
.LBB100_5:	# bb17.preheader
	testl	%edx, %edx
	jle	.LBB100_14	# bb25
.LBB100_6:	# bb17.preheader.bb16_crit_edge
	xorl	%r11d, %r11d
	.align	16
.LBB100_7:	# bb16
	movslq	%r8d, %rbx
	movl	$0, (%r10,%rbx,4)
	addl	%eax, %r8d
	incl	%r11d
	cmpl	%edx, %r11d
	jne	.LBB100_7	# bb16
	jmp	.LBB100_14	# bb25
.LBB100_8:	# bb18
	ucomiss	.LCPI100_0(%rip), %xmm1
	setnp	%r8b
	sete	%r11b
	testb	%r8b, %r11b
	jne	.LBB100_14	# bb25
.LBB100_9:	# bb19
	testl	%eax, %eax
	jg	.LBB100_45	# bb19.bb24.preheader_crit_edge
.LBB100_10:	# bb20
	movl	$1, %r8d
	subl	%edx, %r8d
	imull	%eax, %r8d
.LBB100_11:	# bb24.preheader
	testl	%edx, %edx
	jle	.LBB100_14	# bb25
.LBB100_12:	# bb24.preheader.bb23_crit_edge
	xorl	%r11d, %r11d
	.align	16
.LBB100_13:	# bb23
	movslq	%r8d, %rbx
	movaps	%xmm1, %xmm2
	mulss	(%r10,%rbx,4), %xmm2
	movss	%xmm2, (%r10,%rbx,4)
	addl	%eax, %r8d
	incl	%r11d
	cmpl	%edx, %r11d
	jne	.LBB100_13	# bb23
.LBB100_14:	# bb25
	pxor	%xmm1, %xmm1
	ucomiss	%xmm1, %xmm0
	setnp	%r8b
	sete	%r11b
	testb	%r8b, %r11b
	jne	.LBB100_42	# bb80.thread
.LBB100_15:	# bb26
	cmpl	$121, %esi
	jne	.LBB100_17	# bb29
.LBB100_16:	# bb26
	cmpl	$101, %edi
	je	.LBB100_19	# bb33
.LBB100_17:	# bb29
	cmpl	$122, %esi
	jne	.LBB100_29	# bb51
.LBB100_18:	# bb29
	cmpl	$102, %edi
	jne	.LBB100_29	# bb51
.LBB100_19:	# bb33
	cmpl	$0, 96(%rsp)
	jg	.LBB100_46	# bb33.bb36_crit_edge
.LBB100_20:	# bb34
	movl	$1, %esi
	subl	%edx, %esi
	imull	96(%rsp), %esi
	movl	%esi, 16(%rsp)
.LBB100_21:	# bb36
	testl	%eax, %eax
	jg	.LBB100_47	# bb36.bb50.preheader_crit_edge
.LBB100_22:	# bb37
	movl	$1, %esi
	subl	%edx, %esi
	imull	%eax, %esi
	movl	%esi, 12(%rsp)
.LBB100_23:	# bb50.preheader
	testl	%edx, %edx
	jle	.LBB100_42	# bb80.thread
.LBB100_24:	# bb.nph113
	movl	$1, %esi
	subl	%edx, %esi
	movl	%esi, %edi
	imull	96(%rsp), %edi
	movl	%edi, 20(%rsp)
	imull	%eax, %esi
	movl	%esi, 8(%rsp)
	leal	-1(%rdx), %esi
	incl	24(%rsp)
	xorl	%edi, %edi
	movl	%edi, 32(%rsp)
	movl	%edi, 36(%rsp)
	movl	%edi, 28(%rsp)
	.align	16
.LBB100_25:	# bb40
	movl	32(%rsp), %r8d
	movl	12(%rsp), %r11d
	leal	(%r11,%r8), %r8d
	movslq	%r8d, %r8
	movl	16(%rsp), %r11d
	leal	(%r11,%rdi), %r11d
	movslq	%r11d, %r11
	movaps	%xmm0, %xmm1
	mulss	(%r9,%r11,4), %xmm1
	movslq	36(%rsp), %r11
	movaps	%xmm1, %xmm2
	mulss	(%rcx,%r11,4), %xmm2
	addss	(%r10,%r8,4), %xmm2
	movss	%xmm2, (%r10,%r8,4)
	xorl	%r11d, %r11d
	testl	%eax, %eax
	movl	8(%rsp), %ebx
	cmovg	%r11d, %ebx
	cmpl	$0, 96(%rsp)
	cmovle	20(%rsp), %r11d
	movl	28(%rsp), %r14d
	leal	1(%r14), %r14d
	cmpl	%edx, %r14d
	jge	.LBB100_48	# bb40.bb49_crit_edge
.LBB100_26:	# bb.nph108
	movl	96(%rsp), %r14d
	leal	(%r14,%rdi), %r14d
	movl	32(%rsp), %r15d
	leal	(%rax,%r15), %r15d
	movl	36(%rsp), %r12d
	leal	1(%r12), %r12d
	pxor	%xmm2, %xmm2
	xorl	%r13d, %r13d
	.align	16
.LBB100_27:	# bb47
	addl	%ebx, %r15d
	movslq	%r15d, %rbx
	leal	(%r12,%r13), %ebp
	movslq	%ebp, %rbp
	movaps	%xmm1, %xmm3
	mulss	(%rcx,%rbp,4), %xmm3
	addss	(%r10,%rbx,4), %xmm3
	movss	%xmm3, (%r10,%rbx,4)
	addl	%r11d, %r14d
	movslq	%r14d, %r11
	movss	(%r9,%r11,4), %xmm3
	mulss	(%rcx,%rbp,4), %xmm3
	addss	%xmm3, %xmm2
	incl	%r13d
	cmpl	%esi, %r13d
	movl	%eax, %ebx
	movl	96(%rsp), %r11d
	jne	.LBB100_27	# bb47
.LBB100_28:	# bb49
	mulss	%xmm0, %xmm2
	addss	(%r10,%r8,4), %xmm2
	movss	%xmm2, (%r10,%r8,4)
	addl	96(%rsp), %edi
	addl	%eax, 32(%rsp)
	movl	36(%rsp), %r8d
	addl	24(%rsp), %r8d
	movl	%r8d, 36(%rsp)
	decl	%esi
	movl	28(%rsp), %r8d
	incl	%r8d
	movl	%r8d, 28(%rsp)
	cmpl	%edx, %r8d
	jne	.LBB100_25	# bb40
	jmp	.LBB100_42	# bb80.thread
.LBB100_29:	# bb51
	cmpl	$102, %edi
	sete	%r8b
	cmpl	$121, %esi
	sete	%r11b
	andb	%r8b, %r11b
	cmpl	$101, %edi
	sete	%dil
	cmpl	$122, %esi
	sete	%sil
	testb	%dil, %sil
	jne	.LBB100_31	# bb59
.LBB100_30:	# bb51
	notb	%r11b
	testb	$1, %r11b
	jne	.LBB100_43	# bb82
.LBB100_31:	# bb59
	cmpl	$0, 96(%rsp)
	jg	.LBB100_49	# bb59.bb62_crit_edge
.LBB100_32:	# bb60
	movl	$1, %esi
	subl	%edx, %esi
	imull	96(%rsp), %esi
	movl	%esi, 36(%rsp)
.LBB100_33:	# bb62
	testl	%eax, %eax
	jg	.LBB100_50	# bb62.bb65_crit_edge
.LBB100_34:	# bb63
	movl	$1, %esi
	subl	%edx, %esi
	imull	%eax, %esi
.LBB100_35:	# bb65
	movl	$1, %edi
	subl	%edx, %edi
	movl	%edi, %r8d
	movl	96(%rsp), %r11d
	imull	%r11d, %r8d
	movl	%r8d, 32(%rsp)
	leal	-1(%rdx), %r8d
	imull	%r8d, %r11d
	imull	%eax, %edi
	movl	%edi, 20(%rsp)
	movl	%eax, %edi
	imull	%r8d, %edi
	movl	$4294967295, %r14d
	movl	24(%rsp), %ebx
	subl	%ebx, %r14d
	movl	%r14d, 28(%rsp)
	movl	%ebx, %r14d
	imull	%r8d, %r14d
	leal	1(%rbx), %ebx
	imull	%r8d, %ebx
	addl	%esi, %edi
	addl	%r11d, 36(%rsp)
	jmp	.LBB100_40	# bb76
.LBB100_36:	# bb66
	movslq	36(%rsp), %r8
	movaps	%xmm0, %xmm1
	mulss	(%r9,%r8,4), %xmm1
	movslq	%ebx, %r8
	movaps	%xmm1, %xmm2
	mulss	(%rcx,%r8,4), %xmm2
	movslq	%edi, %r8
	addss	(%r10,%r8,4), %xmm2
	movss	%xmm2, (%r10,%r8,4)
	xorl	%r11d, %r11d
	testl	%eax, %eax
	movl	20(%rsp), %r15d
	cmovg	%r11d, %r15d
	cmpl	$0, 96(%rsp)
	cmovle	32(%rsp), %r11d
	testl	%esi, %esi
	jle	.LBB100_51	# bb66.bb75_crit_edge
.LBB100_37:	# bb.nph96
	leal	-1(%rdx), %esi
	pxor	%xmm2, %xmm2
	xorl	%r12d, %r12d
	.align	16
.LBB100_38:	# bb73
	leal	(%r14,%r12), %r13d
	movslq	%r13d, %r13
	movaps	%xmm1, %xmm3
	mulss	(%rcx,%r13,4), %xmm3
	movslq	%r15d, %rbp
	addss	(%r10,%rbp,4), %xmm3
	movss	%xmm3, (%r10,%rbp,4)
	movslq	%r11d, %rbp
	movss	(%r9,%rbp,4), %xmm3
	mulss	(%rcx,%r13,4), %xmm3
	addss	%xmm3, %xmm2
	addl	96(%rsp), %r11d
	addl	%eax, %r15d
	incl	%r12d
	cmpl	%esi, %r12d
	jne	.LBB100_38	# bb73
.LBB100_39:	# bb75
	mulss	%xmm0, %xmm2
	addss	(%r10,%r8,4), %xmm2
	movss	%xmm2, (%r10,%r8,4)
	addl	28(%rsp), %ebx
	subl	%eax, %edi
	movl	36(%rsp), %esi
	subl	96(%rsp), %esi
	movl	%esi, 36(%rsp)
	subl	24(%rsp), %r14d
	decl	%edx
.LBB100_40:	# bb76
	testl	%edx, %edx
	jle	.LBB100_42	# bb80.thread
.LBB100_41:	# bb77
	leal	-1(%rdx), %esi
	testl	%edx, %edx
	jne	.LBB100_36	# bb66
.LBB100_42:	# bb80.thread
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB100_43:	# bb82
	xorl	%edi, %edi
	leaq	.str118, %rsi
	leaq	.str1119, %rdx
	xorb	%al, %al
	call	cblas_xerbla
	jmp	.LBB100_42	# bb80.thread
.LBB100_44:	# bb12.bb17.preheader_crit_edge
	xorl	%r8d, %r8d
	jmp	.LBB100_5	# bb17.preheader
.LBB100_45:	# bb19.bb24.preheader_crit_edge
	xorl	%r8d, %r8d
	jmp	.LBB100_11	# bb24.preheader
.LBB100_46:	# bb33.bb36_crit_edge
	movl	$0, 16(%rsp)
	jmp	.LBB100_21	# bb36
.LBB100_47:	# bb36.bb50.preheader_crit_edge
	movl	$0, 12(%rsp)
	jmp	.LBB100_23	# bb50.preheader
.LBB100_48:	# bb40.bb49_crit_edge
	pxor	%xmm2, %xmm2
	jmp	.LBB100_28	# bb49
.LBB100_49:	# bb59.bb62_crit_edge
	movl	$0, 36(%rsp)
	jmp	.LBB100_33	# bb62
.LBB100_50:	# bb62.bb65_crit_edge
	xorl	%esi, %esi
	jmp	.LBB100_35	# bb65
.LBB100_51:	# bb66.bb75_crit_edge
	pxor	%xmm2, %xmm2
	jmp	.LBB100_39	# bb75
	.size	cblas_ssymv, .-cblas_ssymv
.Leh_func_end66:


	.align	16
	.globl	cblas_ssyr2
	.type	cblas_ssyr2,@function
cblas_ssyr2:
.Leh_func_begin67:
.Llabel67:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$8, %rsp
	pxor	%xmm1, %xmm1
	ucomiss	%xmm1, %xmm0
	setnp	%al
	sete	%r10b
	testb	%al, %r10b
	movq	72(%rsp), %rax
	movl	64(%rsp), %r10d
	jne	.LBB101_29	# return
.LBB101_1:	# entry
	testl	%edx, %edx
	je	.LBB101_29	# return
.LBB101_2:	# bb7
	cmpl	$121, %esi
	jne	.LBB101_4	# bb10
.LBB101_3:	# bb7
	cmpl	$101, %edi
	je	.LBB101_6	# bb14
.LBB101_4:	# bb10
	cmpl	$122, %esi
	jne	.LBB101_16	# bb26
.LBB101_5:	# bb10
	cmpl	$102, %edi
	jne	.LBB101_16	# bb26
.LBB101_6:	# bb14
	testl	%r8d, %r8d
	jg	.LBB101_30	# bb14.bb17_crit_edge
.LBB101_7:	# bb15
	movl	$1, %esi
	subl	%edx, %esi
	imull	%r8d, %esi
.LBB101_8:	# bb17
	testl	%r10d, %r10d
	jg	.LBB101_31	# bb17.bb25.preheader_crit_edge
.LBB101_9:	# bb18
	movl	$1, %edi
	subl	%edx, %edi
	imull	%r10d, %edi
.LBB101_10:	# bb25.preheader
	testl	%edx, %edx
	jle	.LBB101_29	# return
.LBB101_11:	# bb21.preheader
	movl	80(%rsp), %r11d
	incl	%r11d
	movl	%r11d, (%rsp)
	xorl	%r11d, %r11d
	movl	%edx, %ebx
	movl	%r11d, %r14d
	.align	16
.LBB101_12:	# bb21
	movslq	%edi, %r15
	movaps	%xmm0, %xmm1
	mulss	(%r9,%r15,4), %xmm1
	movslq	%esi, %r15
	movaps	%xmm0, %xmm2
	mulss	(%rcx,%r15,4), %xmm2
	cmpl	%edx, %r14d
	jge	.LBB101_15	# bb24
.LBB101_13:	# bb21.bb22_crit_edge
	xorl	%r15d, %r15d
	movl	%edi, %r12d
	movl	%esi, %r13d
	.align	16
.LBB101_14:	# bb22
	movslq	%r13d, %rbp
	movaps	%xmm1, %xmm3
	mulss	(%rcx,%rbp,4), %xmm3
	movslq	%r12d, %rbp
	movaps	%xmm2, %xmm4
	mulss	(%r9,%rbp,4), %xmm4
	addss	%xmm3, %xmm4
	leal	(%r11,%r15), %ebp
	movslq	%ebp, %rbp
	addss	(%rax,%rbp,4), %xmm4
	movss	%xmm4, (%rax,%rbp,4)
	addl	%r10d, %r12d
	addl	%r8d, %r13d
	incl	%r15d
	cmpl	%ebx, %r15d
	jne	.LBB101_14	# bb22
.LBB101_15:	# bb24
	addl	(%rsp), %r11d
	addl	%r8d, %esi
	addl	%r10d, %edi
	decl	%ebx
	incl	%r14d
	cmpl	%edx, %r14d
	jne	.LBB101_12	# bb21
	jmp	.LBB101_29	# return
.LBB101_16:	# bb26
	cmpl	$102, %edi
	sete	%r11b
	cmpl	$121, %esi
	sete	%bl
	andb	%r11b, %bl
	cmpl	$101, %edi
	sete	%dil
	cmpl	$122, %esi
	sete	%sil
	testb	%dil, %sil
	jne	.LBB101_18	# bb34
.LBB101_17:	# bb26
	notb	%bl
	testb	$1, %bl
	jne	.LBB101_28	# bb52
.LBB101_18:	# bb34
	testl	%r8d, %r8d
	jg	.LBB101_32	# bb34.bb37_crit_edge
.LBB101_19:	# bb35
	movl	$1, %esi
	subl	%edx, %esi
	imull	%r8d, %esi
.LBB101_20:	# bb37
	testl	%r10d, %r10d
	jg	.LBB101_33	# bb37.bb51.preheader_crit_edge
.LBB101_21:	# bb38
	movl	$1, %edi
	subl	%edx, %edi
	imull	%r10d, %edi
.LBB101_22:	# bb51.preheader
	testl	%edx, %edx
	jle	.LBB101_29	# return
.LBB101_23:	# bb.nph61
	movl	$1, %r11d
	subl	%edx, %r11d
	movl	%r11d, %ebx
	imull	%r8d, %ebx
	movl	%ebx, 4(%rsp)
	imull	%r10d, %r11d
	xorl	%ebx, %ebx
	movl	%ebx, %r14d
	.align	16
.LBB101_24:	# bb41
	xorl	%r15d, %r15d
	testl	%r10d, %r10d
	movl	%r11d, %r12d
	cmovg	%r15d, %r12d
	testl	%r8d, %r8d
	cmovle	4(%rsp), %r15d
	movslq	%edi, %r13
	movaps	%xmm0, %xmm1
	mulss	(%r9,%r13,4), %xmm1
	movslq	%esi, %r13
	movaps	%xmm0, %xmm2
	mulss	(%rcx,%r13,4), %xmm2
	testl	%r14d, %r14d
	js	.LBB101_27	# bb50
.LBB101_25:	# bb41.bb48_crit_edge
	xorl	%r13d, %r13d
	.align	16
.LBB101_26:	# bb48
	movslq	%r15d, %rbp
	movaps	%xmm1, %xmm3
	mulss	(%rcx,%rbp,4), %xmm3
	movslq	%r12d, %rbp
	movaps	%xmm2, %xmm4
	mulss	(%r9,%rbp,4), %xmm4
	addss	%xmm3, %xmm4
	leal	(%rbx,%r13), %ebp
	movslq	%ebp, %rbp
	addss	(%rax,%rbp,4), %xmm4
	movss	%xmm4, (%rax,%rbp,4)
	addl	%r8d, %r15d
	addl	%r10d, %r12d
	incl	%r13d
	cmpl	%r14d, %r13d
	jle	.LBB101_26	# bb48
.LBB101_27:	# bb50
	addl	%r8d, %esi
	addl	%r10d, %edi
	addl	80(%rsp), %ebx
	incl	%r14d
	cmpl	%edx, %r14d
	jne	.LBB101_24	# bb41
	jmp	.LBB101_29	# return
.LBB101_28:	# bb52
	xorl	%edi, %edi
	leaq	.str120, %rsi
	leaq	.str1121, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB101_29:	# return
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB101_30:	# bb14.bb17_crit_edge
	xorl	%esi, %esi
	jmp	.LBB101_8	# bb17
.LBB101_31:	# bb17.bb25.preheader_crit_edge
	xorl	%edi, %edi
	jmp	.LBB101_10	# bb25.preheader
.LBB101_32:	# bb34.bb37_crit_edge
	xorl	%esi, %esi
	jmp	.LBB101_20	# bb37
.LBB101_33:	# bb37.bb51.preheader_crit_edge
	xorl	%edi, %edi
	jmp	.LBB101_22	# bb51.preheader
	.size	cblas_ssyr2, .-cblas_ssyr2
.Leh_func_end67:


	.section	.rodata.cst4,"aM",@progbits,4
	.align	4
.LCPI102_0:					
	.long	1065353216	# float 1.000000e+00
	.text
	.align	16
	.globl	cblas_ssyr2k
	.type	cblas_ssyr2k,@function
cblas_ssyr2k:
.Leh_func_begin68:
.Llabel68:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$8, %rsp
	ucomiss	.LCPI102_0(%rip), %xmm1
	movq	88(%rsp), %rax
	movq	72(%rsp), %r10
	jne	.LBB102_2	# bb4
	jp	.LBB102_2	# bb4
.LBB102_1:	# entry
	pxor	%xmm2, %xmm2
	ucomiss	%xmm2, %xmm0
	setnp	%r11b
	sete	%bl
	testb	%r11b, %bl
	jne	.LBB102_81	# return
.LBB102_2:	# bb4
	cmpl	$101, %edi
	je	.LBB102_82	# bb5
.LBB102_3:	# bb9
	cmpl	$121, %esi
	movl	$122, %edi
	movl	$121, %esi
	cmove	%edi, %esi
	addl	$4294967184, %edx
	cmpl	$1, %edx
	jbe	.LBB102_83	# bb9.bb15_crit_edge
.LBB102_4:	# bb14
	movl	$112, 4(%rsp)
.LBB102_5:	# bb15
	pxor	%xmm2, %xmm2
	ucomiss	%xmm2, %xmm1
	jne	.LBB102_19	# bb29
	jp	.LBB102_19	# bb29
.LBB102_6:	# bb16
	cmpl	$121, %esi
	je	.LBB102_9	# bb22.preheader
.LBB102_7:	# bb28.preheader
	testl	%ecx, %ecx
	jle	.LBB102_33	# bb43
.LBB102_8:	# bb28.preheader.bb26.preheader_crit_edge
	xorl	%edx, %edx
	movl	%edx, %edi
	jmp	.LBB102_17	# bb26.preheader
.LBB102_9:	# bb22.preheader
	testl	%ecx, %ecx
	jle	.LBB102_33	# bb43
.LBB102_10:	# bb20.preheader.preheader
	movl	96(%rsp), %edx
	leal	1(%rdx), %edx
	xorl	%edi, %edi
	movl	%ecx, %r11d
	movl	%edi, %ebx
	jmp	.LBB102_13	# bb20.preheader
	.align	16
.LBB102_11:	# bb19
	leal	(%rdi,%r14), %r15d
	movslq	%r15d, %r15
	movl	$0, (%rax,%r15,4)
	incl	%r14d
	cmpl	%r11d, %r14d
	jne	.LBB102_11	# bb19
.LBB102_12:	# bb21
	addl	%edx, %edi
	decl	%r11d
	incl	%ebx
	cmpl	%ecx, %ebx
	je	.LBB102_33	# bb43
.LBB102_13:	# bb20.preheader
	cmpl	%ecx, %ebx
	jge	.LBB102_12	# bb21
.LBB102_14:	# bb20.preheader.bb19_crit_edge
	xorl	%r14d, %r14d
	jmp	.LBB102_11	# bb19
	.align	16
.LBB102_15:	# bb25
	leal	(%rdx,%r11), %ebx
	movslq	%ebx, %rbx
	movl	$0, (%rax,%rbx,4)
	incl	%r11d
	cmpl	%edi, %r11d
	jle	.LBB102_15	# bb25
.LBB102_16:	# bb27
	addl	96(%rsp), %edx
	incl	%edi
	cmpl	%ecx, %edi
	je	.LBB102_33	# bb43
.LBB102_17:	# bb26.preheader
	testl	%edi, %edi
	js	.LBB102_16	# bb27
.LBB102_18:	# bb26.preheader.bb25_crit_edge
	xorl	%r11d, %r11d
	jmp	.LBB102_15	# bb25
.LBB102_19:	# bb29
	ucomiss	.LCPI102_0(%rip), %xmm1
	setnp	%dl
	sete	%dil
	testb	%dl, %dil
	jne	.LBB102_33	# bb43
.LBB102_20:	# bb30
	cmpl	$121, %esi
	je	.LBB102_23	# bb36.preheader
.LBB102_21:	# bb42.preheader
	testl	%ecx, %ecx
	jle	.LBB102_33	# bb43
.LBB102_22:	# bb42.preheader.bb40.preheader_crit_edge
	xorl	%edx, %edx
	movl	%edx, %edi
	jmp	.LBB102_31	# bb40.preheader
.LBB102_23:	# bb36.preheader
	testl	%ecx, %ecx
	jle	.LBB102_33	# bb43
.LBB102_24:	# bb34.preheader.preheader
	movl	96(%rsp), %edx
	leal	1(%rdx), %edx
	xorl	%edi, %edi
	movl	%ecx, %r11d
	movl	%edi, %ebx
	jmp	.LBB102_27	# bb34.preheader
	.align	16
.LBB102_25:	# bb33
	leal	(%rdi,%r14), %r15d
	movslq	%r15d, %r15
	movaps	%xmm1, %xmm2
	mulss	(%rax,%r15,4), %xmm2
	movss	%xmm2, (%rax,%r15,4)
	incl	%r14d
	cmpl	%r11d, %r14d
	jne	.LBB102_25	# bb33
.LBB102_26:	# bb35
	addl	%edx, %edi
	decl	%r11d
	incl	%ebx
	cmpl	%ecx, %ebx
	je	.LBB102_33	# bb43
.LBB102_27:	# bb34.preheader
	cmpl	%ecx, %ebx
	jge	.LBB102_26	# bb35
.LBB102_28:	# bb34.preheader.bb33_crit_edge
	xorl	%r14d, %r14d
	jmp	.LBB102_25	# bb33
	.align	16
.LBB102_29:	# bb39
	leal	(%rdx,%r11), %ebx
	movslq	%ebx, %rbx
	movaps	%xmm1, %xmm2
	mulss	(%rax,%rbx,4), %xmm2
	movss	%xmm2, (%rax,%rbx,4)
	incl	%r11d
	cmpl	%edi, %r11d
	jle	.LBB102_29	# bb39
.LBB102_30:	# bb41
	addl	96(%rsp), %edx
	incl	%edi
	cmpl	%ecx, %edi
	je	.LBB102_33	# bb43
.LBB102_31:	# bb40.preheader
	testl	%edi, %edi
	js	.LBB102_30	# bb41
.LBB102_32:	# bb40.preheader.bb39_crit_edge
	xorl	%r11d, %r11d
	jmp	.LBB102_29	# bb39
.LBB102_33:	# bb43
	pxor	%xmm1, %xmm1
	ucomiss	%xmm1, %xmm0
	setnp	%dl
	sete	%dil
	testb	%dl, %dil
	jne	.LBB102_81	# return
.LBB102_34:	# bb44
	cmpl	$121, %esi
	jne	.LBB102_46	# bb56
.LBB102_35:	# bb44
	cmpl	$111, 4(%rsp)
	jne	.LBB102_46	# bb56
.LBB102_36:	# bb55.preheader
	testl	%ecx, %ecx
	jle	.LBB102_81	# return
.LBB102_37:	# bb.nph141
	movl	96(%rsp), %edx
	incl	%edx
	movl	%edx, (%rsp)
	xorl	%edi, %edi
	movl	%edi, %r15d
	movl	%edi, %edx
	movl	%ecx, %ebx
	movl	%edi, 4(%rsp)
	jmp	.LBB102_44	# bb53.preheader
.LBB102_38:	# bb51.preheader.bb50_crit_edge
	pxor	%xmm1, %xmm1
	xorl	%r14d, %r14d
	.align	16
.LBB102_39:	# bb50
	leal	(%r12,%r14), %r13d
	movslq	%r13d, %r13
	leal	(%rdx,%r14), %ebp
	movslq	%ebp, %rbp
	movss	(%r10,%rbp,4), %xmm2
	mulss	(%r9,%r13,4), %xmm2
	leal	(%rsi,%r14), %r13d
	movslq	%r13d, %r13
	leal	(%r15,%r14), %ebp
	movslq	%ebp, %rbp
	movss	(%r9,%rbp,4), %xmm3
	mulss	(%r10,%r13,4), %xmm3
	addss	%xmm2, %xmm3
	addss	%xmm3, %xmm1
	incl	%r14d
	cmpl	%r8d, %r14d
	jne	.LBB102_39	# bb50
.LBB102_40:	# bb52
	leal	(%rdi,%r11), %r14d
	movslq	%r14d, %r14
	mulss	%xmm0, %xmm1
	addss	(%rax,%r14,4), %xmm1
	movss	%xmm1, (%rax,%r14,4)
	addl	80(%rsp), %esi
	addl	64(%rsp), %r12d
	incl	%r11d
	cmpl	%ebx, %r11d
	je	.LBB102_43	# bb54
.LBB102_41:	# bb51.preheader
	testl	%r8d, %r8d
	jg	.LBB102_38	# bb51.preheader.bb50_crit_edge
.LBB102_42:	# bb51.preheader.bb52_crit_edge
	pxor	%xmm1, %xmm1
	jmp	.LBB102_40	# bb52
.LBB102_43:	# bb54
	addl	(%rsp), %edi
	addl	64(%rsp), %r15d
	addl	80(%rsp), %edx
	decl	%ebx
	movl	4(%rsp), %esi
	incl	%esi
	movl	%esi, 4(%rsp)
	cmpl	%ecx, %esi
	je	.LBB102_81	# return
.LBB102_44:	# bb53.preheader
	cmpl	%ecx, 4(%rsp)
	jge	.LBB102_43	# bb54
.LBB102_45:	# bb53.preheader.bb51.preheader_crit_edge
	xorl	%r11d, %r11d
	movl	%edx, %esi
	movl	%r15d, %r12d
	jmp	.LBB102_41	# bb51.preheader
.LBB102_46:	# bb56
	cmpl	$121, %esi
	jne	.LBB102_57	# bb69
.LBB102_47:	# bb56
	cmpl	$112, 4(%rsp)
	jne	.LBB102_57	# bb69
.LBB102_48:	# bb68.preheader
	testl	%r8d, %r8d
	jle	.LBB102_81	# return
.LBB102_49:	# bb68.preheader
	testl	%ecx, %ecx
	jle	.LBB102_81	# return
.LBB102_50:	# bb68.preheader.bb66.preheader_crit_edge
	xorl	%edx, %edx
	movl	%edx, %esi
	movl	%edx, 4(%rsp)
	jmp	.LBB102_56	# bb66.preheader
	.align	16
.LBB102_51:	# bb62
	leal	(%rsi,%r14), %r15d
	leal	(%rdx,%r14), %r12d
	cmpl	%ecx, %r14d
	movslq	%r12d, %r12
	movaps	%xmm0, %xmm1
	mulss	(%r10,%r12,4), %xmm1
	movslq	%r15d, %r15
	movaps	%xmm0, %xmm2
	mulss	(%r9,%r15,4), %xmm2
	jge	.LBB102_54	# bb65
.LBB102_52:	# bb.nph129
	leal	(%rsi,%r14), %r15d
	leal	(%rdx,%r14), %r12d
	xorl	%r13d, %r13d
	.align	16
.LBB102_53:	# bb63
	leal	(%r15,%r13), %ebp
	movslq	%ebp, %rbp
	movaps	%xmm1, %xmm3
	mulss	(%r9,%rbp,4), %xmm3
	leal	(%r12,%r13), %ebp
	movslq	%ebp, %rbp
	movaps	%xmm2, %xmm4
	mulss	(%r10,%rbp,4), %xmm4
	addss	%xmm3, %xmm4
	leal	(%r11,%r13), %ebp
	movslq	%ebp, %rbp
	addss	(%rax,%rbp,4), %xmm4
	movss	%xmm4, (%rax,%rbp,4)
	incl	%r13d
	cmpl	%ebx, %r13d
	jne	.LBB102_53	# bb63
.LBB102_54:	# bb65
	addl	%edi, %r11d
	decl	%ebx
	incl	%r14d
	cmpl	%ecx, %r14d
	jne	.LBB102_51	# bb62
.LBB102_55:	# bb67
	addl	80(%rsp), %edx
	addl	64(%rsp), %esi
	movl	4(%rsp), %edi
	incl	%edi
	movl	%edi, 4(%rsp)
	cmpl	%r8d, %edi
	je	.LBB102_81	# return
.LBB102_56:	# bb66.preheader
	movl	64(%rsp), %edi
	leal	1(%rdi), %edi
	xorl	%r11d, %r11d
	movl	%ecx, %ebx
	movl	%r11d, %r14d
	jmp	.LBB102_51	# bb62
.LBB102_57:	# bb69
	cmpl	$122, %esi
	jne	.LBB102_69	# bb82
.LBB102_58:	# bb69
	cmpl	$111, 4(%rsp)
	jne	.LBB102_69	# bb82
.LBB102_59:	# bb81.preheader
	testl	%ecx, %ecx
	jle	.LBB102_81	# return
.LBB102_60:	# bb.nph127
	xorl	%r15d, %r15d
	movl	%r15d, %edi
	movl	%r15d, %edx
	movl	%r15d, %esi
	jmp	.LBB102_67	# bb79.preheader
.LBB102_61:	# bb77.preheader.bb76_crit_edge
	pxor	%xmm1, %xmm1
	xorl	%r14d, %r14d
	.align	16
.LBB102_62:	# bb76
	leal	(%r12,%r14), %r13d
	movslq	%r13d, %r13
	leal	(%rdi,%r14), %ebp
	movslq	%ebp, %rbp
	movss	(%r10,%rbp,4), %xmm2
	mulss	(%r9,%r13,4), %xmm2
	leal	(%r11,%r14), %r13d
	movslq	%r13d, %r13
	leal	(%r15,%r14), %ebp
	movslq	%ebp, %rbp
	movss	(%r9,%rbp,4), %xmm3
	mulss	(%r10,%r13,4), %xmm3
	addss	%xmm2, %xmm3
	addss	%xmm3, %xmm1
	incl	%r14d
	cmpl	%r8d, %r14d
	jne	.LBB102_62	# bb76
.LBB102_63:	# bb78
	leal	(%rdx,%rbx), %r14d
	movslq	%r14d, %r14
	mulss	%xmm0, %xmm1
	addss	(%rax,%r14,4), %xmm1
	movss	%xmm1, (%rax,%r14,4)
	addl	80(%rsp), %r11d
	addl	64(%rsp), %r12d
	incl	%ebx
	cmpl	%esi, %ebx
	jg	.LBB102_66	# bb80
.LBB102_64:	# bb77.preheader
	testl	%r8d, %r8d
	jg	.LBB102_61	# bb77.preheader.bb76_crit_edge
.LBB102_65:	# bb77.preheader.bb78_crit_edge
	pxor	%xmm1, %xmm1
	jmp	.LBB102_63	# bb78
.LBB102_66:	# bb80
	addl	64(%rsp), %r15d
	addl	80(%rsp), %edi
	addl	96(%rsp), %edx
	incl	%esi
	cmpl	%ecx, %esi
	je	.LBB102_81	# return
.LBB102_67:	# bb79.preheader
	testl	%esi, %esi
	js	.LBB102_66	# bb80
.LBB102_68:	# bb79.preheader.bb77.preheader_crit_edge
	xorl	%r11d, %r11d
	movl	%r11d, %r12d
	movl	%r11d, %ebx
	jmp	.LBB102_64	# bb77.preheader
.LBB102_69:	# bb82
	cmpl	$122, %esi
	jne	.LBB102_80	# bb95
.LBB102_70:	# bb82
	cmpl	$112, 4(%rsp)
	jne	.LBB102_80	# bb95
.LBB102_71:	# bb94.preheader
	testl	%r8d, %r8d
	jle	.LBB102_81	# return
.LBB102_72:	# bb94.preheader
	testl	%ecx, %ecx
	jle	.LBB102_81	# return
.LBB102_73:	# bb94.preheader.bb92.preheader_crit_edge
	xorl	%edx, %edx
	movl	%edx, %esi
	movl	%edx, %edi
	jmp	.LBB102_79	# bb92.preheader
	.align	16
.LBB102_74:	# bb88
	leal	(%rsi,%r11), %r14d
	movslq	%r14d, %r14
	movaps	%xmm0, %xmm1
	mulss	(%r10,%r14,4), %xmm1
	leal	(%rdx,%r11), %r14d
	movslq	%r14d, %r14
	movaps	%xmm0, %xmm2
	mulss	(%r9,%r14,4), %xmm2
	testl	%r11d, %r11d
	js	.LBB102_77	# bb91
.LBB102_75:	# bb88.bb89_crit_edge
	xorl	%r14d, %r14d
	.align	16
.LBB102_76:	# bb89
	leal	(%rdx,%r14), %r15d
	movslq	%r15d, %r15
	movaps	%xmm1, %xmm3
	mulss	(%r9,%r15,4), %xmm3
	leal	(%rsi,%r14), %r15d
	movslq	%r15d, %r15
	movaps	%xmm2, %xmm4
	mulss	(%r10,%r15,4), %xmm4
	addss	%xmm3, %xmm4
	leal	(%rbx,%r14), %r15d
	movslq	%r15d, %r15
	addss	(%rax,%r15,4), %xmm4
	movss	%xmm4, (%rax,%r15,4)
	incl	%r14d
	cmpl	%r11d, %r14d
	jle	.LBB102_76	# bb89
.LBB102_77:	# bb91
	addl	64(%rsp), %ebx
	incl	%r11d
	cmpl	%ecx, %r11d
	jne	.LBB102_74	# bb88
.LBB102_78:	# bb93
	addl	64(%rsp), %edx
	addl	80(%rsp), %esi
	incl	%edi
	cmpl	%r8d, %edi
	je	.LBB102_81	# return
.LBB102_79:	# bb92.preheader
	xorl	%ebx, %ebx
	movl	%ebx, %r11d
	jmp	.LBB102_74	# bb88
.LBB102_80:	# bb95
	xorl	%edi, %edi
	leaq	.str122, %rsi
	leaq	.str1123, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB102_81:	# return
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB102_82:	# bb5
	cmpl	$113, %edx
	movl	$112, %edi
	cmovne	%edx, %edi
	movl	%edi, 4(%rsp)
	jmp	.LBB102_5	# bb15
.LBB102_83:	# bb9.bb15_crit_edge
	movl	$111, 4(%rsp)
	jmp	.LBB102_5	# bb15
	.size	cblas_ssyr2k, .-cblas_ssyr2k
.Leh_func_end68:


	.align	16
	.globl	cblas_ssyr
	.type	cblas_ssyr,@function
cblas_ssyr:
.Leh_func_begin69:
.Llabel69:
	pushq	%r15
	pushq	%r14
	pushq	%rbx
	pxor	%xmm1, %xmm1
	ucomiss	%xmm1, %xmm0
	setnp	%al
	sete	%r10b
	testb	%al, %r10b
	movl	32(%rsp), %eax
	jne	.LBB103_25	# return
.LBB103_1:	# entry
	testl	%edx, %edx
	je	.LBB103_25	# return
.LBB103_2:	# bb4
	cmpl	$121, %esi
	jne	.LBB103_4	# bb7
.LBB103_3:	# bb4
	cmpl	$101, %edi
	je	.LBB103_6	# bb11
.LBB103_4:	# bb7
	cmpl	$122, %esi
	jne	.LBB103_14	# bb20
.LBB103_5:	# bb7
	cmpl	$102, %edi
	jne	.LBB103_14	# bb20
.LBB103_6:	# bb11
	testl	%r8d, %r8d
	jg	.LBB103_26	# bb11.bb19.preheader_crit_edge
.LBB103_7:	# bb12
	movl	$1, %esi
	subl	%edx, %esi
	imull	%r8d, %esi
.LBB103_8:	# bb19.preheader
	testl	%edx, %edx
	jle	.LBB103_25	# return
.LBB103_9:	# bb15.preheader
	incl	%eax
	xorl	%edi, %edi
	movl	%edx, %r10d
	movl	%edi, %r11d
	.align	16
.LBB103_10:	# bb15
	movslq	%esi, %rbx
	movaps	%xmm0, %xmm1
	mulss	(%rcx,%rbx,4), %xmm1
	cmpl	%edx, %r11d
	jge	.LBB103_13	# bb18
.LBB103_11:	# bb15.bb16_crit_edge
	xorl	%ebx, %ebx
	movl	%esi, %r14d
	.align	16
.LBB103_12:	# bb16
	movslq	%r14d, %r15
	movaps	%xmm1, %xmm2
	mulss	(%rcx,%r15,4), %xmm2
	leal	(%rdi,%rbx), %r15d
	movslq	%r15d, %r15
	addss	(%r9,%r15,4), %xmm2
	movss	%xmm2, (%r9,%r15,4)
	addl	%r8d, %r14d
	incl	%ebx
	cmpl	%r10d, %ebx
	jne	.LBB103_12	# bb16
.LBB103_13:	# bb18
	addl	%eax, %edi
	addl	%r8d, %esi
	decl	%r10d
	incl	%r11d
	cmpl	%edx, %r11d
	jne	.LBB103_10	# bb15
	jmp	.LBB103_25	# return
.LBB103_14:	# bb20
	cmpl	$102, %edi
	sete	%r10b
	cmpl	$121, %esi
	sete	%r11b
	andb	%r10b, %r11b
	cmpl	$101, %edi
	sete	%dil
	cmpl	$122, %esi
	sete	%sil
	testb	%dil, %sil
	jne	.LBB103_16	# bb28
.LBB103_15:	# bb20
	notb	%r11b
	testb	$1, %r11b
	jne	.LBB103_24	# bb40
.LBB103_16:	# bb28
	testl	%r8d, %r8d
	jg	.LBB103_27	# bb28.bb39.preheader_crit_edge
.LBB103_17:	# bb29
	movl	$1, %esi
	subl	%edx, %esi
	imull	%r8d, %esi
.LBB103_18:	# bb39.preheader
	testl	%edx, %edx
	jle	.LBB103_25	# return
.LBB103_19:	# bb.nph47
	movl	$1, %edi
	subl	%edx, %edi
	imull	%r8d, %edi
	xorl	%r10d, %r10d
	movl	%r10d, %r11d
	.align	16
.LBB103_20:	# bb32
	testl	%r8d, %r8d
	movl	$0, %ebx
	cmovle	%edi, %ebx
	movslq	%esi, %r14
	movaps	%xmm0, %xmm1
	mulss	(%rcx,%r14,4), %xmm1
	testl	%r11d, %r11d
	js	.LBB103_23	# bb38
.LBB103_21:	# bb32.bb36_crit_edge
	xorl	%r14d, %r14d
	.align	16
.LBB103_22:	# bb36
	movslq	%ebx, %r15
	movaps	%xmm1, %xmm2
	mulss	(%rcx,%r15,4), %xmm2
	leal	(%r10,%r14), %r15d
	movslq	%r15d, %r15
	addss	(%r9,%r15,4), %xmm2
	movss	%xmm2, (%r9,%r15,4)
	addl	%r8d, %ebx
	incl	%r14d
	cmpl	%r11d, %r14d
	jle	.LBB103_22	# bb36
.LBB103_23:	# bb38
	addl	%r8d, %esi
	addl	%eax, %r10d
	incl	%r11d
	cmpl	%edx, %r11d
	jne	.LBB103_20	# bb32
	jmp	.LBB103_25	# return
.LBB103_24:	# bb40
	xorl	%edi, %edi
	leaq	.str124, %rsi
	leaq	.str1125, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB103_25:	# return
	popq	%rbx
	popq	%r14
	popq	%r15
	ret
.LBB103_26:	# bb11.bb19.preheader_crit_edge
	xorl	%esi, %esi
	jmp	.LBB103_8	# bb19.preheader
.LBB103_27:	# bb28.bb39.preheader_crit_edge
	xorl	%esi, %esi
	jmp	.LBB103_18	# bb39.preheader
	.size	cblas_ssyr, .-cblas_ssyr
.Leh_func_end69:


	.section	.rodata.cst4,"aM",@progbits,4
	.align	4
.LCPI104_0:					
	.long	1065353216	# float 1.000000e+00
	.text
	.align	16
	.globl	cblas_ssyrk
	.type	cblas_ssyrk,@function
cblas_ssyrk:
.Leh_func_begin70:
.Llabel70:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$24, %rsp
	ucomiss	.LCPI104_0(%rip), %xmm1
	movl	96(%rsp), %ebx
	movq	88(%rsp), %r14
	movl	80(%rsp), %r15d
	movss	%xmm1, 16(%rsp)
	movq	%r9, %r12
	movss	%xmm0, 20(%rsp)
	movl	%r8d, %r13d
	movl	%ecx, %ebp
	jne	.LBB104_2	# bb4
	jp	.LBB104_2	# bb4
.LBB104_1:	# entry
	pxor	%xmm0, %xmm0
	movss	20(%rsp), %xmm1
	ucomiss	%xmm0, %xmm1
	setnp	%al
	sete	%cl
	testb	%al, %cl
	jne	.LBB104_108	# return
.LBB104_2:	# bb4
	cmpl	$101, %edi
	je	.LBB104_109	# bb5
.LBB104_3:	# bb9
	cmpl	$121, %esi
	movl	$122, %eax
	movl	$121, %esi
	cmove	%eax, %esi
	addl	$4294967184, %edx
	cmpl	$1, %edx
	jbe	.LBB104_110	# bb9.bb15_crit_edge
.LBB104_4:	# bb14
	movl	$112, 8(%rsp)
.LBB104_5:	# bb15
	movl	%esi, 12(%rsp)
	pxor	%xmm0, %xmm0
	movss	16(%rsp), %xmm1
	ucomiss	%xmm0, %xmm1
	jne	.LBB104_23	# bb29
	jp	.LBB104_23	# bb29
.LBB104_6:	# bb16
	xorl	%edi, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB104_21	# real_catch0
.LBB104_7:	# jump0
	xorl	%edi, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB104_22	# real_end0
.LBB104_8:	# real_try0
	xorl	%edi, %edi
	call	llvm_real_try
	cmpl	$121, 12(%rsp)
	je	.LBB104_11	# bb22.preheader
.LBB104_9:	# bb28.preheader
	testl	%ebp, %ebp
	jle	.LBB104_22	# real_end0
.LBB104_10:	# bb28.preheader.bb26.preheader_crit_edge
	xorl	%eax, %eax
	movl	%eax, %ecx
	jmp	.LBB104_19	# bb26.preheader
.LBB104_11:	# bb22.preheader
	testl	%ebp, %ebp
	jle	.LBB104_22	# real_end0
.LBB104_12:	# bb20.preheader.preheader
	leal	1(%rbx), %eax
	xorl	%ecx, %ecx
	movl	%ebp, %edx
	movl	%ecx, %esi
	jmp	.LBB104_15	# bb20.preheader
	.align	16
.LBB104_13:	# bb19
	leal	(%rcx,%rdi), %r8d
	movslq	%r8d, %r8
	movl	$0, (%r14,%r8,4)
	incl	%edi
	cmpl	%edx, %edi
	jne	.LBB104_13	# bb19
.LBB104_14:	# bb21
	addl	%eax, %ecx
	decl	%edx
	incl	%esi
	cmpl	%ebp, %esi
	je	.LBB104_22	# real_end0
.LBB104_15:	# bb20.preheader
	cmpl	%ebp, %esi
	jge	.LBB104_14	# bb21
.LBB104_16:	# bb20.preheader.bb19_crit_edge
	xorl	%edi, %edi
	jmp	.LBB104_13	# bb19
	.align	16
.LBB104_17:	# bb25
	leal	(%rax,%rdx), %esi
	movslq	%esi, %rsi
	movl	$0, (%r14,%rsi,4)
	incl	%edx
	cmpl	%ecx, %edx
	jle	.LBB104_17	# bb25
.LBB104_18:	# bb27
	addl	%ebx, %eax
	incl	%ecx
	cmpl	%ebp, %ecx
	je	.LBB104_22	# real_end0
.LBB104_19:	# bb26.preheader
	testl	%ecx, %ecx
	js	.LBB104_18	# bb27
.LBB104_20:	# bb26.preheader.bb25_crit_edge
	xorl	%edx, %edx
	jmp	.LBB104_17	# bb25
.LBB104_21:	# real_catch0
	xorl	%edi, %edi
	call	llvm_real_catch
.LBB104_22:	# real_end0
	xorl	%edi, %edi
	call	llvm_real_end
	jmp	.LBB104_41	# bb43
.LBB104_23:	# bb29
	movss	16(%rsp), %xmm0
	ucomiss	.LCPI104_0(%rip), %xmm0
	setnp	%al
	sete	%cl
	testb	%al, %cl
	jne	.LBB104_41	# bb43
.LBB104_24:	# bb30
	movl	$1, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB104_39	# real_catch1
.LBB104_25:	# jump1
	movl	$1, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB104_40	# real_end1
.LBB104_26:	# real_try1
	movl	$1, %edi
	call	llvm_real_try
	cmpl	$121, 12(%rsp)
	je	.LBB104_29	# bb36.preheader
.LBB104_27:	# bb42.preheader
	testl	%ebp, %ebp
	jle	.LBB104_40	# real_end1
.LBB104_28:	# bb42.preheader.bb40.preheader_crit_edge
	xorl	%eax, %eax
	movl	%eax, %ecx
	jmp	.LBB104_37	# bb40.preheader
.LBB104_29:	# bb36.preheader
	testl	%ebp, %ebp
	jle	.LBB104_40	# real_end1
.LBB104_30:	# bb34.preheader.preheader
	leal	1(%rbx), %eax
	xorl	%ecx, %ecx
	movl	%ebp, %edx
	movl	%ecx, %esi
	jmp	.LBB104_33	# bb34.preheader
	.align	16
.LBB104_31:	# bb33
	leal	(%rcx,%rdi), %r8d
	movslq	%r8d, %r8
	movss	16(%rsp), %xmm0
	mulss	(%r14,%r8,4), %xmm0
	movss	%xmm0, (%r14,%r8,4)
	incl	%edi
	cmpl	%edx, %edi
	jne	.LBB104_31	# bb33
.LBB104_32:	# bb35
	addl	%eax, %ecx
	decl	%edx
	incl	%esi
	cmpl	%ebp, %esi
	je	.LBB104_40	# real_end1
.LBB104_33:	# bb34.preheader
	cmpl	%ebp, %esi
	jge	.LBB104_32	# bb35
.LBB104_34:	# bb34.preheader.bb33_crit_edge
	xorl	%edi, %edi
	jmp	.LBB104_31	# bb33
	.align	16
.LBB104_35:	# bb39
	leal	(%rax,%rdx), %esi
	movslq	%esi, %rsi
	movss	16(%rsp), %xmm0
	mulss	(%r14,%rsi,4), %xmm0
	movss	%xmm0, (%r14,%rsi,4)
	incl	%edx
	cmpl	%ecx, %edx
	jle	.LBB104_35	# bb39
.LBB104_36:	# bb41
	addl	%ebx, %eax
	incl	%ecx
	cmpl	%ebp, %ecx
	je	.LBB104_40	# real_end1
.LBB104_37:	# bb40.preheader
	testl	%ecx, %ecx
	js	.LBB104_36	# bb41
.LBB104_38:	# bb40.preheader.bb39_crit_edge
	xorl	%edx, %edx
	jmp	.LBB104_35	# bb39
.LBB104_39:	# real_catch1
	movl	$1, %edi
	call	llvm_real_catch
.LBB104_40:	# real_end1
	movl	$1, %edi
	call	llvm_real_end
.LBB104_41:	# bb43
	pxor	%xmm0, %xmm0
	movss	20(%rsp), %xmm1
	ucomiss	%xmm0, %xmm1
	setnp	%al
	sete	%cl
	testb	%al, %cl
	jne	.LBB104_108	# return
.LBB104_42:	# bb44
	cmpl	$121, 12(%rsp)
	jne	.LBB104_59	# bb56
.LBB104_43:	# bb44
	cmpl	$111, 8(%rsp)
	jne	.LBB104_59	# bb56
.LBB104_44:	# bb47
	movl	$2, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB104_56	# real_catch2
.LBB104_45:	# jump2
	movl	$2, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB104_57	# real_end2
.LBB104_46:	# real_try2
	movl	$2, %edi
	call	llvm_real_try
	testl	%ebp, %ebp
	jle	.LBB104_57	# real_end2
.LBB104_47:	# bb.nph145
	incl	%ebx
	xorl	%edx, %edx
	movl	%edx, %ecx
	movl	%ebp, %eax
	movl	%edx, %r9d
	jmp	.LBB104_54	# bb53.preheader
.LBB104_48:	# bb51.preheader.bb50_crit_edge
	pxor	%xmm0, %xmm0
	xorl	%r8d, %r8d
	.align	16
.LBB104_49:	# bb50
	leal	(%rdi,%r8), %r10d
	movslq	%r10d, %r10
	leal	(%rcx,%r8), %r11d
	movslq	%r11d, %r11
	movss	(%r12,%r11,4), %xmm1
	mulss	(%r12,%r10,4), %xmm1
	addss	%xmm1, %xmm0
	incl	%r8d
	cmpl	%r13d, %r8d
	jne	.LBB104_49	# bb50
.LBB104_50:	# bb52
	leal	(%rdx,%rsi), %r8d
	movslq	%r8d, %r8
	mulss	20(%rsp), %xmm0
	addss	(%r14,%r8,4), %xmm0
	movss	%xmm0, (%r14,%r8,4)
	addl	%r15d, %edi
	incl	%esi
	cmpl	%eax, %esi
	je	.LBB104_53	# bb54
.LBB104_51:	# bb51.preheader
	testl	%r13d, %r13d
	jg	.LBB104_48	# bb51.preheader.bb50_crit_edge
.LBB104_52:	# bb51.preheader.bb52_crit_edge
	pxor	%xmm0, %xmm0
	jmp	.LBB104_50	# bb52
.LBB104_53:	# bb54
	addl	%ebx, %edx
	addl	%r15d, %ecx
	decl	%eax
	incl	%r9d
	cmpl	%ebp, %r9d
	je	.LBB104_57	# real_end2
.LBB104_54:	# bb53.preheader
	cmpl	%ebp, %r9d
	jge	.LBB104_53	# bb54
.LBB104_55:	# bb53.preheader.bb51.preheader_crit_edge
	xorl	%esi, %esi
	movl	%ecx, %edi
	jmp	.LBB104_51	# bb51.preheader
.LBB104_56:	# real_catch2
	movl	$2, %edi
	call	llvm_real_catch
.LBB104_57:	# real_end2
	movl	$2, %edi
.LBB104_58:	# real_end2
	call	llvm_real_end
	jmp	.LBB104_108	# return
.LBB104_59:	# bb56
	cmpl	$121, 12(%rsp)
	jne	.LBB104_75	# bb69
.LBB104_60:	# bb56
	cmpl	$112, 8(%rsp)
	jne	.LBB104_75	# bb69
.LBB104_61:	# bb60
	movl	$3, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB104_73	# real_catch3
.LBB104_62:	# jump3
	movl	$3, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB104_74	# real_end3
.LBB104_63:	# real_try3
	movl	$3, %edi
	call	llvm_real_try
	testl	%ebp, %ebp
	jle	.LBB104_74	# real_end3
.LBB104_64:	# bb.nph137
	incl	%ebx
	xorl	%edx, %edx
	movl	%ebp, %eax
	movl	%edx, %ecx
	jmp	.LBB104_71	# bb66.preheader
.LBB104_65:	# bb64.preheader.bb63_crit_edge
	pxor	%xmm0, %xmm0
	xorl	%edi, %edi
	movl	%ecx, %r8d
	.align	16
.LBB104_66:	# bb63
	leal	(%rsi,%r8), %r9d
	movslq	%r9d, %r9
	movslq	%r8d, %r10
	movss	(%r12,%r10,4), %xmm1
	mulss	(%r12,%r9,4), %xmm1
	addss	%xmm1, %xmm0
	addl	%r15d, %r8d
	incl	%edi
	cmpl	%r13d, %edi
	jne	.LBB104_66	# bb63
.LBB104_67:	# bb65
	leal	(%rdx,%rsi), %edi
	movslq	%edi, %rdi
	mulss	20(%rsp), %xmm0
	addss	(%r14,%rdi,4), %xmm0
	movss	%xmm0, (%r14,%rdi,4)
	incl	%esi
	cmpl	%eax, %esi
	je	.LBB104_70	# bb67
.LBB104_68:	# bb64.preheader
	testl	%r13d, %r13d
	jg	.LBB104_65	# bb64.preheader.bb63_crit_edge
.LBB104_69:	# bb64.preheader.bb65_crit_edge
	pxor	%xmm0, %xmm0
	jmp	.LBB104_67	# bb65
.LBB104_70:	# bb67
	addl	%ebx, %edx
	decl	%eax
	incl	%ecx
	cmpl	%ebp, %ecx
	je	.LBB104_74	# real_end3
.LBB104_71:	# bb66.preheader
	cmpl	%ebp, %ecx
	jge	.LBB104_70	# bb67
.LBB104_72:	# bb66.preheader.bb64.preheader_crit_edge
	xorl	%esi, %esi
	jmp	.LBB104_68	# bb64.preheader
.LBB104_73:	# real_catch3
	movl	$3, %edi
	call	llvm_real_catch
.LBB104_74:	# real_end3
	movl	$3, %edi
	jmp	.LBB104_58	# real_end2
.LBB104_75:	# bb69
	cmpl	$122, 12(%rsp)
	jne	.LBB104_91	# bb82
.LBB104_76:	# bb69
	cmpl	$111, 8(%rsp)
	jne	.LBB104_91	# bb82
.LBB104_77:	# bb73
	movl	$4, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB104_89	# real_catch4
.LBB104_78:	# jump4
	movl	$4, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB104_90	# real_end4
.LBB104_79:	# real_try4
	movl	$4, %edi
	call	llvm_real_try
	testl	%ebp, %ebp
	jle	.LBB104_90	# real_end4
.LBB104_80:	# bb.nph129
	xorl	%edi, %edi
	movl	%edi, %eax
	movl	%edi, %r8d
	jmp	.LBB104_87	# bb79.preheader
.LBB104_81:	# bb77.preheader.bb76_crit_edge
	pxor	%xmm0, %xmm0
	xorl	%edx, %edx
	.align	16
.LBB104_82:	# bb76
	leal	(%rcx,%rdx), %r9d
	movslq	%r9d, %r9
	leal	(%rdi,%rdx), %r10d
	movslq	%r10d, %r10
	movss	(%r12,%r10,4), %xmm1
	mulss	(%r12,%r9,4), %xmm1
	addss	%xmm1, %xmm0
	incl	%edx
	cmpl	%r13d, %edx
	jne	.LBB104_82	# bb76
.LBB104_83:	# bb78
	leal	(%rax,%rsi), %edx
	movslq	%edx, %rdx
	mulss	20(%rsp), %xmm0
	addss	(%r14,%rdx,4), %xmm0
	movss	%xmm0, (%r14,%rdx,4)
	addl	%r15d, %ecx
	incl	%esi
	cmpl	%r8d, %esi
	jg	.LBB104_86	# bb80
.LBB104_84:	# bb77.preheader
	testl	%r13d, %r13d
	jg	.LBB104_81	# bb77.preheader.bb76_crit_edge
.LBB104_85:	# bb77.preheader.bb78_crit_edge
	pxor	%xmm0, %xmm0
	jmp	.LBB104_83	# bb78
.LBB104_86:	# bb80
	addl	%r15d, %edi
	addl	%ebx, %eax
	incl	%r8d
	cmpl	%ebp, %r8d
	je	.LBB104_90	# real_end4
.LBB104_87:	# bb79.preheader
	testl	%r8d, %r8d
	js	.LBB104_86	# bb80
.LBB104_88:	# bb79.preheader.bb77.preheader_crit_edge
	xorl	%ecx, %ecx
	movl	%ecx, %esi
	jmp	.LBB104_84	# bb77.preheader
.LBB104_89:	# real_catch4
	movl	$4, %edi
	call	llvm_real_catch
.LBB104_90:	# real_end4
	movl	$4, %edi
	jmp	.LBB104_58	# real_end2
.LBB104_91:	# bb82
	cmpl	$122, 12(%rsp)
	jne	.LBB104_107	# bb95
.LBB104_92:	# bb82
	cmpl	$112, 8(%rsp)
	jne	.LBB104_107	# bb95
.LBB104_93:	# bb86
	movl	$5, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB104_105	# real_catch5
.LBB104_94:	# jump5
	movl	$5, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB104_106	# real_end5
.LBB104_95:	# real_try5
	movl	$5, %edi
	call	llvm_real_try
	testl	%ebp, %ebp
	jle	.LBB104_106	# real_end5
.LBB104_96:	# bb.nph121
	xorl	%edx, %edx
	movl	%edx, %edi
	jmp	.LBB104_103	# bb92.preheader
.LBB104_97:	# bb90.preheader.bb89_crit_edge
	pxor	%xmm0, %xmm0
	xorl	%ecx, %ecx
	movl	%ecx, %esi
	.align	16
.LBB104_98:	# bb89
	leal	(%rax,%rcx), %r8d
	movslq	%r8d, %r8
	leal	(%rdi,%rcx), %r9d
	movslq	%r9d, %r9
	movss	(%r12,%r9,4), %xmm1
	mulss	(%r12,%r8,4), %xmm1
	addss	%xmm1, %xmm0
	addl	%r15d, %ecx
	incl	%esi
	cmpl	%r13d, %esi
	jne	.LBB104_98	# bb89
.LBB104_99:	# bb91
	leal	(%rdx,%rax), %ecx
	movslq	%ecx, %rcx
	mulss	20(%rsp), %xmm0
	addss	(%r14,%rcx,4), %xmm0
	movss	%xmm0, (%r14,%rcx,4)
	incl	%eax
	cmpl	%edi, %eax
	jg	.LBB104_102	# bb93
.LBB104_100:	# bb90.preheader
	testl	%r13d, %r13d
	jg	.LBB104_97	# bb90.preheader.bb89_crit_edge
.LBB104_101:	# bb90.preheader.bb91_crit_edge
	pxor	%xmm0, %xmm0
	jmp	.LBB104_99	# bb91
.LBB104_102:	# bb93
	addl	%ebx, %edx
	incl	%edi
	cmpl	%ebp, %edi
	je	.LBB104_106	# real_end5
.LBB104_103:	# bb92.preheader
	testl	%edi, %edi
	js	.LBB104_102	# bb93
.LBB104_104:	# bb92.preheader.bb90.preheader_crit_edge
	xorl	%eax, %eax
	jmp	.LBB104_100	# bb90.preheader
.LBB104_105:	# real_catch5
	movl	$5, %edi
	call	llvm_real_catch
.LBB104_106:	# real_end5
	movl	$5, %edi
	jmp	.LBB104_58	# real_end2
.LBB104_107:	# bb95
	xorl	%edi, %edi
	leaq	.str126, %rsi
	leaq	.str1127, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB104_108:	# return
	addq	$24, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB104_109:	# bb5
	cmpl	$113, %edx
	movl	$112, %eax
	cmovne	%edx, %eax
	movl	%eax, 8(%rsp)
	jmp	.LBB104_5	# bb15
.LBB104_110:	# bb9.bb15_crit_edge
	movl	$111, 8(%rsp)
	jmp	.LBB104_5	# bb15
	.size	cblas_ssyrk, .-cblas_ssyrk
.Leh_func_end70:


	.section	.rodata.cst8,"aM",@progbits,8
	.align	8
.LCPI105_0:					
	.quad	4607182418800017408	# double value: 1.000000e+00
	.text
	.align	16
	.globl	cblas_stbmv
	.type	cblas_stbmv,@function
cblas_stbmv:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	cmpl	$113, %edx
	movl	$112, %eax
	cmovne	%edx, %eax
	testl	%r8d, %r8d
	movl	80(%rsp), %edx
	movq	72(%rsp), %r10
	movq	56(%rsp), %r11
	movl	%r9d, -4(%rsp)
	movl	%ecx, -8(%rsp)
	je	.LBB105_30	# bb85.thread
.LBB105_1:	# bb18
	cmpl	$121, %esi
	sete	%cl
	setne	%r9b
	cmpl	$101, %edi
	sete	%bl
	setne	%r14b
	andb	%cl, %bl
	orb	%r9b, %r14b
	testb	%r14b, %r14b
	jne	.LBB105_3	# bb25
.LBB105_2:	# bb18
	cmpl	$111, %eax
	je	.LBB105_5	# bb33
.LBB105_3:	# bb25
	cmpl	$122, %esi
	sete	%cl
	setne	%r9b
	cmpl	$102, %edi
	sete	%r14b
	setne	%r15b
	andb	%cl, %r14b
	orb	%r9b, %r15b
	testb	%r15b, %r15b
	jne	.LBB105_15	# bb48
.LBB105_4:	# bb25
	cmpl	$112, %eax
	jne	.LBB105_15	# bb48
.LBB105_5:	# bb33
	testl	%edx, %edx
	jg	.LBB105_56	# bb33.bb47.preheader_crit_edge
.LBB105_6:	# bb34
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
	movl	%eax, -20(%rsp)
.LBB105_7:	# bb47.preheader
	testl	%r8d, %r8d
	jle	.LBB105_30	# bb85.thread
.LBB105_8:	# bb.nph217
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
	movl	%eax, -24(%rsp)
	movl	-4(%rsp), %eax
	leal	1(%rax), %eax
	movl	%eax, -16(%rsp)
	movl	$4294967294, -12(%rsp)
	xorl	%eax, %eax
	movl	%eax, %ecx
	movl	%eax, %esi
	movl	%eax, %edi
	.align	16
.LBB105_9:	# bb37
	movl	-20(%rsp), %r9d
	leal	(%r9,%rax), %r9d
	cmpl	$131, -8(%rsp)
	jne	.LBB105_57	# bb37.bb40_crit_edge
.LBB105_10:	# bb38
	movslq	%ecx, %rbx
	cvtss2sd	(%r11,%rbx,4), %xmm0
.LBB105_11:	# bb40
	testl	%edx, %edx
	movl	$0, %ebx
	cmovle	-24(%rsp), %ebx
	movl	-16(%rsp), %r14d
	leal	(%r14,%rdi), %r14d
	cmpl	%r8d, %r14d
	cmovg	%r8d, %r14d
	leal	1(%rdi), %r15d
	cmpl	%r14d, %r15d
	movslq	%r9d, %r9
	cvtss2sd	(%r10,%r9,4), %xmm1
	mulsd	%xmm0, %xmm1
	cvtsd2ss	%xmm1, %xmm0
	jge	.LBB105_14	# bb46
.LBB105_12:	# bb.nph213
	movl	$4294967295, %r14d
	subl	%r8d, %r14d
	movl	%esi, %r15d
	negl	%r15d
	subl	-4(%rsp), %r15d
	addl	$4294967294, %r15d
	cmpl	%r15d, %r14d
	cmovg	%r14d, %r15d
	movl	-12(%rsp), %r14d
	subl	%r15d, %r14d
	leal	(%rdx,%rax), %r15d
	leal	1(%rcx), %r12d
	xorl	%r13d, %r13d
	.align	16
.LBB105_13:	# bb44
	leal	(%r12,%r13), %ebp
	movslq	%ebp, %rbp
	addl	%ebx, %r15d
	movslq	%r15d, %rbx
	movss	(%r10,%rbx,4), %xmm1
	mulss	(%r11,%rbp,4), %xmm1
	addss	%xmm1, %xmm0
	incl	%r13d
	cmpl	%r14d, %r13d
	movl	%edx, %ebx
	jne	.LBB105_13	# bb44
.LBB105_14:	# bb46
	movss	%xmm0, (%r10,%r9,4)
	addl	%edx, %eax
	addl	64(%rsp), %ecx
	incl	%esi
	decl	-12(%rsp)
	incl	%edi
	cmpl	%r8d, %edi
	jne	.LBB105_9	# bb37
	jmp	.LBB105_30	# bb85.thread
.LBB105_15:	# bb48
	cmpl	$122, %esi
	sete	%cl
	setne	%r9b
	cmpl	$101, %edi
	sete	%r15b
	setne	%r12b
	andb	%cl, %r15b
	orb	%r9b, %r12b
	testb	%r12b, %r12b
	jne	.LBB105_17	# bb56
.LBB105_16:	# bb48
	cmpl	$111, %eax
	je	.LBB105_19	# bb64
.LBB105_17:	# bb56
	cmpl	$121, %esi
	sete	%cl
	setne	%sil
	cmpl	$102, %edi
	sete	%dil
	setne	%r9b
	andb	%cl, %dil
	orb	%sil, %r9b
	testb	%r9b, %r9b
	jne	.LBB105_31	# bb87
.LBB105_18:	# bb56
	cmpl	$112, %eax
	jne	.LBB105_31	# bb87
.LBB105_19:	# bb64
	testl	%edx, %edx
	jg	.LBB105_58	# bb64.bb67_crit_edge
.LBB105_20:	# bb65
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
.LBB105_21:	# bb67
	movl	$1, %ecx
	subl	%r8d, %ecx
	movl	$1, %edi
	movl	64(%rsp), %esi
	subl	%esi, %edi
	movl	%edi, -12(%rsp)
	movl	$4294967295, %r9d
	movl	-4(%rsp), %edi
	subl	%edi, %r9d
	movl	%r9d, -16(%rsp)
	movl	%edi, %r9d
	subl	%r8d, %r9d
	leal	-1(%r8), %ebx
	imull	%ebx, %esi
	leal	1(%r9,%rsi), %r9d
	addl	%edi, %esi
	imull	%edx, %ecx
	imull	%edx, %ebx
	addl	%eax, %ebx
	jmp	.LBB105_28	# bb81
.LBB105_22:	# bb68
	cmpl	$131, -8(%rsp)
	jne	.LBB105_59	# bb68.bb71_crit_edge
.LBB105_23:	# bb69
	movslq	%esi, %rdi
	cvtss2sd	(%r11,%rdi,4), %xmm0
.LBB105_24:	# bb71
	xorl	%edi, %edi
	testl	%edx, %edx
	movl	%ecx, %r14d
	cmovg	%edi, %r14d
	movl	-16(%rsp), %r15d
	leal	(%r15,%r8), %r15d
	cmpl	-4(%rsp), %eax
	cmovle	%edi, %r15d
	movl	%r15d, %edi
	imull	%edx, %edi
	cmpl	%eax, %r15d
	movslq	%ebx, %rax
	cvtss2sd	(%r10,%rax,4), %xmm1
	mulsd	%xmm0, %xmm1
	cvtsd2ss	%xmm1, %xmm0
	jge	.LBB105_27	# bb80
.LBB105_25:	# bb.nph205
	movl	%r8d, %r12d
	subl	%r15d, %r12d
	decl	%r12d
	addl	%r9d, %r15d
	xorl	%r13d, %r13d
	.align	16
.LBB105_26:	# bb78
	leal	(%r15,%r13), %ebp
	movslq	%ebp, %rbp
	addl	%r14d, %edi
	movslq	%edi, %r14
	movss	(%r10,%r14,4), %xmm1
	mulss	(%r11,%rbp,4), %xmm1
	addss	%xmm1, %xmm0
	incl	%r13d
	cmpl	%r12d, %r13d
	movl	%edx, %r14d
	jne	.LBB105_26	# bb78
.LBB105_27:	# bb80
	movss	%xmm0, (%r10,%rax,4)
	subl	64(%rsp), %esi
	subl	%edx, %ebx
	addl	-12(%rsp), %r9d
	decl	%r8d
.LBB105_28:	# bb81
	testl	%r8d, %r8d
	jle	.LBB105_30	# bb85.thread
.LBB105_29:	# bb82
	leal	-1(%r8), %eax
	testl	%r8d, %r8d
	jne	.LBB105_22	# bb68
.LBB105_30:	# bb85.thread
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB105_31:	# bb87
	cmpl	$111, %eax
	sete	%cl
	andb	%cl, %r14b
	cmpl	$112, %eax
	sete	%cl
	testb	%cl, %bl
	jne	.LBB105_33	# bb103
.LBB105_32:	# bb87
	notb	%r14b
	testb	$1, %r14b
	jne	.LBB105_44	# bb126
.LBB105_33:	# bb103
	testl	%edx, %edx
	jg	.LBB105_60	# bb103.bb106_crit_edge
.LBB105_34:	# bb104
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
.LBB105_35:	# bb106
	movl	$4294967295, %ecx
	subl	-4(%rsp), %ecx
	movl	$1, %esi
	subl	%r8d, %esi
	leal	-1(%r8), %edi
	movl	64(%rsp), %r9d
	imull	%edi, %r9d
	imull	%edx, %esi
	imull	%edx, %edi
	addl	%eax, %edi
	jmp	.LBB105_42	# bb120
.LBB105_36:	# bb107
	xorl	%ebx, %ebx
	testl	%edx, %edx
	movl	%esi, %r14d
	cmovg	%ebx, %r14d
	leal	(%rcx,%r8), %r15d
	cmpl	-4(%rsp), %eax
	cmovl	%ebx, %r15d
	movl	%r15d, %ebx
	imull	%edx, %ebx
	cmpl	%eax, %r15d
	jge	.LBB105_61	# bb107.bb116_crit_edge
.LBB105_37:	# bb.nph197
	movl	64(%rsp), %eax
	leal	-1(%rax), %eax
	movl	%r15d, %r12d
	imull	%eax, %r12d
	leal	-1(%r8,%r12), %r12d
	leal	-1(%r8), %r13d
	subl	%r15d, %r13d
	pxor	%xmm0, %xmm0
	xorl	%r15d, %r15d
	.align	16
.LBB105_38:	# bb114
	leal	(%rax,%r12), %ebp
	addl	%r14d, %ebx
	incl	%r15d
	cmpl	%r13d, %r15d
	movslq	%ebx, %r14
	movss	(%r10,%r14,4), %xmm1
	movslq	%r12d, %r14
	mulss	(%r11,%r14,4), %xmm1
	addss	%xmm1, %xmm0
	movl	%ebp, %r12d
	movl	%edx, %r14d
	jne	.LBB105_38	# bb114
.LBB105_39:	# bb116
	movslq	%edi, %rax
	movss	(%r10,%rax,4), %xmm1
	cmpl	$131, -8(%rsp)
	jne	.LBB105_41	# bb119
.LBB105_40:	# bb117
	movslq	%r9d, %rbx
	mulss	(%r11,%rbx,4), %xmm1
.LBB105_41:	# bb119
	addss	%xmm0, %xmm1
	movss	%xmm1, (%r10,%rax,4)
	subl	%edx, %edi
	subl	64(%rsp), %r9d
	decl	%r8d
.LBB105_42:	# bb120
	testl	%r8d, %r8d
	jle	.LBB105_30	# bb85.thread
.LBB105_43:	# bb121
	leal	-1(%r8), %eax
	testl	%r8d, %r8d
	jne	.LBB105_36	# bb107
	jmp	.LBB105_30	# bb85.thread
.LBB105_44:	# bb126
	cmpl	$111, %eax
	sete	%cl
	andb	%cl, %dil
	cmpl	$112, %eax
	sete	%al
	testb	%al, %r15b
	jne	.LBB105_46	# bb142
.LBB105_45:	# bb126
	notb	%dil
	testb	$1, %dil
	jne	.LBB105_30	# bb85.thread
.LBB105_46:	# bb142
	testl	%edx, %edx
	jg	.LBB105_62	# bb142.bb157.preheader_crit_edge
.LBB105_47:	# bb143
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
	movl	%eax, -28(%rsp)
.LBB105_48:	# bb157.preheader
	testl	%r8d, %r8d
	jle	.LBB105_30	# bb85.thread
.LBB105_49:	# bb.nph190
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
	movl	%eax, -32(%rsp)
	movl	64(%rsp), %eax
	leal	-1(%rax), %eax
	movl	%eax, -20(%rsp)
	movl	-4(%rsp), %eax
	leal	1(%rax), %ecx
	movl	%ecx, -24(%rsp)
	movl	$4294967294, -16(%rsp)
	xorl	%ecx, %ecx
	movl	%ecx, -12(%rsp)
	movl	%ecx, %esi
	.align	16
.LBB105_50:	# bb146
	testl	%edx, %edx
	movl	$0, %edi
	cmovle	-32(%rsp), %edi
	movl	-24(%rsp), %r9d
	leal	(%r9,%rsi), %r9d
	cmpl	%r8d, %r9d
	cmovg	%r8d, %r9d
	movl	-28(%rsp), %ebx
	leal	(%rbx,%rcx), %ebx
	leal	1(%rsi), %r14d
	cmpl	%r9d, %r14d
	jge	.LBB105_63	# bb146.bb153_crit_edge
.LBB105_51:	# bb.nph
	movl	$4294967295, %r9d
	subl	%r8d, %r9d
	movl	-12(%rsp), %r14d
	negl	%r14d
	subl	-4(%rsp), %r14d
	addl	$4294967294, %r14d
	cmpl	%r14d, %r9d
	cmovg	%r9d, %r14d
	movl	-16(%rsp), %r9d
	subl	%r14d, %r9d
	leal	(%rdx,%rcx), %r14d
	movl	-20(%rsp), %r15d
	leal	(%r15,%rax), %r15d
	movl	64(%rsp), %r12d
	leal	-1(%r12), %r12d
	pxor	%xmm0, %xmm0
	xorl	%r13d, %r13d
	.align	16
.LBB105_52:	# bb151
	leal	(%r12,%r15), %ebp
	addl	%edi, %r14d
	incl	%r13d
	cmpl	%r9d, %r13d
	movslq	%r14d, %rdi
	movss	(%r10,%rdi,4), %xmm1
	movslq	%r15d, %rdi
	mulss	(%r11,%rdi,4), %xmm1
	addss	%xmm1, %xmm0
	movl	%ebp, %r15d
	movl	%edx, %edi
	jne	.LBB105_52	# bb151
.LBB105_53:	# bb153
	movslq	%ebx, %rdi
	movss	(%r10,%rdi,4), %xmm1
	cmpl	$131, -8(%rsp)
	jne	.LBB105_55	# bb156
.LBB105_54:	# bb154
	movslq	%eax, %r9
	mulss	(%r11,%r9,4), %xmm1
.LBB105_55:	# bb156
	addss	%xmm0, %xmm1
	movss	%xmm1, (%r10,%rdi,4)
	addl	%edx, %ecx
	addl	64(%rsp), %eax
	incl	-12(%rsp)
	decl	-16(%rsp)
	incl	%esi
	cmpl	%r8d, %esi
	je	.LBB105_30	# bb85.thread
	jmp	.LBB105_50	# bb146
.LBB105_56:	# bb33.bb47.preheader_crit_edge
	movl	$0, -20(%rsp)
	jmp	.LBB105_7	# bb47.preheader
.LBB105_57:	# bb37.bb40_crit_edge
	movsd	.LCPI105_0(%rip), %xmm0
	jmp	.LBB105_11	# bb40
.LBB105_58:	# bb64.bb67_crit_edge
	xorl	%eax, %eax
	jmp	.LBB105_21	# bb67
.LBB105_59:	# bb68.bb71_crit_edge
	movsd	.LCPI105_0(%rip), %xmm0
	jmp	.LBB105_24	# bb71
.LBB105_60:	# bb103.bb106_crit_edge
	xorl	%eax, %eax
	jmp	.LBB105_35	# bb106
.LBB105_61:	# bb107.bb116_crit_edge
	pxor	%xmm0, %xmm0
	jmp	.LBB105_39	# bb116
.LBB105_62:	# bb142.bb157.preheader_crit_edge
	movl	$0, -28(%rsp)
	jmp	.LBB105_48	# bb157.preheader
.LBB105_63:	# bb146.bb153_crit_edge
	pxor	%xmm0, %xmm0
	jmp	.LBB105_53	# bb153
	.size	cblas_stbmv, .-cblas_stbmv


	.align	16
	.globl	cblas_stbsv
	.type	cblas_stbsv,@function
cblas_stbsv:
.Leh_func_begin71:
.Llabel71:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	cmpl	$113, %edx
	movl	$112, %eax
	cmovne	%edx, %eax
	testl	%r8d, %r8d
	movl	120(%rsp), %edx
	movq	112(%rsp), %r10
	movq	96(%rsp), %r11
	movl	%r9d, 36(%rsp)
	movl	%ecx, 32(%rsp)
	je	.LBB106_16	# bb53.thread
.LBB106_1:	# bb20
	cmpl	$121, %esi
	sete	%cl
	setne	%r9b
	cmpl	$101, %edi
	sete	%bl
	setne	%r14b
	andb	%cl, %bl
	orb	%r9b, %r14b
	testb	%r14b, %r14b
	jne	.LBB106_3	# bb27
.LBB106_2:	# bb20
	cmpl	$111, %eax
	je	.LBB106_5	# bb35
.LBB106_3:	# bb27
	cmpl	$122, %esi
	sete	%cl
	setne	%r9b
	cmpl	$102, %edi
	sete	%r14b
	setne	%r15b
	andb	%cl, %r14b
	orb	%r9b, %r15b
	testb	%r15b, %r15b
	jne	.LBB106_17	# bb55
.LBB106_4:	# bb27
	cmpl	$112, %eax
	jne	.LBB106_17	# bb55
.LBB106_5:	# bb35
	testl	%edx, %edx
	jg	.LBB106_57	# bb35.bb38_crit_edge
.LBB106_6:	# bb36
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
.LBB106_7:	# bb38
	movl	$1, %ecx
	subl	%r8d, %ecx
	leal	-1(%r8), %esi
	movl	104(%rsp), %edi
	imull	%esi, %edi
	imull	%edx, %ecx
	movl	%ecx, 16(%rsp)
	movl	%r8d, %ecx
	imull	%edx, %ecx
	movl	%ecx, 24(%rsp)
	imull	%edx, %esi
	addl	%eax, %esi
	movl	%esi, 20(%rsp)
	xorl	%eax, %eax
	movl	%r8d, %ecx
	movl	%eax, 28(%rsp)
	movl	%eax, %esi
	jmp	.LBB106_14	# bb49
.LBB106_8:	# bb39
	testl	%edx, %edx
	movl	$0, %ebx
	cmovle	16(%rsp), %ebx
	movl	36(%rsp), %r14d
	leal	(%r14,%rcx), %r14d
	cmpl	%r8d, %r14d
	cmovg	%r8d, %r14d
	cmpl	%r14d, %ecx
	movslq	%r9d, %r9
	movss	(%r10,%r9,4), %xmm0
	jge	.LBB106_11	# bb45
.LBB106_9:	# bb.nph217
	movl	$4294967295, %r14d
	subl	%r8d, %r14d
	movl	36(%rsp), %r15d
	leal	1(%r15,%r8), %r15d
	movl	28(%rsp), %r12d
	subl	%r15d, %r12d
	cmpl	%r12d, %r14d
	cmovg	%r14d, %r12d
	addl	%r8d, %r12d
	movl	%esi, %r14d
	subl	%r12d, %r14d
	decl	%r14d
	movl	24(%rsp), %r15d
	leal	(%r15,%rax), %r15d
	leal	1(%rdi), %r12d
	xorl	%r13d, %r13d
	.align	16
.LBB106_10:	# bb43
	leal	(%r12,%r13), %ebp
	movslq	%ebp, %rbp
	addl	%ebx, %r15d
	movslq	%r15d, %rbx
	movss	(%r10,%rbx,4), %xmm1
	mulss	(%r11,%rbp,4), %xmm1
	subss	%xmm1, %xmm0
	incl	%r13d
	cmpl	%r14d, %r13d
	movl	%edx, %ebx
	jne	.LBB106_10	# bb43
.LBB106_11:	# bb45
	cmpl	$131, 32(%rsp)
	jne	.LBB106_13	# bb48
.LBB106_12:	# bb46
	movslq	%edi, %rbx
	divss	(%r11,%rbx,4), %xmm0
.LBB106_13:	# bb48
	movss	%xmm0, (%r10,%r9,4)
	subl	%edx, %eax
	subl	104(%rsp), %edi
	decl	%ecx
	incl	28(%rsp)
	incl	%esi
.LBB106_14:	# bb49
	movl	20(%rsp), %r9d
	leal	(%r9,%rax), %r9d
	testl	%ecx, %ecx
	jle	.LBB106_16	# bb53.thread
.LBB106_15:	# bb50
	testl	%ecx, %ecx
	jne	.LBB106_8	# bb39
.LBB106_16:	# bb53.thread
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB106_17:	# bb55
	cmpl	$122, %esi
	sete	%cl
	setne	%r9b
	cmpl	$101, %edi
	sete	%r15b
	setne	%r12b
	andb	%cl, %r15b
	orb	%r9b, %r12b
	testb	%r12b, %r12b
	jne	.LBB106_19	# bb63
.LBB106_18:	# bb55
	cmpl	$111, %eax
	je	.LBB106_21	# bb71
.LBB106_19:	# bb63
	cmpl	$121, %esi
	sete	%cl
	setne	%sil
	cmpl	$102, %edi
	sete	%dil
	setne	%r9b
	andb	%cl, %dil
	orb	%sil, %r9b
	testb	%r9b, %r9b
	jne	.LBB106_31	# bb89
.LBB106_20:	# bb63
	cmpl	$112, %eax
	jne	.LBB106_31	# bb89
.LBB106_21:	# bb71
	testl	%edx, %edx
	jg	.LBB106_58	# bb71.bb88.preheader_crit_edge
.LBB106_22:	# bb72
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
.LBB106_23:	# bb88.preheader
	testl	%r8d, %r8d
	jle	.LBB106_16	# bb53.thread
.LBB106_24:	# bb.nph210
	movl	$1, %ecx
	subl	%r8d, %ecx
	imull	%edx, %ecx
	movl	104(%rsp), %esi
	leal	-1(%rsi), %esi
	movl	%esi, 28(%rsp)
	movl	36(%rsp), %esi
	movl	%esi, %edi
	negl	%edi
	movl	%edi, 24(%rsp)
	xorl	%edi, %edi
	movl	%esi, %r9d
	.align	16
.LBB106_25:	# bb75
	xorl	%ebx, %ebx
	testl	%edx, %edx
	movl	%ecx, %r14d
	cmovg	%ebx, %r14d
	movl	24(%rsp), %r15d
	leal	(%r15,%rdi), %r15d
	cmpl	36(%rsp), %edi
	cmovle	%ebx, %r15d
	movl	%r15d, %ebx
	imull	%edx, %ebx
	cmpl	%edi, %r15d
	movslq	%eax, %rax
	movss	(%r10,%rax,4), %xmm0
	jge	.LBB106_28	# bb84
.LBB106_26:	# bb.nph206
	movl	%edi, %r12d
	subl	%r15d, %r12d
	addl	%esi, %r15d
	xorl	%r13d, %r13d
	.align	16
.LBB106_27:	# bb82
	leal	(%r15,%r13), %ebp
	movslq	%ebp, %rbp
	addl	%r14d, %ebx
	movslq	%ebx, %r14
	movss	(%r10,%r14,4), %xmm1
	mulss	(%r11,%rbp,4), %xmm1
	subss	%xmm1, %xmm0
	incl	%r13d
	cmpl	%r12d, %r13d
	movl	%edx, %r14d
	jne	.LBB106_27	# bb82
.LBB106_28:	# bb84
	cmpl	$131, 32(%rsp)
	jne	.LBB106_30	# bb87
.LBB106_29:	# bb85
	movslq	%r9d, %rbx
	divss	(%r11,%rbx,4), %xmm0
.LBB106_30:	# bb87
	movss	%xmm0, (%r10,%rax,4)
	addl	%edx, %eax
	addl	104(%rsp), %r9d
	addl	28(%rsp), %esi
	incl	%edi
	cmpl	%r8d, %edi
	je	.LBB106_16	# bb53.thread
	jmp	.LBB106_25	# bb75
.LBB106_31:	# bb89
	cmpl	$111, %eax
	sete	%cl
	andb	%cl, %r14b
	cmpl	$112, %eax
	sete	%cl
	testb	%cl, %bl
	jne	.LBB106_33	# bb105
.LBB106_32:	# bb89
	notb	%r14b
	testb	$1, %r14b
	jne	.LBB106_43	# bb123
.LBB106_33:	# bb105
	testl	%edx, %edx
	jg	.LBB106_59	# bb105.bb122.preheader_crit_edge
.LBB106_34:	# bb106
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
.LBB106_35:	# bb122.preheader
	testl	%r8d, %r8d
	jle	.LBB106_16	# bb53.thread
.LBB106_36:	# bb.nph199
	movl	$1, %ecx
	subl	%r8d, %ecx
	imull	%edx, %ecx
	movl	36(%rsp), %esi
	negl	%esi
	movl	%esi, 28(%rsp)
	xorl	%esi, %esi
	movl	%esi, %edi
	.align	16
.LBB106_37:	# bb109
	xorl	%r9d, %r9d
	testl	%edx, %edx
	movl	%ecx, %ebx
	cmovg	%r9d, %ebx
	movl	28(%rsp), %r14d
	leal	(%r14,%rdi), %r14d
	cmpl	36(%rsp), %edi
	cmovl	%r9d, %r14d
	movl	%r14d, %r9d
	imull	%edx, %r9d
	cmpl	%edi, %r14d
	movslq	%eax, %rax
	movss	(%r10,%rax,4), %xmm0
	jge	.LBB106_40	# bb118
.LBB106_38:	# bb.nph195
	movl	104(%rsp), %r15d
	leal	-1(%r15), %r15d
	movl	%r14d, %r12d
	imull	%r15d, %r12d
	addl	%edi, %r12d
	movl	%edi, %r13d
	subl	%r14d, %r13d
	xorl	%r14d, %r14d
	.align	16
.LBB106_39:	# bb116
	leal	(%r15,%r12), %ebp
	addl	%ebx, %r9d
	incl	%r14d
	cmpl	%r13d, %r14d
	movslq	%r9d, %rbx
	movss	(%r10,%rbx,4), %xmm1
	movslq	%r12d, %rbx
	mulss	(%r11,%rbx,4), %xmm1
	subss	%xmm1, %xmm0
	movl	%ebp, %r12d
	movl	%edx, %ebx
	jne	.LBB106_39	# bb116
.LBB106_40:	# bb118
	cmpl	$131, 32(%rsp)
	jne	.LBB106_42	# bb121
.LBB106_41:	# bb119
	movslq	%esi, %r9
	divss	(%r11,%r9,4), %xmm0
.LBB106_42:	# bb121
	movss	%xmm0, (%r10,%rax,4)
	addl	%edx, %eax
	addl	104(%rsp), %esi
	incl	%edi
	cmpl	%r8d, %edi
	je	.LBB106_16	# bb53.thread
	jmp	.LBB106_37	# bb109
.LBB106_43:	# bb123
	cmpl	$111, %eax
	sete	%cl
	andb	%cl, %dil
	cmpl	$112, %eax
	sete	%al
	testb	%al, %r15b
	jne	.LBB106_45	# bb139
.LBB106_44:	# bb123
	notb	%dil
	testb	$1, %dil
	jne	.LBB106_56	# bb160
.LBB106_45:	# bb139
	testl	%edx, %edx
	jg	.LBB106_60	# bb139.bb142_crit_edge
.LBB106_46:	# bb140
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
.LBB106_47:	# bb142
	movl	104(%rsp), %ecx
	movl	%ecx, %esi
	imull	%r8d, %esi
	decl	%esi
	movl	%esi, 16(%rsp)
	movl	$1, %esi
	subl	%r8d, %esi
	leal	-1(%r8), %edi
	imull	%edi, %ecx
	movl	%ecx, 20(%rsp)
	imull	%edx, %esi
	movl	%esi, 4(%rsp)
	movl	%r8d, %ecx
	imull	%edx, %ecx
	movl	%ecx, 12(%rsp)
	imull	%edx, %edi
	addl	%eax, %edi
	movl	%edi, 8(%rsp)
	xorl	%eax, %eax
	movl	36(%rsp), %ecx
	movl	%r8d, %esi
	movl	%eax, 28(%rsp)
	movl	%eax, 24(%rsp)
	jmp	.LBB106_54	# bb154
.LBB106_48:	# bb143
	testl	%edx, %edx
	movl	$0, %r9d
	cmovle	4(%rsp), %r9d
	movl	36(%rsp), %ebx
	leal	(%rbx,%rsi), %ebx
	cmpl	%r8d, %ebx
	cmovg	%r8d, %ebx
	cmpl	%ebx, %esi
	movslq	%edi, %rdi
	movss	(%r10,%rdi,4), %xmm0
	jge	.LBB106_51	# bb150
.LBB106_49:	# bb.nph
	movl	$4294967295, %ebx
	subl	%r8d, %ebx
	movl	36(%rsp), %r14d
	leal	1(%r14,%r8), %r14d
	movl	28(%rsp), %r15d
	subl	%r14d, %r15d
	cmpl	%r15d, %ebx
	cmovg	%ebx, %r15d
	addl	%r8d, %r15d
	movl	24(%rsp), %ebx
	subl	%r15d, %ebx
	decl	%ebx
	movl	12(%rsp), %r14d
	leal	(%r14,%rax), %r14d
	movl	16(%rsp), %r15d
	leal	(%r15,%rcx), %r15d
	movl	104(%rsp), %r12d
	leal	-1(%r12), %r12d
	xorl	%r13d, %r13d
	.align	16
.LBB106_50:	# bb148
	leal	(%r12,%r15), %ebp
	addl	%r9d, %r14d
	incl	%r13d
	cmpl	%ebx, %r13d
	movslq	%r14d, %r9
	movss	(%r10,%r9,4), %xmm1
	movslq	%r15d, %r9
	mulss	(%r11,%r9,4), %xmm1
	subss	%xmm1, %xmm0
	movl	%ebp, %r15d
	movl	%edx, %r9d
	jne	.LBB106_50	# bb148
.LBB106_51:	# bb150
	cmpl	$131, 32(%rsp)
	jne	.LBB106_53	# bb153
.LBB106_52:	# bb151
	movl	20(%rsp), %r9d
	leal	(%r9,%rcx), %r9d
	movslq	%r9d, %r9
	divss	(%r11,%r9,4), %xmm0
.LBB106_53:	# bb153
	movss	%xmm0, (%r10,%rdi,4)
	subl	%edx, %eax
	subl	104(%rsp), %ecx
	decl	%esi
	incl	28(%rsp)
	incl	24(%rsp)
.LBB106_54:	# bb154
	movl	8(%rsp), %edi
	leal	(%rdi,%rax), %edi
	testl	%esi, %esi
	jle	.LBB106_16	# bb53.thread
.LBB106_55:	# bb155
	testl	%esi, %esi
	jne	.LBB106_48	# bb143
	jmp	.LBB106_16	# bb53.thread
.LBB106_56:	# bb160
	xorl	%edi, %edi
	leaq	.str128, %rsi
	leaq	.str1129, %rdx
	xorb	%al, %al
	call	cblas_xerbla
	jmp	.LBB106_16	# bb53.thread
.LBB106_57:	# bb35.bb38_crit_edge
	xorl	%eax, %eax
	jmp	.LBB106_7	# bb38
.LBB106_58:	# bb71.bb88.preheader_crit_edge
	xorl	%eax, %eax
	jmp	.LBB106_23	# bb88.preheader
.LBB106_59:	# bb105.bb122.preheader_crit_edge
	xorl	%eax, %eax
	jmp	.LBB106_35	# bb122.preheader
.LBB106_60:	# bb139.bb142_crit_edge
	xorl	%eax, %eax
	jmp	.LBB106_47	# bb142
	.size	cblas_stbsv, .-cblas_stbsv
.Leh_func_end71:


	.align	16
	.globl	cblas_stpmv
	.type	cblas_stpmv,@function
cblas_stpmv:
.Leh_func_begin72:
.Llabel72:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$24, %rsp
	cmpl	$113, %edx
	movl	$112, %eax
	cmovne	%edx, %eax
	testl	%r8d, %r8d
	movl	88(%rsp), %edx
	movq	80(%rsp), %r10
	movl	%ecx, 20(%rsp)
	je	.LBB107_30	# bb79.thread
.LBB107_1:	# bb15
	cmpl	$121, %esi
	sete	%cl
	setne	%r11b
	cmpl	$101, %edi
	sete	%bl
	setne	%r14b
	andb	%cl, %bl
	orb	%r11b, %r14b
	testb	%r14b, %r14b
	jne	.LBB107_3	# bb22
.LBB107_2:	# bb15
	cmpl	$111, %eax
	je	.LBB107_5	# bb30
.LBB107_3:	# bb22
	cmpl	$122, %esi
	sete	%cl
	setne	%r11b
	cmpl	$102, %edi
	sete	%r14b
	setne	%r15b
	andb	%cl, %r14b
	orb	%r11b, %r15b
	testb	%r15b, %r15b
	jne	.LBB107_15	# bb45
.LBB107_4:	# bb22
	cmpl	$112, %eax
	jne	.LBB107_15	# bb45
.LBB107_5:	# bb30
	testl	%edx, %edx
	jg	.LBB107_56	# bb30.bb44.preheader_crit_edge
.LBB107_6:	# bb31
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
	movl	%eax, 12(%rsp)
.LBB107_7:	# bb44.preheader
	testl	%r8d, %r8d
	jle	.LBB107_30	# bb79.thread
.LBB107_8:	# bb.nph206
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
	movl	%eax, 8(%rsp)
	leal	1(,%r8,2), %eax
	leal	-1(%r8), %ecx
	movl	%ecx, 16(%rsp)
	xorl	%ecx, %ecx
	movl	%ecx, %esi
	movl	%ecx, %edi
	.align	16
.LBB107_9:	# bb34
	movl	12(%rsp), %r11d
	leal	(%r11,%rcx), %r11d
	movslq	%r11d, %r11
	movss	(%r10,%r11,4), %xmm0
	cmpl	$131, 20(%rsp)
	jne	.LBB107_11	# bb37
.LBB107_10:	# bb35
	leal	(%rax,%rsi), %ebx
	imull	%edi, %ebx
	movl	%ebx, %r14d
	shrl	$31, %r14d
	addl	%ebx, %r14d
	sarl	%r14d
	movslq	%r14d, %rbx
	mulss	(%r9,%rbx,4), %xmm0
.LBB107_11:	# bb37
	testl	%edx, %edx
	movl	$0, %ebx
	cmovle	8(%rsp), %ebx
	leal	1(%rdi), %r14d
	cmpl	%r8d, %r14d
	jge	.LBB107_14	# bb43
.LBB107_12:	# bb.nph202
	leal	(%rax,%rsi), %r14d
	imull	%edi, %r14d
	movl	%r14d, %r15d
	shrl	$31, %r15d
	addl	%r14d, %r15d
	sarl	%r15d
	incl	%r15d
	leal	(%rdx,%rcx), %r14d
	movl	16(%rsp), %r12d
	leal	(%r12,%rsi), %r12d
	xorl	%r13d, %r13d
	.align	16
.LBB107_13:	# bb41
	leal	(%r15,%r13), %ebp
	movslq	%ebp, %rbp
	addl	%ebx, %r14d
	movslq	%r14d, %rbx
	movss	(%r10,%rbx,4), %xmm1
	mulss	(%r9,%rbp,4), %xmm1
	addss	%xmm1, %xmm0
	incl	%r13d
	cmpl	%r12d, %r13d
	movl	%edx, %ebx
	jne	.LBB107_13	# bb41
.LBB107_14:	# bb43
	movss	%xmm0, (%r10,%r11,4)
	addl	%edx, %ecx
	decl	%esi
	incl	%edi
	cmpl	%r8d, %edi
	jne	.LBB107_9	# bb34
	jmp	.LBB107_30	# bb79.thread
.LBB107_15:	# bb45
	cmpl	$122, %esi
	sete	%cl
	setne	%r11b
	cmpl	$101, %edi
	sete	%r15b
	setne	%r12b
	andb	%cl, %r15b
	orb	%r11b, %r12b
	testb	%r12b, %r12b
	jne	.LBB107_17	# bb53
.LBB107_16:	# bb45
	cmpl	$111, %eax
	je	.LBB107_19	# bb61
.LBB107_17:	# bb53
	cmpl	$121, %esi
	sete	%cl
	setne	%sil
	cmpl	$102, %edi
	sete	%dil
	setne	%r11b
	andb	%cl, %dil
	orb	%sil, %r11b
	testb	%r11b, %r11b
	jne	.LBB107_31	# bb81
.LBB107_18:	# bb53
	cmpl	$112, %eax
	jne	.LBB107_31	# bb81
.LBB107_19:	# bb61
	testl	%edx, %edx
	jg	.LBB107_57	# bb61.bb64_crit_edge
.LBB107_20:	# bb62
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
.LBB107_21:	# bb64
	movl	$1, %ecx
	subl	%r8d, %ecx
	imull	%edx, %ecx
	leal	-1(%r8), %esi
	imull	%edx, %esi
	addl	%eax, %esi
	jmp	.LBB107_28	# bb75
.LBB107_22:	# bb65
	movslq	%esi, %rdi
	movss	(%r10,%rdi,4), %xmm0
	cmpl	$131, 20(%rsp)
	jne	.LBB107_24	# bb68
.LBB107_23:	# bb66
	movl	%eax, %r11d
	imull	%r8d, %r11d
	movl	%r11d, %ebx
	shrl	$31, %ebx
	addl	%r11d, %ebx
	sarl	%ebx
	leal	-1(%r8,%rbx), %r11d
	movslq	%r11d, %r11
	mulss	(%r9,%r11,4), %xmm0
.LBB107_24:	# bb68
	testl	%edx, %edx
	movl	$0, %r11d
	cmovle	%ecx, %r11d
	testl	%eax, %eax
	jle	.LBB107_27	# bb74
.LBB107_25:	# bb.nph192
	imull	%r8d, %eax
	movl	%eax, %ebx
	shrl	$31, %ebx
	addl	%eax, %ebx
	sarl	%ebx
	leal	-1(%r8), %eax
	xorl	%r14d, %r14d
	.align	16
.LBB107_26:	# bb72
	leal	(%rbx,%r14), %r15d
	movslq	%r15d, %r15
	movslq	%r11d, %r12
	movss	(%r10,%r12,4), %xmm1
	mulss	(%r9,%r15,4), %xmm1
	addss	%xmm1, %xmm0
	addl	%edx, %r11d
	incl	%r14d
	cmpl	%eax, %r14d
	jne	.LBB107_26	# bb72
.LBB107_27:	# bb74
	movss	%xmm0, (%r10,%rdi,4)
	subl	%edx, %esi
	decl	%r8d
.LBB107_28:	# bb75
	testl	%r8d, %r8d
	jle	.LBB107_30	# bb79.thread
.LBB107_29:	# bb76
	leal	-1(%r8), %eax
	testl	%r8d, %r8d
	jne	.LBB107_22	# bb65
.LBB107_30:	# bb79.thread
	addq	$24, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB107_31:	# bb81
	cmpl	$111, %eax
	sete	%cl
	andb	%cl, %r14b
	cmpl	$112, %eax
	sete	%cl
	testb	%cl, %bl
	jne	.LBB107_33	# bb97
.LBB107_32:	# bb81
	notb	%r14b
	testb	$1, %r14b
	jne	.LBB107_44	# bb117
.LBB107_33:	# bb97
	testl	%edx, %edx
	jg	.LBB107_58	# bb97.bb100_crit_edge
.LBB107_34:	# bb98
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
.LBB107_35:	# bb100
	movl	$1, %ecx
	subl	%r8d, %ecx
	imull	%edx, %ecx
	movl	%ecx, 16(%rsp)
	leal	-1(%r8), %ecx
	imull	%edx, %ecx
	addl	%eax, %ecx
	leal	2(%r8), %eax
	movl	%r8d, %esi
	jmp	.LBB107_42	# bb111
.LBB107_36:	# bb101
	movslq	%ecx, %r11
	movss	(%r10,%r11,4), %xmm0
	cmpl	$131, 20(%rsp)
	jne	.LBB107_38	# bb104
.LBB107_37:	# bb102
	movl	%eax, %ebx
	imull	%edi, %ebx
	movl	%ebx, %r14d
	shrl	$31, %r14d
	addl	%ebx, %r14d
	sarl	%r14d
	movslq	%r14d, %rbx
	mulss	(%r9,%rbx,4), %xmm0
.LBB107_38:	# bb104
	testl	%edx, %edx
	movl	$0, %ebx
	cmovle	16(%rsp), %ebx
	testl	%edi, %edi
	jle	.LBB107_41	# bb110
.LBB107_39:	# bb.nph187
	leal	1(,%r8,2), %edi
	leal	-1(%rsi), %r14d
	xorl	%r15d, %r15d
	movl	%r15d, %r12d
	.align	16
.LBB107_40:	# bb108
	leal	(%rdi,%r15), %r13d
	imull	%r12d, %r13d
	movl	%r13d, %ebp
	shrl	$31, %ebp
	addl	%r13d, %ebp
	sarl	%ebp
	leal	(%r14,%r15), %r13d
	addl	%ebp, %r13d
	movslq	%r13d, %r13
	movslq	%ebx, %rbp
	movss	(%r10,%rbp,4), %xmm1
	mulss	(%r9,%r13,4), %xmm1
	addss	%xmm1, %xmm0
	addl	%edx, %ebx
	decl	%r15d
	incl	%r12d
	cmpl	%r14d, %r12d
	jne	.LBB107_40	# bb108
.LBB107_41:	# bb110
	movss	%xmm0, (%r10,%r11,4)
	subl	%edx, %ecx
	decl	%esi
	incl	%eax
.LBB107_42:	# bb111
	testl	%esi, %esi
	jle	.LBB107_30	# bb79.thread
.LBB107_43:	# bb112
	leal	-1(%rsi), %edi
	testl	%esi, %esi
	jne	.LBB107_36	# bb101
	jmp	.LBB107_30	# bb79.thread
.LBB107_44:	# bb117
	cmpl	$111, %eax
	sete	%cl
	andb	%cl, %dil
	cmpl	$112, %eax
	sete	%al
	testb	%al, %r15b
	jne	.LBB107_46	# bb133
.LBB107_45:	# bb117
	notb	%dil
	testb	$1, %dil
	jne	.LBB107_60	# bb148
.LBB107_46:	# bb133
	testl	%edx, %edx
	jg	.LBB107_59	# bb133.bb147.preheader_crit_edge
.LBB107_47:	# bb134
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
	movl	%eax, 16(%rsp)
.LBB107_48:	# bb147.preheader
	testl	%r8d, %r8d
	jle	.LBB107_30	# bb79.thread
.LBB107_49:	# bb.nph183
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
	movl	%eax, 12(%rsp)
	leal	-1(%r8), %eax
	xorl	%ecx, %ecx
	movl	%ecx, %esi
	.align	16
.LBB107_50:	# bb137
	movl	16(%rsp), %edi
	leal	(%rdi,%rcx), %edi
	movslq	%edi, %rdi
	movss	(%r10,%rdi,4), %xmm0
	cmpl	$131, 20(%rsp)
	jne	.LBB107_52	# bb140
.LBB107_51:	# bb138
	leal	1(%rsi), %r11d
	imull	%esi, %r11d
	movl	%r11d, %ebx
	shrl	$31, %ebx
	addl	%r11d, %ebx
	sarl	%ebx
	addl	%esi, %ebx
	movslq	%ebx, %r11
	mulss	(%r9,%r11,4), %xmm0
.LBB107_52:	# bb140
	testl	%edx, %edx
	movl	$0, %r11d
	cmovle	12(%rsp), %r11d
	leal	1(%rsi), %ebx
	cmpl	%r8d, %ebx
	jge	.LBB107_55	# bb146
.LBB107_53:	# bb.nph
	leal	(%rdx,%rcx), %ebx
	leal	1(%rsi), %r14d
	leal	2(%rsi), %r15d
	xorl	%r12d, %r12d
	.align	16
.LBB107_54:	# bb144
	leal	(%r14,%r12), %r13d
	leal	(%r15,%r12), %ebp
	imull	%r13d, %ebp
	movl	%ebp, %r13d
	shrl	$31, %r13d
	addl	%ebp, %r13d
	sarl	%r13d
	addl	%esi, %r13d
	movslq	%r13d, %r13
	addl	%r11d, %ebx
	movslq	%ebx, %r11
	movss	(%r10,%r11,4), %xmm1
	mulss	(%r9,%r13,4), %xmm1
	addss	%xmm1, %xmm0
	incl	%r12d
	cmpl	%eax, %r12d
	movl	%edx, %r11d
	jne	.LBB107_54	# bb144
.LBB107_55:	# bb146
	movss	%xmm0, (%r10,%rdi,4)
	addl	%edx, %ecx
	decl	%eax
	incl	%esi
	cmpl	%r8d, %esi
	je	.LBB107_30	# bb79.thread
	jmp	.LBB107_50	# bb137
.LBB107_56:	# bb30.bb44.preheader_crit_edge
	movl	$0, 12(%rsp)
	jmp	.LBB107_7	# bb44.preheader
.LBB107_57:	# bb61.bb64_crit_edge
	xorl	%eax, %eax
	jmp	.LBB107_21	# bb64
.LBB107_58:	# bb97.bb100_crit_edge
	xorl	%eax, %eax
	jmp	.LBB107_35	# bb100
.LBB107_59:	# bb133.bb147.preheader_crit_edge
	movl	$0, 16(%rsp)
	jmp	.LBB107_48	# bb147.preheader
.LBB107_60:	# bb148
	xorl	%edi, %edi
	leaq	.str130, %rsi
	leaq	.str1131, %rdx
	xorb	%al, %al
	call	cblas_xerbla
	jmp	.LBB107_30	# bb79.thread
	.size	cblas_stpmv, .-cblas_stpmv
.Leh_func_end72:


	.align	16
	.globl	cblas_stpsv
	.type	cblas_stpsv,@function
cblas_stpsv:
.Leh_func_begin73:
.Llabel73:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$24, %rsp
	cmpl	$113, %edx
	movl	$112, %eax
	cmovne	%edx, %eax
	testl	%r8d, %r8d
	movl	88(%rsp), %edx
	movq	80(%rsp), %r10
	movl	%ecx, 20(%rsp)
	je	.LBB108_18	# bb46.thread
.LBB108_1:	# bb14
	cmpl	$121, %esi
	sete	%cl
	setne	%r11b
	cmpl	$101, %edi
	sete	%bl
	setne	%r14b
	andb	%cl, %bl
	orb	%r11b, %r14b
	testb	%r14b, %r14b
	jne	.LBB108_3	# bb21
.LBB108_2:	# bb14
	cmpl	$111, %eax
	je	.LBB108_5	# bb29
.LBB108_3:	# bb21
	cmpl	$122, %esi
	sete	%cl
	setne	%r11b
	cmpl	$102, %edi
	sete	%r14b
	setne	%r15b
	andb	%cl, %r14b
	orb	%r11b, %r15b
	testb	%r15b, %r15b
	jne	.LBB108_19	# bb48
.LBB108_4:	# bb21
	cmpl	$112, %eax
	jne	.LBB108_19	# bb48
.LBB108_5:	# bb29
	testl	%edx, %edx
	jg	.LBB108_66	# bb29.bb32_crit_edge
.LBB108_6:	# bb30
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
.LBB108_7:	# bb32
	leal	-1(%r8), %ecx
	movl	%ecx, %esi
	imull	%edx, %esi
	addl	%eax, %esi
	cmpl	$131, 20(%rsp)
	jne	.LBB108_9	# bb34
.LBB108_8:	# bb33
	leal	(%r8,%r8), %edi
	leal	-2(%r8), %r11d
	subl	%r11d, %edi
	imull	%ecx, %edi
	movl	%edi, %ecx
	shrl	$31, %ecx
	addl	%edi, %ecx
	sarl	%ecx
	movslq	%ecx, %rcx
	movslq	%esi, %rsi
	movss	(%r10,%rsi,4), %xmm0
	divss	(%r9,%rcx,4), %xmm0
	movss	%xmm0, (%r10,%rsi,4)
.LBB108_9:	# bb34
	leal	-1(%r8), %ecx
	imull	%edx, %ecx
	movl	%ecx, 12(%rsp)
	leal	-2(%r8), %ecx
	imull	%edx, %ecx
	movl	%ecx, 8(%rsp)
	leal	3(%r8), %ecx
	movl	%ecx, 16(%rsp)
	xorl	%ecx, %ecx
	movl	%r8d, %esi
	jmp	.LBB108_16	# bb42
.LBB108_10:	# bb35
	cmpl	%r8d, %ebx
	movslq	%r11d, %r11
	movss	(%r10,%r11,4), %xmm0
	jge	.LBB108_13	# bb38
.LBB108_11:	# bb.nph212
	movl	16(%rsp), %ebx
	leal	(%rbx,%rcx), %ebx
	imull	%edi, %ebx
	movl	%ebx, %r14d
	shrl	$31, %r14d
	addl	%ebx, %r14d
	sarl	%r14d
	incl	%r14d
	movl	12(%rsp), %ebx
	leal	(%rbx,%rax), %ebx
	leal	1(%rcx), %r15d
	xorl	%r12d, %r12d
	.align	16
.LBB108_12:	# bb36
	leal	(%r14,%r12), %r13d
	movslq	%r13d, %r13
	movslq	%ebx, %rbp
	movss	(%r10,%rbp,4), %xmm1
	mulss	(%r9,%r13,4), %xmm1
	subss	%xmm1, %xmm0
	addl	%edx, %ebx
	incl	%r12d
	cmpl	%r15d, %r12d
	jne	.LBB108_12	# bb36
.LBB108_13:	# bb38
	cmpl	$131, 20(%rsp)
	jne	.LBB108_15	# bb42.backedge
.LBB108_14:	# bb39
	movl	16(%rsp), %ebx
	leal	(%rbx,%rcx), %ebx
	imull	%edi, %ebx
	movl	%ebx, %edi
	shrl	$31, %edi
	addl	%ebx, %edi
	sarl	%edi
	movslq	%edi, %rdi
	divss	(%r9,%rdi,4), %xmm0
.LBB108_15:	# bb42.backedge
	movss	%xmm0, (%r10,%r11,4)
	subl	%edx, %eax
	decl	%esi
	incl	%ecx
.LBB108_16:	# bb42
	movl	8(%rsp), %edi
	leal	(%rdi,%rax), %r11d
	leal	-1(%rsi), %ebx
	testl	%ebx, %ebx
	jle	.LBB108_18	# bb46.thread
.LBB108_17:	# bb43
	leal	-2(%rsi), %edi
	cmpl	$4294967295, %edi
	jne	.LBB108_10	# bb35
.LBB108_18:	# bb46.thread
	addq	$24, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB108_19:	# bb48
	cmpl	$122, %esi
	sete	%cl
	setne	%r11b
	cmpl	$101, %edi
	sete	%r15b
	setne	%r12b
	andb	%cl, %r15b
	orb	%r11b, %r12b
	testb	%r12b, %r12b
	jne	.LBB108_21	# bb56
.LBB108_20:	# bb48
	cmpl	$111, %eax
	je	.LBB108_23	# bb64
.LBB108_21:	# bb56
	cmpl	$121, %esi
	sete	%cl
	setne	%sil
	cmpl	$102, %edi
	sete	%dil
	setne	%r11b
	andb	%cl, %dil
	orb	%sil, %r11b
	testb	%r11b, %r11b
	jne	.LBB108_36	# bb81
.LBB108_22:	# bb56
	cmpl	$112, %eax
	jne	.LBB108_36	# bb81
.LBB108_23:	# bb64
	testl	%edx, %edx
	jg	.LBB108_67	# bb64.bb67_crit_edge
.LBB108_24:	# bb65
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
.LBB108_25:	# bb67
	cmpl	$131, 20(%rsp)
	jne	.LBB108_27	# bb80.preheader
.LBB108_26:	# bb68
	movslq	%eax, %rcx
	movss	(%r10,%rcx,4), %xmm0
	divss	(%r9), %xmm0
	movss	%xmm0, (%r10,%rcx,4)
.LBB108_27:	# bb80.preheader
	cmpl	$2, %r8d
	jl	.LBB108_18	# bb46.thread
.LBB108_28:	# bb.nph204
	movl	$1, %ecx
	subl	%r8d, %ecx
	imull	%edx, %ecx
	addl	%edx, %eax
	decl	%r8d
	xorl	%esi, %esi
	.align	16
.LBB108_29:	# bb70
	testl	%edx, %edx
	movl	$0, %edi
	cmovle	%ecx, %edi
	movslq	%eax, %rax
	movss	(%r10,%rax,4), %xmm0
	leal	1(%rsi), %r11d
	testl	%r11d, %r11d
	jle	.LBB108_68	# bb70.bb76_crit_edge
.LBB108_30:	# bb.nph197
	leal	2(%rsi), %ebx
	imull	%r11d, %ebx
	movl	%ebx, %r14d
	shrl	$31, %r14d
	addl	%ebx, %r14d
	sarl	%r14d
	xorl	%ebx, %ebx
	.align	16
.LBB108_31:	# bb74
	leal	(%r14,%rbx), %r15d
	movslq	%r15d, %r15
	movslq	%edi, %r12
	movss	(%r10,%r12,4), %xmm1
	mulss	(%r9,%r15,4), %xmm1
	subss	%xmm1, %xmm0
	addl	%edx, %edi
	incl	%ebx
	cmpl	%r11d, %ebx
	jne	.LBB108_31	# bb74
.LBB108_32:	# bb76.loopexit
	leal	1(%rsi), %edi
.LBB108_33:	# bb76
	cmpl	$131, 20(%rsp)
	jne	.LBB108_35	# bb79
.LBB108_34:	# bb77
	leal	2(%rsi), %ebx
	imull	%r11d, %ebx
	movl	%ebx, %r11d
	shrl	$31, %r11d
	addl	%ebx, %r11d
	sarl	%r11d
	addl	%edi, %r11d
	movslq	%r11d, %rdi
	divss	(%r9,%rdi,4), %xmm0
.LBB108_35:	# bb79
	movss	%xmm0, (%r10,%rax,4)
	addl	%edx, %eax
	incl	%esi
	cmpl	%r8d, %esi
	je	.LBB108_18	# bb46.thread
	jmp	.LBB108_29	# bb70
.LBB108_36:	# bb81
	cmpl	$111, %eax
	sete	%cl
	andb	%cl, %r14b
	cmpl	$112, %eax
	sete	%cl
	testb	%cl, %bl
	jne	.LBB108_38	# bb97
.LBB108_37:	# bb81
	notb	%r14b
	testb	$1, %r14b
	jne	.LBB108_50	# bb114
.LBB108_38:	# bb97
	testl	%edx, %edx
	jg	.LBB108_69	# bb97.bb100_crit_edge
.LBB108_39:	# bb98
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
.LBB108_40:	# bb100
	cmpl	$131, 20(%rsp)
	jne	.LBB108_42	# bb113.preheader
.LBB108_41:	# bb101
	movslq	%eax, %rcx
	movss	(%r10,%rcx,4), %xmm0
	divss	(%r9), %xmm0
	movss	%xmm0, (%r10,%rcx,4)
.LBB108_42:	# bb113.preheader
	cmpl	$2, %r8d
	jl	.LBB108_18	# bb46.thread
.LBB108_43:	# bb.nph193
	movl	$1, %ecx
	subl	%r8d, %ecx
	imull	%edx, %ecx
	addl	%edx, %eax
	leal	(%r8,%r8), %esi
	leal	-1(%r8), %edi
	movl	%edi, 16(%rsp)
	xorl	%edi, %edi
	.align	16
.LBB108_44:	# bb103
	testl	%edx, %edx
	movl	$0, %r11d
	cmovle	%ecx, %r11d
	movslq	%eax, %rax
	movss	(%r10,%rax,4), %xmm0
	leal	1(%rdi), %ebx
	testl	%ebx, %ebx
	jle	.LBB108_47	# bb109
.LBB108_45:	# bb107.preheader
	leal	(%r8,%r8), %r14d
	xorl	%r15d, %r15d
	movl	$1, %r12d
	.align	16
.LBB108_46:	# bb107
	leal	(%r14,%r12), %r13d
	imull	%r15d, %r13d
	movl	%r13d, %ebp
	shrl	$31, %ebp
	addl	%r13d, %ebp
	sarl	%ebp
	leal	(%rdi,%r12), %r13d
	addl	%ebp, %r13d
	movslq	%r13d, %r13
	movslq	%r11d, %rbp
	movss	(%r10,%rbp,4), %xmm1
	mulss	(%r9,%r13,4), %xmm1
	subss	%xmm1, %xmm0
	addl	%edx, %r11d
	decl	%r12d
	incl	%r15d
	cmpl	%ebx, %r15d
	jne	.LBB108_46	# bb107
.LBB108_47:	# bb109
	cmpl	$131, 20(%rsp)
	jne	.LBB108_49	# bb112
.LBB108_48:	# bb110
	imull	%esi, %ebx
	movl	%ebx, %r11d
	shrl	$31, %r11d
	addl	%ebx, %r11d
	sarl	%r11d
	movslq	%r11d, %r11
	divss	(%r9,%r11,4), %xmm0
.LBB108_49:	# bb112
	movss	%xmm0, (%r10,%rax,4)
	addl	%edx, %eax
	decl	%esi
	incl	%edi
	cmpl	16(%rsp), %edi
	je	.LBB108_18	# bb46.thread
	jmp	.LBB108_44	# bb103
.LBB108_50:	# bb114
	cmpl	$111, %eax
	sete	%cl
	andb	%cl, %dil
	cmpl	$112, %eax
	sete	%al
	testb	%al, %r15b
	jne	.LBB108_52	# bb130
.LBB108_51:	# bb114
	notb	%dil
	testb	$1, %dil
	jne	.LBB108_65	# bb149
.LBB108_52:	# bb130
	testl	%edx, %edx
	jg	.LBB108_70	# bb130.bb133_crit_edge
.LBB108_53:	# bb131
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
	movl	%eax, 16(%rsp)
.LBB108_54:	# bb133
	leal	-1(%r8), %eax
	movl	%eax, %ecx
	imull	%edx, %ecx
	addl	16(%rsp), %ecx
	cmpl	$131, 20(%rsp)
	jne	.LBB108_56	# bb143.preheader
.LBB108_55:	# bb134
	movl	%eax, %esi
	imull	%r8d, %esi
	movl	%esi, %edi
	shrl	$31, %edi
	addl	%esi, %edi
	sarl	%edi
	addl	%eax, %edi
	movslq	%edi, %rax
	movslq	%ecx, %rcx
	movss	(%r10,%rcx,4), %xmm0
	divss	(%r9,%rax,4), %xmm0
	movss	%xmm0, (%r10,%rcx,4)
.LBB108_56:	# bb143.preheader
	leal	-1(%r8), %eax
	imull	%edx, %eax
	movl	%eax, 12(%rsp)
	leal	-2(%r8), %eax
	imull	%edx, %eax
	movl	%eax, 8(%rsp)
	movl	$1, %eax
	movl	%r8d, %ecx
	jmp	.LBB108_63	# bb143
.LBB108_57:	# bb136
	cmpl	%r8d, %esi
	movslq	%edi, %rdi
	movss	(%r10,%rdi,4), %xmm0
	jge	.LBB108_60	# bb139
.LBB108_58:	# bb.nph
	movl	16(%rsp), %r14d
	movl	12(%rsp), %ebx
	leal	(%rbx,%r14), %ebx
	leal	-2(%rcx), %r14d
	leal	-1(%rcx), %r15d
	xorl	%r12d, %r12d
	.align	16
.LBB108_59:	# bb137
	leal	(%r15,%r12), %r13d
	leal	(%rcx,%r12), %ebp
	imull	%r13d, %ebp
	movl	%ebp, %r13d
	shrl	$31, %r13d
	addl	%ebp, %r13d
	sarl	%r13d
	addl	%r14d, %r13d
	movslq	%r13d, %r13
	movslq	%ebx, %rbp
	movss	(%r10,%rbp,4), %xmm1
	mulss	(%r9,%r13,4), %xmm1
	subss	%xmm1, %xmm0
	addl	%edx, %ebx
	incl	%r12d
	cmpl	%eax, %r12d
	jne	.LBB108_59	# bb137
.LBB108_60:	# bb139
	cmpl	$131, 20(%rsp)
	jne	.LBB108_62	# bb143.backedge
.LBB108_61:	# bb140
	imull	%esi, %r11d
	movl	%r11d, %esi
	shrl	$31, %esi
	addl	%r11d, %esi
	sarl	%esi
	leal	-2(%rcx,%rsi), %esi
	movslq	%esi, %rsi
	divss	(%r9,%rsi,4), %xmm0
.LBB108_62:	# bb143.backedge
	movss	%xmm0, (%r10,%rdi,4)
	subl	%edx, 16(%rsp)
	decl	%ecx
	incl	%eax
.LBB108_63:	# bb143
	movl	16(%rsp), %esi
	movl	8(%rsp), %edi
	leal	(%rdi,%rsi), %edi
	leal	-1(%rcx), %esi
	testl	%esi, %esi
	jle	.LBB108_18	# bb46.thread
.LBB108_64:	# bb144
	leal	-2(%rcx), %r11d
	cmpl	$4294967295, %r11d
	jne	.LBB108_57	# bb136
	jmp	.LBB108_18	# bb46.thread
.LBB108_65:	# bb149
	xorl	%edi, %edi
	leaq	.str132, %rsi
	leaq	.str1133, %rdx
	xorb	%al, %al
	call	cblas_xerbla
	jmp	.LBB108_18	# bb46.thread
.LBB108_66:	# bb29.bb32_crit_edge
	xorl	%eax, %eax
	jmp	.LBB108_7	# bb32
.LBB108_67:	# bb64.bb67_crit_edge
	xorl	%eax, %eax
	jmp	.LBB108_25	# bb67
.LBB108_68:	# bb70.bb76_crit_edge
	xorl	%edi, %edi
	jmp	.LBB108_33	# bb76
.LBB108_69:	# bb97.bb100_crit_edge
	xorl	%eax, %eax
	jmp	.LBB108_40	# bb100
.LBB108_70:	# bb130.bb133_crit_edge
	movl	$0, 16(%rsp)
	jmp	.LBB108_54	# bb133
	.size	cblas_stpsv, .-cblas_stpsv
.Leh_func_end73:


	.align	16
	.globl	cblas_strmm
	.type	cblas_strmm,@function
cblas_strmm:
.Leh_func_begin74:
.Llabel74:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$24, %rsp
	cmpl	$101, %edi
	movl	112(%rsp), %eax
	movq	104(%rsp), %rdi
	movl	96(%rsp), %r10d
	movq	88(%rsp), %r11
	movl	80(%rsp), %ebx
	movl	%r8d, 16(%rsp)
	je	.LBB109_112	# bb
.LBB109_1:	# bb11
	cmpl	$121, %edx
	movl	$122, %r8d
	movl	$121, %edx
	cmove	%r8d, %edx
	cmpl	$141, %esi
	movl	$142, %r8d
	movl	$141, %esi
	cmove	%r8d, %esi
	cmpl	$113, %ecx
	movl	$112, %r8d
	cmovne	%ecx, %r8d
	movl	%r9d, 20(%rsp)
	movl	%ebx, %r9d
.LBB109_2:	# bb21
	cmpl	$121, %edx
	sete	%cl
	setne	%bl
	cmpl	$141, %esi
	sete	%r14b
	setne	%r15b
	andb	%cl, %r14b
	orb	%bl, %r15b
	testb	%r15b, %r15b
	jne	.LBB109_15	# bb40
.LBB109_3:	# bb21
	cmpl	$111, %r8d
	jne	.LBB109_15	# bb40
.LBB109_4:	# bb39.preheader
	testl	%r9d, %r9d
	jle	.LBB109_111	# bb62.thread
.LBB109_5:	# bb.nph301
	cmpl	$0, 20(%rsp)
	jle	.LBB109_111	# bb62.thread
.LBB109_6:	# bb37.preheader.preheader
	incl	%r10d
	leal	-1(%r9), %r8d
	xorl	%esi, %esi
	movl	%esi, %ecx
	movl	%esi, 8(%rsp)
	jmp	.LBB109_14	# bb37.preheader
	.align	16
.LBB109_7:	# bb30
	cmpl	$131, 16(%rsp)
	je	.LBB109_113	# bb31
.LBB109_8:	# bb32
	leal	(%rcx,%rbx), %r15d
	movslq	%r15d, %r15
	movss	(%rdi,%r15,4), %xmm1
.LBB109_9:	# bb35.preheader
	cmpl	%r9d, %r14d
	jge	.LBB109_12	# bb36
.LBB109_10:	# bb.nph296
	movl	12(%rsp), %r15d
	leal	(%r15,%rbx), %r15d
	xorl	%r12d, %r12d
	.align	16
.LBB109_11:	# bb34
	leal	(%rax,%r15), %r13d
	leal	(%rsi,%r12), %ebp
	incl	%r12d
	cmpl	%r8d, %r12d
	movslq	%ebp, %rbp
	movss	(%r11,%rbp,4), %xmm2
	movslq	%r15d, %r15
	mulss	(%rdi,%r15,4), %xmm2
	addss	%xmm2, %xmm1
	movl	%r13d, %r15d
	jne	.LBB109_11	# bb34
.LBB109_12:	# bb36
	leal	(%rcx,%rbx), %r15d
	movslq	%r15d, %r15
	mulss	%xmm0, %xmm1
	movss	%xmm1, (%rdi,%r15,4)
	incl	%ebx
	cmpl	20(%rsp), %ebx
	jne	.LBB109_7	# bb30
.LBB109_13:	# bb38
	movl	%edx, %esi
	addl	%r10d, %esi
	addl	%eax, %ecx
	decl	%r8d
	movl	8(%rsp), %ebx
	incl	%ebx
	movl	%ebx, 8(%rsp)
	cmpl	%r9d, %ebx
	je	.LBB109_111	# bb62.thread
.LBB109_14:	# bb37.preheader
	leal	(%rax,%rcx), %ebx
	movl	%ebx, 12(%rsp)
	movslq	%esi, %rdx
	incl	%esi
	movl	8(%rsp), %ebx
	leal	1(%rbx), %r14d
	xorl	%ebx, %ebx
	jmp	.LBB109_7	# bb30
.LBB109_15:	# bb40
	cmpl	$112, %r8d
	setne	%cl
	notb	%r14b
	orb	%cl, %r14b
	testb	$1, %r14b
	jne	.LBB109_29	# bb64
.LBB109_16:	# bb58.preheader
	leal	-1(%r9), %r8d
	movl	%eax, %ecx
	imull	%r8d, %ecx
	movl	$4294967295, %edx
	subl	%r10d, %edx
	movl	%edx, 8(%rsp)
	leal	1(%r10), %edx
	imull	%r8d, %edx
	movl	%edx, 12(%rsp)
	.align	16
.LBB109_17:	# bb58
	testl	%r9d, %r9d
	jle	.LBB109_111	# bb62.thread
.LBB109_18:	# bb59
	leal	-1(%r9), %r8d
	testl	%r9d, %r9d
	je	.LBB109_111	# bb62.thread
.LBB109_19:	# bb57.preheader
	cmpl	$0, 20(%rsp)
	jle	.LBB109_27	# bb58.loopexit
.LBB109_20:	# bb.nph291
	leal	-1(%r9), %esi
	movslq	12(%rsp), %r15
	xorl	%edx, %edx
	.align	16
.LBB109_21:	# bb52.preheader
	testl	%r8d, %r8d
	jle	.LBB109_28	# bb52.preheader.bb53_crit_edge
.LBB109_22:	# bb52.preheader.bb51_crit_edge
	pxor	%xmm1, %xmm1
	xorl	%r12d, %r12d
	movl	%esi, %ebx
	movl	%edx, %r14d
	.align	16
.LBB109_23:	# bb51
	leal	(%r10,%rbx), %r13d
	leal	(%rax,%r14), %ebp
	incl	%r12d
	cmpl	%esi, %r12d
	movslq	%ebx, %rbx
	movss	(%r11,%rbx,4), %xmm2
	movslq	%r14d, %rbx
	mulss	(%rdi,%rbx,4), %xmm2
	addss	%xmm2, %xmm1
	movl	%r13d, %ebx
	movl	%ebp, %r14d
	jne	.LBB109_23	# bb51
.LBB109_24:	# bb53
	cmpl	$131, 16(%rsp)
	je	.LBB109_114	# bb54
.LBB109_25:	# bb55
	leal	(%rcx,%rdx), %ebx
	movslq	%ebx, %rbx
	movss	(%rdi,%rbx,4), %xmm2
.LBB109_26:	# bb56
	addss	%xmm1, %xmm2
	mulss	%xmm0, %xmm2
	leal	(%rcx,%rdx), %ebx
	movslq	%ebx, %rbx
	movss	%xmm2, (%rdi,%rbx,4)
	incl	%edx
	cmpl	20(%rsp), %edx
	jne	.LBB109_21	# bb52.preheader
.LBB109_27:	# bb58.loopexit
	movl	12(%rsp), %r8d
	addl	8(%rsp), %r8d
	movl	%r8d, 12(%rsp)
	subl	%eax, %ecx
	decl	%r9d
	jmp	.LBB109_17	# bb58
.LBB109_28:	# bb52.preheader.bb53_crit_edge
	pxor	%xmm1, %xmm1
	jmp	.LBB109_24	# bb53
.LBB109_29:	# bb64
	cmpl	$122, %edx
	sete	%cl
	setne	%bl
	cmpl	$141, %esi
	sete	%r14b
	setne	%r15b
	andb	%cl, %r14b
	orb	%bl, %r15b
	testb	%r15b, %r15b
	jne	.LBB109_44	# bb88
.LBB109_30:	# bb64
	cmpl	$111, %r8d
	jne	.LBB109_44	# bb88
.LBB109_31:	# bb82.preheader
	leal	-1(%r9), %r8d
	movl	%eax, %ecx
	imull	%r8d, %ecx
	movl	$4294967295, %edx
	subl	%r10d, %edx
	movl	%edx, 8(%rsp)
	movl	%r10d, %edx
	imull	%r8d, %edx
	leal	1(%r10), %esi
	imull	%r8d, %esi
	movl	%esi, 12(%rsp)
	.align	16
.LBB109_32:	# bb82
	testl	%r9d, %r9d
	jle	.LBB109_111	# bb62.thread
.LBB109_33:	# bb83
	leal	-1(%r9), %r12d
	testl	%r9d, %r9d
	je	.LBB109_111	# bb62.thread
.LBB109_34:	# bb81.preheader
	cmpl	$0, 20(%rsp)
	jle	.LBB109_42	# bb82.loopexit
.LBB109_35:	# bb.nph285
	leal	-1(%r9), %r8d
	movslq	12(%rsp), %rsi
	xorl	%ebx, %ebx
	.align	16
.LBB109_36:	# bb76.preheader
	testl	%r12d, %r12d
	jle	.LBB109_43	# bb76.preheader.bb77_crit_edge
.LBB109_37:	# bb76.preheader.bb75_crit_edge
	pxor	%xmm1, %xmm1
	xorl	%r14d, %r14d
	movl	%ebx, %r15d
	.align	16
.LBB109_38:	# bb75
	leal	(%rax,%r15), %r13d
	leal	(%rdx,%r14), %ebp
	incl	%r14d
	cmpl	%r8d, %r14d
	movslq	%ebp, %rbp
	movss	(%r11,%rbp,4), %xmm2
	movslq	%r15d, %r15
	mulss	(%rdi,%r15,4), %xmm2
	addss	%xmm2, %xmm1
	movl	%r13d, %r15d
	jne	.LBB109_38	# bb75
.LBB109_39:	# bb77
	cmpl	$131, 16(%rsp)
	je	.LBB109_115	# bb78
.LBB109_40:	# bb79
	leal	(%rcx,%rbx), %r14d
	movslq	%r14d, %r14
	movss	(%rdi,%r14,4), %xmm2
.LBB109_41:	# bb80
	addss	%xmm1, %xmm2
	mulss	%xmm0, %xmm2
	leal	(%rcx,%rbx), %r14d
	movslq	%r14d, %r14
	movss	%xmm2, (%rdi,%r14,4)
	incl	%ebx
	cmpl	20(%rsp), %ebx
	jne	.LBB109_36	# bb76.preheader
.LBB109_42:	# bb82.loopexit
	movl	12(%rsp), %r8d
	addl	8(%rsp), %r8d
	movl	%r8d, 12(%rsp)
	subl	%r10d, %edx
	subl	%eax, %ecx
	decl	%r9d
	jmp	.LBB109_32	# bb82
.LBB109_43:	# bb76.preheader.bb77_crit_edge
	pxor	%xmm1, %xmm1
	jmp	.LBB109_39	# bb77
.LBB109_44:	# bb88
	cmpl	$112, %r8d
	setne	%cl
	notb	%r14b
	orb	%cl, %r14b
	testb	$1, %r14b
	jne	.LBB109_56	# bb108
.LBB109_45:	# bb107.preheader
	testl	%r9d, %r9d
	jle	.LBB109_111	# bb62.thread
.LBB109_46:	# bb.nph279
	cmpl	$0, 20(%rsp)
	jle	.LBB109_111	# bb62.thread
.LBB109_47:	# bb105.preheader.preheader
	leal	1(%r10), %r8d
	movl	%r8d, (%rsp)
	leal	-1(%r9), %ecx
	xorl	%ebx, %ebx
	movl	%ebx, %edx
	movl	%ebx, 4(%rsp)
	jmp	.LBB109_55	# bb105.preheader
	.align	16
.LBB109_48:	# bb98
	cmpl	$131, 16(%rsp)
	je	.LBB109_116	# bb99
.LBB109_49:	# bb100
	leal	(%rdx,%rbx), %r14d
	movslq	%r14d, %r14
	movss	(%rdi,%r14,4), %xmm1
.LBB109_50:	# bb103.preheader
	cmpl	%r9d, %r8d
	jge	.LBB109_53	# bb104
.LBB109_51:	# bb.nph274
	movl	8(%rsp), %r14d
	leal	(%r14,%rbx), %r14d
	xorl	%r15d, %r15d
	movl	12(%rsp), %r12d
	.align	16
.LBB109_52:	# bb102
	leal	(%r10,%r12), %r13d
	leal	(%rax,%r14), %ebp
	incl	%r15d
	cmpl	%ecx, %r15d
	movslq	%r12d, %r12
	movss	(%r11,%r12,4), %xmm2
	movslq	%r14d, %r14
	mulss	(%rdi,%r14,4), %xmm2
	addss	%xmm2, %xmm1
	movl	%r13d, %r12d
	movl	%ebp, %r14d
	jne	.LBB109_52	# bb102
.LBB109_53:	# bb104
	leal	(%rdx,%rbx), %r14d
	movslq	%r14d, %r14
	mulss	%xmm0, %xmm1
	movss	%xmm1, (%rdi,%r14,4)
	incl	%ebx
	cmpl	20(%rsp), %ebx
	jne	.LBB109_48	# bb98
.LBB109_54:	# bb106
	movl	%esi, %ebx
	addl	(%rsp), %ebx
	addl	%eax, %edx
	decl	%ecx
	movl	4(%rsp), %r8d
	incl	%r8d
	movl	%r8d, 4(%rsp)
	cmpl	%r9d, %r8d
	je	.LBB109_111	# bb62.thread
.LBB109_55:	# bb105.preheader
	leal	(%r10,%rbx), %r8d
	movl	%r8d, 12(%rsp)
	leal	(%rax,%rdx), %r8d
	movl	%r8d, 8(%rsp)
	movl	4(%rsp), %r8d
	leal	1(%r8), %r8d
	movslq	%ebx, %rsi
	xorl	%ebx, %ebx
	jmp	.LBB109_48	# bb98
.LBB109_56:	# bb108
	cmpl	$121, %edx
	sete	%cl
	setne	%bl
	cmpl	$142, %esi
	sete	%r14b
	setne	%r15b
	andb	%cl, %r14b
	orb	%bl, %r15b
	testb	%r15b, %r15b
	jne	.LBB109_71	# bb133
.LBB109_57:	# bb108
	cmpl	$111, %r8d
	jne	.LBB109_71	# bb133
.LBB109_58:	# bb132.preheader
	testl	%r9d, %r9d
	jle	.LBB109_111	# bb62.thread
.LBB109_59:	# bb132.preheader.bb125.preheader_crit_edge
	xorl	%r8d, %r8d
	movl	%r8d, 12(%rsp)
	jmp	.LBB109_70	# bb125.preheader
.LBB109_60:	# bb.nph266
	leal	-1(%rcx), %r14d
	pxor	%xmm1, %xmm1
	xorl	%r15d, %r15d
	movl	%r14d, %r12d
	.align	16
.LBB109_61:	# bb119
	leal	(%r8,%r15), %r13d
	movslq	%r13d, %r13
	movslq	%r12d, %rbp
	movss	(%r11,%rbp,4), %xmm2
	mulss	(%rdi,%r13,4), %xmm2
	addss	%xmm2, %xmm1
	addl	%r10d, %r12d
	incl	%r15d
	cmpl	%r14d, %r15d
	jne	.LBB109_61	# bb119
.LBB109_62:	# bb121
	cmpl	$131, 16(%rsp)
	je	.LBB109_117	# bb122
.LBB109_63:	# bb123
	leal	(%rbx,%rcx), %r14d
	movslq	%r14d, %r14
	movss	(%rdi,%r14,4), %xmm2
.LBB109_64:	# bb124
	addss	%xmm1, %xmm2
	mulss	%xmm0, %xmm2
	leal	(%rbx,%rcx), %r14d
	movslq	%r14d, %r14
	movss	%xmm2, (%rdi,%r14,4)
	addl	%esi, %edx
	decl	%ecx
.LBB109_65:	# bb125
	testl	%ecx, %ecx
	jle	.LBB109_69	# bb131
.LBB109_66:	# bb126
	leal	-1(%rcx), %r14d
	testl	%ecx, %ecx
	je	.LBB109_69	# bb131
.LBB109_67:	# bb120.preheader
	testl	%r14d, %r14d
	jg	.LBB109_60	# bb.nph266
.LBB109_68:	# bb120.preheader.bb121_crit_edge
	pxor	%xmm1, %xmm1
	jmp	.LBB109_62	# bb121
.LBB109_69:	# bb131
	addl	%eax, %r8d
	movl	12(%rsp), %ecx
	incl	%ecx
	movl	%ecx, 12(%rsp)
	cmpl	%r9d, %ecx
	je	.LBB109_111	# bb62.thread
.LBB109_70:	# bb125.preheader
	movl	$4294967295, %esi
	subl	%r10d, %esi
	leal	1(%r10), %ecx
	movl	20(%rsp), %r14d
	leal	-1(%r14), %edx
	imull	%ecx, %edx
	leal	-1(%r8), %ebx
	movl	%r14d, %ecx
	jmp	.LBB109_65	# bb125
.LBB109_71:	# bb133
	cmpl	$112, %r8d
	setne	%cl
	notb	%r14b
	orb	%cl, %r14b
	testb	$1, %r14b
	jne	.LBB109_83	# bb153
.LBB109_72:	# bb152.preheader
	testl	%r9d, %r9d
	jle	.LBB109_111	# bb62.thread
.LBB109_73:	# bb.nph263
	cmpl	$0, 20(%rsp)
	jle	.LBB109_111	# bb62.thread
.LBB109_74:	# bb.nph263.bb150.preheader_crit_edge
	xorl	%esi, %esi
	movl	%esi, 8(%rsp)
	jmp	.LBB109_82	# bb150.preheader
	.align	16
.LBB109_75:	# bb143
	cmpl	$131, 16(%rsp)
	je	.LBB109_118	# bb144
.LBB109_76:	# bb145
	leal	(%rsi,%rdx), %r14d
	movslq	%r14d, %r14
	movss	(%rdi,%r14,4), %xmm1
.LBB109_77:	# bb148.preheader
	leal	1(%rdx), %r14d
	cmpl	20(%rsp), %r14d
	jge	.LBB109_80	# bb149
.LBB109_78:	# bb.nph258
	leal	(%r8,%rdx), %r14d
	leal	1(%rcx), %r15d
	xorl	%r12d, %r12d
	.align	16
.LBB109_79:	# bb147
	leal	(%r14,%r12), %r13d
	movslq	%r13d, %r13
	leal	(%r15,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r11,%rbp,4), %xmm2
	mulss	(%rdi,%r13,4), %xmm2
	addss	%xmm2, %xmm1
	incl	%r12d
	cmpl	%ebx, %r12d
	jne	.LBB109_79	# bb147
.LBB109_80:	# bb149
	leal	(%rsi,%rdx), %r14d
	movslq	%r14d, %r14
	mulss	%xmm0, %xmm1
	movss	%xmm1, (%rdi,%r14,4)
	addl	12(%rsp), %ecx
	decl	%ebx
	incl	%edx
	cmpl	20(%rsp), %edx
	jne	.LBB109_75	# bb143
.LBB109_81:	# bb151
	addl	%eax, %esi
	movl	8(%rsp), %r8d
	incl	%r8d
	movl	%r8d, 8(%rsp)
	cmpl	%r9d, %r8d
	je	.LBB109_111	# bb62.thread
.LBB109_82:	# bb150.preheader
	leal	1(%r10), %r8d
	movl	%r8d, 12(%rsp)
	movl	20(%rsp), %r8d
	leal	-1(%r8), %ebx
	leal	1(%rsi), %r8d
	xorl	%ecx, %ecx
	movl	%ecx, %edx
	jmp	.LBB109_75	# bb143
.LBB109_83:	# bb153
	cmpl	$122, %edx
	sete	%cl
	setne	%dl
	cmpl	$142, %esi
	sete	%sil
	setne	%bl
	andb	%cl, %sil
	orb	%dl, %bl
	testb	%bl, %bl
	jne	.LBB109_96	# bb173
.LBB109_84:	# bb153
	cmpl	$111, %r8d
	jne	.LBB109_96	# bb173
.LBB109_85:	# bb172.preheader
	testl	%r9d, %r9d
	jle	.LBB109_111	# bb62.thread
.LBB109_86:	# bb.nph253
	cmpl	$0, 20(%rsp)
	jle	.LBB109_111	# bb62.thread
.LBB109_87:	# bb.nph253.bb170.preheader_crit_edge
	xorl	%ebx, %ebx
	movl	%ebx, 8(%rsp)
	jmp	.LBB109_95	# bb170.preheader
	.align	16
.LBB109_88:	# bb163
	cmpl	$131, 16(%rsp)
	je	.LBB109_119	# bb164
.LBB109_89:	# bb165
	leal	(%rbx,%rcx), %r14d
	movslq	%r14d, %r14
	movss	(%rdi,%r14,4), %xmm1
.LBB109_90:	# bb168.preheader
	leal	1(%rcx), %r14d
	cmpl	20(%rsp), %r14d
	jge	.LBB109_93	# bb169
.LBB109_91:	# bb.nph248
	leal	(%r10,%rsi), %r14d
	leal	(%r8,%rcx), %r15d
	xorl	%r12d, %r12d
	.align	16
.LBB109_92:	# bb167
	leal	(%r15,%r12), %r13d
	movslq	%r13d, %r13
	movslq	%r14d, %rbp
	movss	(%r11,%rbp,4), %xmm2
	mulss	(%rdi,%r13,4), %xmm2
	addss	%xmm2, %xmm1
	addl	%r10d, %r14d
	incl	%r12d
	cmpl	%edx, %r12d
	jne	.LBB109_92	# bb167
.LBB109_93:	# bb169
	leal	(%rbx,%rcx), %r14d
	movslq	%r14d, %r14
	mulss	%xmm0, %xmm1
	movss	%xmm1, (%rdi,%r14,4)
	addl	12(%rsp), %esi
	decl	%edx
	incl	%ecx
	cmpl	20(%rsp), %ecx
	jne	.LBB109_88	# bb163
.LBB109_94:	# bb171
	addl	%eax, %ebx
	movl	8(%rsp), %r8d
	incl	%r8d
	movl	%r8d, 8(%rsp)
	cmpl	%r9d, %r8d
	je	.LBB109_111	# bb62.thread
.LBB109_95:	# bb170.preheader
	leal	1(%r10), %r8d
	movl	%r8d, 12(%rsp)
	movl	20(%rsp), %r8d
	leal	-1(%r8), %edx
	leal	1(%rbx), %r8d
	xorl	%esi, %esi
	movl	%esi, %ecx
	jmp	.LBB109_88	# bb163
.LBB109_96:	# bb173
	cmpl	$112, %r8d
	setne	%cl
	notb	%sil
	orb	%cl, %sil
	testb	$1, %sil
	jne	.LBB109_110	# bb198
.LBB109_97:	# bb197.preheader
	testl	%r9d, %r9d
	jle	.LBB109_111	# bb62.thread
.LBB109_98:	# bb197.preheader.bb190.preheader_crit_edge
	xorl	%ecx, %ecx
	movl	%ecx, 12(%rsp)
	jmp	.LBB109_109	# bb190.preheader
.LBB109_99:	# bb.nph
	leal	-1(%rsi), %r15d
	pxor	%xmm1, %xmm1
	xorl	%r12d, %r12d
	.align	16
.LBB109_100:	# bb184
	leal	(%rcx,%r12), %r13d
	movslq	%r13d, %r13
	leal	(%rbx,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%r11,%rbp,4), %xmm2
	mulss	(%rdi,%r13,4), %xmm2
	addss	%xmm2, %xmm1
	incl	%r12d
	cmpl	%r15d, %r12d
	jne	.LBB109_100	# bb184
.LBB109_101:	# bb186
	cmpl	$131, 16(%rsp)
	je	.LBB109_120	# bb187
.LBB109_102:	# bb188
	leal	(%r8,%rsi), %r15d
	movslq	%r15d, %r15
	movss	(%rdi,%r15,4), %xmm2
.LBB109_103:	# bb189
	addss	%xmm1, %xmm2
	mulss	%xmm0, %xmm2
	leal	(%r8,%rsi), %r15d
	movslq	%r15d, %r15
	movss	%xmm2, (%rdi,%r15,4)
	addl	%r14d, %edx
	subl	%r10d, %ebx
	decl	%esi
.LBB109_104:	# bb190
	testl	%esi, %esi
	jle	.LBB109_108	# bb196
.LBB109_105:	# bb191
	leal	-1(%rsi), %r15d
	testl	%esi, %esi
	je	.LBB109_108	# bb196
.LBB109_106:	# bb185.preheader
	testl	%r15d, %r15d
	jg	.LBB109_99	# bb.nph
.LBB109_107:	# bb185.preheader.bb186_crit_edge
	pxor	%xmm1, %xmm1
	jmp	.LBB109_101	# bb186
.LBB109_108:	# bb196
	addl	%eax, %ecx
	movl	12(%rsp), %edx
	incl	%edx
	movl	%edx, 12(%rsp)
	cmpl	%r9d, %edx
	je	.LBB109_111	# bb62.thread
.LBB109_109:	# bb190.preheader
	movl	$4294967295, %r14d
	subl	%r10d, %r14d
	movl	20(%rsp), %esi
	leal	-1(%rsi), %r8d
	movl	%r10d, %ebx
	imull	%r8d, %ebx
	leal	1(%r10), %edx
	imull	%r8d, %edx
	leal	-1(%rcx), %r8d
	jmp	.LBB109_104	# bb190
.LBB109_110:	# bb198
	xorl	%edi, %edi
	leaq	.str134, %rsi
	leaq	.str1135, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB109_111:	# bb62.thread
	addq	$24, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB109_112:	# bb
	cmpl	$113, %ecx
	movl	$112, %r8d
	cmovne	%ecx, %r8d
	movl	%ebx, 20(%rsp)
	jmp	.LBB109_2	# bb21
.LBB109_113:	# bb31
	leal	(%rcx,%rbx), %r15d
	movslq	%r15d, %r15
	movss	(%r11,%rdx,4), %xmm1
	mulss	(%rdi,%r15,4), %xmm1
	jmp	.LBB109_9	# bb35.preheader
.LBB109_114:	# bb54
	leal	(%rcx,%rdx), %ebx
	movslq	%ebx, %rbx
	movss	(%r11,%r15,4), %xmm2
	mulss	(%rdi,%rbx,4), %xmm2
	jmp	.LBB109_26	# bb56
.LBB109_115:	# bb78
	leal	(%rcx,%rbx), %r14d
	movslq	%r14d, %r14
	movss	(%r11,%rsi,4), %xmm2
	mulss	(%rdi,%r14,4), %xmm2
	jmp	.LBB109_41	# bb80
.LBB109_116:	# bb99
	leal	(%rdx,%rbx), %r14d
	movslq	%r14d, %r14
	movss	(%r11,%rsi,4), %xmm1
	mulss	(%rdi,%r14,4), %xmm1
	jmp	.LBB109_50	# bb103.preheader
.LBB109_117:	# bb122
	leal	(%rbx,%rcx), %r14d
	movslq	%r14d, %r14
	movslq	%edx, %r15
	movss	(%r11,%r15,4), %xmm2
	mulss	(%rdi,%r14,4), %xmm2
	jmp	.LBB109_64	# bb124
.LBB109_118:	# bb144
	leal	(%rsi,%rdx), %r14d
	movslq	%r14d, %r14
	movslq	%ecx, %r15
	movss	(%r11,%r15,4), %xmm1
	mulss	(%rdi,%r14,4), %xmm1
	jmp	.LBB109_77	# bb148.preheader
.LBB109_119:	# bb164
	leal	(%rbx,%rcx), %r14d
	movslq	%r14d, %r14
	movslq	%esi, %r15
	movss	(%r11,%r15,4), %xmm1
	mulss	(%rdi,%r14,4), %xmm1
	jmp	.LBB109_90	# bb168.preheader
.LBB109_120:	# bb187
	leal	(%r8,%rsi), %r15d
	movslq	%r15d, %r15
	movslq	%edx, %r12
	movss	(%r11,%r12,4), %xmm2
	mulss	(%rdi,%r15,4), %xmm2
	jmp	.LBB109_103	# bb189
	.size	cblas_strmm, .-cblas_strmm
.Leh_func_end74:


	.align	16
	.globl	cblas_strmv
	.type	cblas_strmv,@function
cblas_strmv:
.Leh_func_begin75:
.Llabel75:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$24, %rsp
	cmpl	$113, %edx
	movl	$112, %eax
	cmovne	%edx, %eax
	cmpl	$121, %esi
	sete	%dl
	setne	%r10b
	cmpl	$101, %edi
	sete	%r11b
	setne	%bl
	andb	%dl, %r11b
	orb	%r10b, %bl
	testb	%bl, %bl
	movl	96(%rsp), %edx
	movq	88(%rsp), %r10
	movl	80(%rsp), %ebx
	movl	%ecx, 20(%rsp)
	jne	.LBB110_2	# bb24
.LBB110_1:	# entry
	cmpl	$111, %eax
	je	.LBB110_4	# bb32
.LBB110_2:	# bb24
	cmpl	$122, %esi
	sete	%cl
	setne	%r14b
	cmpl	$102, %edi
	sete	%r15b
	setne	%r12b
	andb	%cl, %r15b
	orb	%r14b, %r12b
	testb	%r12b, %r12b
	jne	.LBB110_14	# bb47
.LBB110_3:	# bb24
	cmpl	$112, %eax
	jne	.LBB110_14	# bb47
.LBB110_4:	# bb32
	testl	%edx, %edx
	jg	.LBB110_55	# bb32.bb46.preheader_crit_edge
.LBB110_5:	# bb33
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
	movl	%eax, 16(%rsp)
.LBB110_6:	# bb46.preheader
	testl	%r8d, %r8d
	jle	.LBB110_29	# bb81.thread
.LBB110_7:	# bb.nph199
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
	movl	%eax, 12(%rsp)
	leal	-1(%r8), %eax
	incl	%ebx
	xorl	%ecx, %ecx
	movl	%ecx, %esi
	movl	%ecx, %edi
	.align	16
.LBB110_8:	# bb36
	testl	%edx, %edx
	movl	$0, %r11d
	cmovle	12(%rsp), %r11d
	movl	16(%rsp), %r14d
	leal	(%r14,%rcx), %r14d
	leal	1(%rdi), %r15d
	cmpl	%r8d, %r15d
	jge	.LBB110_56	# bb36.bb42_crit_edge
.LBB110_9:	# bb.nph195
	leal	(%rdx,%rcx), %r15d
	leal	1(%rsi), %r12d
	pxor	%xmm0, %xmm0
	xorl	%r13d, %r13d
	.align	16
.LBB110_10:	# bb40
	leal	(%r12,%r13), %ebp
	movslq	%ebp, %rbp
	addl	%r11d, %r15d
	movslq	%r15d, %r11
	movss	(%r10,%r11,4), %xmm1
	mulss	(%r9,%rbp,4), %xmm1
	addss	%xmm1, %xmm0
	incl	%r13d
	cmpl	%eax, %r13d
	movl	%edx, %r11d
	jne	.LBB110_10	# bb40
.LBB110_11:	# bb42
	movslq	%r14d, %r11
	movss	(%r10,%r11,4), %xmm1
	cmpl	$131, 20(%rsp)
	jne	.LBB110_13	# bb45
.LBB110_12:	# bb43
	movslq	%esi, %r14
	mulss	(%r9,%r14,4), %xmm1
.LBB110_13:	# bb45
	addss	%xmm0, %xmm1
	movss	%xmm1, (%r10,%r11,4)
	addl	%edx, %ecx
	addl	%ebx, %esi
	decl	%eax
	incl	%edi
	cmpl	%r8d, %edi
	jne	.LBB110_8	# bb36
	jmp	.LBB110_29	# bb81.thread
.LBB110_14:	# bb47
	cmpl	$122, %esi
	sete	%cl
	setne	%r14b
	cmpl	$101, %edi
	sete	%r12b
	setne	%r13b
	andb	%cl, %r12b
	orb	%r14b, %r13b
	testb	%r13b, %r13b
	jne	.LBB110_16	# bb55
.LBB110_15:	# bb47
	cmpl	$111, %eax
	je	.LBB110_18	# bb63
.LBB110_16:	# bb55
	cmpl	$121, %esi
	sete	%cl
	setne	%sil
	cmpl	$102, %edi
	sete	%dil
	setne	%r14b
	andb	%cl, %dil
	orb	%sil, %r14b
	testb	%r14b, %r14b
	jne	.LBB110_30	# bb83
.LBB110_17:	# bb55
	cmpl	$112, %eax
	jne	.LBB110_30	# bb83
.LBB110_18:	# bb63
	testl	%edx, %edx
	jg	.LBB110_57	# bb63.bb66_crit_edge
.LBB110_19:	# bb64
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
.LBB110_20:	# bb66
	movl	$1, %ecx
	subl	%r8d, %ecx
	imull	%edx, %ecx
	leal	-1(%r8), %esi
	movl	%edx, %edi
	imull	%esi, %edi
	movl	$4294967295, %r11d
	subl	%ebx, %r11d
	movl	%ebx, %r14d
	imull	%esi, %r14d
	leal	1(%rbx), %r15d
	imull	%esi, %r15d
	addl	%eax, %edi
	jmp	.LBB110_27	# bb77
.LBB110_21:	# bb67
	testl	%edx, %edx
	movl	$0, %esi
	cmovle	%ecx, %esi
	testl	%eax, %eax
	jle	.LBB110_58	# bb67.bb73_crit_edge
.LBB110_22:	# bb.nph187
	leal	-1(%r8), %eax
	pxor	%xmm0, %xmm0
	xorl	%r12d, %r12d
	.align	16
.LBB110_23:	# bb71
	leal	(%r14,%r12), %r13d
	movslq	%r13d, %r13
	movslq	%esi, %rbp
	movss	(%r10,%rbp,4), %xmm1
	mulss	(%r9,%r13,4), %xmm1
	addss	%xmm1, %xmm0
	addl	%edx, %esi
	incl	%r12d
	cmpl	%eax, %r12d
	jne	.LBB110_23	# bb71
.LBB110_24:	# bb73
	movslq	%edi, %rax
	movss	(%r10,%rax,4), %xmm1
	cmpl	$131, 20(%rsp)
	jne	.LBB110_26	# bb76
.LBB110_25:	# bb74
	movslq	%r15d, %rsi
	mulss	(%r9,%rsi,4), %xmm1
.LBB110_26:	# bb76
	addss	%xmm0, %xmm1
	movss	%xmm1, (%r10,%rax,4)
	addl	%r11d, %r15d
	subl	%edx, %edi
	subl	%ebx, %r14d
	decl	%r8d
.LBB110_27:	# bb77
	testl	%r8d, %r8d
	jle	.LBB110_29	# bb81.thread
.LBB110_28:	# bb78
	leal	-1(%r8), %eax
	testl	%r8d, %r8d
	jne	.LBB110_21	# bb67
.LBB110_29:	# bb81.thread
	addq	$24, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB110_30:	# bb83
	cmpl	$111, %eax
	sete	%cl
	andb	%cl, %r15b
	cmpl	$112, %eax
	sete	%cl
	testb	%cl, %r11b
	jne	.LBB110_32	# bb99
.LBB110_31:	# bb83
	notb	%r15b
	testb	$1, %r15b
	jne	.LBB110_43	# bb119
.LBB110_32:	# bb99
	testl	%edx, %edx
	jg	.LBB110_59	# bb99.bb102_crit_edge
.LBB110_33:	# bb100
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
.LBB110_34:	# bb102
	movl	$1, %ecx
	subl	%r8d, %ecx
	imull	%edx, %ecx
	leal	-1(%r8), %esi
	movl	%edx, %edi
	imull	%esi, %edi
	movl	$4294967295, %r11d
	subl	%ebx, %r11d
	leal	1(%rbx), %r14d
	imull	%esi, %r14d
	addl	%eax, %edi
	jmp	.LBB110_41	# bb113
.LBB110_35:	# bb103
	testl	%edx, %edx
	movl	$0, %esi
	cmovle	%ecx, %esi
	testl	%eax, %eax
	jle	.LBB110_60	# bb103.bb109_crit_edge
.LBB110_36:	# bb.nph182
	leal	-1(%r8), %eax
	pxor	%xmm0, %xmm0
	xorl	%r15d, %r15d
	movl	%eax, %r12d
	.align	16
.LBB110_37:	# bb107
	leal	(%rdx,%rsi), %r13d
	leal	(%rbx,%r12), %ebp
	incl	%r15d
	cmpl	%eax, %r15d
	movslq	%esi, %rsi
	movss	(%r10,%rsi,4), %xmm1
	movslq	%r12d, %rsi
	mulss	(%r9,%rsi,4), %xmm1
	addss	%xmm1, %xmm0
	movl	%r13d, %esi
	movl	%ebp, %r12d
	jne	.LBB110_37	# bb107
.LBB110_38:	# bb109
	movslq	%edi, %rax
	movss	(%r10,%rax,4), %xmm1
	cmpl	$131, 20(%rsp)
	jne	.LBB110_40	# bb112
.LBB110_39:	# bb110
	movslq	%r14d, %rsi
	mulss	(%r9,%rsi,4), %xmm1
.LBB110_40:	# bb112
	addss	%xmm0, %xmm1
	movss	%xmm1, (%r10,%rax,4)
	addl	%r11d, %r14d
	subl	%edx, %edi
	decl	%r8d
.LBB110_41:	# bb113
	testl	%r8d, %r8d
	jle	.LBB110_29	# bb81.thread
.LBB110_42:	# bb114
	leal	-1(%r8), %eax
	testl	%r8d, %r8d
	jne	.LBB110_35	# bb103
	jmp	.LBB110_29	# bb81.thread
.LBB110_43:	# bb119
	cmpl	$111, %eax
	sete	%cl
	andb	%cl, %dil
	cmpl	$112, %eax
	sete	%al
	testb	%al, %r12b
	jne	.LBB110_45	# bb135
.LBB110_44:	# bb119
	notb	%dil
	testb	$1, %dil
	jne	.LBB110_63	# bb150
.LBB110_45:	# bb135
	testl	%edx, %edx
	jg	.LBB110_61	# bb135.bb149.preheader_crit_edge
.LBB110_46:	# bb136
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
	movl	%eax, 12(%rsp)
.LBB110_47:	# bb149.preheader
	testl	%r8d, %r8d
	jle	.LBB110_29	# bb81.thread
.LBB110_48:	# bb.nph178
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
	movl	%eax, 8(%rsp)
	leal	-1(%r8), %eax
	leal	1(%rbx), %ecx
	movl	%ecx, 16(%rsp)
	xorl	%ecx, %ecx
	movl	%ecx, %esi
	movl	%ecx, %edi
	.align	16
.LBB110_49:	# bb139
	testl	%edx, %edx
	movl	$0, %r11d
	cmovle	8(%rsp), %r11d
	movl	12(%rsp), %r14d
	leal	(%r14,%rcx), %r14d
	leal	1(%rdi), %r15d
	cmpl	%r8d, %r15d
	jge	.LBB110_62	# bb139.bb145_crit_edge
.LBB110_50:	# bb.nph
	leal	(%rdx,%rcx), %r15d
	leal	(%rbx,%rsi), %r12d
	pxor	%xmm0, %xmm0
	xorl	%r13d, %r13d
	.align	16
.LBB110_51:	# bb143
	leal	(%rbx,%r12), %ebp
	addl	%r11d, %r15d
	incl	%r13d
	cmpl	%eax, %r13d
	movslq	%r15d, %r11
	movss	(%r10,%r11,4), %xmm1
	movslq	%r12d, %r11
	mulss	(%r9,%r11,4), %xmm1
	addss	%xmm1, %xmm0
	movl	%ebp, %r12d
	movl	%edx, %r11d
	jne	.LBB110_51	# bb143
.LBB110_52:	# bb145
	movslq	%r14d, %r11
	movss	(%r10,%r11,4), %xmm1
	cmpl	$131, 20(%rsp)
	jne	.LBB110_54	# bb148
.LBB110_53:	# bb146
	movslq	%esi, %r14
	mulss	(%r9,%r14,4), %xmm1
.LBB110_54:	# bb148
	addss	%xmm0, %xmm1
	movss	%xmm1, (%r10,%r11,4)
	addl	%edx, %ecx
	addl	16(%rsp), %esi
	decl	%eax
	incl	%edi
	cmpl	%r8d, %edi
	je	.LBB110_29	# bb81.thread
	jmp	.LBB110_49	# bb139
.LBB110_55:	# bb32.bb46.preheader_crit_edge
	movl	$0, 16(%rsp)
	jmp	.LBB110_6	# bb46.preheader
.LBB110_56:	# bb36.bb42_crit_edge
	pxor	%xmm0, %xmm0
	jmp	.LBB110_11	# bb42
.LBB110_57:	# bb63.bb66_crit_edge
	xorl	%eax, %eax
	jmp	.LBB110_20	# bb66
.LBB110_58:	# bb67.bb73_crit_edge
	pxor	%xmm0, %xmm0
	jmp	.LBB110_24	# bb73
.LBB110_59:	# bb99.bb102_crit_edge
	xorl	%eax, %eax
	jmp	.LBB110_34	# bb102
.LBB110_60:	# bb103.bb109_crit_edge
	pxor	%xmm0, %xmm0
	jmp	.LBB110_38	# bb109
.LBB110_61:	# bb135.bb149.preheader_crit_edge
	movl	$0, 12(%rsp)
	jmp	.LBB110_47	# bb149.preheader
.LBB110_62:	# bb139.bb145_crit_edge
	pxor	%xmm0, %xmm0
	jmp	.LBB110_52	# bb145
.LBB110_63:	# bb150
	xorl	%edi, %edi
	leaq	.str136, %rsi
	leaq	.str1137, %rdx
	xorb	%al, %al
	call	cblas_xerbla
	jmp	.LBB110_29	# bb81.thread
	.size	cblas_strmv, .-cblas_strmv
.Leh_func_end75:


	.section	.rodata.cst4,"aM",@progbits,4
	.align	4
.LCPI111_0:					
	.long	1065353216	# float 1.000000e+00
	.text
	.align	16
	.globl	cblas_strsm
	.type	cblas_strsm,@function
cblas_strsm:
.Leh_func_begin76:
.Llabel76:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$8, %rsp
	cmpl	$101, %edi
	movq	88(%rsp), %rax
	movl	80(%rsp), %edi
	movq	72(%rsp), %r10
	movl	64(%rsp), %r11d
	movl	%r8d, 4(%rsp)
	je	.LBB111_170	# bb
.LBB111_1:	# bb15
	cmpl	$121, %edx
	movl	$122, %r8d
	movl	$121, %edx
	cmove	%r8d, %edx
	cmpl	$141, %esi
	movl	$142, %r8d
	movl	$141, %esi
	cmove	%r8d, %esi
	cmpl	$113, %ecx
	movl	$112, %r8d
	cmovne	%ecx, %r8d
	movl	%r9d, %ecx
	movl	%r11d, %r9d
.LBB111_2:	# bb25
	cmpl	$121, %edx
	sete	%r11b
	setne	%bl
	cmpl	$111, %r8d
	sete	%r14b
	setne	%r15b
	andb	%r11b, %r14b
	orb	%bl, %r15b
	testb	%r15b, %r15b
	jne	.LBB111_25	# bb56
.LBB111_3:	# bb25
	cmpl	$141, %esi
	jne	.LBB111_25	# bb56
.LBB111_4:	# bb32
	ucomiss	.LCPI111_0(%rip), %xmm0
	setnp	%r14b
	sete	%dl
	testb	%r14b, %dl
	jne	.LBB111_171	# bb50.preheader
.LBB111_5:	# bb32
	testl	%r9d, %r9d
	jle	.LBB111_171	# bb50.preheader
.LBB111_6:	# bb32
	testl	%ecx, %ecx
	jle	.LBB111_171	# bb50.preheader
.LBB111_7:	# bb32.bb36.preheader_crit_edge
	xorl	%r8d, %r8d
	movl	%r8d, %edx
	jmp	.LBB111_10	# bb36.preheader
	.align	16
.LBB111_8:	# bb35
	leal	(%r8,%rsi), %r11d
	movslq	%r11d, %r11
	movaps	%xmm0, %xmm1
	mulss	(%rax,%r11,4), %xmm1
	movss	%xmm1, (%rax,%r11,4)
	incl	%esi
	cmpl	%ecx, %esi
	jne	.LBB111_8	# bb35
.LBB111_9:	# bb37
	addl	96(%rsp), %r8d
	incl	%edx
	cmpl	%r9d, %edx
	je	.LBB111_171	# bb50.preheader
.LBB111_10:	# bb36.preheader
	xorl	%esi, %esi
	jmp	.LBB111_8	# bb35
.LBB111_11:	# bb40
	cmpl	$131, 4(%rsp)
	jne	.LBB111_15	# bb49.preheader
.LBB111_12:	# bb41
	movslq	%edx, %r11
	movss	(%r10,%r11,4), %xmm0
	testl	%ecx, %ecx
	jle	.LBB111_15	# bb49.preheader
.LBB111_13:	# bb41.bb42_crit_edge
	xorl	%r11d, %r11d
	.align	16
.LBB111_14:	# bb42
	leal	(%r8,%r11), %ebx
	movslq	%ebx, %rbx
	movss	(%rax,%rbx,4), %xmm1
	divss	%xmm0, %xmm1
	movss	%xmm1, (%rax,%rbx,4)
	incl	%r11d
	cmpl	%ecx, %r11d
	jne	.LBB111_14	# bb42
.LBB111_15:	# bb49.preheader
	testl	%esi, %esi
	jle	.LBB111_21	# bb50.loopexit289
.LBB111_16:	# bb.nph378
	testl	%ecx, %ecx
	jle	.LBB111_21	# bb50.loopexit289
.LBB111_17:	# bb.nph378.split
	leal	-1(%r9), %esi
	xorl	%r11d, %r11d
	movl	%esi, %ebx
	movl	%r11d, %r14d
	.align	16
.LBB111_18:	# bb45
	movslq	%ebx, %r15
	movss	(%r10,%r15,4), %xmm0
	xorl	%r15d, %r15d
	.align	16
.LBB111_19:	# bb46
	leal	(%r8,%r15), %r12d
	movslq	%r12d, %r12
	movaps	%xmm0, %xmm1
	mulss	(%rax,%r12,4), %xmm1
	leal	(%r11,%r15), %r12d
	movslq	%r12d, %r12
	movss	(%rax,%r12,4), %xmm2
	subss	%xmm1, %xmm2
	movss	%xmm2, (%rax,%r12,4)
	incl	%r15d
	cmpl	%ecx, %r15d
	jne	.LBB111_19	# bb46
.LBB111_20:	# bb48
	addl	%edi, %ebx
	addl	96(%rsp), %r11d
	incl	%r14d
	cmpl	%esi, %r14d
	jne	.LBB111_18	# bb45
.LBB111_21:	# bb50.loopexit289
	addl	(%rsp), %edx
	subl	96(%rsp), %r8d
	decl	%r9d
.LBB111_22:	# bb50
	testl	%r9d, %r9d
	jle	.LBB111_24	# bb54.thread
.LBB111_23:	# bb51
	leal	-1(%r9), %esi
	testl	%r9d, %r9d
	jne	.LBB111_11	# bb40
.LBB111_24:	# bb54.thread
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB111_25:	# bb56
	cmpl	$121, %edx
	sete	%r11b
	setne	%bl
	cmpl	$112, %r8d
	sete	%r15b
	setne	%r12b
	andb	%r11b, %r15b
	orb	%bl, %r12b
	testb	%r12b, %r12b
	jne	.LBB111_47	# bb84
.LBB111_26:	# bb56
	cmpl	$141, %esi
	jne	.LBB111_47	# bb84
.LBB111_27:	# bb64
	ucomiss	.LCPI111_0(%rip), %xmm0
	setnp	%r14b
	sete	%r15b
	testb	%r14b, %r15b
	jne	.LBB111_34	# bb83.preheader
.LBB111_28:	# bb64
	testl	%r9d, %r9d
	jle	.LBB111_34	# bb83.preheader
.LBB111_29:	# bb64
	testl	%ecx, %ecx
	jle	.LBB111_34	# bb83.preheader
.LBB111_30:	# bb64.bb68.preheader_crit_edge
	xorl	%r8d, %r8d
	movl	%r8d, %edx
	.align	16
.LBB111_31:	# bb68.preheader
	xorl	%esi, %esi
	.align	16
.LBB111_32:	# bb67
	leal	(%r8,%rsi), %r11d
	movslq	%r11d, %r11
	movaps	%xmm0, %xmm1
	mulss	(%rax,%r11,4), %xmm1
	movss	%xmm1, (%rax,%r11,4)
	incl	%esi
	cmpl	%ecx, %esi
	jne	.LBB111_32	# bb67
.LBB111_33:	# bb69
	addl	96(%rsp), %r8d
	incl	%edx
	cmpl	%r9d, %edx
	jne	.LBB111_31	# bb68.preheader
.LBB111_34:	# bb83.preheader
	testl	%r9d, %r9d
	jle	.LBB111_24	# bb54.thread
.LBB111_35:	# bb.nph368
	leal	-1(%r9), %r8d
	incl	%edi
	xorl	%edx, %edx
	movl	%edx, %esi
	movl	%edx, %r11d
	.align	16
.LBB111_36:	# bb72
	cmpl	$131, 4(%rsp)
	jne	.LBB111_40	# bb81.preheader
.LBB111_37:	# bb73
	movslq	%edx, %rbx
	movss	(%r10,%rbx,4), %xmm0
	testl	%ecx, %ecx
	jle	.LBB111_40	# bb81.preheader
.LBB111_38:	# bb73.bb74_crit_edge
	xorl	%ebx, %ebx
	.align	16
.LBB111_39:	# bb74
	leal	(%rsi,%rbx), %r14d
	movslq	%r14d, %r14
	movss	(%rax,%r14,4), %xmm1
	divss	%xmm0, %xmm1
	movss	%xmm1, (%rax,%r14,4)
	incl	%ebx
	cmpl	%ecx, %ebx
	jne	.LBB111_39	# bb74
.LBB111_40:	# bb81.preheader
	leal	1(%r11), %ebx
	cmpl	%r9d, %ebx
	jge	.LBB111_46	# bb82
.LBB111_41:	# bb.nph366
	testl	%ecx, %ecx
	jle	.LBB111_46	# bb82
.LBB111_42:	# bb.nph366.split
	movl	96(%rsp), %ebx
	leal	(%rbx,%rsi), %ebx
	leal	1(%rdx), %r14d
	xorl	%r15d, %r15d
	.align	16
.LBB111_43:	# bb77
	leal	(%r14,%r15), %r12d
	movslq	%r12d, %r12
	movss	(%r10,%r12,4), %xmm0
	xorl	%r12d, %r12d
	.align	16
.LBB111_44:	# bb78
	leal	(%rsi,%r12), %r13d
	movslq	%r13d, %r13
	movaps	%xmm0, %xmm1
	mulss	(%rax,%r13,4), %xmm1
	leal	(%rbx,%r12), %r13d
	movslq	%r13d, %r13
	movss	(%rax,%r13,4), %xmm2
	subss	%xmm1, %xmm2
	movss	%xmm2, (%rax,%r13,4)
	incl	%r12d
	cmpl	%ecx, %r12d
	jne	.LBB111_44	# bb78
.LBB111_45:	# bb81.loopexit
	addl	96(%rsp), %ebx
	incl	%r15d
	cmpl	%r8d, %r15d
	jne	.LBB111_43	# bb77
.LBB111_46:	# bb82
	addl	%edi, %edx
	addl	96(%rsp), %esi
	decl	%r8d
	incl	%r11d
	cmpl	%r9d, %r11d
	je	.LBB111_24	# bb54.thread
	jmp	.LBB111_36	# bb72
.LBB111_47:	# bb84
	cmpl	$122, %edx
	sete	%r11b
	setne	%bl
	cmpl	$111, %r8d
	sete	%r12b
	setne	%r13b
	andb	%r11b, %r12b
	orb	%bl, %r13b
	testb	%r13b, %r13b
	jne	.LBB111_69	# bb112
.LBB111_48:	# bb84
	cmpl	$141, %esi
	jne	.LBB111_69	# bb112
.LBB111_49:	# bb92
	ucomiss	.LCPI111_0(%rip), %xmm0
	setnp	%r14b
	sete	%r15b
	testb	%r14b, %r15b
	jne	.LBB111_56	# bb111.preheader
.LBB111_50:	# bb92
	testl	%r9d, %r9d
	jle	.LBB111_56	# bb111.preheader
.LBB111_51:	# bb92
	testl	%ecx, %ecx
	jle	.LBB111_56	# bb111.preheader
.LBB111_52:	# bb92.bb96.preheader_crit_edge
	xorl	%r8d, %r8d
	movl	%r8d, %edx
	.align	16
.LBB111_53:	# bb96.preheader
	xorl	%esi, %esi
	.align	16
.LBB111_54:	# bb95
	leal	(%r8,%rsi), %r11d
	movslq	%r11d, %r11
	movaps	%xmm0, %xmm1
	mulss	(%rax,%r11,4), %xmm1
	movss	%xmm1, (%rax,%r11,4)
	incl	%esi
	cmpl	%ecx, %esi
	jne	.LBB111_54	# bb95
.LBB111_55:	# bb97
	addl	96(%rsp), %r8d
	incl	%edx
	cmpl	%r9d, %edx
	jne	.LBB111_53	# bb96.preheader
.LBB111_56:	# bb111.preheader
	testl	%r9d, %r9d
	jle	.LBB111_24	# bb54.thread
.LBB111_57:	# bb.nph354
	leal	-1(%r9), %r8d
	leal	1(%rdi), %edx
	movl	%edx, (%rsp)
	xorl	%edx, %edx
	movl	%edx, %esi
	movl	%edx, %r11d
	.align	16
.LBB111_58:	# bb100
	cmpl	$131, 4(%rsp)
	jne	.LBB111_62	# bb109.preheader
.LBB111_59:	# bb101
	movslq	%edx, %rbx
	movss	(%r10,%rbx,4), %xmm0
	testl	%ecx, %ecx
	jle	.LBB111_62	# bb109.preheader
.LBB111_60:	# bb101.bb102_crit_edge
	xorl	%ebx, %ebx
	.align	16
.LBB111_61:	# bb102
	leal	(%rsi,%rbx), %r14d
	movslq	%r14d, %r14
	movss	(%rax,%r14,4), %xmm1
	divss	%xmm0, %xmm1
	movss	%xmm1, (%rax,%r14,4)
	incl	%ebx
	cmpl	%ecx, %ebx
	jne	.LBB111_61	# bb102
.LBB111_62:	# bb109.preheader
	leal	1(%r11), %ebx
	cmpl	%r9d, %ebx
	jge	.LBB111_68	# bb110
.LBB111_63:	# bb.nph352
	testl	%ecx, %ecx
	jle	.LBB111_68	# bb110
.LBB111_64:	# bb.nph352.split
	leal	(%rdi,%rdx), %ebx
	movl	96(%rsp), %r14d
	leal	(%r14,%rsi), %r14d
	xorl	%r15d, %r15d
	.align	16
.LBB111_65:	# bb105
	movslq	%ebx, %r12
	movss	(%r10,%r12,4), %xmm0
	xorl	%r12d, %r12d
	.align	16
.LBB111_66:	# bb106
	leal	(%rsi,%r12), %r13d
	movslq	%r13d, %r13
	movaps	%xmm0, %xmm1
	mulss	(%rax,%r13,4), %xmm1
	leal	(%r14,%r12), %r13d
	movslq	%r13d, %r13
	movss	(%rax,%r13,4), %xmm2
	subss	%xmm1, %xmm2
	movss	%xmm2, (%rax,%r13,4)
	incl	%r12d
	cmpl	%ecx, %r12d
	jne	.LBB111_66	# bb106
.LBB111_67:	# bb109.loopexit
	addl	%edi, %ebx
	addl	96(%rsp), %r14d
	incl	%r15d
	cmpl	%r8d, %r15d
	jne	.LBB111_65	# bb105
.LBB111_68:	# bb110
	addl	(%rsp), %edx
	addl	96(%rsp), %esi
	decl	%r8d
	incl	%r11d
	cmpl	%r9d, %r11d
	je	.LBB111_24	# bb54.thread
	jmp	.LBB111_58	# bb100
.LBB111_69:	# bb112
	cmpl	$122, %edx
	sete	%dl
	setne	%r11b
	cmpl	$112, %r8d
	sete	%r8b
	setne	%bl
	andb	%dl, %r8b
	orb	%r11b, %bl
	testb	%bl, %bl
	jne	.LBB111_91	# bb144
.LBB111_70:	# bb112
	cmpl	$141, %esi
	jne	.LBB111_91	# bb144
.LBB111_71:	# bb120
	ucomiss	.LCPI111_0(%rip), %xmm0
	setnp	%r8b
	sete	%r14b
	testb	%r8b, %r14b
	jne	.LBB111_172	# bb138.preheader
.LBB111_72:	# bb120
	testl	%r9d, %r9d
	jle	.LBB111_172	# bb138.preheader
.LBB111_73:	# bb120
	testl	%ecx, %ecx
	jle	.LBB111_172	# bb138.preheader
.LBB111_74:	# bb120.bb124.preheader_crit_edge
	xorl	%edx, %edx
	movl	%edx, %esi
	jmp	.LBB111_77	# bb124.preheader
	.align	16
.LBB111_75:	# bb123
	leal	(%rdx,%r8), %r11d
	movslq	%r11d, %r11
	movaps	%xmm0, %xmm1
	mulss	(%rax,%r11,4), %xmm1
	movss	%xmm1, (%rax,%r11,4)
	incl	%r8d
	cmpl	%ecx, %r8d
	jne	.LBB111_75	# bb123
.LBB111_76:	# bb125
	addl	96(%rsp), %edx
	incl	%esi
	cmpl	%r9d, %esi
	je	.LBB111_172	# bb138.preheader
.LBB111_77:	# bb124.preheader
	xorl	%r8d, %r8d
	jmp	.LBB111_75	# bb123
.LBB111_78:	# bb128
	cmpl	$131, 4(%rsp)
	jne	.LBB111_82	# bb137.preheader
.LBB111_79:	# bb129
	movslq	%edx, %r14
	movss	(%r10,%r14,4), %xmm0
	testl	%ecx, %ecx
	jle	.LBB111_82	# bb137.preheader
.LBB111_80:	# bb129.bb130_crit_edge
	xorl	%r14d, %r14d
	.align	16
.LBB111_81:	# bb130
	leal	(%r11,%r14), %r15d
	movslq	%r15d, %r15
	movss	(%rax,%r15,4), %xmm1
	divss	%xmm0, %xmm1
	movss	%xmm1, (%rax,%r15,4)
	incl	%r14d
	cmpl	%ecx, %r14d
	jne	.LBB111_81	# bb130
.LBB111_82:	# bb137.preheader
	testl	%esi, %esi
	jle	.LBB111_88	# bb138.loopexit291
.LBB111_83:	# bb.nph340
	testl	%ecx, %ecx
	jle	.LBB111_88	# bb138.loopexit291
.LBB111_84:	# bb.nph340.split
	leal	-1(%r9), %esi
	xorl	%r14d, %r14d
	movl	%r14d, %r15d
	.align	16
.LBB111_85:	# bb133
	leal	(%r8,%r15), %r12d
	movslq	%r12d, %r12
	movss	(%r10,%r12,4), %xmm0
	xorl	%r12d, %r12d
	.align	16
.LBB111_86:	# bb134
	leal	(%r11,%r12), %r13d
	movslq	%r13d, %r13
	movaps	%xmm0, %xmm1
	mulss	(%rax,%r13,4), %xmm1
	leal	(%r14,%r12), %r13d
	movslq	%r13d, %r13
	movss	(%rax,%r13,4), %xmm2
	subss	%xmm1, %xmm2
	movss	%xmm2, (%rax,%r13,4)
	incl	%r12d
	cmpl	%ecx, %r12d
	jne	.LBB111_86	# bb134
.LBB111_87:	# bb136
	addl	96(%rsp), %r14d
	incl	%r15d
	cmpl	%esi, %r15d
	jne	.LBB111_85	# bb133
.LBB111_88:	# bb138.loopexit291
	addl	%ebx, %edx
	subl	%edi, %r8d
	subl	96(%rsp), %r11d
	decl	%r9d
.LBB111_89:	# bb138
	testl	%r9d, %r9d
	jle	.LBB111_24	# bb54.thread
.LBB111_90:	# bb139
	leal	-1(%r9), %esi
	testl	%r9d, %r9d
	jne	.LBB111_78	# bb128
	jmp	.LBB111_24	# bb54.thread
.LBB111_91:	# bb144
	cmpl	$142, %esi
	setne	%dl
	notb	%r14b
	orb	%dl, %r14b
	testb	$1, %r14b
	jne	.LBB111_110	# bb170
.LBB111_92:	# bb152
	ucomiss	.LCPI111_0(%rip), %xmm0
	setnp	%r8b
	sete	%r15b
	testb	%r8b, %r15b
	jne	.LBB111_106	# bb169.preheader
.LBB111_93:	# bb152
	testl	%r9d, %r9d
	jle	.LBB111_106	# bb169.preheader
.LBB111_94:	# bb152
	testl	%ecx, %ecx
	jle	.LBB111_106	# bb169.preheader
.LBB111_95:	# bb152.bb156.preheader_crit_edge
	xorl	%edx, %edx
	movl	%edx, %esi
	jmp	.LBB111_98	# bb156.preheader
	.align	16
.LBB111_96:	# bb155
	leal	(%rdx,%r8), %r11d
	movslq	%r11d, %r11
	movaps	%xmm0, %xmm1
	mulss	(%rax,%r11,4), %xmm1
	movss	%xmm1, (%rax,%r11,4)
	incl	%r8d
	cmpl	%ecx, %r8d
	jne	.LBB111_96	# bb155
.LBB111_97:	# bb157
	addl	96(%rsp), %edx
	incl	%esi
	cmpl	%r9d, %esi
	je	.LBB111_106	# bb169.preheader
.LBB111_98:	# bb156.preheader
	xorl	%r8d, %r8d
	jmp	.LBB111_96	# bb155
	.align	16
.LBB111_99:	# bb161
	cmpl	$131, 4(%rsp)
	jne	.LBB111_101	# bb163
.LBB111_100:	# bb162
	leal	(%rsi,%r11), %r15d
	movslq	%r15d, %r15
	movss	(%rax,%r15,4), %xmm0
	movslq	%r8d, %r12
	divss	(%r10,%r12,4), %xmm0
	movss	%xmm0, (%rax,%r15,4)
.LBB111_101:	# bb163
	leal	(%rsi,%r11), %r15d
	leal	1(%r11), %r12d
	cmpl	%ecx, %r12d
	movslq	%r15d, %r15
	movss	(%rax,%r15,4), %xmm0
	jge	.LBB111_104	# bb166
.LBB111_102:	# bb.nph326
	leal	(%r14,%r11), %r15d
	leal	1(%r8), %r12d
	xorl	%r13d, %r13d
	.align	16
.LBB111_103:	# bb164
	leal	(%r12,%r13), %ebp
	movslq	%ebp, %rbp
	movaps	%xmm0, %xmm1
	mulss	(%r10,%rbp,4), %xmm1
	leal	(%r15,%r13), %ebp
	movslq	%ebp, %rbp
	movss	(%rax,%rbp,4), %xmm2
	subss	%xmm1, %xmm2
	movss	%xmm2, (%rax,%rbp,4)
	incl	%r13d
	cmpl	%ebx, %r13d
	jne	.LBB111_103	# bb164
.LBB111_104:	# bb166
	addl	%edx, %r8d
	decl	%ebx
	incl	%r11d
	cmpl	%ecx, %r11d
	jne	.LBB111_99	# bb161
.LBB111_105:	# bb168
	addl	96(%rsp), %esi
	movl	(%rsp), %edx
	incl	%edx
	movl	%edx, (%rsp)
	cmpl	%r9d, %edx
	je	.LBB111_24	# bb54.thread
	jmp	.LBB111_109	# bb167.preheader
.LBB111_106:	# bb169.preheader
	testl	%r9d, %r9d
	jle	.LBB111_24	# bb54.thread
.LBB111_107:	# bb.nph330
	testl	%ecx, %ecx
	jle	.LBB111_24	# bb54.thread
.LBB111_108:	# bb.nph330.bb167.preheader_crit_edge
	xorl	%esi, %esi
	movl	%esi, (%rsp)
	.align	16
.LBB111_109:	# bb167.preheader
	leal	1(%rdi), %edx
	leal	-1(%rcx), %ebx
	leal	1(%rsi), %r14d
	xorl	%r8d, %r8d
	movl	%r8d, %r11d
	jmp	.LBB111_99	# bb161
.LBB111_110:	# bb170
	cmpl	$142, %esi
	setne	%dl
	notb	%r15b
	orb	%dl, %r15b
	testb	$1, %r15b
	jne	.LBB111_130	# bb200
.LBB111_111:	# bb178
	ucomiss	.LCPI111_0(%rip), %xmm0
	setnp	%r8b
	sete	%r12b
	testb	%r8b, %r12b
	jne	.LBB111_127	# bb199.preheader
.LBB111_112:	# bb178
	testl	%r9d, %r9d
	jle	.LBB111_127	# bb199.preheader
.LBB111_113:	# bb178
	testl	%ecx, %ecx
	jle	.LBB111_127	# bb199.preheader
.LBB111_114:	# bb178.bb182.preheader_crit_edge
	xorl	%edx, %edx
	movl	%edx, %esi
	jmp	.LBB111_117	# bb182.preheader
	.align	16
.LBB111_115:	# bb181
	leal	(%rdx,%r8), %r11d
	movslq	%r11d, %r11
	movaps	%xmm0, %xmm1
	mulss	(%rax,%r11,4), %xmm1
	movss	%xmm1, (%rax,%r11,4)
	incl	%r8d
	cmpl	%ecx, %r8d
	jne	.LBB111_115	# bb181
.LBB111_116:	# bb183
	addl	96(%rsp), %edx
	incl	%esi
	cmpl	%r9d, %esi
	je	.LBB111_127	# bb199.preheader
.LBB111_117:	# bb182.preheader
	xorl	%r8d, %r8d
	jmp	.LBB111_115	# bb181
.LBB111_118:	# bb187
	cmpl	$131, 4(%rsp)
	jne	.LBB111_120	# bb189
.LBB111_119:	# bb188
	leal	(%r14,%rsi), %r12d
	movslq	%r12d, %r12
	movss	(%rax,%r12,4), %xmm0
	movslq	%ebx, %r13
	divss	(%r10,%r13,4), %xmm0
	movss	%xmm0, (%rax,%r12,4)
.LBB111_120:	# bb189
	leal	(%r14,%rsi), %r12d
	movslq	%r12d, %r12
	movss	(%rax,%r12,4), %xmm0
	testl	%r15d, %r15d
	jle	.LBB111_123	# bb192.loopexit
.LBB111_121:	# bb.nph316
	leal	-1(%rsi), %r15d
	xorl	%r12d, %r12d
	movl	%r15d, %r13d
	.align	16
.LBB111_122:	# bb190
	movslq	%r13d, %rbp
	movaps	%xmm0, %xmm1
	mulss	(%r10,%rbp,4), %xmm1
	leal	(%r11,%r12), %ebp
	movslq	%ebp, %rbp
	movss	(%rax,%rbp,4), %xmm2
	subss	%xmm1, %xmm2
	movss	%xmm2, (%rax,%rbp,4)
	addl	%edi, %r13d
	incl	%r12d
	cmpl	%r15d, %r12d
	jne	.LBB111_122	# bb190
.LBB111_123:	# bb192.loopexit
	addl	%edx, %ebx
	decl	%esi
.LBB111_124:	# bb192
	testl	%esi, %esi
	jle	.LBB111_126	# bb198
.LBB111_125:	# bb193
	leal	-1(%rsi), %r15d
	testl	%esi, %esi
	jne	.LBB111_118	# bb187
.LBB111_126:	# bb198
	addl	96(%rsp), %r11d
	incl	%r8d
	cmpl	%r9d, %r8d
	je	.LBB111_24	# bb54.thread
	jmp	.LBB111_129	# bb192.preheader
.LBB111_127:	# bb199.preheader
	testl	%r9d, %r9d
	jle	.LBB111_24	# bb54.thread
.LBB111_128:	# bb199.preheader.bb192.preheader_crit_edge
	xorl	%r11d, %r11d
	movl	%r11d, %r8d
	.align	16
.LBB111_129:	# bb192.preheader
	movl	$4294967295, %edx
	subl	%edi, %edx
	leal	1(%rdi), %esi
	leal	-1(%rcx), %ebx
	imull	%esi, %ebx
	leal	-1(%r11), %r14d
	movl	%ecx, %esi
	jmp	.LBB111_124	# bb192
.LBB111_130:	# bb200
	cmpl	$142, %esi
	setne	%dl
	notb	%r12b
	orb	%dl, %r12b
	testb	$1, %r12b
	jne	.LBB111_150	# bb230
.LBB111_131:	# bb208
	ucomiss	.LCPI111_0(%rip), %xmm0
	setnp	%r8b
	sete	%dl
	testb	%r8b, %dl
	jne	.LBB111_147	# bb229.preheader
.LBB111_132:	# bb208
	testl	%r9d, %r9d
	jle	.LBB111_147	# bb229.preheader
.LBB111_133:	# bb208
	testl	%ecx, %ecx
	jle	.LBB111_147	# bb229.preheader
.LBB111_134:	# bb208.bb212.preheader_crit_edge
	xorl	%edx, %edx
	movl	%edx, %esi
	jmp	.LBB111_137	# bb212.preheader
	.align	16
.LBB111_135:	# bb211
	leal	(%rdx,%r8), %r11d
	movslq	%r11d, %r11
	movaps	%xmm0, %xmm1
	mulss	(%rax,%r11,4), %xmm1
	movss	%xmm1, (%rax,%r11,4)
	incl	%r8d
	cmpl	%ecx, %r8d
	jne	.LBB111_135	# bb211
.LBB111_136:	# bb213
	addl	96(%rsp), %edx
	incl	%esi
	cmpl	%r9d, %esi
	je	.LBB111_147	# bb229.preheader
.LBB111_137:	# bb212.preheader
	xorl	%r8d, %r8d
	jmp	.LBB111_135	# bb211
.LBB111_138:	# bb217
	cmpl	$131, 4(%rsp)
	jne	.LBB111_140	# bb219
.LBB111_139:	# bb218
	leal	(%rdx,%r11), %r13d
	movslq	%r13d, %r13
	movss	(%rax,%r13,4), %xmm0
	movslq	%r12d, %rbp
	divss	(%r10,%rbp,4), %xmm0
	movss	%xmm0, (%rax,%r13,4)
.LBB111_140:	# bb219
	leal	(%rdx,%r11), %r13d
	movslq	%r13d, %r13
	movss	(%rax,%r13,4), %xmm0
	testl	%r14d, %r14d
	jle	.LBB111_143	# bb222.loopexit
.LBB111_141:	# bb.nph308
	leal	-1(%r11), %r14d
	xorl	%r13d, %r13d
	.align	16
.LBB111_142:	# bb220
	leal	(%rbx,%r13), %ebp
	movslq	%ebp, %rbp
	movaps	%xmm0, %xmm1
	mulss	(%r10,%rbp,4), %xmm1
	leal	(%rsi,%r13), %ebp
	movslq	%ebp, %rbp
	movss	(%rax,%rbp,4), %xmm2
	subss	%xmm1, %xmm2
	movss	%xmm2, (%rax,%rbp,4)
	incl	%r13d
	cmpl	%r14d, %r13d
	jne	.LBB111_142	# bb220
.LBB111_143:	# bb222.loopexit
	addl	%r15d, %r12d
	subl	%edi, %ebx
	decl	%r11d
.LBB111_144:	# bb222
	testl	%r11d, %r11d
	jle	.LBB111_146	# bb228
.LBB111_145:	# bb223
	leal	-1(%r11), %r14d
	testl	%r11d, %r11d
	jne	.LBB111_138	# bb217
.LBB111_146:	# bb228
	addl	96(%rsp), %esi
	incl	%r8d
	cmpl	%r9d, %r8d
	je	.LBB111_24	# bb54.thread
	jmp	.LBB111_149	# bb222.preheader
.LBB111_147:	# bb229.preheader
	testl	%r9d, %r9d
	jle	.LBB111_24	# bb54.thread
.LBB111_148:	# bb229.preheader.bb222.preheader_crit_edge
	xorl	%esi, %esi
	movl	%esi, %r8d
	.align	16
.LBB111_149:	# bb222.preheader
	movl	$4294967295, %r15d
	subl	%edi, %r15d
	leal	-1(%rcx), %edx
	movl	%edi, %ebx
	imull	%edx, %ebx
	leal	1(%rdi), %r12d
	imull	%edx, %r12d
	leal	-1(%rsi), %edx
	movl	%ecx, %r11d
	jmp	.LBB111_144	# bb222
.LBB111_150:	# bb230
	cmpl	$142, %esi
	setne	%dl
	notb	%r8b
	orb	%dl, %r8b
	testb	$1, %r8b
	jne	.LBB111_169	# bb256
.LBB111_151:	# bb238
	ucomiss	.LCPI111_0(%rip), %xmm0
	setnp	%dl
	sete	%sil
	testb	%dl, %sil
	jne	.LBB111_165	# bb255.preheader
.LBB111_152:	# bb238
	testl	%r9d, %r9d
	jle	.LBB111_165	# bb255.preheader
.LBB111_153:	# bb238
	testl	%ecx, %ecx
	jle	.LBB111_165	# bb255.preheader
.LBB111_154:	# bb238.bb242.preheader_crit_edge
	xorl	%edx, %edx
	movl	%edx, %esi
	jmp	.LBB111_157	# bb242.preheader
	.align	16
.LBB111_155:	# bb241
	leal	(%rdx,%r8), %r11d
	movslq	%r11d, %r11
	movaps	%xmm0, %xmm1
	mulss	(%rax,%r11,4), %xmm1
	movss	%xmm1, (%rax,%r11,4)
	incl	%r8d
	cmpl	%ecx, %r8d
	jne	.LBB111_155	# bb241
.LBB111_156:	# bb243
	addl	96(%rsp), %edx
	incl	%esi
	cmpl	%r9d, %esi
	je	.LBB111_165	# bb255.preheader
.LBB111_157:	# bb242.preheader
	xorl	%r8d, %r8d
	jmp	.LBB111_155	# bb241
	.align	16
.LBB111_158:	# bb247
	cmpl	$131, 4(%rsp)
	jne	.LBB111_160	# bb249
.LBB111_159:	# bb248
	leal	(%rsi,%r11), %r15d
	movslq	%r15d, %r15
	movss	(%rax,%r15,4), %xmm0
	movslq	%r14d, %r12
	divss	(%r10,%r12,4), %xmm0
	movss	%xmm0, (%rax,%r15,4)
.LBB111_160:	# bb249
	leal	(%rsi,%r11), %r15d
	leal	1(%r11), %r12d
	cmpl	%ecx, %r12d
	movslq	%r15d, %r15
	movss	(%rax,%r15,4), %xmm0
	jge	.LBB111_163	# bb252
.LBB111_161:	# bb.nph
	leal	(%rdi,%r14), %r15d
	leal	(%r8,%r11), %r12d
	xorl	%r13d, %r13d
	.align	16
.LBB111_162:	# bb250
	movslq	%r15d, %rbp
	movaps	%xmm0, %xmm1
	mulss	(%r10,%rbp,4), %xmm1
	leal	(%r12,%r13), %ebp
	movslq	%ebp, %rbp
	movss	(%rax,%rbp,4), %xmm2
	subss	%xmm1, %xmm2
	movss	%xmm2, (%rax,%rbp,4)
	addl	%edi, %r15d
	incl	%r13d
	cmpl	%ebx, %r13d
	jne	.LBB111_162	# bb250
.LBB111_163:	# bb252
	addl	%edx, %r14d
	decl	%ebx
	incl	%r11d
	cmpl	%ecx, %r11d
	jne	.LBB111_158	# bb247
.LBB111_164:	# bb254
	addl	96(%rsp), %esi
	movl	(%rsp), %edx
	incl	%edx
	movl	%edx, (%rsp)
	cmpl	%r9d, %edx
	je	.LBB111_24	# bb54.thread
	jmp	.LBB111_168	# bb253.preheader
.LBB111_165:	# bb255.preheader
	testl	%r9d, %r9d
	jle	.LBB111_24	# bb54.thread
.LBB111_166:	# bb.nph302
	testl	%ecx, %ecx
	jle	.LBB111_24	# bb54.thread
.LBB111_167:	# bb.nph302.bb253.preheader_crit_edge
	xorl	%esi, %esi
	movl	%esi, (%rsp)
	.align	16
.LBB111_168:	# bb253.preheader
	leal	1(%rdi), %edx
	leal	-1(%rcx), %ebx
	leal	1(%rsi), %r8d
	xorl	%r14d, %r14d
	movl	%r14d, %r11d
	jmp	.LBB111_158	# bb247
.LBB111_169:	# bb256
	xorl	%edi, %edi
	leaq	.str138, %rsi
	leaq	.str1139, %rdx
	xorb	%al, %al
	call	cblas_xerbla
	jmp	.LBB111_24	# bb54.thread
.LBB111_170:	# bb
	cmpl	$113, %ecx
	movl	$112, %r8d
	cmovne	%ecx, %r8d
	movl	%r11d, %ecx
	jmp	.LBB111_2	# bb25
.LBB111_171:	# bb50.preheader
	leal	-1(%r9), %esi
	movl	96(%rsp), %r8d
	imull	%esi, %r8d
	movl	$4294967295, %edx
	subl	%edi, %edx
	movl	%edx, (%rsp)
	leal	1(%rdi), %edx
	imull	%esi, %edx
	jmp	.LBB111_22	# bb50
.LBB111_172:	# bb138.preheader
	leal	-1(%r9), %esi
	movl	96(%rsp), %r11d
	imull	%esi, %r11d
	movl	$4294967295, %ebx
	subl	%edi, %ebx
	movl	%edi, %r8d
	imull	%esi, %r8d
	leal	1(%rdi), %edx
	imull	%esi, %edx
	jmp	.LBB111_89	# bb138
	.size	cblas_strsm, .-cblas_strsm
.Leh_func_end76:


	.align	16
	.globl	cblas_strsv
	.type	cblas_strsv,@function
cblas_strsv:
.Leh_func_begin77:
.Llabel77:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$24, %rsp
	cmpl	$113, %edx
	movl	$112, %eax
	cmovne	%edx, %eax
	testl	%r8d, %r8d
	movl	96(%rsp), %ebx
	movq	88(%rsp), %r14
	movl	80(%rsp), %r15d
	movq	%r9, %r12
	movl	%r8d, 20(%rsp)
	movl	%ecx, 16(%rsp)
	je	.LBB112_81	# return
.LBB112_1:	# bb8
	cmpl	$121, %esi
	sete	%cl
	setne	%dl
	cmpl	$101, %edi
	sete	%r8b
	setne	%r9b
	andb	%cl, %r8b
	orb	%dl, %r9b
	testb	%r9b, %r9b
	jne	.LBB112_3	# bb15
.LBB112_2:	# bb8
	cmpl	$111, %eax
	je	.LBB112_5	# bb23
.LBB112_3:	# bb15
	cmpl	$122, %esi
	sete	%cl
	setne	%dl
	cmpl	$102, %edi
	sete	%r9b
	setne	%r10b
	andb	%cl, %r9b
	orb	%dl, %r10b
	testb	%r10b, %r10b
	jne	.LBB112_23	# bb42
.LBB112_4:	# bb15
	cmpl	$112, %eax
	jne	.LBB112_23	# bb42
.LBB112_5:	# bb23
	xorl	%edi, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB112_20	# real_catch0
.LBB112_6:	# jump0
	xorl	%edi, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB112_21	# real_end0
.LBB112_7:	# real_try0
	xorl	%edi, %edi
	call	llvm_real_try
	testl	%ebx, %ebx
	jg	.LBB112_82	# real_try0.bb26_crit_edge
.LBB112_8:	# bb24
	movl	$1, %eax
	subl	20(%rsp), %eax
	imull	%ebx, %eax
.LBB112_9:	# bb26
	movl	20(%rsp), %ecx
	leal	-1(%rcx), %ecx
	movl	%ecx, %edx
	imull	%ebx, %edx
	addl	%eax, %edx
	cmpl	$131, 16(%rsp)
	jne	.LBB112_11	# bb28
.LBB112_10:	# bb27
	movslq	%edx, %rdx
	movss	(%r14,%rdx,4), %xmm0
	leal	1(%r15), %esi
	imull	%ecx, %esi
	movslq	%esi, %rcx
	divss	(%r12,%rcx,4), %xmm0
	movss	%xmm0, (%r14,%rdx,4)
.LBB112_11:	# bb28
	movl	20(%rsp), %ecx
	leal	-2(%rcx), %edx
	movl	%ebx, %esi
	imull	%edx, %esi
	movl	%esi, 12(%rsp)
	movl	$4294967295, %esi
	subl	%r15d, %esi
	movl	%esi, 4(%rsp)
	movl	%r15d, %esi
	imull	%edx, %esi
	incl	%r15d
	imull	%edx, %r15d
	leal	-1(%rcx,%rsi), %edx
	movl	%edx, 8(%rsp)
	leal	-1(%rcx), %edx
	imull	%ebx, %edx
	movl	$1, %esi
	xorl	%edi, %edi
	jmp	.LBB112_18	# bb36
.LBB112_12:	# bb29
	cmpl	20(%rsp), %r9d
	movslq	%r8d, %r8
	movss	(%r14,%r8,4), %xmm0
	jge	.LBB112_15	# bb32
.LBB112_13:	# bb.nph196
	leal	(%rdx,%rax), %r9d
	movl	8(%rsp), %r10d
	leal	(%r10,%rdi), %r10d
	xorl	%r11d, %r11d
	.align	16
.LBB112_14:	# bb30
	leal	(%r10,%r11), %r13d
	movslq	%r13d, %r13
	movslq	%r9d, %rbp
	movss	(%r14,%rbp,4), %xmm1
	mulss	(%r12,%r13,4), %xmm1
	subss	%xmm1, %xmm0
	addl	%ebx, %r9d
	incl	%r11d
	cmpl	%esi, %r11d
	jne	.LBB112_14	# bb30
.LBB112_15:	# bb32
	cmpl	$131, 16(%rsp)
	jne	.LBB112_17	# bb36.backedge
.LBB112_16:	# bb33
	leal	(%r15,%rdi), %r9d
	movslq	%r9d, %r9
	divss	(%r12,%r9,4), %xmm0
.LBB112_17:	# bb36.backedge
	movss	%xmm0, (%r14,%r8,4)
	addl	4(%rsp), %edi
	subl	%ebx, %eax
	decl	%ecx
	incl	%esi
.LBB112_18:	# bb36
	movl	12(%rsp), %r8d
	leal	(%r8,%rax), %r8d
	leal	-1(%rcx), %r9d
	testl	%r9d, %r9d
	jle	.LBB112_21	# real_end0
.LBB112_19:	# bb37
	cmpl	$1, %ecx
	jne	.LBB112_12	# bb29
	jmp	.LBB112_21	# real_end0
.LBB112_20:	# real_catch0
	xorl	%edi, %edi
	call	llvm_real_catch
.LBB112_21:	# real_end0
	xorl	%edi, %edi
.LBB112_22:	# real_end0
	call	llvm_real_end
	jmp	.LBB112_81	# return
.LBB112_23:	# bb42
	cmpl	$122, %esi
	sete	%cl
	setne	%dl
	cmpl	$101, %edi
	sete	%r10b
	setne	%r11b
	andb	%cl, %r10b
	orb	%dl, %r11b
	testb	%r11b, %r11b
	jne	.LBB112_25	# bb50
.LBB112_24:	# bb42
	cmpl	$111, %eax
	je	.LBB112_27	# bb58
.LBB112_25:	# bb50
	cmpl	$121, %esi
	sete	%cl
	setne	%dl
	cmpl	$102, %edi
	sete	%sil
	setne	%dil
	andb	%cl, %sil
	orb	%dl, %dil
	testb	%dil, %dil
	jne	.LBB112_43	# bb75
.LBB112_26:	# bb50
	cmpl	$112, %eax
	jne	.LBB112_43	# bb75
.LBB112_27:	# bb58
	movl	$1, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB112_41	# real_catch1
.LBB112_28:	# jump1
	movl	$1, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB112_42	# real_end1
.LBB112_29:	# real_try1
	movl	$1, %edi
	call	llvm_real_try
	testl	%ebx, %ebx
	jg	.LBB112_83	# real_try1.bb61_crit_edge
.LBB112_30:	# bb59
	movl	$1, %eax
	subl	20(%rsp), %eax
	imull	%ebx, %eax
.LBB112_31:	# bb61
	cmpl	$131, 16(%rsp)
	jne	.LBB112_33	# bb74.preheader
.LBB112_32:	# bb62
	movslq	%eax, %rcx
	movss	(%r14,%rcx,4), %xmm0
	divss	(%r12), %xmm0
	movss	%xmm0, (%r14,%rcx,4)
.LBB112_33:	# bb74.preheader
	cmpl	$2, 20(%rsp)
	jl	.LBB112_42	# real_end1
.LBB112_34:	# bb.nph188
	movl	$1, %ecx
	movl	20(%rsp), %edx
	subl	%edx, %ecx
	imull	%ebx, %ecx
	addl	%ebx, %eax
	decl	%edx
	movl	%edx, 20(%rsp)
	leal	1(%r15), %edx
	xorl	%esi, %esi
	movl	%edx, %edi
	movl	%r15d, %r8d
	.align	16
.LBB112_35:	# bb64
	testl	%ebx, %ebx
	movl	$0, %r9d
	cmovle	%ecx, %r9d
	movslq	%eax, %rax
	movss	(%r14,%rax,4), %xmm0
	leal	1(%rsi), %r10d
	testl	%r10d, %r10d
	jle	.LBB112_38	# bb70
.LBB112_36:	# bb64.bb68_crit_edge
	xorl	%r11d, %r11d
	.align	16
.LBB112_37:	# bb68
	leal	(%r8,%r11), %r13d
	movslq	%r13d, %r13
	movslq	%r9d, %rbp
	movss	(%r14,%rbp,4), %xmm1
	mulss	(%r12,%r13,4), %xmm1
	subss	%xmm1, %xmm0
	addl	%ebx, %r9d
	incl	%r11d
	cmpl	%r10d, %r11d
	jne	.LBB112_37	# bb68
.LBB112_38:	# bb70
	cmpl	$131, 16(%rsp)
	jne	.LBB112_40	# bb73
.LBB112_39:	# bb71
	movslq	%edi, %r9
	divss	(%r12,%r9,4), %xmm0
.LBB112_40:	# bb73
	movss	%xmm0, (%r14,%rax,4)
	addl	%ebx, %eax
	addl	%edx, %edi
	addl	%r15d, %r8d
	incl	%esi
	cmpl	20(%rsp), %esi
	jne	.LBB112_35	# bb64
	jmp	.LBB112_42	# real_end1
.LBB112_41:	# real_catch1
	movl	$1, %edi
	call	llvm_real_catch
.LBB112_42:	# real_end1
	movl	$1, %edi
	jmp	.LBB112_22	# real_end0
.LBB112_43:	# bb75
	cmpl	$111, %eax
	sete	%cl
	andb	%cl, %r9b
	cmpl	$112, %eax
	sete	%cl
	testb	%cl, %r8b
	jne	.LBB112_45	# bb91
.LBB112_44:	# bb75
	notb	%r9b
	testb	$1, %r9b
	jne	.LBB112_61	# bb108
.LBB112_45:	# bb91
	movl	$2, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB112_59	# real_catch2
.LBB112_46:	# jump2
	movl	$2, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB112_60	# real_end2
.LBB112_47:	# real_try2
	movl	$2, %edi
	call	llvm_real_try
	testl	%ebx, %ebx
	jg	.LBB112_84	# real_try2.bb94_crit_edge
.LBB112_48:	# bb92
	movl	$1, %eax
	subl	20(%rsp), %eax
	imull	%ebx, %eax
.LBB112_49:	# bb94
	cmpl	$131, 16(%rsp)
	jne	.LBB112_51	# bb107.preheader
.LBB112_50:	# bb95
	movslq	%eax, %rcx
	movss	(%r14,%rcx,4), %xmm0
	divss	(%r12), %xmm0
	movss	%xmm0, (%r14,%rcx,4)
.LBB112_51:	# bb107.preheader
	cmpl	$2, 20(%rsp)
	jl	.LBB112_60	# real_end2
.LBB112_52:	# bb.nph178
	movl	$1, %ecx
	movl	20(%rsp), %edx
	subl	%edx, %ecx
	imull	%ebx, %ecx
	addl	%ebx, %eax
	decl	%edx
	movl	%edx, 20(%rsp)
	leal	1(%r15), %edx
	xorl	%esi, %esi
	movl	%edx, %edi
	.align	16
.LBB112_53:	# bb97
	testl	%ebx, %ebx
	movl	$0, %r8d
	cmovle	%ecx, %r8d
	movslq	%eax, %rax
	movss	(%r14,%rax,4), %xmm0
	leal	1(%rsi), %r9d
	testl	%r9d, %r9d
	jle	.LBB112_56	# bb103
.LBB112_54:	# bb101.preheader
	leal	1(%rsi), %r10d
	xorl	%r11d, %r11d
	.align	16
.LBB112_55:	# bb101
	leal	(%rbx,%r8), %r13d
	leal	(%r15,%r10), %ebp
	incl	%r11d
	cmpl	%r9d, %r11d
	movslq	%r8d, %r8
	movss	(%r14,%r8,4), %xmm1
	movslq	%r10d, %r8
	mulss	(%r12,%r8,4), %xmm1
	subss	%xmm1, %xmm0
	movl	%r13d, %r8d
	movl	%ebp, %r10d
	jne	.LBB112_55	# bb101
.LBB112_56:	# bb103
	cmpl	$131, 16(%rsp)
	jne	.LBB112_58	# bb106
.LBB112_57:	# bb104
	movslq	%edi, %r8
	divss	(%r12,%r8,4), %xmm0
.LBB112_58:	# bb106
	movss	%xmm0, (%r14,%rax,4)
	addl	%ebx, %eax
	addl	%edx, %edi
	incl	%esi
	cmpl	20(%rsp), %esi
	jne	.LBB112_53	# bb97
	jmp	.LBB112_60	# real_end2
.LBB112_59:	# real_catch2
	movl	$2, %edi
	call	llvm_real_catch
.LBB112_60:	# real_end2
	movl	$2, %edi
	jmp	.LBB112_22	# real_end0
.LBB112_61:	# bb108
	cmpl	$111, %eax
	sete	%cl
	andb	%cl, %sil
	cmpl	$112, %eax
	sete	%al
	testb	%al, %r10b
	jne	.LBB112_63	# bb124
.LBB112_62:	# bb108
	notb	%sil
	testb	$1, %sil
	jne	.LBB112_80	# bb143
.LBB112_63:	# bb124
	movl	$3, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB112_78	# real_catch3
.LBB112_64:	# jump3
	movl	$3, %edi
	call	llvm_real_split
	cmpl	$0, dummy
	je	.LBB112_79	# real_end3
.LBB112_65:	# real_try3
	movl	$3, %edi
	call	llvm_real_try
	testl	%ebx, %ebx
	jg	.LBB112_85	# real_try3.bb127_crit_edge
.LBB112_66:	# bb125
	movl	$1, %eax
	subl	20(%rsp), %eax
	imull	%ebx, %eax
.LBB112_67:	# bb127
	movl	20(%rsp), %ecx
	leal	-1(%rcx), %ecx
	movl	%ecx, %edx
	imull	%ebx, %edx
	addl	%eax, %edx
	cmpl	$131, 16(%rsp)
	jne	.LBB112_69	# bb129
.LBB112_68:	# bb128
	movslq	%edx, %rdx
	movss	(%r14,%rdx,4), %xmm0
	leal	1(%r15), %esi
	imull	%ecx, %esi
	movslq	%esi, %rcx
	divss	(%r12,%rcx,4), %xmm0
	movss	%xmm0, (%r14,%rdx,4)
.LBB112_69:	# bb129
	movl	$4294967295, %ecx
	subl	%r15d, %ecx
	movl	%ecx, (%rsp)
	movl	20(%rsp), %ecx
	leal	-1(%rcx), %edx
	movl	%r15d, %esi
	imull	%edx, %esi
	leal	-2(%rcx,%rsi), %esi
	movl	%esi, 12(%rsp)
	leal	-2(%rcx), %esi
	movl	%ebx, %edi
	imull	%esi, %edi
	movl	%edi, 8(%rsp)
	leal	1(%r15), %edi
	imull	%esi, %edi
	movl	%edi, 4(%rsp)
	imull	%ebx, %edx
	movl	$1, %esi
	xorl	%edi, %edi
	jmp	.LBB112_76	# bb137
.LBB112_70:	# bb130
	cmpl	20(%rsp), %r9d
	movslq	%r8d, %r8
	movss	(%r14,%r8,4), %xmm0
	jge	.LBB112_73	# bb133
.LBB112_71:	# bb.nph
	leal	(%rdx,%rax), %r9d
	movl	12(%rsp), %r10d
	leal	(%r10,%rdi), %r10d
	xorl	%r11d, %r11d
	.align	16
.LBB112_72:	# bb131
	leal	(%r15,%r10), %r13d
	leal	(%rbx,%r9), %ebp
	incl	%r11d
	cmpl	%esi, %r11d
	movslq	%r9d, %r9
	movss	(%r14,%r9,4), %xmm1
	movslq	%r10d, %r9
	mulss	(%r12,%r9,4), %xmm1
	subss	%xmm1, %xmm0
	movl	%r13d, %r10d
	movl	%ebp, %r9d
	jne	.LBB112_72	# bb131
.LBB112_73:	# bb133
	cmpl	$131, 16(%rsp)
	jne	.LBB112_75	# bb137.backedge
.LBB112_74:	# bb134
	movl	4(%rsp), %r9d
	leal	(%r9,%rdi), %r9d
	movslq	%r9d, %r9
	divss	(%r12,%r9,4), %xmm0
.LBB112_75:	# bb137.backedge
	movss	%xmm0, (%r14,%r8,4)
	subl	%ebx, %eax
	addl	(%rsp), %edi
	decl	%ecx
	incl	%esi
.LBB112_76:	# bb137
	movl	8(%rsp), %r8d
	leal	(%r8,%rax), %r8d
	leal	-1(%rcx), %r9d
	testl	%r9d, %r9d
	jle	.LBB112_79	# real_end3
.LBB112_77:	# bb138
	cmpl	$1, %ecx
	jne	.LBB112_70	# bb130
	jmp	.LBB112_79	# real_end3
.LBB112_78:	# real_catch3
	movl	$3, %edi
	call	llvm_real_catch
.LBB112_79:	# real_end3
	movl	$3, %edi
	jmp	.LBB112_22	# real_end0
.LBB112_80:	# bb143
	xorl	%edi, %edi
	leaq	.str140, %rsi
	leaq	.str1141, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB112_81:	# return
	addq	$24, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB112_82:	# real_try0.bb26_crit_edge
	xorl	%eax, %eax
	jmp	.LBB112_9	# bb26
.LBB112_83:	# real_try1.bb61_crit_edge
	xorl	%eax, %eax
	jmp	.LBB112_31	# bb61
.LBB112_84:	# real_try2.bb94_crit_edge
	xorl	%eax, %eax
	jmp	.LBB112_49	# bb94
.LBB112_85:	# real_try3.bb127_crit_edge
	xorl	%eax, %eax
	jmp	.LBB112_67	# bb127
	.size	cblas_strsv, .-cblas_strsv
.Leh_func_end77:


	.align	16
	.globl	cblas_xerbla
	.type	cblas_xerbla,@function
cblas_xerbla:
.Leh_func_begin78:
.Llabel78:
	pushq	%rbx
	subq	$208, %rsp
	movaps	%xmm7, 160(%rsp)
	movaps	%xmm6, 144(%rsp)
	movaps	%xmm5, 128(%rsp)
	movaps	%xmm4, 112(%rsp)
	movaps	%xmm3, 96(%rsp)
	movaps	%xmm2, 80(%rsp)
	movaps	%xmm1, 64(%rsp)
	movaps	%xmm0, 48(%rsp)
	movq	%r9, 40(%rsp)
	movq	%r8, 32(%rsp)
	movq	%rcx, 24(%rsp)
	leaq	(%rsp), %rax
	movq	%rax, 200(%rsp)
	leaq	224(%rsp), %rax
	movq	%rax, 192(%rsp)
	movl	$48, 188(%rsp)
	movl	$24, 184(%rsp)
	testl	%edi, %edi
	movq	%rdx, %rbx
	movq	%rsi, %rcx
	movl	%edi, %edx
	je	.LBB113_2	# bb1
.LBB113_1:	# bb
	movq	stderr, %rdi
	leaq	.str142, %rsi
	xorb	%al, %al
	call	fprintf
.LBB113_2:	# bb1
	movq	stderr, %rdi
	leaq	184(%rsp), %rdx
	movq	%rbx, %rsi
	call	vfprintf
	call	abort
	.size	cblas_xerbla, .-cblas_xerbla
.Leh_func_end78:


	.align	16
	.globl	cblas_zaxpy
	.type	cblas_zaxpy,@function
cblas_zaxpy:
	pushq	%rbx
	testl	%ecx, %ecx
	jg	.LBB114_9	# entry.bb2_crit_edge
.LBB114_1:	# bb
	movl	$1, %eax
	subl	%edi, %eax
	imull	%ecx, %eax
.LBB114_2:	# bb2
	testl	%r9d, %r9d
	jg	.LBB114_10	# bb2.bb5_crit_edge
.LBB114_3:	# bb3
	movl	$1, %r10d
	subl	%edi, %r10d
	imull	%r9d, %r10d
.LBB114_4:	# bb5
	movsd	(%rsi), %xmm0
	pxor	%xmm1, %xmm1
	ucomisd	%xmm1, %xmm0
	setnp	%r11b
	sete	%bl
	andb	%r11b, %bl
	movsd	8(%rsi), %xmm2
	ucomisd	%xmm1, %xmm2
	setnp	%sil
	sete	%r11b
	andb	%sil, %r11b
	testb	%bl, %r11b
	jne	.LBB114_8	# return
.LBB114_5:	# bb5
	testl	%edi, %edi
	jle	.LBB114_8	# return
.LBB114_6:	# bb.nph
	addl	%r9d, %r9d
	addl	%r10d, %r10d
	addl	%ecx, %ecx
	addl	%eax, %eax
	xorl	%esi, %esi
	.align	16
.LBB114_7:	# bb8
	movslq	%eax, %r11
	movsd	(%rdx,%r11,8), %xmm1
	movapd	%xmm0, %xmm3
	mulsd	%xmm1, %xmm3
	leal	1(%rax), %r11d
	movslq	%r11d, %r11
	movsd	(%rdx,%r11,8), %xmm4
	movapd	%xmm2, %xmm5
	mulsd	%xmm4, %xmm5
	subsd	%xmm5, %xmm3
	movslq	%r10d, %r11
	addsd	(%r8,%r11,8), %xmm3
	movsd	%xmm3, (%r8,%r11,8)
	mulsd	%xmm2, %xmm1
	mulsd	%xmm0, %xmm4
	addsd	%xmm1, %xmm4
	leal	1(%r10), %r11d
	movslq	%r11d, %r11
	addsd	(%r8,%r11,8), %xmm4
	movsd	%xmm4, (%r8,%r11,8)
	addl	%r9d, %r10d
	addl	%ecx, %eax
	incl	%esi
	cmpl	%edi, %esi
	jne	.LBB114_7	# bb8
.LBB114_8:	# return
	popq	%rbx
	ret
.LBB114_9:	# entry.bb2_crit_edge
	xorl	%eax, %eax
	jmp	.LBB114_2	# bb2
.LBB114_10:	# bb2.bb5_crit_edge
	xorl	%r10d, %r10d
	jmp	.LBB114_4	# bb5
	.size	cblas_zaxpy, .-cblas_zaxpy


	.align	16
	.globl	cblas_zcopy
	.type	cblas_zcopy,@function
cblas_zcopy:
	testl	%edx, %edx
	jg	.LBB115_8	# entry.bb2_crit_edge
.LBB115_1:	# bb
	movl	$1, %eax
	subl	%edi, %eax
	imull	%edx, %eax
.LBB115_2:	# bb2
	testl	%r8d, %r8d
	jg	.LBB115_9	# bb2.bb7.preheader_crit_edge
.LBB115_3:	# bb3
	movl	$1, %r9d
	subl	%edi, %r9d
	imull	%r8d, %r9d
.LBB115_4:	# bb7.preheader
	testl	%edi, %edi
	jle	.LBB115_7	# return
.LBB115_5:	# bb.nph
	addl	%r8d, %r8d
	addl	%r9d, %r9d
	addl	%edx, %edx
	addl	%eax, %eax
	xorl	%r10d, %r10d
	.align	16
.LBB115_6:	# bb6
	movslq	%eax, %r11
	movsd	(%rsi,%r11,8), %xmm0
	movslq	%r9d, %r11
	movsd	%xmm0, (%rcx,%r11,8)
	leal	1(%rax), %r11d
	movslq	%r11d, %r11
	movsd	(%rsi,%r11,8), %xmm0
	leal	(%r8,%r9), %r11d
	incl	%r9d
	movslq	%r9d, %r9
	movsd	%xmm0, (%rcx,%r9,8)
	addl	%edx, %eax
	incl	%r10d
	cmpl	%edi, %r10d
	movl	%r11d, %r9d
	jne	.LBB115_6	# bb6
.LBB115_7:	# return
	ret
.LBB115_8:	# entry.bb2_crit_edge
	xorl	%eax, %eax
	jmp	.LBB115_2	# bb2
.LBB115_9:	# bb2.bb7.preheader_crit_edge
	xorl	%r9d, %r9d
	jmp	.LBB115_4	# bb7.preheader
	.size	cblas_zcopy, .-cblas_zcopy


	.align	16
	.globl	cblas_zdotc_sub
	.type	cblas_zdotc_sub,@function
cblas_zdotc_sub:
	pushq	%rbx
	testl	%edx, %edx
	jg	.LBB116_8	# entry.bb2_crit_edge
.LBB116_1:	# bb
	movl	$1, %eax
	subl	%edi, %eax
	imull	%edx, %eax
.LBB116_2:	# bb2
	testl	%r8d, %r8d
	jg	.LBB116_9	# bb2.bb7.preheader_crit_edge
.LBB116_3:	# bb3
	movl	$1, %r10d
	subl	%edi, %r10d
	imull	%r8d, %r10d
.LBB116_4:	# bb7.preheader
	testl	%edi, %edi
	jle	.LBB116_10	# bb7.preheader.bb8_crit_edge
.LBB116_5:	# bb.nph
	addl	%r8d, %r8d
	addl	%r10d, %r10d
	addl	%edx, %edx
	addl	%eax, %eax
	pxor	%xmm0, %xmm0
	xorl	%r11d, %r11d
	movapd	%xmm0, %xmm1
	.align	16
.LBB116_6:	# bb6
	movslq	%r10d, %rbx
	movsd	(%rcx,%rbx,8), %xmm2
	movslq	%eax, %rbx
	movsd	(%rsi,%rbx,8), %xmm3
	movapd	%xmm3, %xmm4
	mulsd	%xmm2, %xmm4
	leal	1(%rax), %ebx
	movslq	%ebx, %rbx
	movsd	(%rsi,%rbx,8), %xmm5
	mulsd	%xmm5, %xmm2
	leal	1(%r10), %ebx
	movslq	%ebx, %rbx
	movsd	(%rcx,%rbx,8), %xmm6
	mulsd	%xmm6, %xmm3
	subsd	%xmm2, %xmm3
	addsd	%xmm3, %xmm1
	mulsd	%xmm6, %xmm5
	addsd	%xmm4, %xmm5
	addsd	%xmm5, %xmm0
	addl	%r8d, %r10d
	addl	%edx, %eax
	incl	%r11d
	cmpl	%edi, %r11d
	jne	.LBB116_6	# bb6
.LBB116_7:	# bb8
	movsd	%xmm0, (%r9)
	movsd	%xmm1, 8(%r9)
	popq	%rbx
	ret
.LBB116_8:	# entry.bb2_crit_edge
	xorl	%eax, %eax
	jmp	.LBB116_2	# bb2
.LBB116_9:	# bb2.bb7.preheader_crit_edge
	xorl	%r10d, %r10d
	jmp	.LBB116_4	# bb7.preheader
.LBB116_10:	# bb7.preheader.bb8_crit_edge
	pxor	%xmm0, %xmm0
	movapd	%xmm0, %xmm1
	jmp	.LBB116_7	# bb8
	.size	cblas_zdotc_sub, .-cblas_zdotc_sub


	.align	16
	.globl	cblas_zdotu_sub
	.type	cblas_zdotu_sub,@function
cblas_zdotu_sub:
	pushq	%r14
	pushq	%rbx
	testl	%edx, %edx
	jg	.LBB117_8	# entry.bb2_crit_edge
.LBB117_1:	# bb
	movl	$1, %eax
	subl	%edi, %eax
	imull	%edx, %eax
.LBB117_2:	# bb2
	testl	%r8d, %r8d
	jg	.LBB117_9	# bb2.bb7.preheader_crit_edge
.LBB117_3:	# bb3
	movl	$1, %r10d
	subl	%edi, %r10d
	imull	%r8d, %r10d
.LBB117_4:	# bb7.preheader
	testl	%edi, %edi
	jle	.LBB117_10	# bb7.preheader.bb8_crit_edge
.LBB117_5:	# bb.nph
	addl	%r8d, %r8d
	addl	%r10d, %r10d
	addl	%edx, %edx
	addl	%eax, %eax
	pxor	%xmm0, %xmm0
	xorl	%r11d, %r11d
	movapd	%xmm0, %xmm1
	.align	16
.LBB117_6:	# bb6
	movslq	%r10d, %rbx
	movsd	(%rcx,%rbx,8), %xmm2
	movslq	%eax, %rbx
	leal	1(%rax), %r14d
	movslq	%r14d, %r14
	movsd	(%rsi,%r14,8), %xmm3
	movapd	%xmm3, %xmm4
	mulsd	%xmm2, %xmm4
	movsd	(%rsi,%rbx,8), %xmm5
	leal	1(%r10), %ebx
	movslq	%ebx, %rbx
	movsd	(%rcx,%rbx,8), %xmm6
	movapd	%xmm5, %xmm7
	mulsd	%xmm6, %xmm7
	addsd	%xmm4, %xmm7
	addsd	%xmm7, %xmm1
	mulsd	%xmm2, %xmm5
	mulsd	%xmm6, %xmm3
	subsd	%xmm3, %xmm5
	addsd	%xmm5, %xmm0
	addl	%r8d, %r10d
	addl	%edx, %eax
	incl	%r11d
	cmpl	%edi, %r11d
	jne	.LBB117_6	# bb6
.LBB117_7:	# bb8
	movsd	%xmm0, (%r9)
	movsd	%xmm1, 8(%r9)
	popq	%rbx
	popq	%r14
	ret
.LBB117_8:	# entry.bb2_crit_edge
	xorl	%eax, %eax
	jmp	.LBB117_2	# bb2
.LBB117_9:	# bb2.bb7.preheader_crit_edge
	xorl	%r10d, %r10d
	jmp	.LBB117_4	# bb7.preheader
.LBB117_10:	# bb7.preheader.bb8_crit_edge
	pxor	%xmm0, %xmm0
	movapd	%xmm0, %xmm1
	jmp	.LBB117_7	# bb8
	.size	cblas_zdotu_sub, .-cblas_zdotu_sub


	.align	16
	.globl	cblas_zdscal
	.type	cblas_zdscal,@function
cblas_zdscal:
	testl	%edx, %edx
	jle	.LBB118_4	# return
.LBB118_1:	# entry
	testl	%edi, %edi
	jle	.LBB118_4	# return
.LBB118_2:	# bb.nph
	addl	%edx, %edx
	xorl	%eax, %eax
	movl	%eax, %ecx
	.align	16
.LBB118_3:	# bb1
	movslq	%eax, %r8
	movapd	%xmm0, %xmm1
	mulsd	(%rsi,%r8,8), %xmm1
	movsd	%xmm1, (%rsi,%r8,8)
	leal	1(%rax), %r8d
	movslq	%r8d, %r8
	movapd	%xmm0, %xmm1
	mulsd	(%rsi,%r8,8), %xmm1
	movsd	%xmm1, (%rsi,%r8,8)
	addl	%edx, %eax
	incl	%ecx
	cmpl	%edi, %ecx
	jne	.LBB118_3	# bb1
.LBB118_4:	# return
	ret
	.size	cblas_zdscal, .-cblas_zdscal


	.section	.rodata.cst8,"aM",@progbits,8
	.align	8
.LCPI119_0:					
	.quad	4607182418800017408	# double value: 1.000000e+00
	.text
	.align	16
	.globl	cblas_zgbmv
	.type	cblas_zgbmv,@function
cblas_zgbmv:
.Leh_func_begin79:
.Llabel79:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	testl	%ecx, %ecx
	movq	136(%rsp), %rax
	movsd	8(%rax), %xmm0
	movsd	(%rax), %xmm1
	movq	96(%rsp), %rax
	movsd	8(%rax), %xmm2
	movsd	(%rax), %xmm3
	movl	152(%rsp), %eax
	movq	144(%rsp), %r10
	movl	128(%rsp), %r11d
	movq	120(%rsp), %rbx
	movq	104(%rsp), %r14
	movl	%edi, 16(%rsp)
	je	.LBB119_69	# return
.LBB119_1:	# entry
	testl	%edx, %edx
	je	.LBB119_69	# return
.LBB119_2:	# bb
	pxor	%xmm4, %xmm4
	ucomisd	%xmm4, %xmm3
	setp	36(%rsp)
	setne	%dil
	setnp	%r15b
	sete	%r12b
	andb	%r15b, %r12b
	ucomisd	%xmm4, %xmm2
	setp	%r15b
	setnp	%r13b
	sete	12(%rsp)
	setne	%bpl
	andb	%r13b, 12(%rsp)
	andb	%r12b, 12(%rsp)
	orb	36(%rsp), %dil
	orb	%r15b, %bpl
	orb	%dil, %bpl
	testb	%bpl, %bpl
	jne	.LBB119_5	# bb36
.LBB119_3:	# bb
	ucomisd	.LCPI119_0(%rip), %xmm1
	jne	.LBB119_5	# bb36
	jp	.LBB119_5	# bb36
.LBB119_4:	# bb
	pxor	%xmm4, %xmm4
	ucomisd	%xmm4, %xmm0
	setnp	%dil
	sete	%r15b
	testb	%dil, %r15b
	jne	.LBB119_69	# return
.LBB119_5:	# bb36
	cmpl	$111, %esi
	je	.LBB119_70	# bb36.bb39_crit_edge
.LBB119_6:	# bb38
	movl	%r8d, 28(%rsp)
	movl	%r9d, %r8d
	movl	%ecx, 36(%rsp)
	movl	%edx, %ecx
.LBB119_7:	# bb39
	movl	%r8d, 24(%rsp)
	movl	%ecx, 32(%rsp)
	pxor	%xmm4, %xmm4
	ucomisd	%xmm4, %xmm0
	jne	.LBB119_14	# bb47
	jp	.LBB119_14	# bb47
.LBB119_8:	# bb39
	pxor	%xmm4, %xmm4
	ucomisd	%xmm4, %xmm1
	jne	.LBB119_14	# bb47
	jp	.LBB119_14	# bb47
.LBB119_9:	# bb41
	testl	%eax, %eax
	jg	.LBB119_71	# bb41.bb46.preheader_crit_edge
.LBB119_10:	# bb42
	movl	$1, %ecx
	subl	36(%rsp), %ecx
	imull	%eax, %ecx
.LBB119_11:	# bb46.preheader
	cmpl	$0, 36(%rsp)
	jle	.LBB119_21	# bb55
.LBB119_12:	# bb.nph
	leal	(%rax,%rax), %edx
	addl	%ecx, %ecx
	xorl	%edi, %edi
	.align	16
.LBB119_13:	# bb45
	movslq	%ecx, %r8
	movq	$0, (%r10,%r8,8)
	leal	(%rdx,%rcx), %r8d
	incl	%ecx
	movslq	%ecx, %rcx
	movq	$0, (%r10,%rcx,8)
	incl	%edi
	cmpl	36(%rsp), %edi
	movl	%r8d, %ecx
	jne	.LBB119_13	# bb45
	jmp	.LBB119_21	# bb55
.LBB119_14:	# bb47
	pxor	%xmm4, %xmm4
	ucomisd	%xmm4, %xmm0
	jne	.LBB119_16	# bb49
	jp	.LBB119_16	# bb49
.LBB119_15:	# bb47
	ucomisd	.LCPI119_0(%rip), %xmm1
	setnp	%cl
	sete	%dl
	testb	%cl, %dl
	jne	.LBB119_21	# bb55
.LBB119_16:	# bb49
	testl	%eax, %eax
	jg	.LBB119_72	# bb49.bb54.preheader_crit_edge
.LBB119_17:	# bb50
	movl	$1, %ecx
	subl	36(%rsp), %ecx
	imull	%eax, %ecx
.LBB119_18:	# bb54.preheader
	cmpl	$0, 36(%rsp)
	jle	.LBB119_21	# bb55
.LBB119_19:	# bb.nph216
	leal	(%rax,%rax), %edx
	addl	%ecx, %ecx
	xorl	%edi, %edi
	.align	16
.LBB119_20:	# bb53
	movslq	%ecx, %r8
	movsd	(%r10,%r8,8), %xmm4
	movapd	%xmm4, %xmm5
	mulsd	%xmm1, %xmm5
	leal	1(%rcx), %r9d
	movslq	%r9d, %r9
	movsd	(%r10,%r9,8), %xmm6
	movapd	%xmm6, %xmm7
	mulsd	%xmm0, %xmm7
	subsd	%xmm7, %xmm5
	movsd	%xmm5, (%r10,%r8,8)
	mulsd	%xmm0, %xmm4
	mulsd	%xmm1, %xmm6
	addsd	%xmm4, %xmm6
	movsd	%xmm6, (%r10,%r9,8)
	addl	%edx, %ecx
	incl	%edi
	cmpl	36(%rsp), %edi
	jne	.LBB119_20	# bb53
.LBB119_21:	# bb55
	testb	$1, 12(%rsp)
	jne	.LBB119_69	# return
.LBB119_22:	# bb57
	cmpl	$111, %esi
	jne	.LBB119_24	# bb61
.LBB119_23:	# bb57
	cmpl	$101, 16(%rsp)
	je	.LBB119_26	# bb65
.LBB119_24:	# bb61
	cmpl	$112, %esi
	jne	.LBB119_34	# bb80
.LBB119_25:	# bb61
	cmpl	$102, 16(%rsp)
	jne	.LBB119_34	# bb80
.LBB119_26:	# bb65
	testl	%eax, %eax
	jg	.LBB119_73	# bb65.bb79.preheader_crit_edge
.LBB119_27:	# bb66
	movl	$1, %ecx
	subl	36(%rsp), %ecx
	imull	%eax, %ecx
.LBB119_28:	# bb79.preheader
	cmpl	$0, 36(%rsp)
	jle	.LBB119_69	# return
.LBB119_29:	# bb.nph213
	movl	$1, %edx
	subl	32(%rsp), %edx
	imull	%r11d, %edx
	movl	%edx, 8(%rsp)
	movl	$4294967294, %edx
	movl	28(%rsp), %esi
	subl	%esi, %edx
	addl	%eax, %eax
	addl	%ecx, %ecx
	movl	112(%rsp), %edi
	decl	%edi
	movl	%edi, 20(%rsp)
	incl	%esi
	movl	%esi, 28(%rsp)
	movl	24(%rsp), %esi
	movl	%esi, %edi
	negl	%edi
	movl	%edi, 12(%rsp)
	xorl	%edi, %edi
	movl	%esi, 16(%rsp)
	.align	16
.LBB119_30:	# bb69
	xorl	%esi, %esi
	testl	%r11d, %r11d
	movl	8(%rsp), %r8d
	cmovg	%esi, %r8d
	movl	12(%rsp), %r9d
	leal	(%r9,%rdi), %r9d
	cmpl	24(%rsp), %edi
	cmovle	%esi, %r9d
	movl	%r9d, %esi
	imull	%r11d, %esi
	movl	28(%rsp), %r15d
	leal	(%r15,%rdi), %r15d
	movl	32(%rsp), %r12d
	cmpl	%r12d, %r15d
	cmovg	%r12d, %r15d
	cmpl	%r15d, %r9d
	jge	.LBB119_74	# bb69.bb78_crit_edge
.LBB119_31:	# bb.nph208
	movl	16(%rsp), %r15d
	leal	(%r9,%r15), %r15d
	addl	%r15d, %r15d
	movl	32(%rsp), %r12d
	notl	%r12d
	cmpl	%r12d, %edx
	cmovge	%edx, %r12d
	addl	%r9d, %r12d
	notl	%r12d
	pxor	%xmm0, %xmm0
	xorl	%r9d, %r9d
	movapd	%xmm0, %xmm1
	.align	16
.LBB119_32:	# bb76
	movslq	%r15d, %r13
	leal	1(%r15), %ebp
	movslq	%ebp, %rbp
	movsd	(%r14,%rbp,8), %xmm4
	addl	%r8d, %esi
	leal	1(,%rsi,2), %r8d
	leal	(%rsi,%rsi), %ebp
	movslq	%ebp, %rbp
	movsd	(%rbx,%rbp,8), %xmm5
	movapd	%xmm4, %xmm6
	mulsd	%xmm5, %xmm6
	movsd	(%r14,%r13,8), %xmm7
	movslq	%r8d, %r8
	movsd	(%rbx,%r8,8), %xmm8
	movapd	%xmm7, %xmm9
	mulsd	%xmm8, %xmm9
	addsd	%xmm6, %xmm9
	addsd	%xmm9, %xmm1
	mulsd	%xmm8, %xmm4
	mulsd	%xmm5, %xmm7
	subsd	%xmm4, %xmm7
	addsd	%xmm7, %xmm0
	addl	$2, %r15d
	incl	%r9d
	cmpl	%r12d, %r9d
	movl	%r11d, %r8d
	jne	.LBB119_32	# bb76
.LBB119_33:	# bb78
	movapd	%xmm2, %xmm4
	mulsd	%xmm1, %xmm4
	movapd	%xmm3, %xmm5
	mulsd	%xmm0, %xmm5
	subsd	%xmm4, %xmm5
	movslq	%ecx, %rsi
	addsd	(%r10,%rsi,8), %xmm5
	movsd	%xmm5, (%r10,%rsi,8)
	mulsd	%xmm2, %xmm0
	mulsd	%xmm3, %xmm1
	addsd	%xmm0, %xmm1
	leal	1(%rcx), %esi
	movslq	%esi, %rsi
	addsd	(%r10,%rsi,8), %xmm1
	movsd	%xmm1, (%r10,%rsi,8)
	addl	%eax, %ecx
	movl	16(%rsp), %esi
	addl	20(%rsp), %esi
	movl	%esi, 16(%rsp)
	decl	%edx
	incl	%edi
	cmpl	36(%rsp), %edi
	jne	.LBB119_30	# bb69
	jmp	.LBB119_69	# return
.LBB119_34:	# bb80
	movl	16(%rsp), %ecx
	cmpl	$102, %ecx
	sete	%dl
	cmpl	$111, %esi
	sete	%dil
	andb	%dl, %dil
	cmpl	$101, %ecx
	sete	%cl
	cmpl	$112, %esi
	sete	%dl
	testb	%cl, %dl
	jne	.LBB119_36	# bb88
.LBB119_35:	# bb80
	notb	%dil
	testb	$1, %dil
	jne	.LBB119_46	# bb106
.LBB119_36:	# bb88
	testl	%r11d, %r11d
	jg	.LBB119_75	# bb88.bb105.preheader_crit_edge
.LBB119_37:	# bb89
	movl	$1, %ecx
	subl	32(%rsp), %ecx
	imull	%r11d, %ecx
.LBB119_38:	# bb105.preheader
	cmpl	$0, 32(%rsp)
	jle	.LBB119_69	# return
.LBB119_39:	# bb.nph200
	movl	$1, %edx
	subl	36(%rsp), %edx
	imull	%eax, %edx
	movl	%edx, 12(%rsp)
	movl	$4294967294, %edx
	movl	24(%rsp), %esi
	subl	%esi, %edx
	addl	%r11d, %r11d
	addl	%ecx, %ecx
	movl	112(%rsp), %edi
	decl	%edi
	movl	%edi, 20(%rsp)
	incl	%esi
	movl	%esi, 24(%rsp)
	movl	28(%rsp), %esi
	movl	%esi, %edi
	negl	%edi
	movl	%edi, 16(%rsp)
	xorl	%edi, %edi
	.align	16
.LBB119_40:	# bb92
	movslq	%ecx, %r8
	movsd	(%rbx,%r8,8), %xmm0
	movapd	%xmm3, %xmm1
	mulsd	%xmm0, %xmm1
	leal	1(%rcx), %r8d
	movslq	%r8d, %r8
	movsd	(%rbx,%r8,8), %xmm4
	movapd	%xmm2, %xmm5
	mulsd	%xmm4, %xmm5
	subsd	%xmm5, %xmm1
	pxor	%xmm5, %xmm5
	ucomisd	%xmm5, %xmm1
	mulsd	%xmm2, %xmm0
	mulsd	%xmm3, %xmm4
	addsd	%xmm0, %xmm4
	jne	.LBB119_42	# bb94
	jp	.LBB119_42	# bb94
.LBB119_41:	# bb92
	pxor	%xmm0, %xmm0
	ucomisd	%xmm0, %xmm4
	setnp	%r8b
	sete	%r9b
	testb	%r8b, %r9b
	jne	.LBB119_45	# bb104
.LBB119_42:	# bb94
	xorl	%r8d, %r8d
	testl	%eax, %eax
	movl	12(%rsp), %r9d
	cmovg	%r8d, %r9d
	movl	16(%rsp), %r15d
	leal	(%r15,%rdi), %r15d
	cmpl	28(%rsp), %edi
	cmovle	%r8d, %r15d
	movl	%r15d, %r8d
	imull	%eax, %r8d
	movl	24(%rsp), %r12d
	leal	(%r12,%rdi), %r12d
	movl	36(%rsp), %r13d
	cmpl	%r13d, %r12d
	cmovg	%r13d, %r12d
	cmpl	%r12d, %r15d
	jge	.LBB119_45	# bb104
.LBB119_43:	# bb.nph197
	leal	(%r15,%rsi), %r12d
	addl	%r12d, %r12d
	movl	36(%rsp), %r13d
	notl	%r13d
	cmpl	%r13d, %edx
	cmovge	%edx, %r13d
	addl	%r15d, %r13d
	notl	%r13d
	xorl	%r15d, %r15d
	.align	16
.LBB119_44:	# bb102
	movslq	%r12d, %rbp
	movsd	(%r14,%rbp,8), %xmm0
	movapd	%xmm0, %xmm5
	mulsd	%xmm1, %xmm5
	leal	1(%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r14,%rbp,8), %xmm6
	movapd	%xmm6, %xmm7
	mulsd	%xmm4, %xmm7
	subsd	%xmm7, %xmm5
	addl	%r9d, %r8d
	leal	(%r8,%r8), %r9d
	movslq	%r9d, %r9
	addsd	(%r10,%r9,8), %xmm5
	movsd	%xmm5, (%r10,%r9,8)
	mulsd	%xmm4, %xmm0
	mulsd	%xmm1, %xmm6
	addsd	%xmm0, %xmm6
	leal	1(,%r8,2), %r9d
	movslq	%r9d, %r9
	addsd	(%r10,%r9,8), %xmm6
	movsd	%xmm6, (%r10,%r9,8)
	addl	$2, %r12d
	incl	%r15d
	cmpl	%r13d, %r15d
	movl	%eax, %r9d
	jne	.LBB119_44	# bb102
.LBB119_45:	# bb104
	addl	%r11d, %ecx
	addl	20(%rsp), %esi
	decl	%edx
	incl	%edi
	cmpl	32(%rsp), %edi
	jne	.LBB119_40	# bb92
	jmp	.LBB119_69	# return
.LBB119_46:	# bb106
	cmpl	$113, %esi
	jne	.LBB119_58	# bb128
.LBB119_47:	# bb106
	cmpl	$101, 16(%rsp)
	jne	.LBB119_58	# bb128
.LBB119_48:	# bb110
	testl	%r11d, %r11d
	jg	.LBB119_76	# bb110.bb127.preheader_crit_edge
.LBB119_49:	# bb111
	movl	$1, %ecx
	subl	32(%rsp), %ecx
	imull	%r11d, %ecx
.LBB119_50:	# bb127.preheader
	cmpl	$0, 32(%rsp)
	jle	.LBB119_69	# return
.LBB119_51:	# bb.nph191
	movl	$1, %edx
	subl	36(%rsp), %edx
	imull	%eax, %edx
	movl	%edx, 12(%rsp)
	movl	$4294967294, %edx
	movl	24(%rsp), %esi
	subl	%esi, %edx
	addl	%r11d, %r11d
	addl	%ecx, %ecx
	movl	112(%rsp), %edi
	decl	%edi
	movl	%edi, 20(%rsp)
	incl	%esi
	movl	%esi, 24(%rsp)
	movl	28(%rsp), %esi
	movl	%esi, %edi
	negl	%edi
	movl	%edi, 16(%rsp)
	xorl	%edi, %edi
	.align	16
.LBB119_52:	# bb114
	movslq	%ecx, %r8
	movsd	(%rbx,%r8,8), %xmm0
	movapd	%xmm3, %xmm1
	mulsd	%xmm0, %xmm1
	leal	1(%rcx), %r8d
	movslq	%r8d, %r8
	movsd	(%rbx,%r8,8), %xmm4
	movapd	%xmm2, %xmm5
	mulsd	%xmm4, %xmm5
	subsd	%xmm5, %xmm1
	pxor	%xmm5, %xmm5
	ucomisd	%xmm5, %xmm1
	mulsd	%xmm2, %xmm0
	mulsd	%xmm3, %xmm4
	addsd	%xmm0, %xmm4
	jne	.LBB119_54	# bb116
	jp	.LBB119_54	# bb116
.LBB119_53:	# bb114
	pxor	%xmm0, %xmm0
	ucomisd	%xmm0, %xmm4
	setnp	%r8b
	sete	%r9b
	testb	%r8b, %r9b
	jne	.LBB119_57	# bb126
.LBB119_54:	# bb116
	xorl	%r8d, %r8d
	testl	%eax, %eax
	movl	12(%rsp), %r9d
	cmovg	%r8d, %r9d
	movl	16(%rsp), %r15d
	leal	(%r15,%rdi), %r15d
	cmpl	28(%rsp), %edi
	cmovle	%r8d, %r15d
	movl	%r15d, %r8d
	imull	%eax, %r8d
	movl	24(%rsp), %r12d
	leal	(%r12,%rdi), %r12d
	movl	36(%rsp), %r13d
	cmpl	%r13d, %r12d
	cmovg	%r13d, %r12d
	cmpl	%r12d, %r15d
	jge	.LBB119_57	# bb126
.LBB119_55:	# bb.nph188
	leal	(%r15,%rsi), %r12d
	addl	%r12d, %r12d
	movl	36(%rsp), %r13d
	notl	%r13d
	cmpl	%r13d, %edx
	cmovge	%edx, %r13d
	addl	%r15d, %r13d
	notl	%r13d
	xorl	%r15d, %r15d
	.align	16
.LBB119_56:	# bb124
	movslq	%r12d, %rbp
	movsd	(%r14,%rbp,8), %xmm0
	movapd	%xmm0, %xmm5
	mulsd	%xmm1, %xmm5
	leal	1(%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r14,%rbp,8), %xmm6
	movapd	%xmm6, %xmm7
	mulsd	%xmm4, %xmm7
	addsd	%xmm5, %xmm7
	addl	%r9d, %r8d
	leal	(%r8,%r8), %r9d
	movslq	%r9d, %r9
	addsd	(%r10,%r9,8), %xmm7
	movsd	%xmm7, (%r10,%r9,8)
	mulsd	%xmm4, %xmm0
	mulsd	%xmm1, %xmm6
	subsd	%xmm6, %xmm0
	leal	1(,%r8,2), %r9d
	movslq	%r9d, %r9
	addsd	(%r10,%r9,8), %xmm0
	movsd	%xmm0, (%r10,%r9,8)
	addl	$2, %r12d
	incl	%r15d
	cmpl	%r13d, %r15d
	movl	%eax, %r9d
	jne	.LBB119_56	# bb124
.LBB119_57:	# bb126
	addl	%r11d, %ecx
	addl	20(%rsp), %esi
	decl	%edx
	incl	%edi
	cmpl	32(%rsp), %edi
	jne	.LBB119_52	# bb114
	jmp	.LBB119_69	# return
.LBB119_58:	# bb128
	cmpl	$113, %esi
	jne	.LBB119_68	# bb148
.LBB119_59:	# bb128
	cmpl	$102, 16(%rsp)
	jne	.LBB119_68	# bb148
.LBB119_60:	# bb132
	testl	%eax, %eax
	jg	.LBB119_77	# bb132.bb147.preheader_crit_edge
.LBB119_61:	# bb133
	movl	$1, %ecx
	subl	36(%rsp), %ecx
	imull	%eax, %ecx
.LBB119_62:	# bb147.preheader
	cmpl	$0, 36(%rsp)
	jle	.LBB119_69	# return
.LBB119_63:	# bb.nph182
	movl	$1, %edx
	subl	32(%rsp), %edx
	imull	%r11d, %edx
	movl	%edx, 12(%rsp)
	movl	$4294967294, %edx
	movl	28(%rsp), %esi
	subl	%esi, %edx
	addl	%eax, %eax
	addl	%ecx, %ecx
	movl	112(%rsp), %edi
	decl	%edi
	movl	%edi, 20(%rsp)
	incl	%esi
	movl	%esi, 28(%rsp)
	movl	24(%rsp), %esi
	movl	%esi, %edi
	negl	%edi
	movl	%edi, 16(%rsp)
	xorl	%edi, %edi
	.align	16
.LBB119_64:	# bb136
	xorl	%r8d, %r8d
	testl	%r11d, %r11d
	movl	12(%rsp), %r9d
	cmovg	%r8d, %r9d
	movl	16(%rsp), %r15d
	leal	(%r15,%rdi), %r15d
	cmpl	24(%rsp), %edi
	cmovle	%r8d, %r15d
	movl	%r15d, %r8d
	imull	%r11d, %r8d
	movl	28(%rsp), %r12d
	leal	(%r12,%rdi), %r12d
	movl	32(%rsp), %r13d
	cmpl	%r13d, %r12d
	cmovg	%r13d, %r12d
	cmpl	%r12d, %r15d
	jge	.LBB119_78	# bb136.bb146_crit_edge
.LBB119_65:	# bb.nph177
	leal	(%r15,%rsi), %r12d
	addl	%r12d, %r12d
	movl	32(%rsp), %r13d
	notl	%r13d
	cmpl	%r13d, %edx
	cmovge	%edx, %r13d
	addl	%r15d, %r13d
	notl	%r13d
	pxor	%xmm0, %xmm0
	xorl	%r15d, %r15d
	movapd	%xmm0, %xmm1
	.align	16
.LBB119_66:	# bb144
	movslq	%r12d, %rbp
	movsd	(%r14,%rbp,8), %xmm4
	addl	%r9d, %r8d
	leal	1(,%r8,2), %r9d
	leal	(%r8,%r8), %ebp
	movslq	%ebp, %rbp
	movsd	(%rbx,%rbp,8), %xmm5
	movapd	%xmm4, %xmm6
	mulsd	%xmm5, %xmm6
	leal	1(%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r14,%rbp,8), %xmm7
	mulsd	%xmm7, %xmm5
	movslq	%r9d, %r9
	movsd	(%rbx,%r9,8), %xmm8
	mulsd	%xmm8, %xmm4
	subsd	%xmm5, %xmm4
	addsd	%xmm4, %xmm0
	mulsd	%xmm8, %xmm7
	addsd	%xmm6, %xmm7
	addsd	%xmm7, %xmm1
	addl	$2, %r12d
	incl	%r15d
	cmpl	%r13d, %r15d
	movl	%r11d, %r9d
	jne	.LBB119_66	# bb144
.LBB119_67:	# bb146
	movapd	%xmm2, %xmm4
	mulsd	%xmm0, %xmm4
	movapd	%xmm3, %xmm5
	mulsd	%xmm1, %xmm5
	subsd	%xmm4, %xmm5
	movslq	%ecx, %r8
	addsd	(%r10,%r8,8), %xmm5
	movsd	%xmm5, (%r10,%r8,8)
	mulsd	%xmm2, %xmm1
	mulsd	%xmm3, %xmm0
	addsd	%xmm1, %xmm0
	leal	1(%rcx), %r8d
	movslq	%r8d, %r8
	addsd	(%r10,%r8,8), %xmm0
	movsd	%xmm0, (%r10,%r8,8)
	addl	%eax, %ecx
	addl	20(%rsp), %esi
	decl	%edx
	incl	%edi
	cmpl	36(%rsp), %edi
	jne	.LBB119_64	# bb136
	jmp	.LBB119_69	# return
.LBB119_68:	# bb148
	xorl	%edi, %edi
	leaq	.str144, %rsi
	leaq	.str1145, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB119_69:	# return
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB119_70:	# bb36.bb39_crit_edge
	movl	%r9d, 28(%rsp)
	movl	%edx, 36(%rsp)
	jmp	.LBB119_7	# bb39
.LBB119_71:	# bb41.bb46.preheader_crit_edge
	xorl	%ecx, %ecx
	jmp	.LBB119_11	# bb46.preheader
.LBB119_72:	# bb49.bb54.preheader_crit_edge
	xorl	%ecx, %ecx
	jmp	.LBB119_18	# bb54.preheader
.LBB119_73:	# bb65.bb79.preheader_crit_edge
	xorl	%ecx, %ecx
	jmp	.LBB119_28	# bb79.preheader
.LBB119_74:	# bb69.bb78_crit_edge
	pxor	%xmm0, %xmm0
	movapd	%xmm0, %xmm1
	jmp	.LBB119_33	# bb78
.LBB119_75:	# bb88.bb105.preheader_crit_edge
	xorl	%ecx, %ecx
	jmp	.LBB119_38	# bb105.preheader
.LBB119_76:	# bb110.bb127.preheader_crit_edge
	xorl	%ecx, %ecx
	jmp	.LBB119_50	# bb127.preheader
.LBB119_77:	# bb132.bb147.preheader_crit_edge
	xorl	%ecx, %ecx
	jmp	.LBB119_62	# bb147.preheader
.LBB119_78:	# bb136.bb146_crit_edge
	pxor	%xmm0, %xmm0
	movapd	%xmm0, %xmm1
	jmp	.LBB119_67	# bb146
	.size	cblas_zgbmv, .-cblas_zgbmv
.Leh_func_end79:


	.section	.rodata.cst8,"aM",@progbits,8
	.align	8
.LCPI120_0:					
	.quad	4607182418800017408	# double value: 1.000000e+00
	.text
	.align	16
	.globl	cblas_zgemm
	.type	cblas_zgemm,@function
cblas_zgemm:
.Leh_func_begin80:
.Llabel80:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	movl	%ecx, 36(%rsp)
	movq	96(%rsp), %rax
	movsd	(%rax), %xmm0
	pxor	%xmm1, %xmm1
	ucomisd	%xmm1, %xmm0
	setp	%cl
	setne	%r10b
	setnp	%r11b
	sete	%bl
	andb	%r11b, %bl
	movsd	8(%rax), %xmm2
	ucomisd	%xmm1, %xmm2
	setp	%al
	setnp	%r11b
	sete	%r14b
	setne	%r15b
	andb	%r11b, %r14b
	andb	%bl, %r14b
	movb	%r14b, 7(%rsp)
	orb	%cl, %r10b
	orb	%al, %r15b
	orb	%r10b, %r15b
	testb	%r15b, %r15b
	movq	136(%rsp), %rax
	movsd	8(%rax), %xmm1
	movsd	(%rax), %xmm3
	movq	144(%rsp), %rax
	movq	120(%rsp), %rcx
	movl	112(%rsp), %r10d
	movl	%r10d, 32(%rsp)
	movq	104(%rsp), %r10
	jne	.LBB120_3	# bb17
.LBB120_1:	# entry
	ucomisd	.LCPI120_0(%rip), %xmm3
	jne	.LBB120_3	# bb17
	jp	.LBB120_3	# bb17
.LBB120_2:	# entry
	pxor	%xmm4, %xmm4
	ucomisd	%xmm4, %xmm1
	setnp	%r11b
	sete	%bl
	testb	%r11b, %bl
	jne	.LBB120_70	# return
.LBB120_3:	# bb17
	cmpl	$101, %edi
	je	.LBB120_71	# bb18
.LBB120_4:	# bb31
	cmpl	$111, %esi
	movl	$111, %edi
	movl	$112, %r11d
	cmove	%edi, %r11d
	movl	%r11d, 16(%rsp)
	cmpl	$113, %esi
	movl	$4294967295, %esi
	movl	$1, %r11d
	cmove	%esi, %r11d
	movl	%r11d, 28(%rsp)
	cmpl	$111, %edx
	movl	$112, %r11d
	cmove	%edi, %r11d
	movl	%r11d, 8(%rsp)
	cmpl	$113, %edx
	movl	$1, %edx
	cmove	%esi, %edx
	movl	%edx, 20(%rsp)
	movq	%r10, %rdx
	movq	%rcx, %r10
	movl	112(%rsp), %ecx
	movl	%ecx, 24(%rsp)
	movl	128(%rsp), %ecx
	movl	%ecx, 32(%rsp)
	movl	36(%rsp), %ecx
	movl	%r8d, 36(%rsp)
.LBB120_5:	# bb44
	pxor	%xmm4, %xmm4
	ucomisd	%xmm4, %xmm1
	jne	.LBB120_13	# bb52
	jp	.LBB120_13	# bb52
.LBB120_6:	# bb44
	pxor	%xmm4, %xmm4
	ucomisd	%xmm4, %xmm3
	jne	.LBB120_13	# bb52
	jp	.LBB120_13	# bb52
.LBB120_7:	# bb51.preheader
	cmpl	$0, 36(%rsp)
	jle	.LBB120_20	# bb60
.LBB120_8:	# bb.nph132
	testl	%ecx, %ecx
	jle	.LBB120_20	# bb60
.LBB120_9:	# bb49.preheader.preheader
	movl	152(%rsp), %esi
	leal	(%rsi,%rsi), %r8d
	xorl	%r11d, %r11d
	movl	%r11d, %ebx
	jmp	.LBB120_12	# bb49.preheader
	.align	16
.LBB120_10:	# bb48
	movslq	%edi, %r14
	movq	$0, (%rax,%r14,8)
	leal	1(%rdi), %r14d
	movslq	%r14d, %r14
	movq	$0, (%rax,%r14,8)
	addl	$2, %edi
	incl	%esi
	cmpl	%ecx, %esi
	jne	.LBB120_10	# bb48
.LBB120_11:	# bb50
	addl	%r8d, %r11d
	incl	%ebx
	cmpl	36(%rsp), %ebx
	je	.LBB120_20	# bb60
.LBB120_12:	# bb49.preheader
	xorl	%esi, %esi
	movl	%r11d, %edi
	jmp	.LBB120_10	# bb48
.LBB120_13:	# bb52
	pxor	%xmm4, %xmm4
	ucomisd	%xmm4, %xmm1
	setnp	%sil
	sete	%dil
	andb	%sil, %dil
	ucomisd	.LCPI120_0(%rip), %xmm3
	setnp	%sil
	sete	%r8b
	andb	%sil, %r8b
	testb	%r8b, %dil
	jne	.LBB120_20	# bb60
.LBB120_14:	# bb52
	cmpl	$0, 36(%rsp)
	jle	.LBB120_20	# bb60
.LBB120_15:	# bb.nph168
	testl	%ecx, %ecx
	jle	.LBB120_20	# bb60
.LBB120_16:	# bb57.preheader.preheader
	movl	152(%rsp), %esi
	leal	(%rsi,%rsi), %r8d
	xorl	%edi, %edi
	movl	%edi, %esi
	.align	16
.LBB120_17:	# bb57.preheader
	xorl	%r11d, %r11d
	movl	%edi, %ebx
	.align	16
.LBB120_18:	# bb56
	movslq	%ebx, %r14
	movsd	(%rax,%r14,8), %xmm4
	movapd	%xmm3, %xmm5
	mulsd	%xmm4, %xmm5
	leal	1(%rbx), %r15d
	movslq	%r15d, %r15
	movsd	(%rax,%r15,8), %xmm6
	movapd	%xmm1, %xmm7
	mulsd	%xmm6, %xmm7
	subsd	%xmm7, %xmm5
	movsd	%xmm5, (%rax,%r14,8)
	mulsd	%xmm1, %xmm4
	mulsd	%xmm3, %xmm6
	addsd	%xmm4, %xmm6
	movsd	%xmm6, (%rax,%r15,8)
	addl	$2, %ebx
	incl	%r11d
	cmpl	%ecx, %r11d
	jne	.LBB120_18	# bb56
.LBB120_19:	# bb58
	addl	%r8d, %edi
	incl	%esi
	cmpl	36(%rsp), %esi
	jne	.LBB120_17	# bb57.preheader
.LBB120_20:	# bb60
	testb	$1, 7(%rsp)
	jne	.LBB120_70	# return
.LBB120_21:	# bb62
	cmpl	$111, 8(%rsp)
	jne	.LBB120_33	# bb76
.LBB120_22:	# bb62
	cmpl	$111, 16(%rsp)
	jne	.LBB120_33	# bb76
.LBB120_23:	# bb75.preheader
	testl	%r9d, %r9d
	jle	.LBB120_70	# return
.LBB120_24:	# bb.nph164
	cmpl	$0, 36(%rsp)
	cvtsi2sd	28(%rsp), %xmm1
	cvtsi2sd	20(%rsp), %xmm3
	jle	.LBB120_70	# return
.LBB120_25:	# bb73.preheader.preheader
	movl	24(%rsp), %esi
	addl	%esi, %esi
	movl	%esi, 24(%rsp)
	xorl	%ebx, %ebx
	movl	%ebx, 20(%rsp)
	jmp	.LBB120_32	# bb73.preheader
	.align	16
.LBB120_26:	# bb67
	movslq	%r14d, %r15
	leal	1(%r14), %r12d
	movslq	%r12d, %r12
	movapd	%xmm3, %xmm4
	mulsd	(%r10,%r12,8), %xmm4
	movapd	%xmm0, %xmm5
	mulsd	%xmm4, %xmm5
	movsd	(%r10,%r15,8), %xmm6
	movapd	%xmm2, %xmm7
	mulsd	%xmm6, %xmm7
	addsd	%xmm5, %xmm7
	pxor	%xmm5, %xmm5
	ucomisd	%xmm5, %xmm7
	setnp	%r15b
	sete	%r12b
	andb	%r15b, %r12b
	mulsd	%xmm2, %xmm4
	mulsd	%xmm0, %xmm6
	subsd	%xmm4, %xmm6
	ucomisd	%xmm5, %xmm6
	setnp	%r15b
	sete	%r13b
	andb	%r15b, %r13b
	testl	%ecx, %ecx
	setle	%r15b
	testb	%r12b, %r13b
	jne	.LBB120_30	# bb72
.LBB120_27:	# bb67
	testb	$1, %r15b
	jne	.LBB120_30	# bb72
.LBB120_28:	# bb.nph160
	leal	1(%rsi), %r15d
	xorl	%r12d, %r12d
	movl	%r12d, %r13d
	.align	16
.LBB120_29:	# bb70
	leal	(%r11,%r12), %ebp
	movslq	%ebp, %rbp
	movapd	%xmm1, %xmm4
	mulsd	(%rdx,%rbp,8), %xmm4
	movapd	%xmm7, %xmm5
	mulsd	%xmm4, %xmm5
	leal	(%rbx,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%rdx,%rbp,8), %xmm8
	movapd	%xmm6, %xmm9
	mulsd	%xmm8, %xmm9
	subsd	%xmm5, %xmm9
	leal	(%rsi,%r12), %ebp
	movslq	%ebp, %rbp
	addsd	(%rax,%rbp,8), %xmm9
	movsd	%xmm9, (%rax,%rbp,8)
	mulsd	%xmm7, %xmm8
	mulsd	%xmm6, %xmm4
	addsd	%xmm8, %xmm4
	leal	(%r15,%r12), %ebp
	movslq	%ebp, %rbp
	addsd	(%rax,%rbp,8), %xmm4
	movsd	%xmm4, (%rax,%rbp,8)
	addl	$2, %r12d
	incl	%r13d
	cmpl	%ecx, %r13d
	jne	.LBB120_29	# bb70
.LBB120_30:	# bb72
	addl	28(%rsp), %r14d
	addl	%r8d, %esi
	incl	%edi
	cmpl	36(%rsp), %edi
	jne	.LBB120_26	# bb67
.LBB120_31:	# bb74
	addl	24(%rsp), %ebx
	movl	20(%rsp), %esi
	incl	%esi
	movl	%esi, 20(%rsp)
	cmpl	%r9d, %esi
	je	.LBB120_70	# return
.LBB120_32:	# bb73.preheader
	leal	1(%rbx), %r11d
	movl	20(%rsp), %esi
	leal	(%rsi,%rsi), %r14d
	movl	32(%rsp), %esi
	leal	(%rsi,%rsi), %esi
	movl	%esi, 28(%rsp)
	movl	152(%rsp), %esi
	leal	(%rsi,%rsi), %r8d
	xorl	%esi, %esi
	movl	%esi, %edi
	jmp	.LBB120_26	# bb67
.LBB120_33:	# bb76
	cmpl	$111, 8(%rsp)
	jne	.LBB120_45	# bb89
.LBB120_34:	# bb76
	cmpl	$112, 16(%rsp)
	jne	.LBB120_45	# bb89
.LBB120_35:	# bb88.preheader
	cmpl	$0, 36(%rsp)
	jle	.LBB120_70	# return
.LBB120_36:	# bb.nph158
	testl	%ecx, %ecx
	cvtsi2sd	28(%rsp), %xmm1
	cvtsi2sd	20(%rsp), %xmm3
	jle	.LBB120_70	# return
.LBB120_37:	# bb86.preheader.preheader
	movl	32(%rsp), %esi
	addl	%esi, %esi
	movl	%esi, 32(%rsp)
	movl	152(%rsp), %esi
	addl	%esi, %esi
	movl	%esi, 12(%rsp)
	xorl	%r11d, %r11d
	movl	%r11d, 28(%rsp)
	movl	%r11d, 20(%rsp)
	jmp	.LBB120_44	# bb86.preheader
.LBB120_38:	# bb.nph152
	leal	1(%rbx), %r15d
	pxor	%xmm4, %xmm4
	xorl	%r12d, %r12d
	movl	%r12d, %r13d
	movapd	%xmm4, %xmm5
	.align	16
.LBB120_39:	# bb83
	leal	(%rbx,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%rdx,%rbp,8), %xmm6
	leal	(%rdi,%r12), %ebp
	movslq	%ebp, %rbp
	movapd	%xmm3, %xmm7
	mulsd	(%r10,%rbp,8), %xmm7
	movapd	%xmm7, %xmm8
	mulsd	%xmm6, %xmm8
	leal	(%r15,%r12), %ebp
	movslq	%ebp, %rbp
	movapd	%xmm1, %xmm9
	mulsd	(%rdx,%rbp,8), %xmm9
	leal	(%r11,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r10,%rbp,8), %xmm10
	movapd	%xmm10, %xmm11
	mulsd	%xmm9, %xmm11
	addsd	%xmm8, %xmm11
	addsd	%xmm11, %xmm5
	mulsd	%xmm7, %xmm9
	mulsd	%xmm6, %xmm10
	subsd	%xmm9, %xmm10
	addsd	%xmm10, %xmm4
	addl	$2, %r12d
	incl	%r13d
	cmpl	%r9d, %r13d
	jne	.LBB120_39	# bb83
.LBB120_40:	# bb85
	movapd	%xmm2, %xmm6
	mulsd	%xmm5, %xmm6
	movapd	%xmm0, %xmm7
	mulsd	%xmm4, %xmm7
	subsd	%xmm6, %xmm7
	movslq	%esi, %r15
	addsd	(%rax,%r15,8), %xmm7
	movsd	%xmm7, (%rax,%r15,8)
	mulsd	%xmm2, %xmm4
	mulsd	%xmm0, %xmm5
	addsd	%xmm4, %xmm5
	leal	1(%rsi), %r15d
	movslq	%r15d, %r15
	addsd	(%rax,%r15,8), %xmm5
	movsd	%xmm5, (%rax,%r15,8)
	addl	%r8d, %ebx
	addl	$2, %esi
	incl	%r14d
	cmpl	%ecx, %r14d
	je	.LBB120_43	# bb87
.LBB120_41:	# bb84.preheader
	testl	%r9d, %r9d
	jg	.LBB120_38	# bb.nph152
.LBB120_42:	# bb84.preheader.bb85_crit_edge
	pxor	%xmm4, %xmm4
	movapd	%xmm4, %xmm5
	jmp	.LBB120_40	# bb85
.LBB120_43:	# bb87
	addl	32(%rsp), %r11d
	movl	28(%rsp), %esi
	addl	12(%rsp), %esi
	movl	%esi, 28(%rsp)
	movl	20(%rsp), %esi
	incl	%esi
	movl	%esi, 20(%rsp)
	cmpl	36(%rsp), %esi
	je	.LBB120_70	# return
.LBB120_44:	# bb86.preheader
	leal	1(%r11), %edi
	movl	24(%rsp), %esi
	leal	(%rsi,%rsi), %r8d
	xorl	%ebx, %ebx
	movl	28(%rsp), %esi
	movl	%ebx, %r14d
	jmp	.LBB120_41	# bb84.preheader
.LBB120_45:	# bb89
	cmpl	$112, 8(%rsp)
	jne	.LBB120_57	# bb104
.LBB120_46:	# bb89
	cmpl	$111, 16(%rsp)
	jne	.LBB120_57	# bb104
.LBB120_47:	# bb103.preheader
	testl	%r9d, %r9d
	jle	.LBB120_70	# return
.LBB120_48:	# bb.nph148
	cmpl	$0, 36(%rsp)
	cvtsi2sd	28(%rsp), %xmm1
	cvtsi2sd	20(%rsp), %xmm3
	jle	.LBB120_70	# return
.LBB120_49:	# bb101.preheader.preheader
	movl	24(%rsp), %esi
	addl	%esi, %esi
	movl	%esi, 24(%rsp)
	movl	32(%rsp), %esi
	addl	%esi, %esi
	movl	%esi, 32(%rsp)
	xorl	%esi, %esi
	movl	%esi, 28(%rsp)
	movl	%esi, 20(%rsp)
	jmp	.LBB120_56	# bb101.preheader
	.align	16
.LBB120_50:	# bb95
	movslq	%r8d, %r15
	leal	1(%r8), %r12d
	movslq	%r12d, %r12
	movapd	%xmm3, %xmm4
	mulsd	(%r10,%r12,8), %xmm4
	movapd	%xmm0, %xmm5
	mulsd	%xmm4, %xmm5
	movsd	(%r10,%r15,8), %xmm6
	movapd	%xmm2, %xmm7
	mulsd	%xmm6, %xmm7
	addsd	%xmm5, %xmm7
	pxor	%xmm5, %xmm5
	ucomisd	%xmm5, %xmm7
	setnp	%r15b
	sete	%r12b
	andb	%r15b, %r12b
	mulsd	%xmm2, %xmm4
	mulsd	%xmm0, %xmm6
	subsd	%xmm4, %xmm6
	ucomisd	%xmm5, %xmm6
	setnp	%r15b
	sete	%r13b
	andb	%r15b, %r13b
	testl	%ecx, %ecx
	setle	%r15b
	testb	%r12b, %r13b
	jne	.LBB120_54	# bb100
.LBB120_51:	# bb95
	testb	$1, %r15b
	jne	.LBB120_54	# bb100
.LBB120_52:	# bb.nph144
	leal	1(%r14), %r15d
	xorl	%r12d, %r12d
	movl	%r12d, %r13d
	.align	16
.LBB120_53:	# bb98
	leal	(%rbx,%r12), %ebp
	movslq	%ebp, %rbp
	movapd	%xmm1, %xmm4
	mulsd	(%rdx,%rbp,8), %xmm4
	movapd	%xmm7, %xmm5
	mulsd	%xmm4, %xmm5
	leal	(%rsi,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%rdx,%rbp,8), %xmm8
	movapd	%xmm6, %xmm9
	mulsd	%xmm8, %xmm9
	subsd	%xmm5, %xmm9
	leal	(%r14,%r12), %ebp
	movslq	%ebp, %rbp
	addsd	(%rax,%rbp,8), %xmm9
	movsd	%xmm9, (%rax,%rbp,8)
	mulsd	%xmm7, %xmm8
	mulsd	%xmm6, %xmm4
	addsd	%xmm8, %xmm4
	leal	(%r15,%r12), %ebp
	movslq	%ebp, %rbp
	addsd	(%rax,%rbp,8), %xmm4
	movsd	%xmm4, (%rax,%rbp,8)
	addl	$2, %r12d
	incl	%r13d
	cmpl	%ecx, %r13d
	jne	.LBB120_53	# bb98
.LBB120_54:	# bb100
	addl	%r11d, %r14d
	addl	$2, %r8d
	incl	%edi
	cmpl	36(%rsp), %edi
	jne	.LBB120_50	# bb95
.LBB120_55:	# bb102
	addl	24(%rsp), %esi
	movl	28(%rsp), %edi
	addl	32(%rsp), %edi
	movl	%edi, 28(%rsp)
	movl	20(%rsp), %edi
	incl	%edi
	movl	%edi, 20(%rsp)
	cmpl	%r9d, %edi
	je	.LBB120_70	# return
.LBB120_56:	# bb101.preheader
	leal	1(%rsi), %ebx
	movl	152(%rsp), %edi
	leal	(%rdi,%rdi), %r11d
	xorl	%r14d, %r14d
	movl	28(%rsp), %r8d
	movl	%r14d, %edi
	jmp	.LBB120_50	# bb95
.LBB120_57:	# bb104
	cmpl	$112, 8(%rsp)
	jne	.LBB120_69	# bb117
.LBB120_58:	# bb104
	cmpl	$112, 16(%rsp)
	jne	.LBB120_69	# bb117
.LBB120_59:	# bb116.preheader
	cmpl	$0, 36(%rsp)
	jle	.LBB120_70	# return
.LBB120_60:	# bb.nph142
	testl	%ecx, %ecx
	cvtsi2sd	28(%rsp), %xmm3
	cvtsi2sd	20(%rsp), %xmm1
	jle	.LBB120_70	# return
.LBB120_61:	# bb114.preheader.preheader
	movl	152(%rsp), %esi
	addl	%esi, %esi
	movl	%esi, 12(%rsp)
	xorl	%esi, %esi
	movl	%esi, 16(%rsp)
	movl	%esi, 20(%rsp)
	jmp	.LBB120_68	# bb114.preheader
.LBB120_62:	# bb.nph136
	movl	32(%rsp), %ebx
	leal	(%rbx,%rbx), %ebx
	pxor	%xmm4, %xmm4
	xorl	%r14d, %r14d
	movl	28(%rsp), %r15d
	movl	%esi, %r12d
	movapd	%xmm4, %xmm5
	.align	16
.LBB120_63:	# bb111
	movslq	%r15d, %r13
	leal	1(%r15), %ebp
	movslq	%ebp, %rbp
	movapd	%xmm1, %xmm6
	mulsd	(%r10,%rbp,8), %xmm6
	movslq	%r12d, %rbp
	movsd	(%rdx,%rbp,8), %xmm7
	movapd	%xmm6, %xmm8
	mulsd	%xmm7, %xmm8
	leal	1(%r12), %ebp
	movslq	%ebp, %rbp
	movapd	%xmm3, %xmm9
	mulsd	(%rdx,%rbp,8), %xmm9
	movsd	(%r10,%r13,8), %xmm10
	movapd	%xmm10, %xmm11
	mulsd	%xmm9, %xmm11
	addsd	%xmm8, %xmm11
	addsd	%xmm11, %xmm4
	mulsd	%xmm6, %xmm9
	mulsd	%xmm7, %xmm10
	subsd	%xmm9, %xmm10
	addsd	%xmm10, %xmm5
	addl	%ebx, %r15d
	addl	$2, %r12d
	incl	%r14d
	cmpl	%r9d, %r14d
	jne	.LBB120_63	# bb111
.LBB120_64:	# bb113
	movapd	%xmm2, %xmm6
	mulsd	%xmm4, %xmm6
	movapd	%xmm0, %xmm7
	mulsd	%xmm5, %xmm7
	subsd	%xmm6, %xmm7
	movslq	%r11d, %rbx
	addsd	(%rax,%rbx,8), %xmm7
	movsd	%xmm7, (%rax,%rbx,8)
	mulsd	%xmm2, %xmm5
	mulsd	%xmm0, %xmm4
	addsd	%xmm5, %xmm4
	leal	1(%r11), %ebx
	movslq	%ebx, %rbx
	addsd	(%rax,%rbx,8), %xmm4
	movsd	%xmm4, (%rax,%rbx,8)
	addl	%r8d, %esi
	addl	$2, %r11d
	incl	%edi
	cmpl	%ecx, %edi
	je	.LBB120_67	# bb115
.LBB120_65:	# bb112.preheader
	testl	%r9d, %r9d
	jg	.LBB120_62	# bb.nph136
.LBB120_66:	# bb112.preheader.bb113_crit_edge
	pxor	%xmm4, %xmm4
	movapd	%xmm4, %xmm5
	jmp	.LBB120_64	# bb113
.LBB120_67:	# bb115
	movl	16(%rsp), %esi
	addl	12(%rsp), %esi
	movl	%esi, 16(%rsp)
	movl	20(%rsp), %esi
	incl	%esi
	movl	%esi, 20(%rsp)
	cmpl	36(%rsp), %esi
	je	.LBB120_70	# return
.LBB120_68:	# bb114.preheader
	movl	20(%rsp), %esi
	leal	(%rsi,%rsi), %esi
	movl	%esi, 28(%rsp)
	movl	24(%rsp), %esi
	leal	(%rsi,%rsi), %r8d
	xorl	%esi, %esi
	movl	16(%rsp), %r11d
	movl	%esi, %edi
	jmp	.LBB120_65	# bb112.preheader
.LBB120_69:	# bb117
	xorl	%edi, %edi
	leaq	.str147, %rsi
	leaq	.str1148, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB120_70:	# return
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB120_71:	# bb18
	cmpl	$111, %edx
	movl	$111, %edi
	movl	$112, %r11d
	cmove	%edi, %r11d
	movl	%r11d, 16(%rsp)
	cmpl	$113, %edx
	movl	$4294967295, %edx
	movl	$1, %r11d
	cmove	%edx, %r11d
	movl	%r11d, 28(%rsp)
	cmpl	$111, %esi
	movl	$112, %r11d
	cmove	%edi, %r11d
	movl	%r11d, 8(%rsp)
	cmpl	$113, %esi
	movl	$1, %esi
	cmove	%edx, %esi
	movl	%esi, 20(%rsp)
	movq	%rcx, %rdx
	movl	128(%rsp), %ecx
	movl	%ecx, 24(%rsp)
	movl	%r8d, %ecx
	jmp	.LBB120_5	# bb44
	.size	cblas_zgemm, .-cblas_zgemm
.Leh_func_end80:


	.section	.rodata.cst8,"aM",@progbits,8
	.align	8
.LCPI121_0:					
	.quad	4607182418800017408	# double value: 1.000000e+00
	.text
	.align	16
	.globl	cblas_zgemv
	.type	cblas_zgemv,@function
cblas_zgemv:
.Leh_func_begin81:
.Llabel81:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$24, %rsp
	testl	%ecx, %ecx
	movq	104(%rsp), %rax
	movsd	8(%rax), %xmm0
	movsd	(%rax), %xmm1
	movsd	8(%r8), %xmm2
	movsd	(%r8), %xmm3
	movq	112(%rsp), %rax
	movq	88(%rsp), %r8
	je	.LBB121_63	# return
.LBB121_1:	# entry
	testl	%edx, %edx
	je	.LBB121_63	# return
.LBB121_2:	# bb
	pxor	%xmm4, %xmm4
	ucomisd	%xmm4, %xmm3
	setp	%r10b
	setne	%r11b
	setnp	%bl
	sete	%r14b
	andb	%bl, %r14b
	ucomisd	%xmm4, %xmm2
	setp	%bl
	setnp	%r15b
	sete	11(%rsp)
	setne	%r12b
	andb	%r15b, 11(%rsp)
	andb	%r14b, 11(%rsp)
	orb	%r10b, %r11b
	orb	%bl, %r12b
	orb	%r11b, %r12b
	testb	%r12b, %r12b
	jne	.LBB121_5	# bb32
.LBB121_3:	# bb
	ucomisd	.LCPI121_0(%rip), %xmm1
	jne	.LBB121_5	# bb32
	jp	.LBB121_5	# bb32
.LBB121_4:	# bb
	pxor	%xmm4, %xmm4
	ucomisd	%xmm4, %xmm0
	setnp	%r10b
	sete	%r11b
	testb	%r10b, %r11b
	jne	.LBB121_63	# return
.LBB121_5:	# bb32
	cmpl	$111, %esi
	movl	%edx, %r10d
	cmove	%ecx, %r10d
	cmove	%edx, %ecx
	pxor	%xmm4, %xmm4
	ucomisd	%xmm4, %xmm0
	jne	.LBB121_12	# bb43
	jp	.LBB121_12	# bb43
.LBB121_6:	# bb32
	pxor	%xmm4, %xmm4
	ucomisd	%xmm4, %xmm1
	jne	.LBB121_12	# bb43
	jp	.LBB121_12	# bb43
.LBB121_7:	# bb37
	cmpl	$0, 120(%rsp)
	jg	.LBB121_64	# bb37.bb42.preheader_crit_edge
.LBB121_8:	# bb38
	movl	$1, %edx
	subl	%ecx, %edx
	imull	120(%rsp), %edx
.LBB121_9:	# bb42.preheader
	testl	%ecx, %ecx
	jle	.LBB121_19	# bb51
.LBB121_10:	# bb.nph
	movl	120(%rsp), %r11d
	leal	(%r11,%r11), %r11d
	addl	%edx, %edx
	xorl	%ebx, %ebx
	.align	16
.LBB121_11:	# bb41
	movslq	%edx, %r14
	movq	$0, (%rax,%r14,8)
	leal	(%r11,%rdx), %r14d
	incl	%edx
	movslq	%edx, %rdx
	movq	$0, (%rax,%rdx,8)
	incl	%ebx
	cmpl	%ecx, %ebx
	movl	%r14d, %edx
	jne	.LBB121_11	# bb41
	jmp	.LBB121_19	# bb51
.LBB121_12:	# bb43
	pxor	%xmm4, %xmm4
	ucomisd	%xmm4, %xmm0
	jne	.LBB121_14	# bb45
	jp	.LBB121_14	# bb45
.LBB121_13:	# bb43
	ucomisd	.LCPI121_0(%rip), %xmm1
	setnp	%dl
	sete	%r11b
	testb	%dl, %r11b
	jne	.LBB121_19	# bb51
.LBB121_14:	# bb45
	cmpl	$0, 120(%rsp)
	jg	.LBB121_65	# bb45.bb50.preheader_crit_edge
.LBB121_15:	# bb46
	movl	$1, %edx
	subl	%ecx, %edx
	imull	120(%rsp), %edx
.LBB121_16:	# bb50.preheader
	testl	%ecx, %ecx
	jle	.LBB121_19	# bb51
.LBB121_17:	# bb.nph172
	movl	120(%rsp), %r11d
	leal	(%r11,%r11), %r11d
	addl	%edx, %edx
	xorl	%ebx, %ebx
	.align	16
.LBB121_18:	# bb49
	movslq	%edx, %r14
	movsd	(%rax,%r14,8), %xmm4
	movapd	%xmm4, %xmm5
	mulsd	%xmm1, %xmm5
	leal	1(%rdx), %r15d
	movslq	%r15d, %r15
	movsd	(%rax,%r15,8), %xmm6
	movapd	%xmm6, %xmm7
	mulsd	%xmm0, %xmm7
	subsd	%xmm7, %xmm5
	movsd	%xmm5, (%rax,%r14,8)
	mulsd	%xmm0, %xmm4
	mulsd	%xmm1, %xmm6
	addsd	%xmm4, %xmm6
	movsd	%xmm6, (%rax,%r15,8)
	addl	%r11d, %edx
	incl	%ebx
	cmpl	%ecx, %ebx
	jne	.LBB121_18	# bb49
.LBB121_19:	# bb51
	testb	$1, 11(%rsp)
	jne	.LBB121_63	# return
.LBB121_20:	# bb53
	cmpl	$111, %esi
	jne	.LBB121_22	# bb57
.LBB121_21:	# bb53
	cmpl	$101, %edi
	je	.LBB121_24	# bb61
.LBB121_22:	# bb57
	cmpl	$112, %esi
	jne	.LBB121_32	# bb73
.LBB121_23:	# bb57
	cmpl	$102, %edi
	jne	.LBB121_32	# bb73
.LBB121_24:	# bb61
	cmpl	$0, 120(%rsp)
	jg	.LBB121_66	# bb61.bb72.preheader_crit_edge
.LBB121_25:	# bb62
	movl	$1, %edx
	subl	%ecx, %edx
	imull	120(%rsp), %edx
.LBB121_26:	# bb72.preheader
	testl	%ecx, %ecx
	jle	.LBB121_63	# return
.LBB121_27:	# bb.nph169
	movl	$1, %esi
	subl	%r10d, %esi
	imull	96(%rsp), %esi
	movl	120(%rsp), %edi
	addl	%edi, %edi
	movl	%edi, 12(%rsp)
	addl	%edx, %edx
	movl	80(%rsp), %edi
	addl	%edi, %edi
	movl	%edi, 20(%rsp)
	xorl	%edi, %edi
	movl	%edi, %r11d
	.align	16
.LBB121_28:	# bb65
	cmpl	$0, 96(%rsp)
	movl	$0, %ebx
	cmovle	%esi, %ebx
	testl	%r10d, %r10d
	jle	.LBB121_67	# bb65.bb71_crit_edge
.LBB121_29:	# bb.nph164
	movl	96(%rsp), %r14d
	leal	(%r14,%r14), %r14d
	addl	%ebx, %ebx
	pxor	%xmm0, %xmm0
	xorl	%r15d, %r15d
	movl	%edi, %r12d
	movapd	%xmm0, %xmm1
	.align	16
.LBB121_30:	# bb69
	movslq	%ebx, %r13
	movsd	(%r8,%r13,8), %xmm4
	movslq	%r12d, %r13
	leal	1(%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r9,%rbp,8), %xmm5
	movapd	%xmm5, %xmm6
	mulsd	%xmm4, %xmm6
	movsd	(%r9,%r13,8), %xmm7
	leal	1(%rbx), %r13d
	movslq	%r13d, %r13
	movsd	(%r8,%r13,8), %xmm8
	movapd	%xmm7, %xmm9
	mulsd	%xmm8, %xmm9
	addsd	%xmm6, %xmm9
	addsd	%xmm9, %xmm1
	mulsd	%xmm4, %xmm7
	mulsd	%xmm8, %xmm5
	subsd	%xmm5, %xmm7
	addsd	%xmm7, %xmm0
	addl	%r14d, %ebx
	addl	$2, %r12d
	incl	%r15d
	cmpl	%r10d, %r15d
	jne	.LBB121_30	# bb69
.LBB121_31:	# bb71
	movapd	%xmm2, %xmm4
	mulsd	%xmm1, %xmm4
	movapd	%xmm3, %xmm5
	mulsd	%xmm0, %xmm5
	subsd	%xmm4, %xmm5
	movslq	%edx, %rbx
	addsd	(%rax,%rbx,8), %xmm5
	movsd	%xmm5, (%rax,%rbx,8)
	mulsd	%xmm2, %xmm0
	mulsd	%xmm3, %xmm1
	addsd	%xmm0, %xmm1
	leal	1(%rdx), %ebx
	movslq	%ebx, %rbx
	addsd	(%rax,%rbx,8), %xmm1
	movsd	%xmm1, (%rax,%rbx,8)
	addl	12(%rsp), %edx
	addl	20(%rsp), %edi
	incl	%r11d
	cmpl	%ecx, %r11d
	jne	.LBB121_28	# bb65
	jmp	.LBB121_63	# return
.LBB121_32:	# bb73
	cmpl	$102, %edi
	sete	%dl
	cmpl	$111, %esi
	sete	%r11b
	andb	%dl, %r11b
	cmpl	$101, %edi
	sete	%dl
	cmpl	$112, %esi
	sete	%bl
	testb	%dl, %bl
	jne	.LBB121_34	# bb81
.LBB121_33:	# bb73
	notb	%r11b
	testb	$1, %r11b
	jne	.LBB121_42	# bb93
.LBB121_34:	# bb81
	cmpl	$0, 96(%rsp)
	jg	.LBB121_68	# bb81.bb92.preheader_crit_edge
.LBB121_35:	# bb82
	movl	$1, %edx
	subl	%r10d, %edx
	imull	96(%rsp), %edx
.LBB121_36:	# bb92.preheader
	testl	%r10d, %r10d
	jle	.LBB121_63	# return
.LBB121_37:	# bb.nph159
	movl	$1, %esi
	subl	%ecx, %esi
	imull	120(%rsp), %esi
	movl	96(%rsp), %edi
	addl	%edi, %edi
	movl	%edi, 16(%rsp)
	addl	%edx, %edx
	movl	80(%rsp), %edi
	addl	%edi, %edi
	movl	%edi, 20(%rsp)
	xorl	%edi, %edi
	movl	%edi, %r11d
	.align	16
.LBB121_38:	# bb85
	cmpl	$0, 120(%rsp)
	movl	$0, %ebx
	cmovle	%esi, %ebx
	movslq	%edx, %r14
	movsd	(%r8,%r14,8), %xmm0
	movapd	%xmm2, %xmm1
	mulsd	%xmm0, %xmm1
	leal	1(%rdx), %r14d
	movslq	%r14d, %r14
	movsd	(%r8,%r14,8), %xmm4
	movapd	%xmm3, %xmm5
	mulsd	%xmm4, %xmm5
	addsd	%xmm1, %xmm5
	mulsd	%xmm3, %xmm0
	mulsd	%xmm2, %xmm4
	subsd	%xmm4, %xmm0
	testl	%ecx, %ecx
	jle	.LBB121_41	# bb91
.LBB121_39:	# bb.nph156
	movl	120(%rsp), %r14d
	leal	(%r14,%r14), %r14d
	addl	%ebx, %ebx
	xorl	%r15d, %r15d
	movl	%edi, %r12d
	.align	16
.LBB121_40:	# bb89
	movslq	%r12d, %r13
	movsd	(%r9,%r13,8), %xmm1
	movapd	%xmm1, %xmm4
	mulsd	%xmm0, %xmm4
	leal	1(%r12), %r13d
	movslq	%r13d, %r13
	movsd	(%r9,%r13,8), %xmm6
	movapd	%xmm6, %xmm7
	mulsd	%xmm5, %xmm7
	subsd	%xmm7, %xmm4
	movslq	%ebx, %r13
	addsd	(%rax,%r13,8), %xmm4
	movsd	%xmm4, (%rax,%r13,8)
	mulsd	%xmm5, %xmm1
	mulsd	%xmm0, %xmm6
	addsd	%xmm1, %xmm6
	leal	1(%rbx), %r13d
	movslq	%r13d, %r13
	addsd	(%rax,%r13,8), %xmm6
	movsd	%xmm6, (%rax,%r13,8)
	addl	%r14d, %ebx
	addl	$2, %r12d
	incl	%r15d
	cmpl	%ecx, %r15d
	jne	.LBB121_40	# bb89
.LBB121_41:	# bb91
	addl	16(%rsp), %edx
	addl	20(%rsp), %edi
	incl	%r11d
	cmpl	%r10d, %r11d
	jne	.LBB121_38	# bb85
	jmp	.LBB121_63	# return
.LBB121_42:	# bb93
	cmpl	$113, %esi
	jne	.LBB121_52	# bb109
.LBB121_43:	# bb93
	cmpl	$101, %edi
	jne	.LBB121_52	# bb109
.LBB121_44:	# bb97
	cmpl	$0, 96(%rsp)
	jg	.LBB121_69	# bb97.bb108.preheader_crit_edge
.LBB121_45:	# bb98
	movl	$1, %edx
	subl	%r10d, %edx
	imull	96(%rsp), %edx
.LBB121_46:	# bb108.preheader
	testl	%r10d, %r10d
	jle	.LBB121_63	# return
.LBB121_47:	# bb.nph153
	movl	$1, %esi
	subl	%ecx, %esi
	imull	120(%rsp), %esi
	movl	96(%rsp), %edi
	addl	%edi, %edi
	movl	%edi, 16(%rsp)
	addl	%edx, %edx
	movl	80(%rsp), %edi
	addl	%edi, %edi
	movl	%edi, 20(%rsp)
	xorl	%edi, %edi
	movl	%edi, %r11d
	.align	16
.LBB121_48:	# bb101
	cmpl	$0, 120(%rsp)
	movl	$0, %ebx
	cmovle	%esi, %ebx
	movslq	%edx, %r14
	movsd	(%r8,%r14,8), %xmm0
	movapd	%xmm2, %xmm1
	mulsd	%xmm0, %xmm1
	leal	1(%rdx), %r14d
	movslq	%r14d, %r14
	movsd	(%r8,%r14,8), %xmm4
	movapd	%xmm3, %xmm5
	mulsd	%xmm4, %xmm5
	addsd	%xmm1, %xmm5
	mulsd	%xmm3, %xmm0
	mulsd	%xmm2, %xmm4
	subsd	%xmm4, %xmm0
	testl	%ecx, %ecx
	jle	.LBB121_51	# bb107
.LBB121_49:	# bb.nph150
	movl	120(%rsp), %r14d
	leal	(%r14,%r14), %r14d
	addl	%ebx, %ebx
	xorl	%r15d, %r15d
	movl	%edi, %r12d
	.align	16
.LBB121_50:	# bb105
	movslq	%r12d, %r13
	movsd	(%r9,%r13,8), %xmm1
	movapd	%xmm1, %xmm4
	mulsd	%xmm0, %xmm4
	leal	1(%r12), %r13d
	movslq	%r13d, %r13
	movsd	(%r9,%r13,8), %xmm6
	movapd	%xmm6, %xmm7
	mulsd	%xmm5, %xmm7
	addsd	%xmm4, %xmm7
	movslq	%ebx, %r13
	addsd	(%rax,%r13,8), %xmm7
	movsd	%xmm7, (%rax,%r13,8)
	mulsd	%xmm5, %xmm1
	mulsd	%xmm0, %xmm6
	subsd	%xmm6, %xmm1
	leal	1(%rbx), %r13d
	movslq	%r13d, %r13
	addsd	(%rax,%r13,8), %xmm1
	movsd	%xmm1, (%rax,%r13,8)
	addl	%r14d, %ebx
	addl	$2, %r12d
	incl	%r15d
	cmpl	%ecx, %r15d
	jne	.LBB121_50	# bb105
.LBB121_51:	# bb107
	addl	16(%rsp), %edx
	addl	20(%rsp), %edi
	incl	%r11d
	cmpl	%r10d, %r11d
	jne	.LBB121_48	# bb101
	jmp	.LBB121_63	# return
.LBB121_52:	# bb109
	cmpl	$113, %esi
	jne	.LBB121_62	# bb125
.LBB121_53:	# bb109
	cmpl	$102, %edi
	jne	.LBB121_62	# bb125
.LBB121_54:	# bb113
	cmpl	$0, 120(%rsp)
	jg	.LBB121_70	# bb113.bb124.preheader_crit_edge
.LBB121_55:	# bb114
	movl	$1, %edx
	subl	%ecx, %edx
	imull	120(%rsp), %edx
.LBB121_56:	# bb124.preheader
	testl	%ecx, %ecx
	jle	.LBB121_63	# return
.LBB121_57:	# bb.nph147
	movl	$1, %esi
	subl	%r10d, %esi
	imull	96(%rsp), %esi
	movl	120(%rsp), %edi
	addl	%edi, %edi
	movl	%edi, 12(%rsp)
	addl	%edx, %edx
	movl	80(%rsp), %edi
	addl	%edi, %edi
	movl	%edi, 20(%rsp)
	xorl	%edi, %edi
	movl	%edi, %r11d
	.align	16
.LBB121_58:	# bb117
	cmpl	$0, 96(%rsp)
	movl	$0, %ebx
	cmovle	%esi, %ebx
	testl	%r10d, %r10d
	jle	.LBB121_71	# bb117.bb123_crit_edge
.LBB121_59:	# bb.nph142
	movl	96(%rsp), %r14d
	leal	(%r14,%r14), %r14d
	addl	%ebx, %ebx
	pxor	%xmm0, %xmm0
	xorl	%r15d, %r15d
	movl	%edi, %r12d
	movapd	%xmm0, %xmm1
	.align	16
.LBB121_60:	# bb121
	movslq	%ebx, %r13
	movsd	(%r8,%r13,8), %xmm4
	movslq	%r12d, %r13
	movsd	(%r9,%r13,8), %xmm5
	movapd	%xmm5, %xmm6
	mulsd	%xmm4, %xmm6
	leal	1(%r12), %r13d
	movslq	%r13d, %r13
	movsd	(%r9,%r13,8), %xmm7
	mulsd	%xmm7, %xmm4
	leal	1(%rbx), %r13d
	movslq	%r13d, %r13
	movsd	(%r8,%r13,8), %xmm8
	mulsd	%xmm8, %xmm5
	subsd	%xmm4, %xmm5
	addsd	%xmm5, %xmm0
	mulsd	%xmm8, %xmm7
	addsd	%xmm6, %xmm7
	addsd	%xmm7, %xmm1
	addl	%r14d, %ebx
	addl	$2, %r12d
	incl	%r15d
	cmpl	%r10d, %r15d
	jne	.LBB121_60	# bb121
.LBB121_61:	# bb123
	movapd	%xmm2, %xmm4
	mulsd	%xmm0, %xmm4
	movapd	%xmm3, %xmm5
	mulsd	%xmm1, %xmm5
	subsd	%xmm4, %xmm5
	movslq	%edx, %rbx
	addsd	(%rax,%rbx,8), %xmm5
	movsd	%xmm5, (%rax,%rbx,8)
	mulsd	%xmm2, %xmm1
	mulsd	%xmm3, %xmm0
	addsd	%xmm1, %xmm0
	leal	1(%rdx), %ebx
	movslq	%ebx, %rbx
	addsd	(%rax,%rbx,8), %xmm0
	movsd	%xmm0, (%rax,%rbx,8)
	addl	12(%rsp), %edx
	addl	20(%rsp), %edi
	incl	%r11d
	cmpl	%ecx, %r11d
	jne	.LBB121_58	# bb117
	jmp	.LBB121_63	# return
.LBB121_62:	# bb125
	xorl	%edi, %edi
	leaq	.str149, %rsi
	leaq	.str1150, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB121_63:	# return
	addq	$24, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB121_64:	# bb37.bb42.preheader_crit_edge
	xorl	%edx, %edx
	jmp	.LBB121_9	# bb42.preheader
.LBB121_65:	# bb45.bb50.preheader_crit_edge
	xorl	%edx, %edx
	jmp	.LBB121_16	# bb50.preheader
.LBB121_66:	# bb61.bb72.preheader_crit_edge
	xorl	%edx, %edx
	jmp	.LBB121_26	# bb72.preheader
.LBB121_67:	# bb65.bb71_crit_edge
	pxor	%xmm0, %xmm0
	movapd	%xmm0, %xmm1
	jmp	.LBB121_31	# bb71
.LBB121_68:	# bb81.bb92.preheader_crit_edge
	xorl	%edx, %edx
	jmp	.LBB121_36	# bb92.preheader
.LBB121_69:	# bb97.bb108.preheader_crit_edge
	xorl	%edx, %edx
	jmp	.LBB121_46	# bb108.preheader
.LBB121_70:	# bb113.bb124.preheader_crit_edge
	xorl	%edx, %edx
	jmp	.LBB121_56	# bb124.preheader
.LBB121_71:	# bb117.bb123_crit_edge
	pxor	%xmm0, %xmm0
	movapd	%xmm0, %xmm1
	jmp	.LBB121_61	# bb123
	.size	cblas_zgemv, .-cblas_zgemv
.Leh_func_end81:


	.align	16
	.globl	cblas_zgerc
	.type	cblas_zgerc,@function
cblas_zgerc:
.Leh_func_begin82:
.Llabel82:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$8, %rsp
	cmpl	$102, %edi
	movsd	8(%rcx), %xmm0
	movsd	(%rcx), %xmm1
	movq	80(%rsp), %rax
	movl	72(%rsp), %ecx
	movq	64(%rsp), %r10
	je	.LBB122_10	# bb21
.LBB122_1:	# entry
	cmpl	$101, %edi
	jne	.LBB122_18	# bb33
.LBB122_2:	# bb
	testl	%r9d, %r9d
	jg	.LBB122_20	# bb.bb19.preheader_crit_edge
.LBB122_3:	# bb9
	movl	$1, %edi
	subl	%esi, %edi
	imull	%r9d, %edi
.LBB122_4:	# bb19.preheader
	testl	%esi, %esi
	jle	.LBB122_19	# return
.LBB122_5:	# bb.nph46
	movl	$1, %r11d
	subl	%edx, %r11d
	imull	%ecx, %r11d
	addl	%r9d, %r9d
	movl	%r9d, (%rsp)
	addl	%edi, %edi
	movl	88(%rsp), %r9d
	addl	%r9d, %r9d
	movl	%r9d, 4(%rsp)
	xorl	%r9d, %r9d
	movl	%r9d, %ebx
	.align	16
.LBB122_6:	# bb12
	testl	%ecx, %ecx
	movl	$0, %r14d
	cmovle	%r11d, %r14d
	movslq	%edi, %r15
	movsd	(%r8,%r15,8), %xmm2
	movapd	%xmm0, %xmm3
	mulsd	%xmm2, %xmm3
	leal	1(%rdi), %r15d
	movslq	%r15d, %r15
	movsd	(%r8,%r15,8), %xmm4
	movapd	%xmm1, %xmm5
	mulsd	%xmm4, %xmm5
	addsd	%xmm3, %xmm5
	mulsd	%xmm1, %xmm2
	mulsd	%xmm0, %xmm4
	subsd	%xmm4, %xmm2
	testl	%edx, %edx
	jle	.LBB122_9	# bb18
.LBB122_7:	# bb.nph43
	leal	(%rcx,%rcx), %r15d
	addl	%r14d, %r14d
	xorl	%r12d, %r12d
	movl	%r9d, %r13d
	.align	16
.LBB122_8:	# bb16
	movslq	%r14d, %rbp
	movsd	(%r10,%rbp,8), %xmm3
	movapd	%xmm3, %xmm4
	mulsd	%xmm2, %xmm4
	leal	1(%r14), %ebp
	movslq	%ebp, %rbp
	movsd	(%r10,%rbp,8), %xmm6
	movapd	%xmm5, %xmm7
	mulsd	%xmm6, %xmm7
	addsd	%xmm4, %xmm7
	movslq	%r13d, %rbp
	addsd	(%rax,%rbp,8), %xmm7
	movsd	%xmm7, (%rax,%rbp,8)
	mulsd	%xmm5, %xmm3
	mulsd	%xmm2, %xmm6
	subsd	%xmm6, %xmm3
	leal	1(%r13), %ebp
	movslq	%ebp, %rbp
	addsd	(%rax,%rbp,8), %xmm3
	movsd	%xmm3, (%rax,%rbp,8)
	addl	%r15d, %r14d
	addl	$2, %r13d
	incl	%r12d
	cmpl	%edx, %r12d
	jne	.LBB122_8	# bb16
.LBB122_9:	# bb18
	addl	(%rsp), %edi
	addl	4(%rsp), %r9d
	incl	%ebx
	cmpl	%esi, %ebx
	jne	.LBB122_6	# bb12
	jmp	.LBB122_19	# return
.LBB122_10:	# bb21
	testl	%ecx, %ecx
	jg	.LBB122_21	# bb21.bb32.preheader_crit_edge
.LBB122_11:	# bb22
	movl	$1, %edi
	subl	%edx, %edi
	imull	%ecx, %edi
.LBB122_12:	# bb32.preheader
	testl	%edx, %edx
	jle	.LBB122_19	# return
.LBB122_13:	# bb.nph40
	movl	$1, %r11d
	subl	%esi, %r11d
	imull	%r9d, %r11d
	movl	%r11d, (%rsp)
	addl	%ecx, %ecx
	addl	%edi, %edi
	movl	88(%rsp), %r11d
	addl	%r11d, %r11d
	movl	%r11d, 4(%rsp)
	xorl	%r11d, %r11d
	movl	%r11d, %ebx
	.align	16
.LBB122_14:	# bb25
	testl	%r9d, %r9d
	movl	$0, %r14d
	cmovle	(%rsp), %r14d
	movslq	%edi, %r15
	movsd	(%r10,%r15,8), %xmm2
	movapd	%xmm0, %xmm3
	mulsd	%xmm2, %xmm3
	leal	1(%rdi), %r15d
	movslq	%r15d, %r15
	movsd	(%r10,%r15,8), %xmm4
	movapd	%xmm1, %xmm5
	mulsd	%xmm4, %xmm5
	subsd	%xmm5, %xmm3
	mulsd	%xmm1, %xmm2
	mulsd	%xmm0, %xmm4
	addsd	%xmm2, %xmm4
	testl	%esi, %esi
	jle	.LBB122_17	# bb31
.LBB122_15:	# bb.nph
	leal	(%r9,%r9), %r15d
	addl	%r14d, %r14d
	xorl	%r12d, %r12d
	movl	%r11d, %r13d
	.align	16
.LBB122_16:	# bb29
	movslq	%r14d, %rbp
	movsd	(%r8,%rbp,8), %xmm2
	movapd	%xmm2, %xmm5
	mulsd	%xmm4, %xmm5
	leal	1(%r14), %ebp
	movslq	%ebp, %rbp
	movsd	(%r8,%rbp,8), %xmm6
	movapd	%xmm6, %xmm7
	mulsd	%xmm3, %xmm7
	subsd	%xmm7, %xmm5
	movslq	%r13d, %rbp
	addsd	(%rax,%rbp,8), %xmm5
	movsd	%xmm5, (%rax,%rbp,8)
	mulsd	%xmm3, %xmm2
	mulsd	%xmm4, %xmm6
	addsd	%xmm2, %xmm6
	leal	1(%r13), %ebp
	movslq	%ebp, %rbp
	addsd	(%rax,%rbp,8), %xmm6
	movsd	%xmm6, (%rax,%rbp,8)
	addl	%r15d, %r14d
	addl	$2, %r13d
	incl	%r12d
	cmpl	%esi, %r12d
	jne	.LBB122_16	# bb29
.LBB122_17:	# bb31
	addl	%ecx, %edi
	addl	4(%rsp), %r11d
	incl	%ebx
	cmpl	%edx, %ebx
	jne	.LBB122_14	# bb25
	jmp	.LBB122_19	# return
.LBB122_18:	# bb33
	xorl	%edi, %edi
	leaq	.str151, %rsi
	leaq	.str1152, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB122_19:	# return
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB122_20:	# bb.bb19.preheader_crit_edge
	xorl	%edi, %edi
	jmp	.LBB122_4	# bb19.preheader
.LBB122_21:	# bb21.bb32.preheader_crit_edge
	xorl	%edi, %edi
	jmp	.LBB122_12	# bb32.preheader
	.size	cblas_zgerc, .-cblas_zgerc
.Leh_func_end82:


	.align	16
	.globl	cblas_zgeru
	.type	cblas_zgeru,@function
cblas_zgeru:
.Leh_func_begin83:
.Llabel83:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$8, %rsp
	cmpl	$102, %edi
	movsd	8(%rcx), %xmm0
	movsd	(%rcx), %xmm1
	movq	80(%rsp), %rax
	movl	72(%rsp), %ecx
	movq	64(%rsp), %r10
	je	.LBB123_10	# bb21
.LBB123_1:	# entry
	cmpl	$101, %edi
	jne	.LBB123_18	# bb33
.LBB123_2:	# bb
	testl	%r9d, %r9d
	jg	.LBB123_20	# bb.bb19.preheader_crit_edge
.LBB123_3:	# bb9
	movl	$1, %edi
	subl	%esi, %edi
	imull	%r9d, %edi
.LBB123_4:	# bb19.preheader
	testl	%esi, %esi
	jle	.LBB123_19	# return
.LBB123_5:	# bb.nph46
	movl	$1, %r11d
	subl	%edx, %r11d
	imull	%ecx, %r11d
	addl	%r9d, %r9d
	movl	%r9d, (%rsp)
	addl	%edi, %edi
	movl	88(%rsp), %r9d
	addl	%r9d, %r9d
	movl	%r9d, 4(%rsp)
	xorl	%r9d, %r9d
	movl	%r9d, %ebx
	.align	16
.LBB123_6:	# bb12
	testl	%ecx, %ecx
	movl	$0, %r14d
	cmovle	%r11d, %r14d
	movslq	%edi, %r15
	movsd	(%r8,%r15,8), %xmm2
	movapd	%xmm0, %xmm3
	mulsd	%xmm2, %xmm3
	leal	1(%rdi), %r15d
	movslq	%r15d, %r15
	movsd	(%r8,%r15,8), %xmm4
	movapd	%xmm1, %xmm5
	mulsd	%xmm4, %xmm5
	addsd	%xmm3, %xmm5
	mulsd	%xmm1, %xmm2
	mulsd	%xmm0, %xmm4
	subsd	%xmm4, %xmm2
	testl	%edx, %edx
	jle	.LBB123_9	# bb18
.LBB123_7:	# bb.nph43
	leal	(%rcx,%rcx), %r15d
	addl	%r14d, %r14d
	xorl	%r12d, %r12d
	movl	%r9d, %r13d
	.align	16
.LBB123_8:	# bb16
	movslq	%r14d, %rbp
	movsd	(%r10,%rbp,8), %xmm3
	movapd	%xmm3, %xmm4
	mulsd	%xmm2, %xmm4
	leal	1(%r14), %ebp
	movslq	%ebp, %rbp
	movsd	(%r10,%rbp,8), %xmm6
	movapd	%xmm6, %xmm7
	mulsd	%xmm5, %xmm7
	subsd	%xmm7, %xmm4
	movslq	%r13d, %rbp
	addsd	(%rax,%rbp,8), %xmm4
	movsd	%xmm4, (%rax,%rbp,8)
	mulsd	%xmm5, %xmm3
	mulsd	%xmm2, %xmm6
	addsd	%xmm3, %xmm6
	leal	1(%r13), %ebp
	movslq	%ebp, %rbp
	addsd	(%rax,%rbp,8), %xmm6
	movsd	%xmm6, (%rax,%rbp,8)
	addl	%r15d, %r14d
	addl	$2, %r13d
	incl	%r12d
	cmpl	%edx, %r12d
	jne	.LBB123_8	# bb16
.LBB123_9:	# bb18
	addl	(%rsp), %edi
	addl	4(%rsp), %r9d
	incl	%ebx
	cmpl	%esi, %ebx
	jne	.LBB123_6	# bb12
	jmp	.LBB123_19	# return
.LBB123_10:	# bb21
	testl	%ecx, %ecx
	jg	.LBB123_21	# bb21.bb32.preheader_crit_edge
.LBB123_11:	# bb22
	movl	$1, %edi
	subl	%edx, %edi
	imull	%ecx, %edi
.LBB123_12:	# bb32.preheader
	testl	%edx, %edx
	jle	.LBB123_19	# return
.LBB123_13:	# bb.nph40
	movl	$1, %r11d
	subl	%esi, %r11d
	imull	%r9d, %r11d
	movl	%r11d, (%rsp)
	addl	%ecx, %ecx
	addl	%edi, %edi
	movl	88(%rsp), %r11d
	addl	%r11d, %r11d
	movl	%r11d, 4(%rsp)
	xorl	%r11d, %r11d
	movl	%r11d, %ebx
	.align	16
.LBB123_14:	# bb25
	testl	%r9d, %r9d
	movl	$0, %r14d
	cmovle	(%rsp), %r14d
	movslq	%edi, %r15
	movsd	(%r10,%r15,8), %xmm2
	movapd	%xmm0, %xmm3
	mulsd	%xmm2, %xmm3
	leal	1(%rdi), %r15d
	movslq	%r15d, %r15
	movsd	(%r10,%r15,8), %xmm4
	movapd	%xmm1, %xmm5
	mulsd	%xmm4, %xmm5
	addsd	%xmm3, %xmm5
	mulsd	%xmm1, %xmm2
	mulsd	%xmm0, %xmm4
	subsd	%xmm4, %xmm2
	testl	%esi, %esi
	jle	.LBB123_17	# bb31
.LBB123_15:	# bb.nph
	leal	(%r9,%r9), %r15d
	addl	%r14d, %r14d
	xorl	%r12d, %r12d
	movl	%r11d, %r13d
	.align	16
.LBB123_16:	# bb29
	movslq	%r14d, %rbp
	movsd	(%r8,%rbp,8), %xmm3
	movapd	%xmm3, %xmm4
	mulsd	%xmm2, %xmm4
	leal	1(%r14), %ebp
	movslq	%ebp, %rbp
	movsd	(%r8,%rbp,8), %xmm6
	movapd	%xmm6, %xmm7
	mulsd	%xmm5, %xmm7
	subsd	%xmm7, %xmm4
	movslq	%r13d, %rbp
	addsd	(%rax,%rbp,8), %xmm4
	movsd	%xmm4, (%rax,%rbp,8)
	mulsd	%xmm5, %xmm3
	mulsd	%xmm2, %xmm6
	addsd	%xmm3, %xmm6
	leal	1(%r13), %ebp
	movslq	%ebp, %rbp
	addsd	(%rax,%rbp,8), %xmm6
	movsd	%xmm6, (%rax,%rbp,8)
	addl	%r15d, %r14d
	addl	$2, %r13d
	incl	%r12d
	cmpl	%esi, %r12d
	jne	.LBB123_16	# bb29
.LBB123_17:	# bb31
	addl	%ecx, %edi
	addl	4(%rsp), %r11d
	incl	%ebx
	cmpl	%edx, %ebx
	jne	.LBB123_14	# bb25
	jmp	.LBB123_19	# return
.LBB123_18:	# bb33
	xorl	%edi, %edi
	leaq	.str154, %rsi
	leaq	.str1155, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB123_19:	# return
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB123_20:	# bb.bb19.preheader_crit_edge
	xorl	%edi, %edi
	jmp	.LBB123_4	# bb19.preheader
.LBB123_21:	# bb21.bb32.preheader_crit_edge
	xorl	%edi, %edi
	jmp	.LBB123_12	# bb32.preheader
	.size	cblas_zgeru, .-cblas_zgeru
.Leh_func_end83:


	.section	.rodata.cst8,"aM",@progbits,8
	.align	8
.LCPI124_0:					
	.quad	4607182418800017408	# double value: 1.000000e+00
	.text
	.align	16
	.globl	cblas_zhbmv
	.type	cblas_zhbmv,@function
cblas_zhbmv:
.Leh_func_begin84:
.Llabel84:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$56, %rsp
	cmpl	$102, %edi
	movl	$4294967295, %eax
	movl	$1, %r10d
	cmove	%eax, %r10d
	movl	%r10d, 32(%rsp)
	testl	%edx, %edx
	movq	136(%rsp), %rax
	movsd	8(%rax), %xmm0
	movsd	(%rax), %xmm1
	movsd	8(%r8), %xmm2
	movsd	(%r8), %xmm3
	movq	144(%rsp), %rax
	movq	120(%rsp), %r8
	movl	%ecx, 44(%rsp)
	movl	%esi, 52(%rsp)
	movl	%edi, 48(%rsp)
	je	.LBB124_46	# return
.LBB124_1:	# bb20
	pxor	%xmm4, %xmm4
	ucomisd	%xmm4, %xmm3
	setp	%cl
	setne	%sil
	setnp	%dil
	sete	%r10b
	andb	%dil, %r10b
	ucomisd	%xmm4, %xmm2
	setp	%dil
	setnp	%r11b
	sete	%bl
	setne	%r14b
	andb	%r11b, %bl
	andb	%r10b, %bl
	movb	%bl, 40(%rsp)
	orb	%cl, %sil
	orb	%dil, %r14b
	orb	%sil, %r14b
	testb	%r14b, %r14b
	jne	.LBB124_4	# bb24
.LBB124_2:	# bb20
	ucomisd	.LCPI124_0(%rip), %xmm1
	jne	.LBB124_4	# bb24
	jp	.LBB124_4	# bb24
.LBB124_3:	# bb20
	pxor	%xmm4, %xmm4
	ucomisd	%xmm4, %xmm0
	setnp	%cl
	sete	%sil
	testb	%cl, %sil
	jne	.LBB124_46	# return
.LBB124_4:	# bb24
	pxor	%xmm4, %xmm4
	ucomisd	%xmm4, %xmm0
	jne	.LBB124_11	# bb32
	jp	.LBB124_11	# bb32
.LBB124_5:	# bb24
	pxor	%xmm4, %xmm4
	ucomisd	%xmm4, %xmm1
	jne	.LBB124_11	# bb32
	jp	.LBB124_11	# bb32
.LBB124_6:	# bb26
	cmpl	$0, 152(%rsp)
	jg	.LBB124_47	# bb26.bb31.preheader_crit_edge
.LBB124_7:	# bb27
	movl	$1, %ecx
	subl	%edx, %ecx
	imull	152(%rsp), %ecx
.LBB124_8:	# bb31.preheader
	testl	%edx, %edx
	jle	.LBB124_18	# bb40
.LBB124_9:	# bb.nph
	movl	152(%rsp), %esi
	leal	(%rsi,%rsi), %esi
	addl	%ecx, %ecx
	xorl	%edi, %edi
	.align	16
.LBB124_10:	# bb30
	movslq	%ecx, %r10
	movq	$0, (%rax,%r10,8)
	leal	(%rsi,%rcx), %r10d
	incl	%ecx
	movslq	%ecx, %rcx
	movq	$0, (%rax,%rcx,8)
	incl	%edi
	cmpl	%edx, %edi
	movl	%r10d, %ecx
	jne	.LBB124_10	# bb30
	jmp	.LBB124_18	# bb40
.LBB124_11:	# bb32
	pxor	%xmm4, %xmm4
	ucomisd	%xmm4, %xmm0
	jne	.LBB124_13	# bb34
	jp	.LBB124_13	# bb34
.LBB124_12:	# bb32
	ucomisd	.LCPI124_0(%rip), %xmm1
	setnp	%cl
	sete	%sil
	testb	%cl, %sil
	jne	.LBB124_18	# bb40
.LBB124_13:	# bb34
	cmpl	$0, 152(%rsp)
	jg	.LBB124_48	# bb34.bb39.preheader_crit_edge
.LBB124_14:	# bb35
	movl	$1, %ecx
	subl	%edx, %ecx
	imull	152(%rsp), %ecx
.LBB124_15:	# bb39.preheader
	testl	%edx, %edx
	jle	.LBB124_18	# bb40
.LBB124_16:	# bb.nph153
	movl	152(%rsp), %esi
	leal	(%rsi,%rsi), %esi
	addl	%ecx, %ecx
	xorl	%edi, %edi
	.align	16
.LBB124_17:	# bb38
	movslq	%ecx, %r10
	movsd	(%rax,%r10,8), %xmm4
	movapd	%xmm4, %xmm5
	mulsd	%xmm1, %xmm5
	leal	1(%rcx), %r11d
	movslq	%r11d, %r11
	movsd	(%rax,%r11,8), %xmm6
	movapd	%xmm6, %xmm7
	mulsd	%xmm0, %xmm7
	subsd	%xmm7, %xmm5
	movsd	%xmm5, (%rax,%r10,8)
	mulsd	%xmm0, %xmm4
	mulsd	%xmm1, %xmm6
	addsd	%xmm4, %xmm6
	movsd	%xmm6, (%rax,%r11,8)
	addl	%esi, %ecx
	incl	%edi
	cmpl	%edx, %edi
	jne	.LBB124_17	# bb38
.LBB124_18:	# bb40
	testb	$1, 40(%rsp)
	jne	.LBB124_46	# return
.LBB124_19:	# bb42
	cmpl	$121, 52(%rsp)
	jne	.LBB124_21	# bb45
.LBB124_20:	# bb42
	cmpl	$101, 48(%rsp)
	je	.LBB124_23	# bb49
.LBB124_21:	# bb45
	cmpl	$122, 52(%rsp)
	jne	.LBB124_33	# bb67
.LBB124_22:	# bb45
	cmpl	$102, 48(%rsp)
	jne	.LBB124_33	# bb67
.LBB124_23:	# bb49
	cmpl	$0, 128(%rsp)
	jg	.LBB124_49	# bb49.bb52_crit_edge
.LBB124_24:	# bb50
	movl	$1, %ecx
	subl	%edx, %ecx
	imull	128(%rsp), %ecx
	movl	%ecx, 48(%rsp)
.LBB124_25:	# bb52
	cmpl	$0, 152(%rsp)
	jg	.LBB124_50	# bb52.bb66.preheader_crit_edge
.LBB124_26:	# bb53
	movl	$1, %ecx
	subl	%edx, %ecx
	imull	152(%rsp), %ecx
.LBB124_27:	# bb66.preheader
	testl	%edx, %edx
	jle	.LBB124_46	# return
.LBB124_28:	# bb.nph150
	movl	$1, %esi
	subl	%edx, %esi
	movl	%esi, %edi
	movl	128(%rsp), %r10d
	imull	%r10d, %edi
	movl	%edi, 8(%rsp)
	movl	152(%rsp), %edi
	imull	%edi, %esi
	movl	%esi, 4(%rsp)
	addl	%ecx, %ecx
	movl	48(%rsp), %esi
	addl	%esi, %esi
	movl	%esi, 48(%rsp)
	movl	112(%rsp), %esi
	addl	%esi, %esi
	movl	%esi, 28(%rsp)
	movl	44(%rsp), %esi
	leal	1(%rsi), %esi
	movl	%esi, 20(%rsp)
	cvtsi2sd	32(%rsp), %xmm0
	leal	(%rdi,%rdi), %esi
	movl	%esi, 16(%rsp)
	leal	(%r10,%r10), %esi
	movl	%esi, 12(%rsp)
	movl	$4294967294, 24(%rsp)
	xorl	%esi, %esi
	movl	%esi, 52(%rsp)
	movl	%r10d, 40(%rsp)
	movl	%edi, 36(%rsp)
	movl	%esi, 32(%rsp)
	.align	16
.LBB124_29:	# bb56
	movl	48(%rsp), %edi
	movslq	%edi, %r10
	movsd	(%r8,%r10,8), %xmm1
	movapd	%xmm3, %xmm4
	mulsd	%xmm1, %xmm4
	leal	1(%rdi), %edi
	movslq	%edi, %rdi
	movsd	(%r8,%rdi,8), %xmm5
	movapd	%xmm2, %xmm6
	mulsd	%xmm5, %xmm6
	subsd	%xmm6, %xmm4
	movslq	52(%rsp), %rdi
	movsd	(%r9,%rdi,8), %xmm6
	movapd	%xmm4, %xmm7
	mulsd	%xmm6, %xmm7
	movslq	%ecx, %rdi
	addsd	(%rax,%rdi,8), %xmm7
	movsd	%xmm7, (%rax,%rdi,8)
	mulsd	%xmm2, %xmm1
	mulsd	%xmm3, %xmm5
	addsd	%xmm1, %xmm5
	mulsd	%xmm5, %xmm6
	incl	%ecx
	movslq	%ecx, %rcx
	addsd	(%rax,%rcx,8), %xmm6
	movsd	%xmm6, (%rax,%rcx,8)
	xorl	%r10d, %r10d
	cmpl	$0, 152(%rsp)
	movl	4(%rsp), %r11d
	cmovg	%r10d, %r11d
	cmpl	$0, 128(%rsp)
	cmovle	8(%rsp), %r10d
	movl	20(%rsp), %ebx
	leal	(%rbx,%rsi), %ebx
	cmpl	%edx, %ebx
	cmovg	%edx, %ebx
	leal	1(%rsi), %r14d
	cmpl	%ebx, %r14d
	jge	.LBB124_51	# bb56.bb65_crit_edge
.LBB124_30:	# bb.nph144
	movl	$4294967295, %ebx
	subl	%edx, %ebx
	movl	32(%rsp), %r14d
	negl	%r14d
	subl	44(%rsp), %r14d
	addl	$4294967294, %r14d
	cmpl	%r14d, %ebx
	cmovg	%ebx, %r14d
	movl	24(%rsp), %ebx
	subl	%r14d, %ebx
	pxor	%xmm1, %xmm1
	xorl	%r14d, %r14d
	movl	52(%rsp), %r15d
	movl	36(%rsp), %r12d
	movl	40(%rsp), %r13d
	movapd	%xmm1, %xmm6
	.align	16
.LBB124_31:	# bb63
	leal	3(%r15), %ebp
	movslq	%ebp, %rbp
	movapd	%xmm0, %xmm7
	mulsd	(%r9,%rbp,8), %xmm7
	movapd	%xmm7, %xmm8
	mulsd	%xmm5, %xmm8
	addl	$2, %r15d
	movslq	%r15d, %rbp
	movsd	(%r9,%rbp,8), %xmm9
	movapd	%xmm4, %xmm10
	mulsd	%xmm9, %xmm10
	addsd	%xmm8, %xmm10
	addl	%r11d, %r12d
	leal	(%r12,%r12), %r11d
	movslq	%r11d, %r11
	addsd	(%rax,%r11,8), %xmm10
	movsd	%xmm10, (%rax,%r11,8)
	movapd	%xmm4, %xmm8
	mulsd	%xmm7, %xmm8
	movapd	%xmm5, %xmm10
	mulsd	%xmm9, %xmm10
	subsd	%xmm8, %xmm10
	leal	1(,%r12,2), %r11d
	movslq	%r11d, %r11
	addsd	(%rax,%r11,8), %xmm10
	movsd	%xmm10, (%rax,%r11,8)
	addl	%r10d, %r13d
	leal	1(,%r13,2), %r10d
	movslq	%r10d, %r10
	movsd	(%r8,%r10,8), %xmm8
	movapd	%xmm8, %xmm10
	mulsd	%xmm9, %xmm10
	leal	(%r13,%r13), %r10d
	movslq	%r10d, %r10
	movsd	(%r8,%r10,8), %xmm11
	movapd	%xmm11, %xmm12
	mulsd	%xmm7, %xmm12
	addsd	%xmm10, %xmm12
	addsd	%xmm12, %xmm6
	mulsd	%xmm8, %xmm7
	mulsd	%xmm9, %xmm11
	subsd	%xmm7, %xmm11
	addsd	%xmm11, %xmm1
	incl	%r14d
	cmpl	%ebx, %r14d
	movl	152(%rsp), %r11d
	movl	128(%rsp), %r10d
	jne	.LBB124_31	# bb63
.LBB124_32:	# bb65
	movapd	%xmm2, %xmm4
	mulsd	%xmm6, %xmm4
	movapd	%xmm3, %xmm5
	mulsd	%xmm1, %xmm5
	subsd	%xmm4, %xmm5
	addsd	(%rax,%rdi,8), %xmm5
	movsd	%xmm5, (%rax,%rdi,8)
	mulsd	%xmm2, %xmm1
	mulsd	%xmm3, %xmm6
	addsd	%xmm1, %xmm6
	addsd	(%rax,%rcx,8), %xmm6
	movsd	%xmm6, (%rax,%rcx,8)
	movl	%edi, %ecx
	addl	16(%rsp), %ecx
	movl	48(%rsp), %edi
	addl	12(%rsp), %edi
	movl	%edi, 48(%rsp)
	movl	128(%rsp), %edi
	addl	%edi, 40(%rsp)
	movl	152(%rsp), %edi
	addl	%edi, 36(%rsp)
	movl	52(%rsp), %edi
	addl	28(%rsp), %edi
	movl	%edi, 52(%rsp)
	incl	32(%rsp)
	decl	24(%rsp)
	incl	%esi
	cmpl	%edx, %esi
	jne	.LBB124_29	# bb56
	jmp	.LBB124_46	# return
.LBB124_33:	# bb67
	movl	48(%rsp), %ecx
	cmpl	$102, %ecx
	sete	%sil
	movl	52(%rsp), %edi
	cmpl	$121, %edi
	sete	%r10b
	andb	%sil, %r10b
	cmpl	$101, %ecx
	sete	%cl
	cmpl	$122, %edi
	sete	%sil
	testb	%cl, %sil
	jne	.LBB124_35	# bb75
.LBB124_34:	# bb67
	notb	%r10b
	testb	$1, %r10b
	jne	.LBB124_45	# bb96
.LBB124_35:	# bb75
	cmpl	$0, 128(%rsp)
	jg	.LBB124_52	# bb75.bb78_crit_edge
.LBB124_36:	# bb76
	movl	$1, %ecx
	subl	%edx, %ecx
	imull	128(%rsp), %ecx
	movl	%ecx, 52(%rsp)
.LBB124_37:	# bb78
	cmpl	$0, 152(%rsp)
	jg	.LBB124_53	# bb78.bb95.preheader_crit_edge
.LBB124_38:	# bb79
	movl	$1, %ecx
	subl	%edx, %ecx
	imull	152(%rsp), %ecx
.LBB124_39:	# bb95.preheader
	testl	%edx, %edx
	jle	.LBB124_46	# return
.LBB124_40:	# bb.nph132
	movl	$1, %esi
	subl	%edx, %esi
	movl	%esi, %edi
	movl	152(%rsp), %r10d
	imull	%r10d, %edi
	movl	%edi, 20(%rsp)
	movl	128(%rsp), %edi
	imull	%edi, %esi
	movl	%esi, 12(%rsp)
	movl	52(%rsp), %esi
	addl	%esi, %esi
	movl	%esi, 52(%rsp)
	addl	%ecx, %ecx
	movl	112(%rsp), %esi
	leal	(%rsi,%rsi), %r11d
	movl	%r11d, 36(%rsp)
	decl	%esi
	movl	%esi, 28(%rsp)
	movl	44(%rsp), %esi
	leal	(%rsi,%rsi), %r11d
	movl	%r11d, 40(%rsp)
	movl	%esi, %r11d
	negl	%r11d
	movl	%r11d, 16(%rsp)
	cvtsi2sd	32(%rsp), %xmm0
	leal	(%rdi,%rdi), %edi
	movl	%edi, 32(%rsp)
	leal	(%r10,%r10), %edi
	movl	%edi, 24(%rsp)
	xorl	%edi, %edi
	movl	%esi, 48(%rsp)
	.align	16
.LBB124_41:	# bb82
	xorl	%esi, %esi
	movl	152(%rsp), %r10d
	testl	%r10d, %r10d
	movl	20(%rsp), %r11d
	cmovg	%esi, %r11d
	movl	128(%rsp), %ebx
	testl	%ebx, %ebx
	movl	12(%rsp), %r14d
	cmovg	%esi, %r14d
	movl	16(%rsp), %r15d
	leal	(%r15,%rdi), %r15d
	cmpl	44(%rsp), %edi
	cmovl	%esi, %r15d
	movl	%r15d, %esi
	imull	%r10d, %esi
	movl	%r15d, %r10d
	imull	%ebx, %r10d
	movl	52(%rsp), %ebx
	movslq	%ebx, %r12
	leal	1(%rbx), %ebx
	cmpl	%edi, %r15d
	movsd	(%r8,%r12,8), %xmm1
	movapd	%xmm2, %xmm4
	mulsd	%xmm1, %xmm4
	movslq	%ebx, %rbx
	movsd	(%r8,%rbx,8), %xmm5
	movapd	%xmm3, %xmm6
	mulsd	%xmm5, %xmm6
	addsd	%xmm4, %xmm6
	mulsd	%xmm3, %xmm1
	mulsd	%xmm2, %xmm5
	subsd	%xmm5, %xmm1
	jge	.LBB124_54	# bb82.bb94_crit_edge
.LBB124_42:	# bb.nph124
	movl	%edi, %ebx
	subl	%r15d, %ebx
	addl	48(%rsp), %r15d
	addl	%r15d, %r15d
	pxor	%xmm4, %xmm4
	xorl	%r12d, %r12d
	movapd	%xmm4, %xmm5
	.align	16
.LBB124_43:	# bb92
	movslq	%r15d, %r13
	leal	1(%r15), %ebp
	movslq	%ebp, %rbp
	movapd	%xmm0, %xmm7
	mulsd	(%r9,%rbp,8), %xmm7
	movapd	%xmm7, %xmm8
	mulsd	%xmm6, %xmm8
	movsd	(%r9,%r13,8), %xmm9
	movapd	%xmm1, %xmm10
	mulsd	%xmm9, %xmm10
	addsd	%xmm8, %xmm10
	addl	%r11d, %esi
	leal	(%rsi,%rsi), %r11d
	movslq	%r11d, %r11
	addsd	(%rax,%r11,8), %xmm10
	movsd	%xmm10, (%rax,%r11,8)
	movapd	%xmm1, %xmm8
	mulsd	%xmm7, %xmm8
	movapd	%xmm6, %xmm10
	mulsd	%xmm9, %xmm10
	subsd	%xmm8, %xmm10
	leal	1(,%rsi,2), %r11d
	movslq	%r11d, %r11
	addsd	(%rax,%r11,8), %xmm10
	movsd	%xmm10, (%rax,%r11,8)
	addl	%r14d, %r10d
	leal	1(,%r10,2), %r11d
	movslq	%r11d, %r11
	movsd	(%r8,%r11,8), %xmm8
	movapd	%xmm8, %xmm10
	mulsd	%xmm9, %xmm10
	leal	(%r10,%r10), %r11d
	movslq	%r11d, %r11
	movsd	(%r8,%r11,8), %xmm11
	movapd	%xmm11, %xmm12
	mulsd	%xmm7, %xmm12
	addsd	%xmm10, %xmm12
	addsd	%xmm12, %xmm4
	mulsd	%xmm8, %xmm7
	mulsd	%xmm9, %xmm11
	subsd	%xmm7, %xmm11
	addsd	%xmm11, %xmm5
	addl	$2, %r15d
	incl	%r12d
	cmpl	%ebx, %r12d
	movl	152(%rsp), %r11d
	movl	128(%rsp), %r14d
	jne	.LBB124_43	# bb92
.LBB124_44:	# bb94
	movl	40(%rsp), %esi
	movslq	%esi, %r10
	movsd	(%r9,%r10,8), %xmm7
	mulsd	%xmm7, %xmm1
	movslq	%ecx, %r10
	addsd	(%rax,%r10,8), %xmm1
	movsd	%xmm1, (%rax,%r10,8)
	mulsd	%xmm7, %xmm6
	leal	1(%rcx), %r11d
	movslq	%r11d, %r11
	addsd	(%rax,%r11,8), %xmm6
	movsd	%xmm6, (%rax,%r11,8)
	movapd	%xmm2, %xmm1
	mulsd	%xmm4, %xmm1
	movapd	%xmm3, %xmm6
	mulsd	%xmm5, %xmm6
	subsd	%xmm1, %xmm6
	addsd	(%rax,%r10,8), %xmm6
	movsd	%xmm6, (%rax,%r10,8)
	mulsd	%xmm2, %xmm5
	mulsd	%xmm3, %xmm4
	addsd	%xmm5, %xmm4
	addsd	(%rax,%r11,8), %xmm4
	movsd	%xmm4, (%rax,%r11,8)
	movl	52(%rsp), %r10d
	addl	32(%rsp), %r10d
	movl	%r10d, 52(%rsp)
	addl	24(%rsp), %ecx
	addl	36(%rsp), %esi
	movl	%esi, 40(%rsp)
	movl	48(%rsp), %esi
	addl	28(%rsp), %esi
	movl	%esi, 48(%rsp)
	incl	%edi
	cmpl	%edx, %edi
	jne	.LBB124_41	# bb82
	jmp	.LBB124_46	# return
.LBB124_45:	# bb96
	xorl	%edi, %edi
	leaq	.str157, %rsi
	leaq	.str1158, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB124_46:	# return
	addq	$56, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB124_47:	# bb26.bb31.preheader_crit_edge
	xorl	%ecx, %ecx
	jmp	.LBB124_8	# bb31.preheader
.LBB124_48:	# bb34.bb39.preheader_crit_edge
	xorl	%ecx, %ecx
	jmp	.LBB124_15	# bb39.preheader
.LBB124_49:	# bb49.bb52_crit_edge
	movl	$0, 48(%rsp)
	jmp	.LBB124_25	# bb52
.LBB124_50:	# bb52.bb66.preheader_crit_edge
	xorl	%ecx, %ecx
	jmp	.LBB124_27	# bb66.preheader
.LBB124_51:	# bb56.bb65_crit_edge
	pxor	%xmm1, %xmm1
	movapd	%xmm1, %xmm6
	jmp	.LBB124_32	# bb65
.LBB124_52:	# bb75.bb78_crit_edge
	movl	$0, 52(%rsp)
	jmp	.LBB124_37	# bb78
.LBB124_53:	# bb78.bb95.preheader_crit_edge
	xorl	%ecx, %ecx
	jmp	.LBB124_39	# bb95.preheader
.LBB124_54:	# bb82.bb94_crit_edge
	pxor	%xmm4, %xmm4
	movapd	%xmm4, %xmm5
	jmp	.LBB124_44	# bb94
	.size	cblas_zhbmv, .-cblas_zhbmv
.Leh_func_end84:


	.section	.rodata.cst8,"aM",@progbits,8
	.align	8
.LCPI125_0:					
	.quad	4607182418800017408	# double value: 1.000000e+00
	.text
	.align	16
	.globl	cblas_zhemm
	.type	cblas_zhemm,@function
cblas_zhemm:
.Leh_func_begin85:
.Llabel85:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$88, %rsp
	movl	%ecx, 60(%rsp)
	movsd	(%r9), %xmm0
	pxor	%xmm1, %xmm1
	ucomisd	%xmm1, %xmm0
	setp	%al
	setne	%cl
	setnp	%r10b
	sete	%r11b
	andb	%r10b, %r11b
	movsd	8(%r9), %xmm2
	ucomisd	%xmm1, %xmm2
	setp	%r9b
	setnp	%r10b
	sete	%bl
	setne	%r14b
	andb	%r10b, %bl
	andb	%r11b, %bl
	movb	%bl, 80(%rsp)
	orb	%al, %cl
	orb	%r9b, %r14b
	orb	%cl, %r14b
	testb	%r14b, %r14b
	movq	176(%rsp), %rax
	movsd	8(%rax), %xmm1
	movsd	(%rax), %xmm3
	movq	184(%rsp), %rax
	movq	160(%rsp), %rcx
	jne	.LBB125_3	# bb31
.LBB125_1:	# entry
	ucomisd	.LCPI125_0(%rip), %xmm3
	jne	.LBB125_3	# bb31
	jp	.LBB125_3	# bb31
.LBB125_2:	# entry
	pxor	%xmm4, %xmm4
	ucomisd	%xmm4, %xmm1
	setnp	%r9b
	sete	%r10b
	testb	%r9b, %r10b
	jne	.LBB125_66	# return
.LBB125_3:	# bb31
	cmpl	$101, %edi
	je	.LBB125_67	# bb31.bb40_crit_edge
.LBB125_4:	# bb33
	cmpl	$141, %esi
	movl	$142, %edi
	movl	$141, %esi
	cmove	%edi, %esi
	cmpl	$121, %edx
	movl	$122, %edi
	movl	$121, %edx
	cmove	%edi, %edx
	movl	60(%rsp), %edi
	movl	%edi, 84(%rsp)
	movl	%r8d, 60(%rsp)
.LBB125_5:	# bb40
	pxor	%xmm4, %xmm4
	ucomisd	%xmm4, %xmm1
	jne	.LBB125_13	# bb48
	jp	.LBB125_13	# bb48
.LBB125_6:	# bb40
	pxor	%xmm4, %xmm4
	ucomisd	%xmm4, %xmm3
	jne	.LBB125_13	# bb48
	jp	.LBB125_13	# bb48
.LBB125_7:	# bb47.preheader
	cmpl	$0, 60(%rsp)
	jle	.LBB125_20	# bb56
.LBB125_8:	# bb.nph122
	cmpl	$0, 84(%rsp)
	jle	.LBB125_20	# bb56
.LBB125_9:	# bb45.preheader.preheader
	movl	192(%rsp), %edi
	leal	(%rdi,%rdi), %edi
	xorl	%r8d, %r8d
	movl	%r8d, %r9d
	jmp	.LBB125_12	# bb45.preheader
	.align	16
.LBB125_10:	# bb44
	movslq	%r10d, %rbx
	movq	$0, (%rax,%rbx,8)
	leal	1(%r10), %ebx
	movslq	%ebx, %rbx
	movq	$0, (%rax,%rbx,8)
	addl	$2, %r10d
	incl	%r11d
	cmpl	84(%rsp), %r11d
	jne	.LBB125_10	# bb44
.LBB125_11:	# bb46
	addl	%edi, %r8d
	incl	%r9d
	cmpl	60(%rsp), %r9d
	je	.LBB125_20	# bb56
.LBB125_12:	# bb45.preheader
	xorl	%r11d, %r11d
	movl	%r8d, %r10d
	jmp	.LBB125_10	# bb44
.LBB125_13:	# bb48
	pxor	%xmm4, %xmm4
	ucomisd	%xmm4, %xmm1
	setnp	%dil
	sete	%r8b
	andb	%dil, %r8b
	ucomisd	.LCPI125_0(%rip), %xmm3
	setnp	%dil
	sete	%r9b
	andb	%dil, %r9b
	testb	%r9b, %r8b
	jne	.LBB125_20	# bb56
.LBB125_14:	# bb48
	cmpl	$0, 60(%rsp)
	jle	.LBB125_20	# bb56
.LBB125_15:	# bb.nph173
	cmpl	$0, 84(%rsp)
	jle	.LBB125_20	# bb56
.LBB125_16:	# bb53.preheader.preheader
	movl	192(%rsp), %edi
	leal	(%rdi,%rdi), %r9d
	xorl	%r8d, %r8d
	movl	%r8d, %r11d
	.align	16
.LBB125_17:	# bb53.preheader
	xorl	%edi, %edi
	movl	%r8d, %r10d
	.align	16
.LBB125_18:	# bb52
	movslq	%r10d, %rbx
	movsd	(%rax,%rbx,8), %xmm4
	movapd	%xmm3, %xmm5
	mulsd	%xmm4, %xmm5
	leal	1(%r10), %r14d
	movslq	%r14d, %r14
	movsd	(%rax,%r14,8), %xmm6
	movapd	%xmm1, %xmm7
	mulsd	%xmm6, %xmm7
	subsd	%xmm7, %xmm5
	movsd	%xmm5, (%rax,%rbx,8)
	mulsd	%xmm1, %xmm4
	mulsd	%xmm3, %xmm6
	addsd	%xmm4, %xmm6
	movsd	%xmm6, (%rax,%r14,8)
	addl	$2, %r10d
	incl	%edi
	cmpl	84(%rsp), %edi
	jne	.LBB125_18	# bb52
.LBB125_19:	# bb54
	addl	%r9d, %r8d
	incl	%r11d
	cmpl	60(%rsp), %r11d
	jne	.LBB125_17	# bb53.preheader
.LBB125_20:	# bb56
	testb	$1, 80(%rsp)
	jne	.LBB125_66	# return
.LBB125_21:	# bb58
	cmpl	$121, %edx
	jne	.LBB125_32	# bb70
.LBB125_22:	# bb58
	cmpl	$141, %esi
	jne	.LBB125_32	# bb70
.LBB125_23:	# bb69.preheader
	cmpl	$0, 60(%rsp)
	jle	.LBB125_66	# return
.LBB125_24:	# bb.nph169
	cmpl	$0, 84(%rsp)
	jle	.LBB125_66	# return
.LBB125_25:	# bb67.preheader.preheader
	movl	152(%rsp), %edx
	leal	2(,%rdx,2), %edx
	movl	%edx, (%rsp)
	movl	168(%rsp), %edx
	leal	(%rdx,%rdx), %edx
	movl	%edx, 16(%rsp)
	movl	192(%rsp), %edx
	leal	(%rdx,%rdx), %edx
	movl	%edx, 12(%rsp)
	movl	60(%rsp), %edx
	leal	-1(%rdx), %esi
	xorl	%edx, %edx
	movl	%edx, 56(%rsp)
	movl	%edx, 76(%rsp)
	movl	%edx, 64(%rsp)
	movl	%edx, 20(%rsp)
	jmp	.LBB125_31	# bb67.preheader
	.align	16
.LBB125_26:	# bb63
	movl	40(%rsp), %edi
	leal	(%rdi,%rdx), %edi
	movslq	%edi, %rdi
	movsd	(%rcx,%rdi,8), %xmm1
	movapd	%xmm2, %xmm3
	mulsd	%xmm1, %xmm3
	movl	76(%rsp), %edi
	leal	(%rdi,%rdx), %edi
	movslq	%edi, %rdi
	movsd	(%rcx,%rdi,8), %xmm4
	movapd	%xmm0, %xmm5
	mulsd	%xmm4, %xmm5
	subsd	%xmm3, %xmm5
	movq	144(%rsp), %rdi
	movq	24(%rsp), %r8
	movsd	(%rdi,%r8,8), %xmm3
	movapd	%xmm5, %xmm6
	mulsd	%xmm3, %xmm6
	movl	64(%rsp), %edi
	leal	(%rdi,%rdx), %edi
	movslq	%edi, %rdi
	addsd	(%rax,%rdi,8), %xmm6
	movsd	%xmm6, (%rax,%rdi,8)
	mulsd	%xmm2, %xmm4
	mulsd	%xmm0, %xmm1
	addsd	%xmm4, %xmm1
	mulsd	%xmm1, %xmm3
	movl	44(%rsp), %r8d
	leal	(%r8,%rdx), %r8d
	movslq	%r8d, %r8
	addsd	(%rax,%r8,8), %xmm3
	movsd	%xmm3, (%rax,%r8,8)
	movl	36(%rsp), %r9d
	cmpl	60(%rsp), %r9d
	jge	.LBB125_68	# bb63.bb66_crit_edge
.LBB125_27:	# bb.nph163
	movl	52(%rsp), %r9d
	leal	(%r9,%rdx), %r9d
	movl	48(%rsp), %r10d
	leal	(%r10,%rdx), %r10d
	movl	168(%rsp), %r11d
	leal	(%r11,%r11), %r11d
	movl	192(%rsp), %ebx
	leal	(%rbx,%rbx), %ebx
	pxor	%xmm3, %xmm3
	xorl	%r14d, %r14d
	movl	56(%rsp), %r15d
	movapd	%xmm3, %xmm4
	.align	16
.LBB125_28:	# bb64
	leal	3(%r15), %r12d
	movslq	%r12d, %r12
	movq	144(%rsp), %r13
	movsd	(%r13,%r12,8), %xmm6
	movapd	%xmm6, %xmm7
	mulsd	%xmm1, %xmm7
	addl	$2, %r15d
	movslq	%r15d, %r12
	movsd	(%r13,%r12,8), %xmm8
	movapd	%xmm8, %xmm9
	mulsd	%xmm5, %xmm9
	addsd	%xmm7, %xmm9
	movslq	%r10d, %r12
	addsd	(%rax,%r12,8), %xmm9
	movslq	%r9d, %r13
	movsd	(%rcx,%r13,8), %xmm7
	leal	1(%r9), %r13d
	movslq	%r13d, %r13
	movsd	(%rcx,%r13,8), %xmm10
	movsd	%xmm9, (%rax,%r12,8)
	movapd	%xmm5, %xmm9
	mulsd	%xmm6, %xmm9
	movapd	%xmm8, %xmm11
	mulsd	%xmm1, %xmm11
	subsd	%xmm9, %xmm11
	leal	1(%r10), %r12d
	movslq	%r12d, %r12
	addsd	(%rax,%r12,8), %xmm11
	movsd	%xmm11, (%rax,%r12,8)
	movapd	%xmm6, %xmm9
	mulsd	%xmm7, %xmm9
	movapd	%xmm8, %xmm11
	mulsd	%xmm10, %xmm11
	addsd	%xmm9, %xmm11
	addsd	%xmm11, %xmm4
	mulsd	%xmm10, %xmm6
	mulsd	%xmm7, %xmm8
	subsd	%xmm6, %xmm8
	addsd	%xmm8, %xmm3
	addl	%r11d, %r9d
	addl	%ebx, %r10d
	incl	%r14d
	cmpl	%esi, %r14d
	jne	.LBB125_28	# bb64
.LBB125_29:	# bb66
	movapd	%xmm2, %xmm1
	mulsd	%xmm4, %xmm1
	movapd	%xmm0, %xmm5
	mulsd	%xmm3, %xmm5
	subsd	%xmm1, %xmm5
	addsd	(%rax,%rdi,8), %xmm5
	movsd	%xmm5, (%rax,%rdi,8)
	mulsd	%xmm2, %xmm3
	mulsd	%xmm0, %xmm4
	addsd	%xmm3, %xmm4
	addsd	(%rax,%r8,8), %xmm4
	movsd	%xmm4, (%rax,%r8,8)
	addl	$2, %edx
	movl	80(%rsp), %edi
	incl	%edi
	movl	%edi, 80(%rsp)
	cmpl	84(%rsp), %edi
	jne	.LBB125_26	# bb63
.LBB125_30:	# bb68
	movl	56(%rsp), %edx
	addl	(%rsp), %edx
	movl	%edx, 56(%rsp)
	movl	76(%rsp), %edx
	addl	16(%rsp), %edx
	movl	%edx, 76(%rsp)
	movl	64(%rsp), %edx
	addl	12(%rsp), %edx
	movl	%edx, 64(%rsp)
	decl	%esi
	movl	20(%rsp), %edx
	incl	%edx
	movl	%edx, 20(%rsp)
	cmpl	60(%rsp), %edx
	je	.LBB125_66	# return
.LBB125_31:	# bb67.preheader
	movl	76(%rsp), %edi
	movl	16(%rsp), %edx
	leal	(%rdx,%rdi), %edx
	movl	%edx, 52(%rsp)
	movl	64(%rsp), %r8d
	movl	12(%rsp), %edx
	leal	(%rdx,%r8), %edx
	movl	%edx, 48(%rsp)
	leal	1(%r8), %edx
	movl	%edx, 44(%rsp)
	leal	1(%rdi), %edx
	movl	%edx, 40(%rsp)
	movl	20(%rsp), %edx
	leal	1(%rdx), %edx
	movl	%edx, 36(%rsp)
	movslq	56(%rsp), %rdx
	movq	%rdx, 24(%rsp)
	xorl	%edx, %edx
	movl	%edx, 80(%rsp)
	jmp	.LBB125_26	# bb63
.LBB125_32:	# bb70
	cmpl	$122, %edx
	jne	.LBB125_43	# bb83
.LBB125_33:	# bb70
	cmpl	$141, %esi
	jne	.LBB125_43	# bb83
.LBB125_34:	# bb82.preheader
	cmpl	$0, 60(%rsp)
	jle	.LBB125_66	# return
.LBB125_35:	# bb.nph157
	cmpl	$0, 84(%rsp)
	jle	.LBB125_66	# return
.LBB125_36:	# bb80.preheader.preheader
	movl	152(%rsp), %esi
	leal	2(,%rsi,2), %edi
	movl	%edi, 48(%rsp)
	addl	%esi, %esi
	movl	%esi, (%rsp)
	movl	168(%rsp), %esi
	leal	(%rsi,%rsi), %esi
	movl	%esi, 44(%rsp)
	movl	192(%rsp), %esi
	leal	(%rsi,%rsi), %esi
	movl	%esi, 40(%rsp)
	xorl	%esi, %esi
	movl	%esi, 64(%rsp)
	movl	%esi, 80(%rsp)
	movl	%esi, 76(%rsp)
	movl	%esi, %edi
	jmp	.LBB125_42	# bb80.preheader
	.align	16
.LBB125_37:	# bb76
	movl	80(%rsp), %r9d
	leal	(%r9,%r8), %r9d
	movslq	%r9d, %r9
	movsd	(%rcx,%r9,8), %xmm1
	movapd	%xmm2, %xmm3
	mulsd	%xmm1, %xmm3
	movl	52(%rsp), %r9d
	leal	(%r9,%r8), %r9d
	movslq	%r9d, %r9
	movsd	(%rcx,%r9,8), %xmm4
	movapd	%xmm0, %xmm5
	mulsd	%xmm4, %xmm5
	addsd	%xmm3, %xmm5
	mulsd	%xmm2, %xmm4
	mulsd	%xmm0, %xmm1
	subsd	%xmm4, %xmm1
	testl	%edi, %edi
	jle	.LBB125_69	# bb76.bb79_crit_edge
.LBB125_38:	# bb77.preheader
	movl	168(%rsp), %r9d
	leal	(%r9,%r9), %r9d
	movl	192(%rsp), %r10d
	leal	(%r10,%r10), %r10d
	pxor	%xmm3, %xmm3
	xorl	%r11d, %r11d
	movl	%r8d, %ebx
	movl	%r8d, %r14d
	movl	64(%rsp), %r15d
	movapd	%xmm3, %xmm4
	.align	16
.LBB125_39:	# bb77
	movslq	%r15d, %r12
	movq	144(%rsp), %r13
	movsd	(%r13,%r12,8), %xmm6
	movapd	%xmm6, %xmm7
	mulsd	%xmm1, %xmm7
	leal	1(%r15), %r12d
	movslq	%r12d, %r12
	movsd	(%r13,%r12,8), %xmm8
	movapd	%xmm8, %xmm9
	mulsd	%xmm5, %xmm9
	addsd	%xmm7, %xmm9
	movslq	%r14d, %r12
	addsd	(%rax,%r12,8), %xmm9
	movslq	%ebx, %r13
	movsd	(%rcx,%r13,8), %xmm7
	leal	1(%rbx), %r13d
	movslq	%r13d, %r13
	movsd	(%rcx,%r13,8), %xmm10
	movsd	%xmm9, (%rax,%r12,8)
	movapd	%xmm6, %xmm9
	mulsd	%xmm5, %xmm9
	movapd	%xmm1, %xmm11
	mulsd	%xmm8, %xmm11
	subsd	%xmm11, %xmm9
	leal	1(%r14), %r12d
	movslq	%r12d, %r12
	addsd	(%rax,%r12,8), %xmm9
	movsd	%xmm9, (%rax,%r12,8)
	movapd	%xmm8, %xmm9
	mulsd	%xmm7, %xmm9
	movapd	%xmm6, %xmm11
	mulsd	%xmm10, %xmm11
	addsd	%xmm9, %xmm11
	addsd	%xmm11, %xmm4
	mulsd	%xmm7, %xmm6
	mulsd	%xmm10, %xmm8
	subsd	%xmm8, %xmm6
	addsd	%xmm6, %xmm3
	addl	%r9d, %ebx
	addl	%r10d, %r14d
	addl	$2, %r15d
	incl	%r11d
	cmpl	%edi, %r11d
	jne	.LBB125_39	# bb77
.LBB125_40:	# bb79
	movl	76(%rsp), %r9d
	leal	(%r9,%r8), %r9d
	movslq	%r9d, %r9
	movq	144(%rsp), %r10
	movsd	(%r10,%rdx,8), %xmm6
	mulsd	%xmm6, %xmm1
	addsd	(%rax,%r9,8), %xmm1
	movsd	%xmm1, (%rax,%r9,8)
	movl	56(%rsp), %r10d
	leal	(%r10,%r8), %r10d
	movslq	%r10d, %r10
	mulsd	%xmm6, %xmm5
	addsd	(%rax,%r10,8), %xmm5
	movsd	%xmm5, (%rax,%r10,8)
	movapd	%xmm2, %xmm1
	mulsd	%xmm4, %xmm1
	movapd	%xmm0, %xmm5
	mulsd	%xmm3, %xmm5
	subsd	%xmm1, %xmm5
	addsd	(%rax,%r9,8), %xmm5
	movsd	%xmm5, (%rax,%r9,8)
	mulsd	%xmm2, %xmm3
	mulsd	%xmm0, %xmm4
	addsd	%xmm3, %xmm4
	addsd	(%rax,%r10,8), %xmm4
	movsd	%xmm4, (%rax,%r10,8)
	addl	$2, %r8d
	incl	%esi
	cmpl	84(%rsp), %esi
	jne	.LBB125_37	# bb76
.LBB125_41:	# bb81
	movl	%edx, %esi
	addl	48(%rsp), %esi
	movl	64(%rsp), %r8d
	addl	(%rsp), %r8d
	movl	%r8d, 64(%rsp)
	movl	80(%rsp), %r8d
	addl	44(%rsp), %r8d
	movl	%r8d, 80(%rsp)
	movl	76(%rsp), %r8d
	addl	40(%rsp), %r8d
	movl	%r8d, 76(%rsp)
	incl	%edi
	cmpl	60(%rsp), %edi
	je	.LBB125_66	# return
.LBB125_42:	# bb80.preheader
	movl	76(%rsp), %r8d
	leal	1(%r8), %r8d
	movl	%r8d, 56(%rsp)
	movl	80(%rsp), %r8d
	leal	1(%r8), %r8d
	movl	%r8d, 52(%rsp)
	movslq	%esi, %rdx
	xorl	%r8d, %r8d
	movl	%r8d, %esi
	jmp	.LBB125_37	# bb76
.LBB125_43:	# bb83
	cmpl	$121, %edx
	jne	.LBB125_54	# bb96
.LBB125_44:	# bb83
	cmpl	$142, %esi
	jne	.LBB125_54	# bb96
.LBB125_45:	# bb95.preheader
	cmpl	$0, 60(%rsp)
	jle	.LBB125_66	# return
.LBB125_46:	# bb.nph145
	cmpl	$0, 84(%rsp)
	jle	.LBB125_66	# return
.LBB125_47:	# bb93.preheader.preheader
	movl	168(%rsp), %edx
	addl	%edx, %edx
	movl	%edx, 8(%rsp)
	movl	192(%rsp), %edx
	addl	%edx, %edx
	movl	%edx, 4(%rsp)
	xorl	%edx, %edx
	movl	%edx, 52(%rsp)
	movl	%edx, 56(%rsp)
	movl	%edx, 12(%rsp)
	jmp	.LBB125_53	# bb93.preheader
	.align	16
.LBB125_48:	# bb89
	movl	16(%rsp), %edi
	leal	(%rdi,%rsi), %edi
	movslq	%edi, %rdi
	movsd	(%rcx,%rdi,8), %xmm1
	movapd	%xmm2, %xmm3
	mulsd	%xmm1, %xmm3
	movl	52(%rsp), %edi
	leal	(%rdi,%rsi), %edi
	movslq	%edi, %rdi
	movsd	(%rcx,%rdi,8), %xmm4
	movapd	%xmm0, %xmm5
	mulsd	%xmm4, %xmm5
	subsd	%xmm3, %xmm5
	movslq	80(%rsp), %rdi
	movq	144(%rsp), %r8
	movsd	(%r8,%rdi,8), %xmm3
	movapd	%xmm5, %xmm6
	mulsd	%xmm3, %xmm6
	movl	56(%rsp), %edi
	leal	(%rdi,%rsi), %edi
	movslq	%edi, %rdi
	movq	%rdi, 64(%rsp)
	addsd	(%rax,%rdi,8), %xmm6
	movsd	%xmm6, (%rax,%rdi,8)
	mulsd	%xmm2, %xmm4
	mulsd	%xmm0, %xmm1
	addsd	%xmm4, %xmm1
	mulsd	%xmm1, %xmm3
	movl	20(%rsp), %edi
	leal	(%rdi,%rsi), %edi
	movslq	%edi, %rdi
	addsd	(%rax,%rdi,8), %xmm3
	movsd	%xmm3, (%rax,%rdi,8)
	movl	76(%rsp), %r8d
	leal	1(%r8), %r8d
	cmpl	84(%rsp), %r8d
	jge	.LBB125_70	# bb89.bb92_crit_edge
.LBB125_49:	# bb.nph139
	movl	44(%rsp), %r8d
	leal	(%r8,%rsi), %r8d
	movl	40(%rsp), %r9d
	leal	(%r9,%rsi), %r9d
	movl	36(%rsp), %r10d
	leal	(%r10,%rsi), %r10d
	movl	24(%rsp), %r11d
	leal	(%r11,%rsi), %r11d
	movl	80(%rsp), %ebx
	leal	3(%rbx), %r14d
	leal	2(%rbx), %ebx
	pxor	%xmm3, %xmm3
	xorl	%r15d, %r15d
	movl	%r15d, %r12d
	movapd	%xmm3, %xmm4
	.align	16
.LBB125_50:	# bb90
	leal	(%r14,%r15), %r13d
	movslq	%r13d, %r13
	movq	144(%rsp), %rbp
	movsd	(%rbp,%r13,8), %xmm6
	movapd	%xmm1, %xmm7
	mulsd	%xmm6, %xmm7
	leal	(%rbx,%r15), %r13d
	movslq	%r13d, %r13
	movsd	(%rbp,%r13,8), %xmm8
	movapd	%xmm5, %xmm9
	mulsd	%xmm8, %xmm9
	subsd	%xmm7, %xmm9
	leal	(%r9,%r15), %r13d
	movslq	%r13d, %r13
	addsd	(%rax,%r13,8), %xmm9
	leal	(%r10,%r15), %ebp
	movslq	%ebp, %rbp
	movsd	(%rcx,%rbp,8), %xmm7
	leal	(%r11,%r15), %ebp
	movslq	%ebp, %rbp
	movsd	(%rcx,%rbp,8), %xmm10
	movsd	%xmm9, (%rax,%r13,8)
	movapd	%xmm1, %xmm9
	mulsd	%xmm8, %xmm9
	movapd	%xmm5, %xmm11
	mulsd	%xmm6, %xmm11
	addsd	%xmm9, %xmm11
	leal	(%r8,%r15), %r13d
	movslq	%r13d, %r13
	addsd	(%rax,%r13,8), %xmm11
	movsd	%xmm11, (%rax,%r13,8)
	movapd	%xmm10, %xmm9
	mulsd	%xmm6, %xmm9
	mulsd	%xmm7, %xmm6
	mulsd	%xmm8, %xmm7
	subsd	%xmm9, %xmm7
	addsd	%xmm7, %xmm4
	mulsd	%xmm8, %xmm10
	addsd	%xmm6, %xmm10
	addsd	%xmm10, %xmm3
	addl	$2, %r15d
	incl	%r12d
	cmpl	%edx, %r12d
	jne	.LBB125_50	# bb90
.LBB125_51:	# bb92
	movapd	%xmm2, %xmm1
	mulsd	%xmm4, %xmm1
	movapd	%xmm0, %xmm5
	mulsd	%xmm3, %xmm5
	subsd	%xmm1, %xmm5
	movq	64(%rsp), %r8
	addsd	(%rax,%r8,8), %xmm5
	movsd	%xmm5, (%rax,%r8,8)
	mulsd	%xmm2, %xmm3
	mulsd	%xmm0, %xmm4
	addsd	%xmm3, %xmm4
	addsd	(%rax,%rdi,8), %xmm4
	movsd	%xmm4, (%rax,%rdi,8)
	movl	80(%rsp), %edi
	addl	48(%rsp), %edi
	movl	%edi, 80(%rsp)
	addl	$2, %esi
	decl	%edx
	movl	76(%rsp), %edi
	incl	%edi
	movl	%edi, 76(%rsp)
	cmpl	84(%rsp), %edi
	jne	.LBB125_48	# bb89
.LBB125_52:	# bb94
	movl	52(%rsp), %edx
	addl	8(%rsp), %edx
	movl	%edx, 52(%rsp)
	movl	56(%rsp), %edx
	addl	4(%rsp), %edx
	movl	%edx, 56(%rsp)
	movl	12(%rsp), %edx
	incl	%edx
	movl	%edx, 12(%rsp)
	cmpl	60(%rsp), %edx
	je	.LBB125_66	# return
.LBB125_53:	# bb93.preheader
	movl	152(%rsp), %edx
	leal	2(,%rdx,2), %edx
	movl	%edx, 48(%rsp)
	movl	56(%rsp), %edx
	leal	3(%rdx), %esi
	movl	%esi, 44(%rsp)
	leal	2(%rdx), %esi
	movl	%esi, 40(%rsp)
	movl	52(%rsp), %esi
	leal	3(%rsi), %edi
	movl	%edi, 36(%rsp)
	leal	2(%rsi), %edi
	movl	%edi, 24(%rsp)
	leal	1(%rdx), %edx
	movl	%edx, 20(%rsp)
	leal	1(%rsi), %edx
	movl	%edx, 16(%rsp)
	movl	84(%rsp), %edx
	leal	-1(%rdx), %edx
	xorl	%edi, %edi
	movl	%edi, 80(%rsp)
	movl	%edi, %esi
	movl	%edi, 76(%rsp)
	jmp	.LBB125_48	# bb89
.LBB125_54:	# bb96
	cmpl	$122, %edx
	jne	.LBB125_65	# bb109
.LBB125_55:	# bb96
	cmpl	$142, %esi
	jne	.LBB125_65	# bb109
.LBB125_56:	# bb108.preheader
	cmpl	$0, 60(%rsp)
	jle	.LBB125_66	# return
.LBB125_57:	# bb.nph133
	cmpl	$0, 84(%rsp)
	jle	.LBB125_66	# return
.LBB125_58:	# bb106.preheader.preheader
	movl	168(%rsp), %edx
	addl	%edx, %edx
	movl	%edx, 8(%rsp)
	movl	192(%rsp), %edx
	addl	%edx, %edx
	movl	%edx, 4(%rsp)
	xorl	%r10d, %r10d
	movl	%r10d, %r9d
	movl	%r10d, 64(%rsp)
	jmp	.LBB125_64	# bb106.preheader
	.align	16
.LBB125_59:	# bb102
	leal	(%r10,%rdx), %r14d
	movslq	%r14d, %r14
	movsd	(%rcx,%r14,8), %xmm1
	movapd	%xmm2, %xmm3
	mulsd	%xmm1, %xmm3
	leal	(%rbx,%rdx), %r14d
	movslq	%r14d, %r14
	movsd	(%rcx,%r14,8), %xmm4
	movapd	%xmm0, %xmm5
	mulsd	%xmm4, %xmm5
	addsd	%xmm3, %xmm5
	mulsd	%xmm2, %xmm4
	mulsd	%xmm0, %xmm1
	subsd	%xmm4, %xmm1
	testl	%edi, %edi
	jle	.LBB125_71	# bb102.bb105_crit_edge
.LBB125_60:	# bb.nph126
	leal	1(%rsi), %r14d
	pxor	%xmm3, %xmm3
	xorl	%r15d, %r15d
	movl	%r15d, %r12d
	movapd	%xmm3, %xmm4
	.align	16
.LBB125_61:	# bb103
	leal	(%r14,%r15), %r13d
	movslq	%r13d, %r13
	movq	144(%rsp), %rbp
	movsd	(%rbp,%r13,8), %xmm6
	movapd	%xmm5, %xmm7
	mulsd	%xmm6, %xmm7
	leal	(%rsi,%r15), %r13d
	movslq	%r13d, %r13
	movsd	(%rbp,%r13,8), %xmm8
	movapd	%xmm1, %xmm9
	mulsd	%xmm8, %xmm9
	subsd	%xmm7, %xmm9
	leal	(%r9,%r15), %r13d
	movslq	%r13d, %r13
	addsd	(%rax,%r13,8), %xmm9
	leal	(%rbx,%r15), %ebp
	movslq	%ebp, %rbp
	movsd	(%rcx,%rbp,8), %xmm7
	leal	(%r10,%r15), %ebp
	movslq	%ebp, %rbp
	movsd	(%rcx,%rbp,8), %xmm10
	movsd	%xmm9, (%rax,%r13,8)
	movapd	%xmm5, %xmm9
	mulsd	%xmm8, %xmm9
	movapd	%xmm1, %xmm11
	mulsd	%xmm6, %xmm11
	addsd	%xmm9, %xmm11
	leal	(%r8,%r15), %r13d
	movslq	%r13d, %r13
	addsd	(%rax,%r13,8), %xmm11
	movsd	%xmm11, (%rax,%r13,8)
	movapd	%xmm10, %xmm9
	mulsd	%xmm6, %xmm9
	mulsd	%xmm7, %xmm6
	mulsd	%xmm8, %xmm7
	subsd	%xmm9, %xmm7
	addsd	%xmm7, %xmm4
	mulsd	%xmm8, %xmm10
	addsd	%xmm6, %xmm10
	addsd	%xmm10, %xmm3
	addl	$2, %r15d
	incl	%r12d
	cmpl	%edi, %r12d
	jne	.LBB125_61	# bb103
.LBB125_62:	# bb105
	leal	(%r9,%rdx), %r14d
	movslq	%r14d, %r14
	movslq	%r11d, %r15
	movq	144(%rsp), %r12
	movsd	(%r12,%r15,8), %xmm6
	mulsd	%xmm6, %xmm1
	addsd	(%rax,%r14,8), %xmm1
	movsd	%xmm1, (%rax,%r14,8)
	leal	(%r8,%rdx), %r15d
	movslq	%r15d, %r15
	mulsd	%xmm6, %xmm5
	addsd	(%rax,%r15,8), %xmm5
	movsd	%xmm5, (%rax,%r15,8)
	movapd	%xmm2, %xmm1
	mulsd	%xmm4, %xmm1
	movapd	%xmm0, %xmm5
	mulsd	%xmm3, %xmm5
	subsd	%xmm1, %xmm5
	addsd	(%rax,%r14,8), %xmm5
	movsd	%xmm5, (%rax,%r14,8)
	mulsd	%xmm2, %xmm3
	mulsd	%xmm0, %xmm4
	addsd	%xmm3, %xmm4
	addsd	(%rax,%r15,8), %xmm4
	movsd	%xmm4, (%rax,%r15,8)
	addl	80(%rsp), %r11d
	addl	76(%rsp), %esi
	addl	$2, %edx
	incl	%edi
	cmpl	84(%rsp), %edi
	jne	.LBB125_59	# bb102
.LBB125_63:	# bb107
	addl	8(%rsp), %r10d
	addl	4(%rsp), %r9d
	movl	64(%rsp), %edx
	incl	%edx
	movl	%edx, 64(%rsp)
	cmpl	60(%rsp), %edx
	je	.LBB125_66	# return
.LBB125_64:	# bb106.preheader
	leal	1(%r10), %ebx
	leal	1(%r9), %r8d
	movl	152(%rsp), %edx
	leal	2(,%rdx,2), %esi
	movl	%esi, 80(%rsp)
	leal	(%rdx,%rdx), %edx
	movl	%edx, 76(%rsp)
	xorl	%r11d, %r11d
	movl	%r11d, %esi
	movl	%r11d, %edx
	movl	%r11d, %edi
	jmp	.LBB125_59	# bb102
.LBB125_65:	# bb109
	xorl	%edi, %edi
	leaq	.str159, %rsi
	leaq	.str1160, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB125_66:	# return
	addq	$88, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB125_67:	# bb31.bb40_crit_edge
	movl	%r8d, 84(%rsp)
	jmp	.LBB125_5	# bb40
.LBB125_68:	# bb63.bb66_crit_edge
	pxor	%xmm3, %xmm3
	movapd	%xmm3, %xmm4
	jmp	.LBB125_29	# bb66
.LBB125_69:	# bb76.bb79_crit_edge
	pxor	%xmm3, %xmm3
	movapd	%xmm3, %xmm4
	jmp	.LBB125_40	# bb79
.LBB125_70:	# bb89.bb92_crit_edge
	pxor	%xmm3, %xmm3
	movapd	%xmm3, %xmm4
	jmp	.LBB125_51	# bb92
.LBB125_71:	# bb102.bb105_crit_edge
	pxor	%xmm3, %xmm3
	movapd	%xmm3, %xmm4
	jmp	.LBB125_62	# bb105
	.size	cblas_zhemm, .-cblas_zhemm
.Leh_func_end85:


	.section	.rodata.cst8,"aM",@progbits,8
	.align	8
.LCPI126_0:					
	.quad	4607182418800017408	# double value: 1.000000e+00
	.text
	.align	16
	.globl	cblas_zhemv
	.type	cblas_zhemv,@function
cblas_zhemv:
.Leh_func_begin86:
.Llabel86:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	movl	%r9d, 20(%rsp)
	movsd	(%rcx), %xmm0
	pxor	%xmm1, %xmm1
	ucomisd	%xmm1, %xmm0
	setp	%al
	setne	%r9b
	setnp	%r10b
	sete	%r11b
	andb	%r10b, %r11b
	movsd	8(%rcx), %xmm2
	ucomisd	%xmm1, %xmm2
	setp	%cl
	setnp	%r10b
	sete	%bl
	setne	%r14b
	andb	%r10b, %bl
	andb	%r11b, %bl
	orb	%al, %r9b
	orb	%cl, %r14b
	orb	%r9b, %r14b
	cmpl	$102, %edi
	movl	$4294967295, %eax
	movl	$1, %ecx
	cmove	%eax, %ecx
	movl	%ecx, 16(%rsp)
	testb	%r14b, %r14b
	movq	112(%rsp), %rax
	movsd	8(%rax), %xmm1
	movsd	(%rax), %xmm3
	movq	120(%rsp), %rax
	movq	96(%rsp), %rcx
	jne	.LBB126_3	# bb23
.LBB126_1:	# entry
	ucomisd	.LCPI126_0(%rip), %xmm3
	jne	.LBB126_3	# bb23
	jp	.LBB126_3	# bb23
.LBB126_2:	# entry
	pxor	%xmm4, %xmm4
	ucomisd	%xmm4, %xmm1
	setnp	%r9b
	sete	%r10b
	testb	%r9b, %r10b
	jne	.LBB126_45	# bb95.thread
.LBB126_3:	# bb23
	pxor	%xmm4, %xmm4
	ucomisd	%xmm4, %xmm1
	jne	.LBB126_10	# bb31
	jp	.LBB126_10	# bb31
.LBB126_4:	# bb23
	pxor	%xmm4, %xmm4
	ucomisd	%xmm4, %xmm3
	jne	.LBB126_10	# bb31
	jp	.LBB126_10	# bb31
.LBB126_5:	# bb25
	cmpl	$0, 128(%rsp)
	jg	.LBB126_47	# bb25.bb30.preheader_crit_edge
.LBB126_6:	# bb26
	movl	$1, %r9d
	subl	%edx, %r9d
	imull	128(%rsp), %r9d
.LBB126_7:	# bb30.preheader
	testl	%edx, %edx
	jle	.LBB126_17	# bb39
.LBB126_8:	# bb.nph
	movl	128(%rsp), %r10d
	leal	(%r10,%r10), %r10d
	addl	%r9d, %r9d
	xorl	%r11d, %r11d
	.align	16
.LBB126_9:	# bb29
	movslq	%r9d, %r14
	movq	$0, (%rax,%r14,8)
	leal	(%r10,%r9), %r14d
	incl	%r9d
	movslq	%r9d, %r9
	movq	$0, (%rax,%r9,8)
	incl	%r11d
	cmpl	%edx, %r11d
	movl	%r14d, %r9d
	jne	.LBB126_9	# bb29
	jmp	.LBB126_17	# bb39
.LBB126_10:	# bb31
	pxor	%xmm4, %xmm4
	ucomisd	%xmm4, %xmm1
	jne	.LBB126_12	# bb33
	jp	.LBB126_12	# bb33
.LBB126_11:	# bb31
	ucomisd	.LCPI126_0(%rip), %xmm3
	setnp	%r9b
	sete	%r10b
	testb	%r9b, %r10b
	jne	.LBB126_17	# bb39
.LBB126_12:	# bb33
	cmpl	$0, 128(%rsp)
	jg	.LBB126_48	# bb33.bb38.preheader_crit_edge
.LBB126_13:	# bb34
	movl	$1, %r9d
	subl	%edx, %r9d
	imull	128(%rsp), %r9d
.LBB126_14:	# bb38.preheader
	testl	%edx, %edx
	jle	.LBB126_17	# bb39
.LBB126_15:	# bb.nph140
	movl	128(%rsp), %r10d
	leal	(%r10,%r10), %r10d
	addl	%r9d, %r9d
	xorl	%r11d, %r11d
	.align	16
.LBB126_16:	# bb37
	movslq	%r9d, %r14
	movsd	(%rax,%r14,8), %xmm4
	movapd	%xmm4, %xmm5
	mulsd	%xmm3, %xmm5
	leal	1(%r9), %r15d
	movslq	%r15d, %r15
	movsd	(%rax,%r15,8), %xmm6
	movapd	%xmm6, %xmm7
	mulsd	%xmm1, %xmm7
	subsd	%xmm7, %xmm5
	movsd	%xmm5, (%rax,%r14,8)
	mulsd	%xmm1, %xmm4
	mulsd	%xmm3, %xmm6
	addsd	%xmm4, %xmm6
	movsd	%xmm6, (%rax,%r15,8)
	addl	%r10d, %r9d
	incl	%r11d
	cmpl	%edx, %r11d
	jne	.LBB126_16	# bb37
.LBB126_17:	# bb39
	testb	$1, %bl
	jne	.LBB126_45	# bb95.thread
.LBB126_18:	# bb41
	cmpl	$121, %esi
	jne	.LBB126_20	# bb44
.LBB126_19:	# bb41
	cmpl	$101, %edi
	je	.LBB126_22	# bb48
.LBB126_20:	# bb44
	cmpl	$122, %esi
	jne	.LBB126_32	# bb66
.LBB126_21:	# bb44
	cmpl	$102, %edi
	jne	.LBB126_32	# bb66
.LBB126_22:	# bb48
	cmpl	$0, 104(%rsp)
	jg	.LBB126_49	# bb48.bb51_crit_edge
.LBB126_23:	# bb49
	movl	$1, %esi
	subl	%edx, %esi
	imull	104(%rsp), %esi
	movl	%esi, 32(%rsp)
.LBB126_24:	# bb51
	cmpl	$0, 128(%rsp)
	jg	.LBB126_50	# bb51.bb65.preheader_crit_edge
.LBB126_25:	# bb52
	movl	$1, %esi
	subl	%edx, %esi
	imull	128(%rsp), %esi
.LBB126_26:	# bb65.preheader
	testl	%edx, %edx
	jle	.LBB126_45	# bb95.thread
.LBB126_27:	# bb.nph137
	movl	$1, %edi
	subl	%edx, %edi
	movl	%edi, %r9d
	movl	104(%rsp), %r10d
	imull	%r10d, %r9d
	movl	%r9d, 8(%rsp)
	movl	128(%rsp), %r9d
	imull	%r9d, %edi
	movl	%edi, 4(%rsp)
	addl	%esi, %esi
	movl	32(%rsp), %edi
	addl	%edi, %edi
	movl	%edi, 32(%rsp)
	movl	20(%rsp), %edi
	leal	2(,%rdi,2), %edi
	movl	%edi, 20(%rsp)
	leal	-1(%rdx), %edi
	cvtsi2sd	16(%rsp), %xmm1
	leal	(%r9,%r9), %r11d
	movl	%r11d, 16(%rsp)
	leal	(%r10,%r10), %r11d
	movl	%r11d, 12(%rsp)
	xorl	%r11d, %r11d
	movl	%r11d, 36(%rsp)
	movl	%r10d, 28(%rsp)
	movl	%r9d, 24(%rsp)
	.align	16
.LBB126_28:	# bb55
	movl	32(%rsp), %r9d
	movslq	%r9d, %r10
	movsd	(%rcx,%r10,8), %xmm3
	movapd	%xmm0, %xmm4
	mulsd	%xmm3, %xmm4
	leal	1(%r9), %r9d
	movslq	%r9d, %r9
	movsd	(%rcx,%r9,8), %xmm5
	movapd	%xmm2, %xmm6
	mulsd	%xmm5, %xmm6
	subsd	%xmm6, %xmm4
	movslq	36(%rsp), %r9
	movsd	(%r8,%r9,8), %xmm6
	movapd	%xmm4, %xmm7
	mulsd	%xmm6, %xmm7
	movslq	%esi, %r9
	addsd	(%rax,%r9,8), %xmm7
	movsd	%xmm7, (%rax,%r9,8)
	mulsd	%xmm2, %xmm3
	mulsd	%xmm0, %xmm5
	addsd	%xmm3, %xmm5
	mulsd	%xmm5, %xmm6
	incl	%esi
	movslq	%esi, %rsi
	addsd	(%rax,%rsi,8), %xmm6
	movsd	%xmm6, (%rax,%rsi,8)
	xorl	%r10d, %r10d
	cmpl	$0, 128(%rsp)
	movl	4(%rsp), %ebx
	cmovg	%r10d, %ebx
	cmpl	$0, 104(%rsp)
	cmovle	8(%rsp), %r10d
	leal	1(%r11), %r14d
	cmpl	%edx, %r14d
	jge	.LBB126_51	# bb55.bb64_crit_edge
.LBB126_29:	# bb55.bb62_crit_edge
	pxor	%xmm3, %xmm3
	xorl	%r14d, %r14d
	movl	36(%rsp), %r15d
	movl	24(%rsp), %r12d
	movl	28(%rsp), %r13d
	movapd	%xmm3, %xmm6
	.align	16
.LBB126_30:	# bb62
	leal	3(%r15), %ebp
	movslq	%ebp, %rbp
	movapd	%xmm1, %xmm7
	mulsd	(%r8,%rbp,8), %xmm7
	movapd	%xmm7, %xmm8
	mulsd	%xmm5, %xmm8
	addl	$2, %r15d
	movslq	%r15d, %rbp
	movsd	(%r8,%rbp,8), %xmm9
	movapd	%xmm4, %xmm10
	mulsd	%xmm9, %xmm10
	addsd	%xmm8, %xmm10
	addl	%ebx, %r12d
	leal	(%r12,%r12), %ebx
	movslq	%ebx, %rbx
	addsd	(%rax,%rbx,8), %xmm10
	movsd	%xmm10, (%rax,%rbx,8)
	movapd	%xmm4, %xmm8
	mulsd	%xmm7, %xmm8
	movapd	%xmm5, %xmm10
	mulsd	%xmm9, %xmm10
	subsd	%xmm8, %xmm10
	leal	1(,%r12,2), %ebx
	movslq	%ebx, %rbx
	addsd	(%rax,%rbx,8), %xmm10
	movsd	%xmm10, (%rax,%rbx,8)
	addl	%r10d, %r13d
	leal	1(,%r13,2), %r10d
	movslq	%r10d, %r10
	movsd	(%rcx,%r10,8), %xmm8
	movapd	%xmm8, %xmm10
	mulsd	%xmm9, %xmm10
	leal	(%r13,%r13), %r10d
	movslq	%r10d, %r10
	movsd	(%rcx,%r10,8), %xmm11
	movapd	%xmm11, %xmm12
	mulsd	%xmm7, %xmm12
	addsd	%xmm10, %xmm12
	addsd	%xmm12, %xmm6
	mulsd	%xmm8, %xmm7
	mulsd	%xmm9, %xmm11
	subsd	%xmm7, %xmm11
	addsd	%xmm11, %xmm3
	incl	%r14d
	cmpl	%edi, %r14d
	movl	128(%rsp), %ebx
	movl	104(%rsp), %r10d
	jne	.LBB126_30	# bb62
.LBB126_31:	# bb64
	movapd	%xmm2, %xmm4
	mulsd	%xmm6, %xmm4
	movapd	%xmm0, %xmm5
	mulsd	%xmm3, %xmm5
	subsd	%xmm4, %xmm5
	addsd	(%rax,%r9,8), %xmm5
	movsd	%xmm5, (%rax,%r9,8)
	mulsd	%xmm2, %xmm3
	mulsd	%xmm0, %xmm6
	addsd	%xmm3, %xmm6
	addsd	(%rax,%rsi,8), %xmm6
	movsd	%xmm6, (%rax,%rsi,8)
	movl	%r9d, %esi
	addl	16(%rsp), %esi
	movl	32(%rsp), %r9d
	addl	12(%rsp), %r9d
	movl	%r9d, 32(%rsp)
	movl	104(%rsp), %r9d
	addl	%r9d, 28(%rsp)
	movl	128(%rsp), %r9d
	addl	%r9d, 24(%rsp)
	movl	36(%rsp), %r9d
	addl	20(%rsp), %r9d
	movl	%r9d, 36(%rsp)
	decl	%edi
	incl	%r11d
	cmpl	%edx, %r11d
	jne	.LBB126_28	# bb55
	jmp	.LBB126_45	# bb95.thread
.LBB126_32:	# bb66
	cmpl	$102, %edi
	sete	%r9b
	cmpl	$121, %esi
	sete	%r10b
	andb	%r9b, %r10b
	cmpl	$101, %edi
	sete	%dil
	cmpl	$122, %esi
	sete	%sil
	testb	%dil, %sil
	jne	.LBB126_34	# bb74
.LBB126_33:	# bb66
	notb	%r10b
	testb	$1, %r10b
	jne	.LBB126_46	# bb97
.LBB126_34:	# bb74
	cmpl	$0, 104(%rsp)
	jg	.LBB126_52	# bb74.bb77_crit_edge
.LBB126_35:	# bb75
	movl	$1, %esi
	subl	%edx, %esi
	imull	104(%rsp), %esi
	movl	%esi, 36(%rsp)
.LBB126_36:	# bb77
	cmpl	$0, 128(%rsp)
	jg	.LBB126_53	# bb77.bb80_crit_edge
.LBB126_37:	# bb78
	movl	$1, %esi
	subl	%edx, %esi
	imull	128(%rsp), %esi
.LBB126_38:	# bb80
	leal	-1(%rdx), %edi
	movl	20(%rsp), %r9d
	leal	2(,%r9,2), %r10d
	movl	%r9d, %r11d
	imull	%edi, %r11d
	movl	104(%rsp), %ebx
	movl	%ebx, %r14d
	imull	%edi, %r14d
	movl	128(%rsp), %r15d
	movl	%r15d, %r12d
	imull	%edi, %r12d
	imull	%edi, %r10d
	movl	%r10d, 32(%rsp)
	addl	%r9d, %r9d
	movl	%r9d, 20(%rsp)
	movl	$4294967294, %edi
	subl	%r9d, %edi
	movl	%edi, 4(%rsp)
	addl	%r11d, %r11d
	movl	%r11d, 28(%rsp)
	addl	%esi, %r12d
	addl	%r12d, %r12d
	movl	%r12d, 24(%rsp)
	movl	36(%rsp), %esi
	addl	%r14d, %esi
	addl	%esi, %esi
	movl	%esi, 36(%rsp)
	movl	$1, %esi
	subl	%edx, %esi
	movl	%esi, %edi
	imull	%ebx, %edi
	movl	%edi, 8(%rsp)
	imull	%r15d, %esi
	movl	%esi, (%rsp)
	cvtsi2sd	16(%rsp), %xmm1
	leal	(%r15,%r15), %esi
	movl	%esi, 16(%rsp)
	leal	(%rbx,%rbx), %esi
	movl	%esi, 12(%rsp)
	jmp	.LBB126_43	# bb91
.LBB126_39:	# bb81
	movl	36(%rsp), %edi
	movslq	%edi, %r9
	movsd	(%rcx,%r9,8), %xmm3
	movapd	%xmm0, %xmm4
	mulsd	%xmm3, %xmm4
	leal	1(%rdi), %edi
	movslq	%edi, %rdi
	movsd	(%rcx,%rdi,8), %xmm5
	movapd	%xmm2, %xmm6
	mulsd	%xmm5, %xmm6
	subsd	%xmm6, %xmm4
	movslq	32(%rsp), %rdi
	movsd	(%r8,%rdi,8), %xmm6
	movapd	%xmm4, %xmm7
	mulsd	%xmm6, %xmm7
	movl	24(%rsp), %edi
	movslq	%edi, %r9
	addsd	(%rax,%r9,8), %xmm7
	movsd	%xmm7, (%rax,%r9,8)
	mulsd	%xmm2, %xmm3
	mulsd	%xmm0, %xmm5
	addsd	%xmm3, %xmm5
	mulsd	%xmm5, %xmm6
	leal	1(%rdi), %edi
	movslq	%edi, %rdi
	addsd	(%rax,%rdi,8), %xmm6
	movsd	%xmm6, (%rax,%rdi,8)
	xorl	%r10d, %r10d
	cmpl	$0, 128(%rsp)
	movl	(%rsp), %r11d
	cmovg	%r10d, %r11d
	cmpl	$0, 104(%rsp)
	cmovle	8(%rsp), %r10d
	testl	%esi, %esi
	jle	.LBB126_54	# bb81.bb90_crit_edge
.LBB126_40:	# bb.nph117
	movl	104(%rsp), %esi
	leal	(%rsi,%rsi), %esi
	addl	%r10d, %r10d
	movl	128(%rsp), %ebx
	leal	(%rbx,%rbx), %ebx
	addl	%r11d, %r11d
	leal	-1(%rdx), %r14d
	pxor	%xmm3, %xmm3
	xorl	%r15d, %r15d
	movl	28(%rsp), %r12d
	movapd	%xmm3, %xmm6
	.align	16
.LBB126_41:	# bb88
	movslq	%r12d, %r13
	leal	1(%r12), %ebp
	movslq	%ebp, %rbp
	movapd	%xmm1, %xmm7
	mulsd	(%r8,%rbp,8), %xmm7
	movapd	%xmm7, %xmm8
	mulsd	%xmm5, %xmm8
	movsd	(%r8,%r13,8), %xmm9
	movapd	%xmm4, %xmm10
	mulsd	%xmm9, %xmm10
	addsd	%xmm8, %xmm10
	movslq	%r11d, %r13
	addsd	(%rax,%r13,8), %xmm10
	movsd	%xmm10, (%rax,%r13,8)
	movapd	%xmm4, %xmm8
	mulsd	%xmm7, %xmm8
	movapd	%xmm5, %xmm10
	mulsd	%xmm9, %xmm10
	subsd	%xmm8, %xmm10
	leal	1(%r11), %r13d
	movslq	%r13d, %r13
	addsd	(%rax,%r13,8), %xmm10
	movsd	%xmm10, (%rax,%r13,8)
	movslq	%r10d, %r13
	leal	1(%r10), %ebp
	movslq	%ebp, %rbp
	movsd	(%rcx,%rbp,8), %xmm8
	movapd	%xmm8, %xmm10
	mulsd	%xmm9, %xmm10
	movsd	(%rcx,%r13,8), %xmm11
	movapd	%xmm11, %xmm12
	mulsd	%xmm7, %xmm12
	addsd	%xmm10, %xmm12
	addsd	%xmm12, %xmm3
	mulsd	%xmm7, %xmm8
	mulsd	%xmm9, %xmm11
	subsd	%xmm8, %xmm11
	addsd	%xmm11, %xmm6
	addl	%esi, %r10d
	addl	%ebx, %r11d
	addl	$2, %r12d
	incl	%r15d
	cmpl	%r14d, %r15d
	jne	.LBB126_41	# bb88
.LBB126_42:	# bb90
	movapd	%xmm2, %xmm4
	mulsd	%xmm3, %xmm4
	movapd	%xmm0, %xmm5
	mulsd	%xmm6, %xmm5
	subsd	%xmm4, %xmm5
	addsd	(%rax,%r9,8), %xmm5
	movsd	%xmm5, (%rax,%r9,8)
	mulsd	%xmm2, %xmm6
	mulsd	%xmm0, %xmm3
	addsd	%xmm6, %xmm3
	addsd	(%rax,%rdi,8), %xmm3
	movsd	%xmm3, (%rax,%rdi,8)
	movl	32(%rsp), %esi
	addl	4(%rsp), %esi
	movl	%esi, 32(%rsp)
	movl	24(%rsp), %esi
	subl	16(%rsp), %esi
	movl	%esi, 24(%rsp)
	movl	36(%rsp), %esi
	subl	12(%rsp), %esi
	movl	%esi, 36(%rsp)
	movl	28(%rsp), %esi
	subl	20(%rsp), %esi
	movl	%esi, 28(%rsp)
	decl	%edx
.LBB126_43:	# bb91
	testl	%edx, %edx
	jle	.LBB126_45	# bb95.thread
.LBB126_44:	# bb92
	leal	-1(%rdx), %esi
	testl	%edx, %edx
	jne	.LBB126_39	# bb81
.LBB126_45:	# bb95.thread
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB126_46:	# bb97
	xorl	%edi, %edi
	leaq	.str161, %rsi
	leaq	.str1162, %rdx
	xorb	%al, %al
	call	cblas_xerbla
	jmp	.LBB126_45	# bb95.thread
.LBB126_47:	# bb25.bb30.preheader_crit_edge
	xorl	%r9d, %r9d
	jmp	.LBB126_7	# bb30.preheader
.LBB126_48:	# bb33.bb38.preheader_crit_edge
	xorl	%r9d, %r9d
	jmp	.LBB126_14	# bb38.preheader
.LBB126_49:	# bb48.bb51_crit_edge
	movl	$0, 32(%rsp)
	jmp	.LBB126_24	# bb51
.LBB126_50:	# bb51.bb65.preheader_crit_edge
	xorl	%esi, %esi
	jmp	.LBB126_26	# bb65.preheader
.LBB126_51:	# bb55.bb64_crit_edge
	pxor	%xmm3, %xmm3
	movapd	%xmm3, %xmm6
	jmp	.LBB126_31	# bb64
.LBB126_52:	# bb74.bb77_crit_edge
	movl	$0, 36(%rsp)
	jmp	.LBB126_36	# bb77
.LBB126_53:	# bb77.bb80_crit_edge
	xorl	%esi, %esi
	jmp	.LBB126_38	# bb80
.LBB126_54:	# bb81.bb90_crit_edge
	pxor	%xmm3, %xmm3
	movapd	%xmm3, %xmm6
	jmp	.LBB126_42	# bb90
	.size	cblas_zhemv, .-cblas_zhemv
.Leh_func_end86:


	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI127_0:					
	.quad	9223372036854775808	# double value: -0.000000e+00
	.quad	9223372036854775808	# double value: -0.000000e+00
	.text
	.align	16
	.globl	cblas_zher2
	.type	cblas_zher2,@function
cblas_zher2:
.Leh_func_begin87:
.Llabel87:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$56, %rsp
	cmpl	$102, %edi
	movl	$4294967295, %eax
	movl	$1, %r10d
	cmove	%eax, %r10d
	movl	%r10d, 52(%rsp)
	movsd	8(%rcx), %xmm0
	pxor	%xmm1, %xmm1
	ucomisd	%xmm1, %xmm0
	movsd	(%rcx), %xmm1
	movq	128(%rsp), %rax
	movq	112(%rsp), %rcx
	movl	%edx, 48(%rsp)
	jne	.LBB127_2	# bb20
	jp	.LBB127_2	# bb20
.LBB127_1:	# entry
	pxor	%xmm2, %xmm2
	ucomisd	%xmm2, %xmm1
	setnp	%dl
	sete	%r10b
	testb	%dl, %r10b
	jne	.LBB127_29	# return
.LBB127_2:	# bb20
	cmpl	$121, %esi
	jne	.LBB127_4	# bb23
.LBB127_3:	# bb20
	cmpl	$101, %edi
	je	.LBB127_6	# bb27
.LBB127_4:	# bb23
	cmpl	$122, %esi
	jne	.LBB127_16	# bb39
.LBB127_5:	# bb23
	cmpl	$102, %edi
	jne	.LBB127_16	# bb39
.LBB127_6:	# bb27
	testl	%r9d, %r9d
	jg	.LBB127_30	# bb27.bb30_crit_edge
.LBB127_7:	# bb28
	movl	$1, %edx
	subl	48(%rsp), %edx
	imull	%r9d, %edx
	movl	%edx, 40(%rsp)
.LBB127_8:	# bb30
	cmpl	$0, 120(%rsp)
	jg	.LBB127_31	# bb30.bb38.preheader_crit_edge
.LBB127_9:	# bb31
	movl	$1, %edx
	subl	48(%rsp), %edx
	imull	120(%rsp), %edx
	movl	%edx, 36(%rsp)
.LBB127_10:	# bb38.preheader
	cmpl	$0, 48(%rsp)
	jle	.LBB127_29	# return
.LBB127_11:	# bb.nph88
	movl	40(%rsp), %edx
	leal	(%r9,%rdx), %esi
	addl	%esi, %esi
	movl	%esi, 8(%rsp)
	movl	36(%rsp), %esi
	movl	120(%rsp), %edi
	leal	(%rdi,%rsi), %r10d
	addl	%r10d, %r10d
	movl	%r10d, 4(%rsp)
	movl	136(%rsp), %r10d
	leal	2(,%r10,2), %r10d
	movl	%r10d, 32(%rsp)
	leal	1(,%rdx,2), %r10d
	movl	%r10d, 24(%rsp)
	addl	%edx, %edx
	movl	%edx, 40(%rsp)
	leal	(%r9,%r9), %edx
	movl	%edx, 20(%rsp)
	leal	1(,%rsi,2), %edx
	movl	%edx, 16(%rsp)
	addl	%esi, %esi
	movl	%esi, 36(%rsp)
	leal	(%rdi,%rdi), %edx
	movl	%edx, 12(%rsp)
	movl	48(%rsp), %edx
	leal	-1(%rdx), %edx
	cvtsi2sd	52(%rsp), %xmm2
	movapd	%xmm0, %xmm3
	xorpd	.LCPI127_0(%rip), %xmm3
	xorl	%esi, %esi
	movl	%esi, 52(%rsp)
	movl	%esi, %edi
	movl	%esi, 44(%rsp)
	.align	16
.LBB127_12:	# bb34
	movl	52(%rsp), %r10d
	movl	24(%rsp), %r11d
	leal	(%r11,%r10), %r11d
	movslq	%r11d, %r11
	movsd	(%r8,%r11,8), %xmm4
	movapd	%xmm1, %xmm5
	mulsd	%xmm4, %xmm5
	movl	40(%rsp), %r11d
	leal	(%r11,%r10), %r10d
	movslq	%r10d, %r10
	movsd	(%r8,%r10,8), %xmm6
	movapd	%xmm0, %xmm7
	mulsd	%xmm6, %xmm7
	addsd	%xmm5, %xmm7
	movl	16(%rsp), %r10d
	leal	(%r10,%rdi), %r10d
	movslq	%r10d, %r10
	movsd	(%rcx,%r10,8), %xmm5
	movapd	%xmm7, %xmm8
	mulsd	%xmm5, %xmm8
	mulsd	%xmm0, %xmm4
	mulsd	%xmm1, %xmm6
	subsd	%xmm4, %xmm6
	movl	36(%rsp), %r10d
	leal	(%r10,%rdi), %r10d
	movslq	%r10d, %r10
	movsd	(%rcx,%r10,8), %xmm4
	movapd	%xmm6, %xmm9
	mulsd	%xmm4, %xmm9
	addsd	%xmm8, %xmm9
	addsd	%xmm9, %xmm9
	movslq	%esi, %r10
	addsd	(%rax,%r10,8), %xmm9
	movsd	%xmm9, (%rax,%r10,8)
	leal	1(%rsi), %r10d
	movslq	%r10d, %r10
	movq	$0, (%rax,%r10,8)
	movapd	%xmm0, %xmm8
	mulsd	%xmm5, %xmm8
	movapd	%xmm1, %xmm9
	mulsd	%xmm4, %xmm9
	addsd	%xmm8, %xmm9
	mulsd	%xmm1, %xmm5
	mulsd	%xmm3, %xmm4
	addsd	%xmm5, %xmm4
	movl	44(%rsp), %r10d
	leal	1(%r10), %r10d
	cmpl	48(%rsp), %r10d
	jge	.LBB127_15	# bb37
.LBB127_13:	# bb.nph84
	movl	52(%rsp), %r11d
	movl	8(%rsp), %r10d
	leal	(%r10,%r11), %r10d
	movl	4(%rsp), %r11d
	leal	(%r11,%rdi), %r11d
	movl	120(%rsp), %ebx
	leal	(%rbx,%rbx), %ebx
	leal	(%r9,%r9), %r14d
	xorl	%r15d, %r15d
	movl	%esi, %r12d
	.align	16
.LBB127_14:	# bb35
	movslq	%r10d, %r13
	movsd	(%r8,%r13,8), %xmm5
	movapd	%xmm9, %xmm8
	mulsd	%xmm5, %xmm8
	leal	1(%r10), %r13d
	movslq	%r13d, %r13
	movsd	(%r8,%r13,8), %xmm10
	movapd	%xmm4, %xmm11
	mulsd	%xmm10, %xmm11
	addsd	%xmm8, %xmm11
	movslq	%r11d, %r13
	movsd	(%rcx,%r13,8), %xmm8
	movapd	%xmm6, %xmm12
	mulsd	%xmm8, %xmm12
	leal	1(%r11), %r13d
	movslq	%r13d, %r13
	movsd	(%rcx,%r13,8), %xmm13
	movapd	%xmm7, %xmm14
	mulsd	%xmm13, %xmm14
	addsd	%xmm12, %xmm14
	addsd	%xmm11, %xmm14
	leal	2(%r12), %r13d
	movslq	%r13d, %rbp
	addsd	(%rax,%rbp,8), %xmm14
	movsd	%xmm14, (%rax,%rbp,8)
	mulsd	%xmm4, %xmm5
	mulsd	%xmm9, %xmm10
	subsd	%xmm10, %xmm5
	mulsd	%xmm7, %xmm8
	mulsd	%xmm6, %xmm13
	subsd	%xmm13, %xmm8
	addsd	%xmm5, %xmm8
	mulsd	%xmm2, %xmm8
	addl	$3, %r12d
	movslq	%r12d, %r12
	addsd	(%rax,%r12,8), %xmm8
	movsd	%xmm8, (%rax,%r12,8)
	addl	%ebx, %r11d
	addl	%r14d, %r10d
	incl	%r15d
	cmpl	%edx, %r15d
	movl	%r13d, %r12d
	jne	.LBB127_14	# bb35
.LBB127_15:	# bb37
	addl	32(%rsp), %esi
	movl	52(%rsp), %r10d
	addl	20(%rsp), %r10d
	movl	%r10d, 52(%rsp)
	addl	12(%rsp), %edi
	decl	%edx
	movl	44(%rsp), %r10d
	incl	%r10d
	movl	%r10d, 44(%rsp)
	cmpl	48(%rsp), %r10d
	jne	.LBB127_12	# bb34
	jmp	.LBB127_29	# return
.LBB127_16:	# bb39
	cmpl	$102, %edi
	sete	%dl
	cmpl	$121, %esi
	sete	%r10b
	andb	%dl, %r10b
	cmpl	$101, %edi
	sete	%dl
	cmpl	$122, %esi
	sete	%sil
	testb	%dl, %sil
	jne	.LBB127_18	# bb47
.LBB127_17:	# bb39
	notb	%r10b
	testb	$1, %r10b
	jne	.LBB127_28	# bb65
.LBB127_18:	# bb47
	testl	%r9d, %r9d
	jg	.LBB127_32	# bb47.bb50_crit_edge
.LBB127_19:	# bb48
	movl	$1, %edx
	subl	48(%rsp), %edx
	imull	%r9d, %edx
.LBB127_20:	# bb50
	cmpl	$0, 120(%rsp)
	jg	.LBB127_33	# bb50.bb64.preheader_crit_edge
.LBB127_21:	# bb51
	movl	$1, %esi
	subl	48(%rsp), %esi
	imull	120(%rsp), %esi
.LBB127_22:	# bb64.preheader
	cmpl	$0, 48(%rsp)
	jle	.LBB127_29	# return
.LBB127_23:	# bb.nph74
	movl	$1, %edi
	subl	48(%rsp), %edi
	movl	%edi, %r10d
	imull	%r9d, %r10d
	movl	%r10d, 36(%rsp)
	movl	120(%rsp), %r10d
	imull	%r10d, %edi
	movl	%edi, 16(%rsp)
	addl	%esi, %esi
	addl	%edx, %edx
	movl	136(%rsp), %edi
	leal	2(,%rdi,2), %r11d
	movl	%r11d, 44(%rsp)
	addl	%edi, %edi
	movl	%edi, 20(%rsp)
	cvtsi2sd	52(%rsp), %xmm2
	movapd	%xmm0, %xmm3
	xorpd	.LCPI127_0(%rip), %xmm3
	movsd	%xmm3, 24(%rsp)
	leal	(%r10,%r10), %edi
	movl	%edi, 40(%rsp)
	leal	(%r9,%r9), %edi
	movl	%edi, 32(%rsp)
	xorl	%edi, %edi
	movl	%edi, 52(%rsp)
	movl	%edi, %r10d
	.align	16
.LBB127_24:	# bb54
	xorl	%r11d, %r11d
	cmpl	$0, 120(%rsp)
	movl	16(%rsp), %ebx
	cmovg	%r11d, %ebx
	testl	%r9d, %r9d
	cmovle	36(%rsp), %r11d
	movslq	%esi, %r14
	movsd	(%rcx,%r14,8), %xmm3
	movapd	%xmm1, %xmm4
	mulsd	%xmm3, %xmm4
	movapd	%xmm3, %xmm5
	mulsd	24(%rsp), %xmm5
	leal	1(%rsi), %r14d
	movslq	%r14d, %r14
	movsd	(%rcx,%r14,8), %xmm6
	movapd	%xmm1, %xmm7
	mulsd	%xmm6, %xmm7
	addsd	%xmm5, %xmm7
	movapd	%xmm0, %xmm5
	mulsd	%xmm6, %xmm5
	addsd	%xmm4, %xmm5
	movslq	%edx, %r14
	movsd	(%r8,%r14,8), %xmm4
	movapd	%xmm0, %xmm8
	mulsd	%xmm4, %xmm8
	leal	1(%rdx), %r14d
	movslq	%r14d, %r14
	movsd	(%r8,%r14,8), %xmm9
	movapd	%xmm1, %xmm10
	mulsd	%xmm9, %xmm10
	addsd	%xmm8, %xmm10
	mulsd	%xmm1, %xmm4
	mulsd	%xmm0, %xmm9
	subsd	%xmm9, %xmm4
	testl	%r10d, %r10d
	jle	.LBB127_27	# bb63
.LBB127_25:	# bb.nph
	leal	(%r9,%r9), %r14d
	addl	%r11d, %r11d
	movl	120(%rsp), %r15d
	leal	(%r15,%r15), %r15d
	addl	%ebx, %ebx
	xorl	%r12d, %r12d
	movl	52(%rsp), %r13d
	.align	16
.LBB127_26:	# bb61
	movslq	%r11d, %rbp
	movsd	(%r8,%rbp,8), %xmm8
	movapd	%xmm5, %xmm9
	mulsd	%xmm8, %xmm9
	leal	1(%r11), %ebp
	movslq	%ebp, %rbp
	movsd	(%r8,%rbp,8), %xmm11
	movapd	%xmm7, %xmm12
	mulsd	%xmm11, %xmm12
	addsd	%xmm9, %xmm12
	movslq	%ebx, %rbp
	movsd	(%rcx,%rbp,8), %xmm9
	movapd	%xmm4, %xmm13
	mulsd	%xmm9, %xmm13
	leal	1(%rbx), %ebp
	movslq	%ebp, %rbp
	movsd	(%rcx,%rbp,8), %xmm14
	movapd	%xmm10, %xmm15
	mulsd	%xmm14, %xmm15
	addsd	%xmm13, %xmm15
	addsd	%xmm12, %xmm15
	movslq	%r13d, %rbp
	addsd	(%rax,%rbp,8), %xmm15
	movsd	%xmm15, (%rax,%rbp,8)
	mulsd	%xmm7, %xmm8
	mulsd	%xmm5, %xmm11
	subsd	%xmm11, %xmm8
	mulsd	%xmm10, %xmm9
	mulsd	%xmm4, %xmm14
	subsd	%xmm14, %xmm9
	addsd	%xmm8, %xmm9
	mulsd	%xmm2, %xmm9
	leal	1(%r13), %ebp
	movslq	%ebp, %rbp
	addsd	(%rax,%rbp,8), %xmm9
	movsd	%xmm9, (%rax,%rbp,8)
	addl	%r14d, %r11d
	addl	%r15d, %ebx
	addl	$2, %r13d
	incl	%r12d
	cmpl	%r10d, %r12d
	jne	.LBB127_26	# bb61
.LBB127_27:	# bb63
	mulsd	%xmm6, %xmm10
	mulsd	%xmm3, %xmm4
	addsd	%xmm10, %xmm4
	addsd	%xmm4, %xmm4
	movslq	%edi, %r11
	addsd	(%rax,%r11,8), %xmm4
	movsd	%xmm4, (%rax,%r11,8)
	movl	44(%rsp), %r11d
	leal	(%r11,%rdi), %r11d
	incl	%edi
	movslq	%edi, %rdi
	movq	$0, (%rax,%rdi,8)
	addl	40(%rsp), %esi
	addl	32(%rsp), %edx
	movl	52(%rsp), %edi
	addl	20(%rsp), %edi
	movl	%edi, 52(%rsp)
	incl	%r10d
	cmpl	48(%rsp), %r10d
	movl	%r11d, %edi
	jne	.LBB127_24	# bb54
	jmp	.LBB127_29	# return
.LBB127_28:	# bb65
	xorl	%edi, %edi
	leaq	.str163, %rsi
	leaq	.str1164, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB127_29:	# return
	addq	$56, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB127_30:	# bb27.bb30_crit_edge
	movl	$0, 40(%rsp)
	jmp	.LBB127_8	# bb30
.LBB127_31:	# bb30.bb38.preheader_crit_edge
	movl	$0, 36(%rsp)
	jmp	.LBB127_10	# bb38.preheader
.LBB127_32:	# bb47.bb50_crit_edge
	xorl	%edx, %edx
	jmp	.LBB127_20	# bb50
.LBB127_33:	# bb50.bb64.preheader_crit_edge
	xorl	%esi, %esi
	jmp	.LBB127_22	# bb64.preheader
	.size	cblas_zher2, .-cblas_zher2
.Leh_func_end87:


	.section	.rodata.cst8,"aM",@progbits,8
	.align	16
.LCPI128_0:					
	.quad	4607182418800017408	# double value: 1.000000e+00
	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI128_1:					
	.quad	9223372036854775808	# double value: -0.000000e+00
	.quad	9223372036854775808	# double value: -0.000000e+00
	.text
	.align	16
	.globl	cblas_zher2k
	.type	cblas_zher2k,@function
cblas_zher2k:
.Leh_func_begin88:
.Llabel88:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$72, %rsp
	ucomisd	.LCPI128_0(%rip), %xmm0
	movsd	8(%r9), %xmm1
	movsd	(%r9), %xmm2
	movq	160(%rsp), %rax
	movq	144(%rsp), %r9
	movq	128(%rsp), %r10
	movl	%r8d, 68(%rsp)
	movl	%ecx, 64(%rsp)
	jne	.LBB128_3	# bb47
	jp	.LBB128_3	# bb47
.LBB128_1:	# bb
	pxor	%xmm3, %xmm3
	ucomisd	%xmm3, %xmm2
	setnp	%cl
	sete	%r8b
	andb	%cl, %r8b
	ucomisd	%xmm3, %xmm1
	setnp	%cl
	sete	%r11b
	andb	%cl, %r11b
	testb	%r8b, %r11b
	jne	.LBB128_93	# return
.LBB128_2:	# bb
	cmpl	$0, 68(%rsp)
	je	.LBB128_93	# return
.LBB128_3:	# bb47
	cmpl	$101, %edi
	je	.LBB128_5	# bb56
.LBB128_4:	# bb49
	cmpl	$111, %edx
	movl	$113, %ecx
	movl	$111, %edx
	cmove	%ecx, %edx
	cmpl	$121, %esi
	movl	$122, %ecx
	movl	$121, %esi
	cmove	%ecx, %esi
	xorpd	.LCPI128_1(%rip), %xmm1
.LBB128_5:	# bb56
	movl	%edx, 56(%rsp)
	movl	%esi, 60(%rsp)
	pxor	%xmm3, %xmm3
	ucomisd	%xmm3, %xmm0
	jne	.LBB128_25	# bb70
	jp	.LBB128_25	# bb70
.LBB128_6:	# bb57
	cmpl	$121, 60(%rsp)
	je	.LBB128_17	# bb63.preheader
.LBB128_7:	# bb69.preheader
	cmpl	$0, 64(%rsp)
	jle	.LBB128_11	# bb87
.LBB128_8:	# bb.nph172
	movl	168(%rsp), %ecx
	leal	(%rcx,%rcx), %r8d
	xorl	%esi, %esi
	movl	%esi, %edx
	jmp	.LBB128_23	# bb67.preheader
	.align	16
.LBB128_9:	# bb60
	movslq	%ecx, %rbx
	movq	$0, (%rax,%rbx,8)
	leal	1(%rcx), %ebx
	movslq	%ebx, %rbx
	movq	$0, (%rax,%rbx,8)
	addl	$2, %ecx
	incl	%edx
	cmpl	%esi, %edx
	jne	.LBB128_9	# bb60
.LBB128_10:	# bb62
	addl	%edi, %r8d
	decl	%esi
	incl	%r11d
	cmpl	64(%rsp), %r11d
	jne	.LBB128_19	# bb61.preheader
.LBB128_11:	# bb87
	pxor	%xmm0, %xmm0
	ucomisd	%xmm0, %xmm1
	jne	.LBB128_13	# bb89
	jp	.LBB128_13	# bb89
.LBB128_12:	# bb87
	pxor	%xmm0, %xmm0
	ucomisd	%xmm0, %xmm2
	setnp	%cl
	sete	%dl
	testb	%cl, %dl
	jne	.LBB128_93	# return
.LBB128_13:	# bb89
	cmpl	$121, 60(%rsp)
	jne	.LBB128_53	# bb104
.LBB128_14:	# bb89
	cmpl	$111, 56(%rsp)
	jne	.LBB128_53	# bb104
.LBB128_15:	# bb103.preheader
	cmpl	$0, 64(%rsp)
	jle	.LBB128_93	# return
.LBB128_16:	# bb.nph216
	movl	168(%rsp), %ecx
	leal	2(,%rcx,2), %ecx
	movl	%ecx, 28(%rsp)
	movl	136(%rsp), %ecx
	leal	(%rcx,%rcx), %ecx
	movl	%ecx, 36(%rsp)
	movl	152(%rsp), %ecx
	leal	(%rcx,%rcx), %ecx
	movl	%ecx, 32(%rsp)
	movl	64(%rsp), %ecx
	leal	-1(%rcx), %ecx
	movl	%ecx, 56(%rsp)
	xorl	%esi, %esi
	movl	%esi, 44(%rsp)
	movl	%esi, %ecx
	movl	%esi, %edx
	movl	%esi, 40(%rsp)
	jmp	.LBB128_51	# bb95.preheader
.LBB128_17:	# bb63.preheader
	cmpl	$0, 64(%rsp)
	jle	.LBB128_11	# bb87
.LBB128_18:	# bb.nph220
	movl	168(%rsp), %ecx
	leal	2(,%rcx,2), %edi
	xorl	%r8d, %r8d
	movl	64(%rsp), %esi
	movl	%r8d, %r11d
	.align	16
.LBB128_19:	# bb61.preheader
	cmpl	64(%rsp), %r11d
	jge	.LBB128_10	# bb62
.LBB128_20:	# bb61.preheader.bb60_crit_edge
	xorl	%edx, %edx
	movl	%r8d, %ecx
	jmp	.LBB128_9	# bb60
	.align	16
.LBB128_21:	# bb66
	movslq	%ecx, %r11
	movq	$0, (%rax,%r11,8)
	leal	1(%rcx), %r11d
	movslq	%r11d, %r11
	movq	$0, (%rax,%r11,8)
	addl	$2, %ecx
	incl	%edi
	cmpl	%edx, %edi
	jle	.LBB128_21	# bb66
.LBB128_22:	# bb68
	addl	%r8d, %esi
	incl	%edx
	cmpl	64(%rsp), %edx
	je	.LBB128_11	# bb87
.LBB128_23:	# bb67.preheader
	testl	%edx, %edx
	js	.LBB128_22	# bb68
.LBB128_24:	# bb67.preheader.bb66_crit_edge
	xorl	%edi, %edi
	movl	%esi, %ecx
	jmp	.LBB128_21	# bb66
.LBB128_25:	# bb70
	ucomisd	.LCPI128_0(%rip), %xmm0
	jne	.LBB128_29	# bb71
	jp	.LBB128_29	# bb71
.LBB128_26:	# bb86.preheader
	cmpl	$0, 64(%rsp)
	jle	.LBB128_11	# bb87
.LBB128_27:	# bb.nph
	movl	168(%rsp), %ecx
	leal	2(,%rcx,2), %ecx
	xorl	%edx, %edx
	movl	$1, %esi
	.align	16
.LBB128_28:	# bb85
	movslq	%esi, %rdi
	movq	$0, (%rax,%rdi,8)
	addl	%ecx, %esi
	incl	%edx
	cmpl	64(%rsp), %edx
	jne	.LBB128_28	# bb85
	jmp	.LBB128_11	# bb87
.LBB128_29:	# bb71
	cmpl	$121, 60(%rsp)
	jne	.LBB128_94	# bb83.preheader
.LBB128_30:	# bb77.preheader
	cmpl	$0, 64(%rsp)
	jle	.LBB128_11	# bb87
.LBB128_31:	# bb.nph168
	movl	168(%rsp), %ecx
	leal	2(,%rcx,2), %ecx
	movl	64(%rsp), %edx
	leal	-1(%rdx), %edx
	xorl	%esi, %esi
	movl	%esi, %edi
	.align	16
.LBB128_32:	# bb73
	movslq	%esi, %r8
	movapd	%xmm0, %xmm3
	mulsd	(%rax,%r8,8), %xmm3
	movsd	%xmm3, (%rax,%r8,8)
	leal	1(%rsi), %r8d
	movslq	%r8d, %r8
	movq	$0, (%rax,%r8,8)
	leal	1(%rdi), %r8d
	cmpl	64(%rsp), %r8d
	jge	.LBB128_35	# bb76
.LBB128_33:	# bb73.bb74_crit_edge
	xorl	%r8d, %r8d
	movl	%esi, %r11d
	.align	16
.LBB128_34:	# bb74
	leal	2(%r11), %ebx
	movslq	%ebx, %r14
	movapd	%xmm0, %xmm3
	mulsd	(%rax,%r14,8), %xmm3
	movsd	%xmm3, (%rax,%r14,8)
	addl	$3, %r11d
	movslq	%r11d, %r11
	movapd	%xmm0, %xmm3
	mulsd	(%rax,%r11,8), %xmm3
	movsd	%xmm3, (%rax,%r11,8)
	incl	%r8d
	cmpl	%edx, %r8d
	movl	%ebx, %r11d
	jne	.LBB128_34	# bb74
.LBB128_35:	# bb76
	addl	%ecx, %esi
	decl	%edx
	incl	%edi
	cmpl	64(%rsp), %edi
	je	.LBB128_11	# bb87
	jmp	.LBB128_32	# bb73
	.align	16
.LBB128_36:	# bb80
	movslq	%edi, %r14
	movapd	%xmm0, %xmm3
	mulsd	(%rax,%r14,8), %xmm3
	movsd	%xmm3, (%rax,%r14,8)
	leal	1(%rdi), %r14d
	movslq	%r14d, %r14
	movapd	%xmm0, %xmm3
	mulsd	(%rax,%r14,8), %xmm3
	movsd	%xmm3, (%rax,%r14,8)
	addl	$2, %edi
	incl	%ebx
	cmpl	%edx, %ebx
	jne	.LBB128_36	# bb80
.LBB128_37:	# bb82
	movslq	%r8d, %rdi
	movapd	%xmm0, %xmm3
	mulsd	(%rax,%rdi,8), %xmm3
	movsd	%xmm3, (%rax,%rdi,8)
	leal	(%r11,%r8), %edi
	incl	%r8d
	movslq	%r8d, %r8
	movq	$0, (%rax,%r8,8)
	addl	%ecx, %esi
	incl	%edx
	cmpl	64(%rsp), %edx
	je	.LBB128_11	# bb87
.LBB128_38:	# bb81.preheader
	movl	%edi, %r8d
	testl	%edx, %edx
	jle	.LBB128_37	# bb82
.LBB128_39:	# bb81.preheader.bb80_crit_edge
	xorl	%ebx, %ebx
	movl	%esi, %edi
	jmp	.LBB128_36	# bb80
.LBB128_40:	# bb.nph203
	leal	1(%rdx), %esi
	leal	1(%rcx), %edi
	pxor	%xmm0, %xmm0
	xorl	%r8d, %r8d
	movl	%r8d, %r11d
	.align	16
.LBB128_41:	# bb94
	leal	(%rcx,%r8), %ebx
	movslq	%ebx, %rbx
	movsd	(%r10,%rbx,8), %xmm3
	movapd	%xmm1, %xmm4
	mulsd	%xmm3, %xmm4
	leal	(%rdi,%r8), %ebx
	movslq	%ebx, %rbx
	movsd	(%r10,%rbx,8), %xmm5
	movapd	%xmm2, %xmm6
	mulsd	%xmm5, %xmm6
	addsd	%xmm4, %xmm6
	leal	(%rsi,%r8), %ebx
	movslq	%ebx, %rbx
	mulsd	(%r9,%rbx,8), %xmm6
	mulsd	%xmm1, %xmm5
	mulsd	%xmm2, %xmm3
	subsd	%xmm5, %xmm3
	leal	(%rdx,%r8), %ebx
	movslq	%ebx, %rbx
	mulsd	(%r9,%rbx,8), %xmm3
	addsd	%xmm6, %xmm3
	addsd	%xmm3, %xmm0
	addl	$2, %r8d
	incl	%r11d
	cmpl	68(%rsp), %r11d
	jne	.LBB128_41	# bb94
.LBB128_42:	# bb95.bb96_crit_edge
	addsd	%xmm0, %xmm0
.LBB128_43:	# bb96
	movl	44(%rsp), %esi
	movslq	%esi, %rdi
	addsd	(%rax,%rdi,8), %xmm0
	movsd	%xmm0, (%rax,%rdi,8)
	leal	1(%rsi), %esi
	movslq	%esi, %rsi
	movq	$0, (%rax,%rsi,8)
	movl	40(%rsp), %esi
	leal	1(%rsi), %esi
	cmpl	64(%rsp), %esi
	jge	.LBB128_50	# bb102
.LBB128_44:	# bb.nph214
	movl	32(%rsp), %esi
	leal	(%rsi,%rdx), %edi
	movl	36(%rsp), %esi
	leal	(%rsi,%rcx), %esi
	leal	1(%rcx), %r11d
	leal	1(%rdx), %r8d
	movl	136(%rsp), %ebx
	leal	(%rbx,%rbx), %ebx
	movl	%ebx, 52(%rsp)
	movl	152(%rsp), %ebx
	leal	(%rbx,%rbx), %ebx
	movl	%ebx, 48(%rsp)
	movl	$0, 60(%rsp)
	movl	44(%rsp), %r14d
	jmp	.LBB128_48	# bb99.preheader
.LBB128_45:	# bb.nph208
	leal	1(%rdi), %r14d
	leal	1(%rsi), %r15d
	pxor	%xmm0, %xmm0
	xorl	%r12d, %r12d
	movl	%r12d, %r13d
	movapd	%xmm0, %xmm3
	.align	16
.LBB128_46:	# bb98
	leal	(%rsi,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r10,%rbp,8), %xmm4
	movapd	%xmm1, %xmm5
	mulsd	%xmm4, %xmm5
	leal	(%r15,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r10,%rbp,8), %xmm6
	movapd	%xmm2, %xmm7
	mulsd	%xmm6, %xmm7
	addsd	%xmm5, %xmm7
	leal	(%rdx,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r9,%rbp,8), %xmm5
	movapd	%xmm5, %xmm8
	mulsd	%xmm7, %xmm8
	mulsd	%xmm1, %xmm6
	mulsd	%xmm2, %xmm4
	subsd	%xmm6, %xmm4
	leal	(%r8,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r9,%rbp,8), %xmm6
	movapd	%xmm6, %xmm9
	mulsd	%xmm4, %xmm9
	subsd	%xmm8, %xmm9
	leal	(%r11,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r10,%rbp,8), %xmm8
	movapd	%xmm1, %xmm10
	mulsd	%xmm8, %xmm10
	leal	(%rcx,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r10,%rbp,8), %xmm11
	movapd	%xmm2, %xmm12
	mulsd	%xmm11, %xmm12
	subsd	%xmm10, %xmm12
	leal	(%r14,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r9,%rbp,8), %xmm10
	movapd	%xmm12, %xmm13
	mulsd	%xmm10, %xmm13
	mulsd	%xmm1, %xmm11
	mulsd	%xmm2, %xmm8
	addsd	%xmm11, %xmm8
	leal	(%rdi,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r9,%rbp,8), %xmm11
	movapd	%xmm8, %xmm14
	mulsd	%xmm11, %xmm14
	subsd	%xmm13, %xmm14
	addsd	%xmm9, %xmm14
	addsd	%xmm14, %xmm3
	mulsd	%xmm7, %xmm6
	mulsd	%xmm5, %xmm4
	addsd	%xmm6, %xmm4
	mulsd	%xmm10, %xmm8
	mulsd	%xmm11, %xmm12
	addsd	%xmm8, %xmm12
	addsd	%xmm4, %xmm12
	addsd	%xmm12, %xmm0
	addl	$2, %r12d
	incl	%r13d
	cmpl	68(%rsp), %r13d
	jne	.LBB128_46	# bb98
.LBB128_47:	# bb100
	leal	2(%rbx), %r14d
	movslq	%r14d, %r15
	addsd	(%rax,%r15,8), %xmm0
	movsd	%xmm0, (%rax,%r15,8)
	addl	$3, %ebx
	movslq	%ebx, %rbx
	addsd	(%rax,%rbx,8), %xmm3
	movsd	%xmm3, (%rax,%rbx,8)
	addl	52(%rsp), %esi
	addl	48(%rsp), %edi
	movl	60(%rsp), %ebx
	incl	%ebx
	movl	%ebx, 60(%rsp)
	cmpl	56(%rsp), %ebx
	je	.LBB128_50	# bb102
.LBB128_48:	# bb99.preheader
	movl	%r14d, %ebx
	cmpl	$0, 68(%rsp)
	jg	.LBB128_45	# bb.nph208
.LBB128_49:	# bb99.preheader.bb100_crit_edge
	pxor	%xmm0, %xmm0
	movapd	%xmm0, %xmm3
	jmp	.LBB128_47	# bb100
.LBB128_50:	# bb102
	movl	44(%rsp), %esi
	addl	28(%rsp), %esi
	movl	%esi, 44(%rsp)
	addl	36(%rsp), %ecx
	addl	32(%rsp), %edx
	decl	56(%rsp)
	movl	40(%rsp), %esi
	incl	%esi
	movl	%esi, 40(%rsp)
	cmpl	64(%rsp), %esi
	je	.LBB128_93	# return
.LBB128_51:	# bb95.preheader
	cmpl	$0, 68(%rsp)
	jg	.LBB128_40	# bb.nph203
.LBB128_52:	# bb95.preheader.bb96_crit_edge
	pxor	%xmm0, %xmm0
	jmp	.LBB128_43	# bb96
.LBB128_53:	# bb104
	cmpl	$121, 60(%rsp)
	jne	.LBB128_64	# bb117
.LBB128_54:	# bb104
	cmpl	$113, 56(%rsp)
	jne	.LBB128_64	# bb117
.LBB128_55:	# bb116.preheader
	cmpl	$0, 68(%rsp)
	jle	.LBB128_93	# return
.LBB128_56:	# bb.nph200
	cmpl	$0, 64(%rsp)
	jle	.LBB128_93	# return
.LBB128_57:	# bb114.preheader.preheader
	movl	136(%rsp), %ecx
	leal	(%rcx,%rcx), %ecx
	movl	%ecx, 12(%rsp)
	movl	152(%rsp), %ecx
	addl	%ecx, %ecx
	movl	%ecx, 16(%rsp)
	xorl	%ecx, %ecx
	movl	%ecx, 52(%rsp)
	movl	%ecx, 56(%rsp)
	movl	%ecx, 20(%rsp)
	jmp	.LBB128_63	# bb114.preheader
	.align	16
.LBB128_58:	# bb110
	movl	52(%rsp), %edi
	leal	(%rdi,%rcx), %edi
	movslq	%edi, %rdi
	movsd	(%r10,%rdi,8), %xmm0
	movl	24(%rsp), %edi
	leal	(%rdi,%rcx), %edi
	movslq	%edi, %rdi
	movsd	(%r10,%rdi,8), %xmm3
	movapd	%xmm2, %xmm4
	mulsd	%xmm3, %xmm4
	mulsd	%xmm1, %xmm3
	movapd	%xmm1, %xmm5
	mulsd	%xmm0, %xmm5
	subsd	%xmm4, %xmm5
	movl	28(%rsp), %edi
	leal	(%rdi,%rcx), %edi
	movslq	%edi, %rdi
	movsd	(%r9,%rdi,8), %xmm4
	movapd	%xmm5, %xmm6
	mulsd	%xmm4, %xmm6
	mulsd	%xmm2, %xmm0
	addsd	%xmm3, %xmm0
	movl	56(%rsp), %edi
	leal	(%rdi,%rcx), %edi
	movslq	%edi, %rdi
	movsd	(%r9,%rdi,8), %xmm3
	movapd	%xmm0, %xmm7
	mulsd	%xmm3, %xmm7
	subsd	%xmm6, %xmm7
	addsd	%xmm7, %xmm7
	movslq	%edx, %rdi
	addsd	(%rax,%rdi,8), %xmm7
	movsd	%xmm7, (%rax,%rdi,8)
	leal	1(%rdx), %edi
	movslq	%edi, %rdi
	movq	$0, (%rax,%rdi,8)
	movapd	%xmm1, %xmm6
	mulsd	%xmm4, %xmm6
	movapd	%xmm2, %xmm7
	mulsd	%xmm3, %xmm7
	subsd	%xmm6, %xmm7
	mulsd	%xmm1, %xmm3
	mulsd	%xmm2, %xmm4
	addsd	%xmm3, %xmm4
	xorpd	.LCPI128_1(%rip), %xmm4
	movl	60(%rsp), %edi
	leal	1(%rdi), %edi
	cmpl	64(%rsp), %edi
	jge	.LBB128_61	# bb113
.LBB128_59:	# bb.nph196
	movl	44(%rsp), %edi
	leal	(%rdi,%rcx), %edi
	movl	40(%rsp), %r8d
	leal	(%r8,%rcx), %r8d
	movl	36(%rsp), %r11d
	leal	(%r11,%rcx), %r11d
	movl	32(%rsp), %ebx
	leal	(%rbx,%rcx), %ebx
	leal	3(%rdx), %r14d
	leal	2(%rdx), %r15d
	xorl	%r12d, %r12d
	movl	%r12d, %r13d
	.align	16
.LBB128_60:	# bb111
	leal	(%r11,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r10,%rbp,8), %xmm3
	movapd	%xmm3, %xmm6
	mulsd	%xmm4, %xmm6
	leal	(%rbx,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r10,%rbp,8), %xmm8
	movapd	%xmm7, %xmm9
	mulsd	%xmm8, %xmm9
	subsd	%xmm6, %xmm9
	leal	(%rdi,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r9,%rbp,8), %xmm6
	movapd	%xmm5, %xmm10
	mulsd	%xmm6, %xmm10
	leal	(%r8,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r9,%rbp,8), %xmm11
	movapd	%xmm0, %xmm12
	mulsd	%xmm11, %xmm12
	subsd	%xmm10, %xmm12
	addsd	%xmm9, %xmm12
	leal	(%r15,%r12), %ebp
	movslq	%ebp, %rbp
	addsd	(%rax,%rbp,8), %xmm12
	movsd	%xmm12, (%rax,%rbp,8)
	mulsd	%xmm4, %xmm8
	mulsd	%xmm7, %xmm3
	addsd	%xmm8, %xmm3
	mulsd	%xmm5, %xmm11
	mulsd	%xmm0, %xmm6
	addsd	%xmm11, %xmm6
	addsd	%xmm3, %xmm6
	leal	(%r14,%r12), %ebp
	movslq	%ebp, %rbp
	addsd	(%rax,%rbp,8), %xmm6
	movsd	%xmm6, (%rax,%rbp,8)
	addl	$2, %r12d
	incl	%r13d
	cmpl	%esi, %r13d
	jne	.LBB128_60	# bb111
.LBB128_61:	# bb113
	addl	48(%rsp), %edx
	addl	$2, %ecx
	decl	%esi
	movl	60(%rsp), %edi
	incl	%edi
	movl	%edi, 60(%rsp)
	cmpl	64(%rsp), %edi
	jne	.LBB128_58	# bb110
.LBB128_62:	# bb115
	movl	52(%rsp), %ecx
	addl	12(%rsp), %ecx
	movl	%ecx, 52(%rsp)
	movl	56(%rsp), %ecx
	addl	16(%rsp), %ecx
	movl	%ecx, 56(%rsp)
	movl	20(%rsp), %ecx
	incl	%ecx
	movl	%ecx, 20(%rsp)
	cmpl	68(%rsp), %ecx
	je	.LBB128_93	# return
.LBB128_63:	# bb114.preheader
	movl	136(%rsp), %ecx
	leal	2(,%rcx,2), %ecx
	movl	%ecx, 48(%rsp)
	movl	56(%rsp), %ecx
	leal	3(%rcx), %edx
	movl	%edx, 44(%rsp)
	leal	2(%rcx), %edx
	movl	%edx, 40(%rsp)
	movl	52(%rsp), %edx
	leal	3(%rdx), %esi
	movl	%esi, 36(%rsp)
	leal	2(%rdx), %esi
	movl	%esi, 32(%rsp)
	leal	1(%rcx), %ecx
	movl	%ecx, 28(%rsp)
	leal	1(%rdx), %ecx
	movl	%ecx, 24(%rsp)
	movl	64(%rsp), %ecx
	leal	-1(%rcx), %esi
	xorl	%edx, %edx
	movl	%edx, %ecx
	movl	%edx, 60(%rsp)
	jmp	.LBB128_58	# bb110
.LBB128_64:	# bb117
	cmpl	$122, 60(%rsp)
	jne	.LBB128_81	# bb133
.LBB128_65:	# bb117
	cmpl	$111, 56(%rsp)
	jne	.LBB128_81	# bb133
.LBB128_66:	# bb132.preheader
	cmpl	$0, 64(%rsp)
	jle	.LBB128_93	# return
.LBB128_67:	# bb.nph192
	movl	136(%rsp), %edx
	leal	(%rdx,%rdx), %edx
	movl	%edx, 36(%rsp)
	movl	152(%rsp), %edx
	leal	(%rdx,%rdx), %edx
	movl	%edx, 32(%rsp)
	movl	168(%rsp), %edx
	leal	2(,%rdx,2), %edi
	movl	%edi, 28(%rsp)
	addl	%edx, %edx
	movl	%edx, 24(%rsp)
	xorl	%edx, %edx
	movl	%edx, %edi
	movl	%edx, %ebx
	movl	%edx, 44(%rsp)
	movl	%edx, 56(%rsp)
	jmp	.LBB128_80	# bb127.preheader
.LBB128_68:	# bb.nph182
	leal	1(%rdx), %r14d
	leal	1(%r8), %r15d
	pxor	%xmm0, %xmm0
	xorl	%r12d, %r12d
	movl	%r12d, %r13d
	movapd	%xmm0, %xmm3
	.align	16
.LBB128_69:	# bb124
	leal	(%r8,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r10,%rbp,8), %xmm4
	movapd	%xmm1, %xmm5
	mulsd	%xmm4, %xmm5
	leal	(%r15,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r10,%rbp,8), %xmm6
	movapd	%xmm2, %xmm7
	mulsd	%xmm6, %xmm7
	addsd	%xmm5, %xmm7
	leal	(%rbx,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r9,%rbp,8), %xmm5
	movapd	%xmm5, %xmm8
	mulsd	%xmm7, %xmm8
	mulsd	%xmm1, %xmm6
	mulsd	%xmm2, %xmm4
	subsd	%xmm6, %xmm4
	leal	(%rcx,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r9,%rbp,8), %xmm6
	movapd	%xmm6, %xmm9
	mulsd	%xmm4, %xmm9
	subsd	%xmm8, %xmm9
	leal	(%r11,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r10,%rbp,8), %xmm8
	movapd	%xmm1, %xmm10
	mulsd	%xmm8, %xmm10
	leal	(%rdi,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r10,%rbp,8), %xmm11
	movapd	%xmm2, %xmm12
	mulsd	%xmm11, %xmm12
	subsd	%xmm10, %xmm12
	leal	(%r14,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r9,%rbp,8), %xmm10
	movapd	%xmm12, %xmm13
	mulsd	%xmm10, %xmm13
	mulsd	%xmm1, %xmm11
	mulsd	%xmm2, %xmm8
	addsd	%xmm11, %xmm8
	leal	(%rdx,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r9,%rbp,8), %xmm11
	movapd	%xmm8, %xmm14
	mulsd	%xmm11, %xmm14
	subsd	%xmm13, %xmm14
	addsd	%xmm9, %xmm14
	addsd	%xmm14, %xmm3
	mulsd	%xmm7, %xmm6
	mulsd	%xmm5, %xmm4
	addsd	%xmm6, %xmm4
	mulsd	%xmm10, %xmm8
	mulsd	%xmm11, %xmm12
	addsd	%xmm8, %xmm12
	addsd	%xmm4, %xmm12
	addsd	%xmm12, %xmm0
	addl	$2, %r12d
	incl	%r13d
	cmpl	68(%rsp), %r13d
	jne	.LBB128_69	# bb124
.LBB128_70:	# bb126
	movslq	%esi, %r14
	addsd	(%rax,%r14,8), %xmm0
	movsd	%xmm0, (%rax,%r14,8)
	leal	1(%rsi), %r14d
	movslq	%r14d, %r14
	addsd	(%rax,%r14,8), %xmm3
	movsd	%xmm3, (%rax,%r14,8)
	addl	52(%rsp), %r8d
	addl	48(%rsp), %edx
	addl	$2, %esi
	movl	60(%rsp), %r14d
	incl	%r14d
	movl	%r14d, 60(%rsp)
	cmpl	56(%rsp), %r14d
	jne	.LBB128_74	# bb125.preheader
.LBB128_71:	# bb130.preheader
	cmpl	$0, 68(%rsp)
	jg	.LBB128_76	# bb.nph189
.LBB128_72:	# bb130.preheader.bb131_crit_edge
	pxor	%xmm0, %xmm0
	jmp	.LBB128_79	# bb131
.LBB128_73:	# bb.nph186
	leal	1(%rdi), %r11d
	leal	1(%rbx), %ecx
	movl	136(%rsp), %edx
	leal	(%rdx,%rdx), %edx
	movl	%edx, 52(%rsp)
	movl	152(%rsp), %edx
	leal	(%rdx,%rdx), %edx
	movl	%edx, 48(%rsp)
	xorl	%r8d, %r8d
	movl	%r8d, %edx
	movl	44(%rsp), %esi
	movl	%r8d, 60(%rsp)
	.align	16
.LBB128_74:	# bb125.preheader
	cmpl	$0, 68(%rsp)
	jg	.LBB128_68	# bb.nph182
.LBB128_75:	# bb125.preheader.bb126_crit_edge
	pxor	%xmm0, %xmm0
	movapd	%xmm0, %xmm3
	jmp	.LBB128_70	# bb126
.LBB128_76:	# bb.nph189
	leal	1(%rbx), %ecx
	leal	1(%rdi), %edx
	pxor	%xmm0, %xmm0
	xorl	%esi, %esi
	movl	%esi, %r8d
	.align	16
.LBB128_77:	# bb129
	leal	(%rdi,%rsi), %r11d
	movslq	%r11d, %r11
	movsd	(%r10,%r11,8), %xmm3
	movapd	%xmm1, %xmm4
	mulsd	%xmm3, %xmm4
	leal	(%rdx,%rsi), %r11d
	movslq	%r11d, %r11
	movsd	(%r10,%r11,8), %xmm5
	movapd	%xmm2, %xmm6
	mulsd	%xmm5, %xmm6
	addsd	%xmm4, %xmm6
	leal	(%rcx,%rsi), %r11d
	movslq	%r11d, %r11
	mulsd	(%r9,%r11,8), %xmm6
	mulsd	%xmm1, %xmm5
	mulsd	%xmm2, %xmm3
	subsd	%xmm5, %xmm3
	leal	(%rbx,%rsi), %r11d
	movslq	%r11d, %r11
	mulsd	(%r9,%r11,8), %xmm3
	addsd	%xmm6, %xmm3
	addsd	%xmm3, %xmm0
	addl	$2, %esi
	incl	%r8d
	cmpl	68(%rsp), %r8d
	jne	.LBB128_77	# bb129
.LBB128_78:	# bb130.bb131_crit_edge
	addsd	%xmm0, %xmm0
.LBB128_79:	# bb131
	movl	40(%rsp), %ecx
	movslq	%ecx, %rdx
	addsd	(%rax,%rdx,8), %xmm0
	movsd	%xmm0, (%rax,%rdx,8)
	movl	28(%rsp), %edx
	leal	(%rdx,%rcx), %edx
	incl	%ecx
	movslq	%ecx, %rcx
	movq	$0, (%rax,%rcx,8)
	addl	36(%rsp), %edi
	addl	32(%rsp), %ebx
	movl	44(%rsp), %ecx
	addl	24(%rsp), %ecx
	movl	%ecx, 44(%rsp)
	movl	56(%rsp), %ecx
	incl	%ecx
	movl	%ecx, 56(%rsp)
	cmpl	64(%rsp), %ecx
	je	.LBB128_93	# return
.LBB128_80:	# bb127.preheader
	movl	%edx, 40(%rsp)
	cmpl	$0, 56(%rsp)
	jg	.LBB128_73	# bb.nph186
	jmp	.LBB128_71	# bb130.preheader
.LBB128_81:	# bb133
	cmpl	$122, 60(%rsp)
	jne	.LBB128_92	# bb146
.LBB128_82:	# bb133
	cmpl	$113, 56(%rsp)
	jne	.LBB128_92	# bb146
.LBB128_83:	# bb145.preheader
	cmpl	$0, 68(%rsp)
	jle	.LBB128_93	# return
.LBB128_84:	# bb.nph178
	cmpl	$0, 64(%rsp)
	jle	.LBB128_93	# return
.LBB128_85:	# bb143.preheader.preheader
	movl	136(%rsp), %ecx
	leal	(%rcx,%rcx), %ecx
	movl	%ecx, 48(%rsp)
	movl	152(%rsp), %ecx
	addl	%ecx, %ecx
	movl	%ecx, 16(%rsp)
	xorl	%ebx, %ebx
	movl	%ebx, %r8d
	movl	%ebx, 52(%rsp)
	jmp	.LBB128_91	# bb143.preheader
	.align	16
.LBB128_86:	# bb139
	leal	(%r14,%r11), %r15d
	movslq	%r15d, %r15
	movsd	(%r9,%r15,8), %xmm0
	leal	(%rdx,%r11), %r15d
	movslq	%r15d, %r15
	movsd	(%r10,%r15,8), %xmm3
	movapd	%xmm2, %xmm4
	mulsd	%xmm3, %xmm4
	mulsd	%xmm1, %xmm3
	movapd	%xmm1, %xmm5
	mulsd	%xmm0, %xmm5
	leal	(%r8,%r11), %r15d
	movslq	%r15d, %r15
	movsd	(%r9,%r15,8), %xmm6
	movapd	%xmm2, %xmm7
	mulsd	%xmm6, %xmm7
	subsd	%xmm5, %xmm7
	leal	(%rbx,%r11), %r15d
	movslq	%r15d, %r15
	movsd	(%r10,%r15,8), %xmm5
	movapd	%xmm1, %xmm8
	mulsd	%xmm5, %xmm8
	subsd	%xmm4, %xmm8
	mulsd	%xmm2, %xmm5
	addsd	%xmm3, %xmm5
	movapd	%xmm1, %xmm3
	mulsd	%xmm6, %xmm3
	movapd	%xmm2, %xmm4
	mulsd	%xmm0, %xmm4
	addsd	%xmm3, %xmm4
	xorpd	.LCPI128_1(%rip), %xmm4
	testl	%esi, %esi
	jle	.LBB128_89	# bb142
.LBB128_87:	# bb.nph174
	leal	1(%rdi), %r15d
	xorl	%r12d, %r12d
	movl	%r12d, %r13d
	.align	16
.LBB128_88:	# bb140
	leal	(%rdx,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r10,%rbp,8), %xmm3
	movapd	%xmm3, %xmm9
	mulsd	%xmm4, %xmm9
	leal	(%rbx,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r10,%rbp,8), %xmm10
	movapd	%xmm7, %xmm11
	mulsd	%xmm10, %xmm11
	subsd	%xmm9, %xmm11
	leal	(%r14,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r9,%rbp,8), %xmm9
	movapd	%xmm8, %xmm12
	mulsd	%xmm9, %xmm12
	leal	(%r8,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r9,%rbp,8), %xmm13
	movapd	%xmm5, %xmm14
	mulsd	%xmm13, %xmm14
	subsd	%xmm12, %xmm14
	addsd	%xmm11, %xmm14
	leal	(%rdi,%r12), %ebp
	movslq	%ebp, %rbp
	addsd	(%rax,%rbp,8), %xmm14
	movsd	%xmm14, (%rax,%rbp,8)
	mulsd	%xmm4, %xmm10
	mulsd	%xmm7, %xmm3
	addsd	%xmm10, %xmm3
	mulsd	%xmm8, %xmm13
	mulsd	%xmm5, %xmm9
	addsd	%xmm13, %xmm9
	addsd	%xmm3, %xmm9
	leal	(%r15,%r12), %ebp
	movslq	%ebp, %rbp
	addsd	(%rax,%rbp,8), %xmm9
	movsd	%xmm9, (%rax,%rbp,8)
	addl	$2, %r12d
	incl	%r13d
	cmpl	%esi, %r13d
	jne	.LBB128_88	# bb140
.LBB128_89:	# bb142
	mulsd	%xmm0, %xmm8
	mulsd	%xmm6, %xmm5
	subsd	%xmm8, %xmm5
	addsd	%xmm5, %xmm5
	movslq	%ecx, %r15
	addsd	(%rax,%r15,8), %xmm5
	movsd	%xmm5, (%rax,%r15,8)
	movl	60(%rsp), %r15d
	leal	(%r15,%rcx), %r15d
	incl	%ecx
	movslq	%ecx, %rcx
	movq	$0, (%rax,%rcx,8)
	addl	56(%rsp), %edi
	addl	$2, %r11d
	incl	%esi
	cmpl	64(%rsp), %esi
	movl	%r15d, %ecx
	jne	.LBB128_86	# bb139
.LBB128_90:	# bb144
	addl	48(%rsp), %ebx
	addl	16(%rsp), %r8d
	movl	52(%rsp), %ecx
	incl	%ecx
	movl	%ecx, 52(%rsp)
	cmpl	68(%rsp), %ecx
	je	.LBB128_93	# return
.LBB128_91:	# bb143.preheader
	leal	1(%rbx), %edx
	leal	1(%r8), %r14d
	movl	136(%rsp), %ecx
	leal	2(,%rcx,2), %esi
	movl	%esi, 60(%rsp)
	leal	(%rcx,%rcx), %ecx
	movl	%ecx, 56(%rsp)
	xorl	%ecx, %ecx
	movl	%ecx, %edi
	movl	%ecx, %r11d
	movl	%ecx, %esi
	jmp	.LBB128_86	# bb139
.LBB128_92:	# bb146
	xorl	%edi, %edi
	leaq	.str165, %rsi
	leaq	.str1166, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB128_93:	# return
	addq	$72, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB128_94:	# bb83.preheader
	cmpl	$0, 64(%rsp)
	jle	.LBB128_11	# bb87
.LBB128_95:	# bb.nph162
	movl	168(%rsp), %edi
	leal	2(,%rdi,2), %r11d
	leal	(%rdi,%rdi), %ecx
	xorl	%edi, %edi
	movl	%edi, %esi
	movl	%edi, %edx
	jmp	.LBB128_38	# bb81.preheader
	.size	cblas_zher2k, .-cblas_zher2k
.Leh_func_end88:


	.align	16
	.globl	cblas_zher
	.type	cblas_zher,@function
cblas_zher:
.Leh_func_begin89:
.Llabel89:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$24, %rsp
	cmpl	$102, %edi
	movl	$4294967295, %eax
	movl	$1, %r10d
	cmove	%eax, %r10d
	pxor	%xmm1, %xmm1
	ucomisd	%xmm1, %xmm0
	setnp	%al
	sete	%r11b
	testb	%al, %r11b
	jne	.LBB129_24	# return
.LBB129_1:	# bb13
	cmpl	$121, %esi
	jne	.LBB129_3	# bb16
.LBB129_2:	# bb13
	cmpl	$101, %edi
	je	.LBB129_5	# bb20
.LBB129_3:	# bb16
	cmpl	$122, %esi
	jne	.LBB129_13	# bb29
.LBB129_4:	# bb16
	cmpl	$102, %edi
	jne	.LBB129_13	# bb29
.LBB129_5:	# bb20
	testl	%r8d, %r8d
	jg	.LBB129_25	# bb20.bb28.preheader_crit_edge
.LBB129_6:	# bb21
	movl	$1, %eax
	subl	%edx, %eax
	imull	%r8d, %eax
.LBB129_7:	# bb28.preheader
	testl	%edx, %edx
	jle	.LBB129_24	# return
.LBB129_8:	# bb.nph65
	leal	(%r8,%rax), %esi
	addl	%esi, %esi
	movl	%esi, 16(%rsp)
	cvtsi2sd	%r10d, %xmm1
	mulsd	%xmm0, %xmm1
	negl	%r10d
	cvtsi2sd	%r10d, %xmm2
	movl	80(%rsp), %r10d
	leal	2(,%r10,2), %r10d
	movl	%r10d, 20(%rsp)
	leal	1(,%rax,2), %r10d
	movl	%r10d, 12(%rsp)
	addl	%eax, %eax
	leal	(%r8,%r8), %r10d
	movl	%r10d, 8(%rsp)
	leal	-1(%rdx), %r10d
	xorl	%esi, %esi
	movl	%esi, %edi
	movl	%esi, %r11d
	.align	16
.LBB129_9:	# bb24
	movl	12(%rsp), %ebx
	leal	(%rbx,%rdi), %ebx
	movslq	%ebx, %rbx
	movsd	(%rcx,%rbx,8), %xmm3
	movapd	%xmm1, %xmm4
	mulsd	%xmm3, %xmm4
	mulsd	%xmm2, %xmm3
	mulsd	%xmm4, %xmm3
	leal	(%rax,%rdi), %ebx
	movslq	%ebx, %rbx
	movsd	(%rcx,%rbx,8), %xmm5
	movapd	%xmm5, %xmm6
	mulsd	%xmm0, %xmm6
	mulsd	%xmm6, %xmm5
	subsd	%xmm3, %xmm5
	movslq	%esi, %rbx
	addsd	(%r9,%rbx,8), %xmm5
	movsd	%xmm5, (%r9,%rbx,8)
	leal	1(%rsi), %ebx
	movslq	%ebx, %rbx
	movq	$0, (%r9,%rbx,8)
	leal	1(%r11), %ebx
	cmpl	%edx, %ebx
	jge	.LBB129_12	# bb27
.LBB129_10:	# bb.nph62
	movl	16(%rsp), %ebx
	leal	(%rbx,%rdi), %ebx
	leal	(%r8,%r8), %r14d
	xorl	%r15d, %r15d
	movl	%esi, %r12d
	.align	16
.LBB129_11:	# bb25
	movslq	%ebx, %r13
	leal	1(%rbx), %ebp
	movslq	%ebp, %rbp
	movapd	%xmm2, %xmm3
	mulsd	(%rcx,%rbp,8), %xmm3
	movapd	%xmm3, %xmm5
	mulsd	%xmm4, %xmm5
	movsd	(%rcx,%r13,8), %xmm7
	movapd	%xmm7, %xmm8
	mulsd	%xmm6, %xmm8
	subsd	%xmm5, %xmm8
	leal	2(%r12), %r13d
	movslq	%r13d, %rbp
	addsd	(%r9,%rbp,8), %xmm8
	movsd	%xmm8, (%r9,%rbp,8)
	mulsd	%xmm6, %xmm3
	mulsd	%xmm4, %xmm7
	addsd	%xmm3, %xmm7
	addl	$3, %r12d
	movslq	%r12d, %r12
	addsd	(%r9,%r12,8), %xmm7
	movsd	%xmm7, (%r9,%r12,8)
	addl	%r14d, %ebx
	incl	%r15d
	cmpl	%r10d, %r15d
	movl	%r13d, %r12d
	jne	.LBB129_11	# bb25
.LBB129_12:	# bb27
	addl	20(%rsp), %esi
	addl	8(%rsp), %edi
	decl	%r10d
	incl	%r11d
	cmpl	%edx, %r11d
	jne	.LBB129_9	# bb24
	jmp	.LBB129_24	# return
.LBB129_13:	# bb29
	cmpl	$102, %edi
	sete	%al
	cmpl	$121, %esi
	sete	%r11b
	andb	%al, %r11b
	cmpl	$101, %edi
	sete	%al
	cmpl	$122, %esi
	sete	%sil
	testb	%al, %sil
	jne	.LBB129_15	# bb37
.LBB129_14:	# bb29
	notb	%r11b
	testb	$1, %r11b
	jne	.LBB129_23	# bb49
.LBB129_15:	# bb37
	testl	%r8d, %r8d
	jg	.LBB129_26	# bb37.bb48.preheader_crit_edge
.LBB129_16:	# bb38
	movl	$1, %eax
	subl	%edx, %eax
	imull	%r8d, %eax
.LBB129_17:	# bb48.preheader
	testl	%edx, %edx
	jle	.LBB129_24	# return
.LBB129_18:	# bb.nph
	cvtsi2sd	%r10d, %xmm1
	mulsd	%xmm0, %xmm1
	movl	$1, %esi
	subl	%edx, %esi
	imull	%r8d, %esi
	movl	%esi, 8(%rsp)
	negl	%r10d
	cvtsi2sd	%r10d, %xmm2
	addl	%eax, %eax
	movl	80(%rsp), %esi
	leal	2(,%rsi,2), %edi
	movl	%edi, 20(%rsp)
	addl	%esi, %esi
	movl	%esi, 12(%rsp)
	leal	(%r8,%r8), %esi
	movl	%esi, 16(%rsp)
	xorl	%esi, %esi
	movl	%esi, %edi
	movl	%esi, %r10d
	.align	16
.LBB129_19:	# bb41
	xorl	%r11d, %r11d
	testl	%r10d, %r10d
	movl	%r10d, %ebx
	cmovs	%r11d, %ebx
	movslq	%eax, %r14
	movapd	%xmm0, %xmm3
	mulsd	(%rcx,%r14,8), %xmm3
	testl	%r8d, %r8d
	movl	8(%rsp), %r14d
	cmovg	%r11d, %r14d
	addl	%r14d, %r14d
	leal	1(%rax), %r15d
	movslq	%r15d, %r15
	movapd	%xmm1, %xmm4
	mulsd	(%rcx,%r15,8), %xmm4
	leal	(%r8,%r8), %r15d
	movl	%edi, %r12d
	jmp	.LBB129_21	# bb46
.LBB129_20:	# bb45
	movapd	%xmm5, %xmm7
	mulsd	%xmm4, %xmm7
	movapd	%xmm6, %xmm8
	mulsd	%xmm3, %xmm8
	subsd	%xmm7, %xmm8
	movslq	%r12d, %r13
	addsd	(%r9,%r13,8), %xmm8
	movsd	%xmm8, (%r9,%r13,8)
	mulsd	%xmm4, %xmm6
	mulsd	%xmm3, %xmm5
	addsd	%xmm6, %xmm5
	leal	1(%r12), %r13d
	movslq	%r13d, %r13
	addsd	(%r9,%r13,8), %xmm5
	movsd	%xmm5, (%r9,%r13,8)
	addl	%r15d, %r14d
	addl	$2, %r12d
	incl	%r11d
.LBB129_21:	# bb46
	movslq	%r14d, %r13
	leal	1(%r14), %ebp
	movslq	%ebp, %rbp
	movapd	%xmm2, %xmm5
	mulsd	(%rcx,%rbp,8), %xmm5
	cmpl	%ebx, %r11d
	movsd	(%rcx,%r13,8), %xmm6
	jne	.LBB129_20	# bb45
.LBB129_22:	# bb47
	mulsd	%xmm4, %xmm5
	mulsd	%xmm3, %xmm6
	subsd	%xmm5, %xmm6
	movslq	%esi, %r11
	addsd	(%r9,%r11,8), %xmm6
	movsd	%xmm6, (%r9,%r11,8)
	movl	20(%rsp), %r11d
	leal	(%r11,%rsi), %r11d
	incl	%esi
	movslq	%esi, %rsi
	movq	$0, (%r9,%rsi,8)
	addl	16(%rsp), %eax
	addl	12(%rsp), %edi
	incl	%r10d
	cmpl	%edx, %r10d
	movl	%r11d, %esi
	jne	.LBB129_19	# bb41
	jmp	.LBB129_24	# return
.LBB129_23:	# bb49
	xorl	%edi, %edi
	leaq	.str167, %rsi
	leaq	.str1168, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB129_24:	# return
	addq	$24, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB129_25:	# bb20.bb28.preheader_crit_edge
	xorl	%eax, %eax
	jmp	.LBB129_7	# bb28.preheader
.LBB129_26:	# bb37.bb48.preheader_crit_edge
	xorl	%eax, %eax
	jmp	.LBB129_17	# bb48.preheader
	.size	cblas_zher, .-cblas_zher
.Leh_func_end89:


	.section	.rodata.cst8,"aM",@progbits,8
	.align	8
.LCPI130_0:					
	.quad	4607182418800017408	# double value: 1.000000e+00
	.text
	.align	16
	.globl	cblas_zherk
	.type	cblas_zherk,@function
cblas_zherk:
.Leh_func_begin90:
.Llabel90:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$24, %rsp
	ucomisd	.LCPI130_0(%rip), %xmm1
	movq	88(%rsp), %rax
	jne	.LBB130_3	# bb17
	jp	.LBB130_3	# bb17
.LBB130_1:	# bb
	pxor	%xmm2, %xmm2
	ucomisd	%xmm2, %xmm0
	setnp	%r10b
	sete	%r11b
	testb	%r10b, %r11b
	jne	.LBB130_84	# return
.LBB130_2:	# bb
	testl	%r8d, %r8d
	je	.LBB130_84	# return
.LBB130_3:	# bb17
	cmpl	$101, %edi
	je	.LBB130_5	# bb26
.LBB130_4:	# bb19
	cmpl	$111, %edx
	movl	$113, %edi
	movl	$111, %edx
	cmove	%edi, %edx
	cmpl	$121, %esi
	movl	$122, %edi
	movl	$121, %esi
	cmove	%edi, %esi
.LBB130_5:	# bb26
	movl	%edx, 20(%rsp)
	pxor	%xmm2, %xmm2
	ucomisd	%xmm2, %xmm1
	jne	.LBB130_24	# bb40
	jp	.LBB130_24	# bb40
.LBB130_6:	# bb27
	cmpl	$121, %esi
	je	.LBB130_16	# bb33.preheader
.LBB130_7:	# bb39.preheader
	testl	%ecx, %ecx
	jle	.LBB130_11	# bb57
.LBB130_8:	# bb.nph134
	movl	96(%rsp), %edx
	leal	(%rdx,%rdx), %edx
	xorl	%r10d, %r10d
	movl	%r10d, %r11d
	jmp	.LBB130_22	# bb37.preheader
	.align	16
.LBB130_9:	# bb30
	movslq	%edx, %r15
	movq	$0, (%rax,%r15,8)
	leal	1(%rdx), %r15d
	movslq	%r15d, %r15
	movq	$0, (%rax,%r15,8)
	addl	$2, %edx
	incl	%edi
	cmpl	%r14d, %edi
	jne	.LBB130_9	# bb30
.LBB130_10:	# bb32
	addl	%r10d, %ebx
	decl	%r14d
	incl	%r11d
	cmpl	%ecx, %r11d
	jne	.LBB130_18	# bb31.preheader
.LBB130_11:	# bb57
	pxor	%xmm1, %xmm1
	ucomisd	%xmm1, %xmm0
	setnp	%dl
	sete	%dil
	testb	%dl, %dil
	jne	.LBB130_84	# return
.LBB130_12:	# bb58
	cmpl	$121, %esi
	jne	.LBB130_47	# bb71
.LBB130_13:	# bb58
	cmpl	$111, 20(%rsp)
	jne	.LBB130_47	# bb71
.LBB130_14:	# bb70.preheader
	testl	%ecx, %ecx
	jle	.LBB130_84	# return
.LBB130_15:	# bb.nph174
	movl	80(%rsp), %esi
	leal	(%rsi,%rsi), %esi
	movl	%esi, 12(%rsp)
	movl	96(%rsp), %esi
	leal	2(,%rsi,2), %esi
	movl	%esi, 16(%rsp)
	xorl	%r14d, %r14d
	movl	%r14d, 20(%rsp)
	movl	%ecx, %esi
	movl	%r14d, %edx
	jmp	.LBB130_43	# bb68.preheader
.LBB130_16:	# bb33.preheader
	testl	%ecx, %ecx
	jle	.LBB130_11	# bb57
.LBB130_17:	# bb.nph178
	movl	96(%rsp), %edx
	leal	2(,%rdx,2), %r10d
	xorl	%ebx, %ebx
	movl	%ecx, %r14d
	movl	%ebx, %r11d
	.align	16
.LBB130_18:	# bb31.preheader
	cmpl	%ecx, %r11d
	jge	.LBB130_10	# bb32
.LBB130_19:	# bb31.preheader.bb30_crit_edge
	xorl	%edi, %edi
	movl	%ebx, %edx
	jmp	.LBB130_9	# bb30
	.align	16
.LBB130_20:	# bb36
	movslq	%edi, %r14
	movq	$0, (%rax,%r14,8)
	leal	1(%rdi), %r14d
	movslq	%r14d, %r14
	movq	$0, (%rax,%r14,8)
	addl	$2, %edi
	incl	%ebx
	cmpl	%r11d, %ebx
	jle	.LBB130_20	# bb36
.LBB130_21:	# bb38
	addl	%edx, %r10d
	incl	%r11d
	cmpl	%ecx, %r11d
	je	.LBB130_11	# bb57
.LBB130_22:	# bb37.preheader
	testl	%r11d, %r11d
	js	.LBB130_21	# bb38
.LBB130_23:	# bb37.preheader.bb36_crit_edge
	xorl	%ebx, %ebx
	movl	%r10d, %edi
	jmp	.LBB130_20	# bb36
.LBB130_24:	# bb40
	ucomisd	.LCPI130_0(%rip), %xmm1
	jne	.LBB130_28	# bb41
	jp	.LBB130_28	# bb41
.LBB130_25:	# bb56.preheader
	testl	%ecx, %ecx
	jle	.LBB130_11	# bb57
.LBB130_26:	# bb.nph
	movl	96(%rsp), %edx
	leal	2(,%rdx,2), %edx
	xorl	%edi, %edi
	movl	$1, %r10d
	.align	16
.LBB130_27:	# bb55
	movslq	%r10d, %r11
	movq	$0, (%rax,%r11,8)
	addl	%edx, %r10d
	incl	%edi
	cmpl	%ecx, %edi
	jne	.LBB130_27	# bb55
	jmp	.LBB130_11	# bb57
.LBB130_28:	# bb41
	cmpl	$121, %esi
	jne	.LBB130_85	# bb53.preheader
.LBB130_29:	# bb47.preheader
	testl	%ecx, %ecx
	jle	.LBB130_11	# bb57
.LBB130_30:	# bb.nph130
	movl	96(%rsp), %edx
	leal	2(,%rdx,2), %edx
	leal	-1(%rcx), %edi
	xorl	%r10d, %r10d
	movl	%r10d, %r11d
	.align	16
.LBB130_31:	# bb43
	movslq	%r10d, %rbx
	movapd	%xmm1, %xmm2
	mulsd	(%rax,%rbx,8), %xmm2
	movsd	%xmm2, (%rax,%rbx,8)
	leal	1(%r10), %ebx
	movslq	%ebx, %rbx
	movq	$0, (%rax,%rbx,8)
	leal	1(%r11), %ebx
	cmpl	%ecx, %ebx
	jge	.LBB130_34	# bb46
.LBB130_32:	# bb43.bb44_crit_edge
	xorl	%ebx, %ebx
	movl	%r10d, %r14d
	.align	16
.LBB130_33:	# bb44
	leal	2(%r14), %r15d
	movslq	%r15d, %r12
	movapd	%xmm1, %xmm2
	mulsd	(%rax,%r12,8), %xmm2
	movsd	%xmm2, (%rax,%r12,8)
	addl	$3, %r14d
	movslq	%r14d, %r14
	movapd	%xmm1, %xmm2
	mulsd	(%rax,%r14,8), %xmm2
	movsd	%xmm2, (%rax,%r14,8)
	incl	%ebx
	cmpl	%edi, %ebx
	movl	%r15d, %r14d
	jne	.LBB130_33	# bb44
.LBB130_34:	# bb46
	addl	%edx, %r10d
	decl	%edi
	incl	%r11d
	cmpl	%ecx, %r11d
	je	.LBB130_11	# bb57
	jmp	.LBB130_31	# bb43
	.align	16
.LBB130_35:	# bb50
	movslq	%r15d, %r12
	movapd	%xmm1, %xmm2
	mulsd	(%rax,%r12,8), %xmm2
	movsd	%xmm2, (%rax,%r12,8)
	leal	1(%r15), %r12d
	movslq	%r12d, %r12
	movapd	%xmm1, %xmm2
	mulsd	(%rax,%r12,8), %xmm2
	movsd	%xmm2, (%rax,%r12,8)
	addl	$2, %r15d
	incl	%ebx
	cmpl	%r10d, %ebx
	jne	.LBB130_35	# bb50
.LBB130_36:	# bb52
	movslq	%r14d, %rbx
	movapd	%xmm1, %xmm2
	mulsd	(%rax,%rbx,8), %xmm2
	movsd	%xmm2, (%rax,%rbx,8)
	leal	(%rdi,%r14), %ebx
	incl	%r14d
	movslq	%r14d, %r14
	movq	$0, (%rax,%r14,8)
	addl	%edx, %r11d
	incl	%r10d
	cmpl	%ecx, %r10d
	je	.LBB130_11	# bb57
.LBB130_37:	# bb51.preheader
	movl	%ebx, %r14d
	testl	%r10d, %r10d
	jle	.LBB130_36	# bb52
.LBB130_38:	# bb51.preheader.bb50_crit_edge
	xorl	%ebx, %ebx
	movl	%r11d, %r15d
	jmp	.LBB130_35	# bb50
.LBB130_39:	# bb.nph168
	leal	1(%rbx), %r15d
	pxor	%xmm1, %xmm1
	xorl	%r12d, %r12d
	movl	%r14d, %r13d
	movapd	%xmm1, %xmm2
	.align	16
.LBB130_40:	# bb65
	movslq	%r13d, %rbp
	movsd	(%r9,%rbp,8), %xmm3
	leal	(%r15,%r13), %ebp
	movslq	%ebp, %rbp
	movsd	(%r9,%rbp,8), %xmm4
	movapd	%xmm3, %xmm5
	mulsd	%xmm4, %xmm5
	leal	(%rbx,%r13), %ebp
	movslq	%ebp, %rbp
	movsd	(%r9,%rbp,8), %xmm6
	leal	1(%r13), %ebp
	movslq	%ebp, %rbp
	movsd	(%r9,%rbp,8), %xmm7
	movapd	%xmm7, %xmm8
	mulsd	%xmm6, %xmm8
	subsd	%xmm5, %xmm8
	addsd	%xmm8, %xmm2
	mulsd	%xmm4, %xmm7
	mulsd	%xmm6, %xmm3
	addsd	%xmm7, %xmm3
	addsd	%xmm3, %xmm1
	addl	$2, %r13d
	incl	%r12d
	cmpl	%r8d, %r12d
	jne	.LBB130_40	# bb65
.LBB130_41:	# bb67
	mulsd	%xmm0, %xmm1
	movslq	%r11d, %r15
	addsd	(%rax,%r15,8), %xmm1
	movsd	%xmm1, (%rax,%r15,8)
	mulsd	%xmm0, %xmm2
	leal	1(%r11), %r15d
	movslq	%r15d, %r15
	addsd	(%rax,%r15,8), %xmm2
	movsd	%xmm2, (%rax,%r15,8)
	addl	%edi, %ebx
	addl	$2, %r11d
	incl	%r10d
	cmpl	%esi, %r10d
	jne	.LBB130_45	# bb66.preheader
.LBB130_42:	# bb69
	addl	12(%rsp), %r14d
	movl	20(%rsp), %edi
	addl	16(%rsp), %edi
	movl	%edi, 20(%rsp)
	decl	%esi
	incl	%edx
	cmpl	%ecx, %edx
	je	.LBB130_84	# return
.LBB130_43:	# bb68.preheader
	cmpl	%ecx, %edx
	jge	.LBB130_42	# bb69
.LBB130_44:	# bb.nph172
	movl	80(%rsp), %edi
	leal	(%rdi,%rdi), %edi
	xorl	%ebx, %ebx
	movl	20(%rsp), %r11d
	movl	%ebx, %r10d
	.align	16
.LBB130_45:	# bb66.preheader
	testl	%r8d, %r8d
	jg	.LBB130_39	# bb.nph168
.LBB130_46:	# bb66.preheader.bb67_crit_edge
	pxor	%xmm1, %xmm1
	movapd	%xmm1, %xmm2
	jmp	.LBB130_41	# bb67
.LBB130_47:	# bb71
	cmpl	$121, %esi
	jne	.LBB130_59	# bb84
.LBB130_48:	# bb71
	cmpl	$113, 20(%rsp)
	jne	.LBB130_59	# bb84
.LBB130_49:	# bb83.preheader
	testl	%ecx, %ecx
	jle	.LBB130_84	# return
.LBB130_50:	# bb.nph164
	movl	96(%rsp), %esi
	leal	2(,%rsi,2), %esi
	movl	%esi, 12(%rsp)
	xorl	%r10d, %r10d
	movl	%ecx, 20(%rsp)
	movl	%r10d, 16(%rsp)
	jmp	.LBB130_55	# bb81.preheader
.LBB130_51:	# bb78.preheader
	leal	1(%rdx), %ebx
	movl	80(%rsp), %r11d
	leal	(%r11,%r11), %r11d
	pxor	%xmm2, %xmm2
	xorl	%r12d, %r12d
	movl	%esi, %r14d
	movapd	%xmm2, %xmm1
	.align	16
.LBB130_52:	# bb78
	movslq	%r14d, %r13
	movsd	(%r9,%r13,8), %xmm3
	leal	(%rdx,%r14), %r13d
	movslq	%r13d, %r13
	movsd	(%r9,%r13,8), %xmm4
	movapd	%xmm3, %xmm5
	mulsd	%xmm4, %xmm5
	leal	(%rbx,%r14), %r13d
	leal	1(%r14), %ebp
	movslq	%ebp, %rbp
	movsd	(%r9,%rbp,8), %xmm6
	mulsd	%xmm6, %xmm4
	movslq	%r13d, %r13
	movsd	(%r9,%r13,8), %xmm7
	mulsd	%xmm7, %xmm3
	subsd	%xmm4, %xmm3
	addsd	%xmm3, %xmm1
	mulsd	%xmm6, %xmm7
	addsd	%xmm5, %xmm7
	addsd	%xmm7, %xmm2
	addl	%r11d, %r14d
	incl	%r12d
	cmpl	%r8d, %r12d
	jne	.LBB130_52	# bb78
.LBB130_53:	# bb80
	leal	(%r10,%rdx), %r11d
	movslq	%r11d, %r11
	mulsd	%xmm0, %xmm2
	addsd	(%rax,%r11,8), %xmm2
	movsd	%xmm2, (%rax,%r11,8)
	leal	(%r15,%rdx), %r11d
	movslq	%r11d, %r11
	mulsd	%xmm0, %xmm1
	addsd	(%rax,%r11,8), %xmm1
	movsd	%xmm1, (%rax,%r11,8)
	addl	$2, %edx
	incl	%edi
	cmpl	20(%rsp), %edi
	jne	.LBB130_57	# bb79.preheader
.LBB130_54:	# bb82
	addl	12(%rsp), %r10d
	decl	20(%rsp)
	movl	16(%rsp), %esi
	incl	%esi
	movl	%esi, 16(%rsp)
	cmpl	%ecx, %esi
	je	.LBB130_84	# return
.LBB130_55:	# bb81.preheader
	cmpl	%ecx, 16(%rsp)
	jge	.LBB130_54	# bb82
.LBB130_56:	# bb.nph162
	movl	16(%rsp), %esi
	leal	(%rsi,%rsi), %esi
	leal	1(%r10), %r15d
	xorl	%edx, %edx
	movl	%edx, %edi
	.align	16
.LBB130_57:	# bb79.preheader
	testl	%r8d, %r8d
	jg	.LBB130_51	# bb78.preheader
.LBB130_58:	# bb79.preheader.bb80_crit_edge
	pxor	%xmm2, %xmm2
	movapd	%xmm2, %xmm1
	jmp	.LBB130_53	# bb80
.LBB130_59:	# bb84
	cmpl	$122, %esi
	jne	.LBB130_71	# bb97
.LBB130_60:	# bb84
	cmpl	$111, 20(%rsp)
	jne	.LBB130_71	# bb97
.LBB130_61:	# bb96.preheader
	testl	%ecx, %ecx
	jle	.LBB130_84	# return
.LBB130_62:	# bb.nph154
	movl	80(%rsp), %esi
	leal	(%rsi,%rsi), %esi
	movl	%esi, 12(%rsp)
	movl	96(%rsp), %esi
	addl	%esi, %esi
	movl	%esi, 16(%rsp)
	xorl	%edi, %edi
	movl	%edi, 20(%rsp)
	movl	%edi, %esi
	jmp	.LBB130_67	# bb94.preheader
.LBB130_63:	# bb.nph148
	leal	1(%r10), %r15d
	pxor	%xmm1, %xmm1
	xorl	%r12d, %r12d
	movl	%r12d, %r13d
	movapd	%xmm1, %xmm2
	.align	16
.LBB130_64:	# bb91
	leal	(%r15,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r9,%rbp,8), %xmm3
	leal	(%rdi,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r9,%rbp,8), %xmm4
	movapd	%xmm4, %xmm5
	mulsd	%xmm3, %xmm5
	leal	(%r10,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r9,%rbp,8), %xmm6
	leal	(%r14,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r9,%rbp,8), %xmm7
	movapd	%xmm7, %xmm8
	mulsd	%xmm6, %xmm8
	subsd	%xmm5, %xmm8
	addsd	%xmm8, %xmm2
	mulsd	%xmm3, %xmm7
	mulsd	%xmm6, %xmm4
	addsd	%xmm7, %xmm4
	addsd	%xmm4, %xmm1
	addl	$2, %r12d
	incl	%r13d
	cmpl	%r8d, %r13d
	jne	.LBB130_64	# bb91
.LBB130_65:	# bb93
	mulsd	%xmm0, %xmm1
	movslq	%ebx, %r15
	addsd	(%rax,%r15,8), %xmm1
	movsd	%xmm1, (%rax,%r15,8)
	mulsd	%xmm0, %xmm2
	leal	1(%rbx), %r15d
	movslq	%r15d, %r15
	addsd	(%rax,%r15,8), %xmm2
	movsd	%xmm2, (%rax,%r15,8)
	addl	%edx, %r10d
	addl	$2, %ebx
	incl	%r11d
	cmpl	%esi, %r11d
	jle	.LBB130_69	# bb92.preheader
.LBB130_66:	# bb95
	addl	12(%rsp), %edi
	movl	20(%rsp), %edx
	addl	16(%rsp), %edx
	movl	%edx, 20(%rsp)
	incl	%esi
	cmpl	%ecx, %esi
	je	.LBB130_84	# return
.LBB130_67:	# bb94.preheader
	testl	%esi, %esi
	js	.LBB130_66	# bb95
.LBB130_68:	# bb.nph152
	leal	1(%rdi), %r14d
	movl	80(%rsp), %edx
	leal	(%rdx,%rdx), %edx
	xorl	%r10d, %r10d
	movl	20(%rsp), %ebx
	movl	%r10d, %r11d
	.align	16
.LBB130_69:	# bb92.preheader
	testl	%r8d, %r8d
	jg	.LBB130_63	# bb.nph148
.LBB130_70:	# bb92.preheader.bb93_crit_edge
	pxor	%xmm1, %xmm1
	movapd	%xmm1, %xmm2
	jmp	.LBB130_65	# bb93
.LBB130_71:	# bb97
	cmpl	$122, %esi
	jne	.LBB130_83	# bb110
.LBB130_72:	# bb97
	cmpl	$113, 20(%rsp)
	jne	.LBB130_83	# bb110
.LBB130_73:	# bb109.preheader
	testl	%ecx, %ecx
	jle	.LBB130_84	# return
.LBB130_74:	# bb.nph144
	movl	96(%rsp), %r11d
	addl	%r11d, %r11d
	movl	%r11d, 16(%rsp)
	xorl	%r11d, %r11d
	movl	%r11d, 20(%rsp)
	movl	%r11d, %r12d
	jmp	.LBB130_79	# bb107.preheader
.LBB130_75:	# bb104.preheader
	leal	1(%r15), %ebx
	movl	80(%rsp), %edx
	leal	(%rdx,%rdx), %esi
	pxor	%xmm1, %xmm1
	xorl	%r13d, %r13d
	movl	%r13d, %edx
	movapd	%xmm1, %xmm2
	.align	16
.LBB130_76:	# bb104
	leal	(%r15,%r13), %ebp
	movslq	%ebp, %rbp
	movsd	(%r9,%rbp,8), %xmm3
	leal	(%r12,%r13), %ebp
	movslq	%ebp, %rbp
	movsd	(%r9,%rbp,8), %xmm4
	movapd	%xmm4, %xmm5
	mulsd	%xmm3, %xmm5
	leal	(%rdi,%r13), %ebp
	movslq	%ebp, %rbp
	movsd	(%r9,%rbp,8), %xmm6
	mulsd	%xmm6, %xmm3
	leal	(%rbx,%r13), %ebp
	movslq	%ebp, %rbp
	movsd	(%r9,%rbp,8), %xmm7
	mulsd	%xmm7, %xmm4
	subsd	%xmm3, %xmm4
	addsd	%xmm4, %xmm2
	mulsd	%xmm6, %xmm7
	addsd	%xmm5, %xmm7
	addsd	%xmm7, %xmm1
	addl	%esi, %r13d
	incl	%edx
	cmpl	%r8d, %edx
	jne	.LBB130_76	# bb104
.LBB130_77:	# bb106
	movl	20(%rsp), %edx
	leal	(%rdx,%r15), %edx
	movslq	%edx, %rdx
	mulsd	%xmm0, %xmm1
	addsd	(%rax,%rdx,8), %xmm1
	movsd	%xmm1, (%rax,%rdx,8)
	leal	(%r10,%r15), %edx
	movslq	%edx, %rdx
	mulsd	%xmm0, %xmm2
	addsd	(%rax,%rdx,8), %xmm2
	movsd	%xmm2, (%rax,%rdx,8)
	addl	$2, %r15d
	incl	%r14d
	cmpl	%r11d, %r14d
	jle	.LBB130_81	# bb105.preheader
.LBB130_78:	# bb108
	movl	20(%rsp), %edx
	addl	16(%rsp), %edx
	movl	%edx, 20(%rsp)
	addl	$2, %r12d
	incl	%r11d
	cmpl	%ecx, %r11d
	je	.LBB130_84	# return
.LBB130_79:	# bb107.preheader
	testl	%r11d, %r11d
	js	.LBB130_78	# bb108
.LBB130_80:	# bb.nph142
	leal	1(%r12), %edi
	movl	20(%rsp), %edx
	leal	1(%rdx), %r10d
	xorl	%r15d, %r15d
	movl	%r15d, %r14d
	.align	16
.LBB130_81:	# bb105.preheader
	testl	%r8d, %r8d
	jg	.LBB130_75	# bb104.preheader
.LBB130_82:	# bb105.preheader.bb106_crit_edge
	pxor	%xmm1, %xmm1
	movapd	%xmm1, %xmm2
	jmp	.LBB130_77	# bb106
.LBB130_83:	# bb110
	xorl	%edi, %edi
	leaq	.str169, %rsi
	leaq	.str1170, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB130_84:	# return
	addq	$24, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB130_85:	# bb53.preheader
	testl	%ecx, %ecx
	jle	.LBB130_11	# bb57
.LBB130_86:	# bb.nph124
	movl	96(%rsp), %ebx
	leal	2(,%rbx,2), %edi
	leal	(%rbx,%rbx), %edx
	xorl	%ebx, %ebx
	movl	%ebx, %r11d
	movl	%ebx, %r10d
	jmp	.LBB130_37	# bb51.preheader
	.size	cblas_zherk, .-cblas_zherk
.Leh_func_end90:


	.section	.rodata.cst8,"aM",@progbits,8
	.align	8
.LCPI131_0:					
	.quad	4607182418800017408	# double value: 1.000000e+00
	.text
	.align	16
	.globl	cblas_zhpmv
	.type	cblas_zhpmv,@function
cblas_zhpmv:
.Leh_func_begin91:
.Llabel91:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	movsd	(%rcx), %xmm0
	pxor	%xmm1, %xmm1
	ucomisd	%xmm1, %xmm0
	setp	%al
	setne	%r10b
	setnp	%r11b
	sete	%bl
	andb	%r11b, %bl
	movsd	8(%rcx), %xmm2
	ucomisd	%xmm1, %xmm2
	setp	%cl
	setnp	%r11b
	sete	%r14b
	setne	%r15b
	andb	%r11b, %r14b
	andb	%bl, %r14b
	orb	%al, %r10b
	orb	%cl, %r15b
	orb	%r10b, %r15b
	cmpl	$102, %edi
	movl	$4294967295, %eax
	movl	$1, %ecx
	cmove	%eax, %ecx
	testb	%r15b, %r15b
	movq	104(%rsp), %rax
	movsd	8(%rax), %xmm1
	movsd	(%rax), %xmm3
	movq	112(%rsp), %rax
	jne	.LBB131_3	# bb23
.LBB131_1:	# entry
	ucomisd	.LCPI131_0(%rip), %xmm3
	jne	.LBB131_3	# bb23
	jp	.LBB131_3	# bb23
.LBB131_2:	# entry
	pxor	%xmm4, %xmm4
	ucomisd	%xmm4, %xmm1
	setnp	%r10b
	sete	%r11b
	testb	%r10b, %r11b
	jne	.LBB131_45	# return
.LBB131_3:	# bb23
	pxor	%xmm4, %xmm4
	ucomisd	%xmm4, %xmm1
	jne	.LBB131_10	# bb31
	jp	.LBB131_10	# bb31
.LBB131_4:	# bb23
	pxor	%xmm4, %xmm4
	ucomisd	%xmm4, %xmm3
	jne	.LBB131_10	# bb31
	jp	.LBB131_10	# bb31
.LBB131_5:	# bb25
	cmpl	$0, 120(%rsp)
	jg	.LBB131_46	# bb25.bb30.preheader_crit_edge
.LBB131_6:	# bb26
	movl	$1, %r10d
	subl	%edx, %r10d
	imull	120(%rsp), %r10d
.LBB131_7:	# bb30.preheader
	testl	%edx, %edx
	jle	.LBB131_17	# bb39
.LBB131_8:	# bb.nph
	movl	120(%rsp), %r11d
	leal	(%r11,%r11), %r11d
	addl	%r10d, %r10d
	xorl	%ebx, %ebx
	.align	16
.LBB131_9:	# bb29
	movslq	%r10d, %r15
	movq	$0, (%rax,%r15,8)
	leal	(%r11,%r10), %r15d
	incl	%r10d
	movslq	%r10d, %r10
	movq	$0, (%rax,%r10,8)
	incl	%ebx
	cmpl	%edx, %ebx
	movl	%r15d, %r10d
	jne	.LBB131_9	# bb29
	jmp	.LBB131_17	# bb39
.LBB131_10:	# bb31
	pxor	%xmm4, %xmm4
	ucomisd	%xmm4, %xmm1
	jne	.LBB131_12	# bb33
	jp	.LBB131_12	# bb33
.LBB131_11:	# bb31
	ucomisd	.LCPI131_0(%rip), %xmm3
	setnp	%r10b
	sete	%r11b
	testb	%r10b, %r11b
	jne	.LBB131_17	# bb39
.LBB131_12:	# bb33
	cmpl	$0, 120(%rsp)
	jg	.LBB131_47	# bb33.bb38.preheader_crit_edge
.LBB131_13:	# bb34
	movl	$1, %r10d
	subl	%edx, %r10d
	imull	120(%rsp), %r10d
.LBB131_14:	# bb38.preheader
	testl	%edx, %edx
	jle	.LBB131_17	# bb39
.LBB131_15:	# bb.nph140
	movl	120(%rsp), %r11d
	leal	(%r11,%r11), %r11d
	addl	%r10d, %r10d
	xorl	%ebx, %ebx
	.align	16
.LBB131_16:	# bb37
	movslq	%r10d, %r15
	movsd	(%rax,%r15,8), %xmm4
	movapd	%xmm4, %xmm5
	mulsd	%xmm3, %xmm5
	leal	1(%r10), %r12d
	movslq	%r12d, %r12
	movsd	(%rax,%r12,8), %xmm6
	movapd	%xmm6, %xmm7
	mulsd	%xmm1, %xmm7
	subsd	%xmm7, %xmm5
	movsd	%xmm5, (%rax,%r15,8)
	mulsd	%xmm1, %xmm4
	mulsd	%xmm3, %xmm6
	addsd	%xmm4, %xmm6
	movsd	%xmm6, (%rax,%r12,8)
	addl	%r11d, %r10d
	incl	%ebx
	cmpl	%edx, %ebx
	jne	.LBB131_16	# bb37
.LBB131_17:	# bb39
	testb	$1, %r14b
	jne	.LBB131_45	# return
.LBB131_18:	# bb41
	cmpl	$121, %esi
	jne	.LBB131_20	# bb44
.LBB131_19:	# bb41
	cmpl	$101, %edi
	je	.LBB131_22	# bb48
.LBB131_20:	# bb44
	cmpl	$122, %esi
	jne	.LBB131_32	# bb66
.LBB131_21:	# bb44
	cmpl	$102, %edi
	jne	.LBB131_32	# bb66
.LBB131_22:	# bb48
	cmpl	$0, 96(%rsp)
	jg	.LBB131_48	# bb48.bb51_crit_edge
.LBB131_23:	# bb49
	movl	$1, %esi
	subl	%edx, %esi
	imull	96(%rsp), %esi
	movl	%esi, 36(%rsp)
.LBB131_24:	# bb51
	cmpl	$0, 120(%rsp)
	jg	.LBB131_49	# bb51.bb65.preheader_crit_edge
.LBB131_25:	# bb52
	movl	$1, %esi
	subl	%edx, %esi
	imull	120(%rsp), %esi
.LBB131_26:	# bb65.preheader
	testl	%edx, %edx
	jle	.LBB131_45	# return
.LBB131_27:	# bb.nph137
	movl	$1, %edi
	subl	%edx, %edi
	movl	%edi, %r10d
	movl	96(%rsp), %r11d
	imull	%r11d, %r10d
	movl	%r10d, 4(%rsp)
	movl	120(%rsp), %r10d
	imull	%r10d, %edi
	movl	%edi, (%rsp)
	addl	%esi, %esi
	movl	36(%rsp), %edi
	addl	%edi, %edi
	movl	%edi, 36(%rsp)
	leal	1(,%rdx,2), %edi
	movl	%edi, 20(%rsp)
	leal	-1(%rdx), %edi
	movl	%edi, 16(%rsp)
	cvtsi2sd	%ecx, %xmm1
	leal	(%r10,%r10), %ecx
	movl	%ecx, 12(%rsp)
	leal	(%r11,%r11), %ecx
	movl	%ecx, 8(%rsp)
	xorl	%ecx, %ecx
	movl	%ecx, 32(%rsp)
	movl	%r11d, 28(%rsp)
	movl	%r10d, 24(%rsp)
	.align	16
.LBB131_28:	# bb55
	movl	32(%rsp), %r10d
	movl	20(%rsp), %edi
	leal	(%rdi,%r10), %edi
	imull	%ecx, %edi
	movl	%edi, %r10d
	shrl	$31, %r10d
	addl	%edi, %r10d
	movl	%r10d, %edi
	andl	$4294967294, %edi
	movslq	%edi, %rdi
	movsd	(%r8,%rdi,8), %xmm3
	movl	36(%rsp), %edi
	movslq	%edi, %r11
	movsd	(%r9,%r11,8), %xmm4
	movapd	%xmm0, %xmm5
	mulsd	%xmm4, %xmm5
	leal	1(%rdi), %edi
	movslq	%edi, %rdi
	movsd	(%r9,%rdi,8), %xmm6
	movapd	%xmm2, %xmm7
	mulsd	%xmm6, %xmm7
	subsd	%xmm7, %xmm5
	movapd	%xmm5, %xmm7
	mulsd	%xmm3, %xmm7
	movslq	%esi, %rdi
	addsd	(%rax,%rdi,8), %xmm7
	movsd	%xmm7, (%rax,%rdi,8)
	mulsd	%xmm2, %xmm4
	mulsd	%xmm0, %xmm6
	addsd	%xmm4, %xmm6
	mulsd	%xmm6, %xmm3
	incl	%esi
	movslq	%esi, %rsi
	addsd	(%rax,%rsi,8), %xmm3
	movsd	%xmm3, (%rax,%rsi,8)
	xorl	%r11d, %r11d
	cmpl	$0, 120(%rsp)
	movl	(%rsp), %ebx
	cmovg	%r11d, %ebx
	cmpl	$0, 96(%rsp)
	cmovle	4(%rsp), %r11d
	sarl	%r10d
	leal	1(%rcx), %r14d
	cmpl	%edx, %r14d
	jge	.LBB131_50	# bb55.bb64_crit_edge
.LBB131_29:	# bb.nph131
	movl	32(%rsp), %r14d
	movl	16(%rsp), %r15d
	leal	(%r15,%r14), %r14d
	addl	%r10d, %r10d
	pxor	%xmm3, %xmm3
	xorl	%r15d, %r15d
	movl	24(%rsp), %r12d
	movl	28(%rsp), %r13d
	movapd	%xmm3, %xmm4
	.align	16
.LBB131_30:	# bb62
	leal	3(%r10), %ebp
	movslq	%ebp, %rbp
	movapd	%xmm1, %xmm7
	mulsd	(%r8,%rbp,8), %xmm7
	movapd	%xmm7, %xmm8
	mulsd	%xmm6, %xmm8
	addl	$2, %r10d
	movslq	%r10d, %rbp
	movsd	(%r8,%rbp,8), %xmm9
	movapd	%xmm5, %xmm10
	mulsd	%xmm9, %xmm10
	addsd	%xmm8, %xmm10
	addl	%ebx, %r12d
	leal	(%r12,%r12), %ebx
	movslq	%ebx, %rbx
	addsd	(%rax,%rbx,8), %xmm10
	movsd	%xmm10, (%rax,%rbx,8)
	movapd	%xmm5, %xmm8
	mulsd	%xmm7, %xmm8
	movapd	%xmm6, %xmm10
	mulsd	%xmm9, %xmm10
	subsd	%xmm8, %xmm10
	leal	1(,%r12,2), %ebx
	movslq	%ebx, %rbx
	addsd	(%rax,%rbx,8), %xmm10
	movsd	%xmm10, (%rax,%rbx,8)
	addl	%r11d, %r13d
	leal	1(,%r13,2), %r11d
	movslq	%r11d, %r11
	movsd	(%r9,%r11,8), %xmm8
	movapd	%xmm8, %xmm10
	mulsd	%xmm9, %xmm10
	leal	(%r13,%r13), %r11d
	movslq	%r11d, %r11
	movsd	(%r9,%r11,8), %xmm11
	movapd	%xmm11, %xmm12
	mulsd	%xmm7, %xmm12
	addsd	%xmm10, %xmm12
	addsd	%xmm12, %xmm4
	mulsd	%xmm8, %xmm7
	mulsd	%xmm9, %xmm11
	subsd	%xmm7, %xmm11
	addsd	%xmm11, %xmm3
	incl	%r15d
	cmpl	%r14d, %r15d
	movl	120(%rsp), %ebx
	movl	96(%rsp), %r11d
	jne	.LBB131_30	# bb62
.LBB131_31:	# bb64
	movapd	%xmm2, %xmm5
	mulsd	%xmm4, %xmm5
	movapd	%xmm0, %xmm6
	mulsd	%xmm3, %xmm6
	subsd	%xmm5, %xmm6
	addsd	(%rax,%rdi,8), %xmm6
	movsd	%xmm6, (%rax,%rdi,8)
	mulsd	%xmm2, %xmm3
	mulsd	%xmm0, %xmm4
	addsd	%xmm3, %xmm4
	addsd	(%rax,%rsi,8), %xmm4
	movsd	%xmm4, (%rax,%rsi,8)
	movl	%edi, %esi
	addl	12(%rsp), %esi
	movl	36(%rsp), %edi
	addl	8(%rsp), %edi
	movl	%edi, 36(%rsp)
	movl	96(%rsp), %edi
	addl	%edi, 28(%rsp)
	movl	120(%rsp), %edi
	addl	%edi, 24(%rsp)
	decl	32(%rsp)
	incl	%ecx
	cmpl	%edx, %ecx
	jne	.LBB131_28	# bb55
	jmp	.LBB131_45	# return
.LBB131_32:	# bb66
	cmpl	$102, %edi
	sete	%r10b
	cmpl	$121, %esi
	sete	%r11b
	andb	%r10b, %r11b
	cmpl	$101, %edi
	sete	%dil
	cmpl	$122, %esi
	sete	%sil
	testb	%dil, %sil
	jne	.LBB131_34	# bb74
.LBB131_33:	# bb66
	notb	%r11b
	testb	$1, %r11b
	jne	.LBB131_44	# bb92
.LBB131_34:	# bb74
	cmpl	$0, 96(%rsp)
	jg	.LBB131_51	# bb74.bb77_crit_edge
.LBB131_35:	# bb75
	movl	$1, %esi
	subl	%edx, %esi
	imull	96(%rsp), %esi
	movl	%esi, 36(%rsp)
.LBB131_36:	# bb77
	cmpl	$0, 120(%rsp)
	jg	.LBB131_52	# bb77.bb91.preheader_crit_edge
.LBB131_37:	# bb78
	movl	$1, %esi
	subl	%edx, %esi
	imull	120(%rsp), %esi
.LBB131_38:	# bb91.preheader
	testl	%edx, %edx
	jle	.LBB131_45	# return
.LBB131_39:	# bb.nph119
	movl	$1, %edi
	subl	%edx, %edi
	movl	%edi, %r10d
	movl	120(%rsp), %r11d
	imull	%r11d, %r10d
	movl	%r10d, 24(%rsp)
	movl	96(%rsp), %r10d
	imull	%r10d, %edi
	movl	%edi, 20(%rsp)
	movl	36(%rsp), %edi
	addl	%edi, %edi
	movl	%edi, 36(%rsp)
	addl	%esi, %esi
	cvtsi2sd	%ecx, %xmm1
	leal	(%r10,%r10), %ecx
	movl	%ecx, 32(%rsp)
	leal	(%r11,%r11), %ecx
	movl	%ecx, 28(%rsp)
	xorl	%ecx, %ecx
	.align	16
.LBB131_40:	# bb81
	leal	1(%rcx), %edi
	imull	%ecx, %edi
	movl	%edi, %r10d
	shrl	$31, %r10d
	addl	%edi, %r10d
	sarl	%r10d
	leal	(%r10,%rcx), %edi
	addl	%edi, %edi
	movslq	%edi, %rdi
	movsd	(%r8,%rdi,8), %xmm3
	movl	36(%rsp), %edi
	movslq	%edi, %r11
	movsd	(%r9,%r11,8), %xmm4
	movapd	%xmm0, %xmm5
	mulsd	%xmm4, %xmm5
	leal	1(%rdi), %edi
	movslq	%edi, %rdi
	movsd	(%r9,%rdi,8), %xmm6
	movapd	%xmm2, %xmm7
	mulsd	%xmm6, %xmm7
	subsd	%xmm7, %xmm5
	movapd	%xmm5, %xmm7
	mulsd	%xmm3, %xmm7
	movslq	%esi, %rdi
	addsd	(%rax,%rdi,8), %xmm7
	movsd	%xmm7, (%rax,%rdi,8)
	mulsd	%xmm2, %xmm4
	mulsd	%xmm0, %xmm6
	addsd	%xmm4, %xmm6
	mulsd	%xmm6, %xmm3
	incl	%esi
	movslq	%esi, %rsi
	addsd	(%rax,%rsi,8), %xmm3
	movsd	%xmm3, (%rax,%rsi,8)
	xorl	%r11d, %r11d
	cmpl	$0, 120(%rsp)
	movl	24(%rsp), %ebx
	cmovg	%r11d, %ebx
	cmpl	$0, 96(%rsp)
	cmovle	20(%rsp), %r11d
	testl	%ecx, %ecx
	jle	.LBB131_53	# bb81.bb90_crit_edge
.LBB131_41:	# bb88.preheader
	movl	96(%rsp), %r14d
	leal	(%r14,%r14), %r14d
	addl	%r11d, %r11d
	movl	120(%rsp), %r15d
	leal	(%r15,%r15), %r15d
	addl	%ebx, %ebx
	addl	%r10d, %r10d
	pxor	%xmm3, %xmm3
	xorl	%r12d, %r12d
	movapd	%xmm3, %xmm4
	.align	16
.LBB131_42:	# bb88
	movslq	%r10d, %r13
	leal	1(%r10), %ebp
	movslq	%ebp, %rbp
	movapd	%xmm1, %xmm7
	mulsd	(%r8,%rbp,8), %xmm7
	movapd	%xmm7, %xmm8
	mulsd	%xmm6, %xmm8
	movsd	(%r8,%r13,8), %xmm9
	movapd	%xmm5, %xmm10
	mulsd	%xmm9, %xmm10
	addsd	%xmm8, %xmm10
	movslq	%ebx, %r13
	addsd	(%rax,%r13,8), %xmm10
	movsd	%xmm10, (%rax,%r13,8)
	movapd	%xmm5, %xmm8
	mulsd	%xmm7, %xmm8
	movapd	%xmm6, %xmm10
	mulsd	%xmm9, %xmm10
	subsd	%xmm8, %xmm10
	leal	1(%rbx), %r13d
	movslq	%r13d, %r13
	addsd	(%rax,%r13,8), %xmm10
	movsd	%xmm10, (%rax,%r13,8)
	movslq	%r11d, %r13
	leal	1(%r11), %ebp
	movslq	%ebp, %rbp
	movsd	(%r9,%rbp,8), %xmm8
	movapd	%xmm8, %xmm10
	mulsd	%xmm9, %xmm10
	movsd	(%r9,%r13,8), %xmm11
	movapd	%xmm11, %xmm12
	mulsd	%xmm7, %xmm12
	addsd	%xmm10, %xmm12
	addsd	%xmm12, %xmm3
	mulsd	%xmm7, %xmm8
	mulsd	%xmm9, %xmm11
	subsd	%xmm8, %xmm11
	addsd	%xmm11, %xmm4
	addl	%r14d, %r11d
	addl	%r15d, %ebx
	addl	$2, %r10d
	incl	%r12d
	cmpl	%ecx, %r12d
	jne	.LBB131_42	# bb88
.LBB131_43:	# bb90
	movapd	%xmm2, %xmm5
	mulsd	%xmm3, %xmm5
	movapd	%xmm0, %xmm6
	mulsd	%xmm4, %xmm6
	subsd	%xmm5, %xmm6
	addsd	(%rax,%rdi,8), %xmm6
	movsd	%xmm6, (%rax,%rdi,8)
	mulsd	%xmm2, %xmm4
	mulsd	%xmm0, %xmm3
	addsd	%xmm4, %xmm3
	addsd	(%rax,%rsi,8), %xmm3
	movsd	%xmm3, (%rax,%rsi,8)
	movl	%edi, %esi
	addl	28(%rsp), %esi
	movl	36(%rsp), %edi
	addl	32(%rsp), %edi
	movl	%edi, 36(%rsp)
	incl	%ecx
	cmpl	%edx, %ecx
	jne	.LBB131_40	# bb81
	jmp	.LBB131_45	# return
.LBB131_44:	# bb92
	xorl	%edi, %edi
	leaq	.str171, %rsi
	leaq	.str1172, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB131_45:	# return
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB131_46:	# bb25.bb30.preheader_crit_edge
	xorl	%r10d, %r10d
	jmp	.LBB131_7	# bb30.preheader
.LBB131_47:	# bb33.bb38.preheader_crit_edge
	xorl	%r10d, %r10d
	jmp	.LBB131_14	# bb38.preheader
.LBB131_48:	# bb48.bb51_crit_edge
	movl	$0, 36(%rsp)
	jmp	.LBB131_24	# bb51
.LBB131_49:	# bb51.bb65.preheader_crit_edge
	xorl	%esi, %esi
	jmp	.LBB131_26	# bb65.preheader
.LBB131_50:	# bb55.bb64_crit_edge
	pxor	%xmm3, %xmm3
	movapd	%xmm3, %xmm4
	jmp	.LBB131_31	# bb64
.LBB131_51:	# bb74.bb77_crit_edge
	movl	$0, 36(%rsp)
	jmp	.LBB131_36	# bb77
.LBB131_52:	# bb77.bb91.preheader_crit_edge
	xorl	%esi, %esi
	jmp	.LBB131_38	# bb91.preheader
.LBB131_53:	# bb81.bb90_crit_edge
	pxor	%xmm3, %xmm3
	movapd	%xmm3, %xmm4
	jmp	.LBB131_43	# bb90
	.size	cblas_zhpmv, .-cblas_zhpmv
.Leh_func_end91:


	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI132_0:					
	.quad	9223372036854775808	# double value: -0.000000e+00
	.quad	9223372036854775808	# double value: -0.000000e+00
	.text
	.align	16
	.globl	cblas_zhpr2
	.type	cblas_zhpr2,@function
cblas_zhpr2:
.Leh_func_begin92:
.Llabel92:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$56, %rsp
	cmpl	$102, %edi
	movl	$4294967295, %eax
	movl	$1, %r10d
	cmove	%eax, %r10d
	movl	%r10d, 52(%rsp)
	movsd	8(%rcx), %xmm0
	pxor	%xmm1, %xmm1
	ucomisd	%xmm1, %xmm0
	movsd	(%rcx), %xmm1
	movq	128(%rsp), %rax
	movq	112(%rsp), %rcx
	movl	%edx, 48(%rsp)
	jne	.LBB132_2	# bb20
	jp	.LBB132_2	# bb20
.LBB132_1:	# entry
	pxor	%xmm2, %xmm2
	ucomisd	%xmm2, %xmm1
	setnp	%dl
	sete	%r10b
	testb	%dl, %r10b
	jne	.LBB132_29	# return
.LBB132_2:	# bb20
	cmpl	$121, %esi
	jne	.LBB132_4	# bb23
.LBB132_3:	# bb20
	cmpl	$101, %edi
	je	.LBB132_6	# bb27
.LBB132_4:	# bb23
	cmpl	$122, %esi
	jne	.LBB132_16	# bb39
.LBB132_5:	# bb23
	cmpl	$102, %edi
	jne	.LBB132_16	# bb39
.LBB132_6:	# bb27
	testl	%r9d, %r9d
	jg	.LBB132_30	# bb27.bb30_crit_edge
.LBB132_7:	# bb28
	movl	$1, %edx
	subl	48(%rsp), %edx
	imull	%r9d, %edx
	movl	%edx, 40(%rsp)
.LBB132_8:	# bb30
	cmpl	$0, 120(%rsp)
	jg	.LBB132_31	# bb30.bb38.preheader_crit_edge
.LBB132_9:	# bb31
	movl	$1, %edx
	subl	48(%rsp), %edx
	imull	120(%rsp), %edx
	movl	%edx, 32(%rsp)
.LBB132_10:	# bb38.preheader
	cmpl	$0, 48(%rsp)
	jle	.LBB132_29	# return
.LBB132_11:	# bb.nph95
	movl	40(%rsp), %edx
	leal	(%r9,%rdx), %esi
	addl	%esi, %esi
	movl	%esi, 4(%rsp)
	movl	32(%rsp), %esi
	movl	120(%rsp), %edi
	leal	(%rdi,%rsi), %r10d
	addl	%r10d, %r10d
	movl	%r10d, (%rsp)
	leal	1(,%rdx,2), %r10d
	movl	%r10d, 28(%rsp)
	addl	%edx, %edx
	movl	%edx, 40(%rsp)
	leal	(%r9,%r9), %edx
	movl	%edx, 24(%rsp)
	leal	1(,%rsi,2), %edx
	movl	%edx, 20(%rsp)
	addl	%esi, %esi
	movl	%esi, 32(%rsp)
	leal	(%rdi,%rdi), %edx
	movl	%edx, 16(%rsp)
	movl	48(%rsp), %edx
	leal	1(,%rdx,2), %esi
	movl	%esi, 12(%rsp)
	leal	-1(%rdx), %edx
	movl	%edx, 8(%rsp)
	cvtsi2sd	52(%rsp), %xmm2
	movapd	%xmm0, %xmm3
	xorpd	.LCPI132_0(%rip), %xmm3
	xorl	%edx, %edx
	movl	%edx, 52(%rsp)
	movl	%edx, 44(%rsp)
	movl	%edx, %esi
	.align	16
.LBB132_12:	# bb34
	movl	28(%rsp), %edi
	leal	(%rdi,%rdx), %edi
	movslq	%edi, %rdi
	movsd	(%r8,%rdi,8), %xmm4
	movapd	%xmm1, %xmm5
	mulsd	%xmm4, %xmm5
	movl	40(%rsp), %edi
	leal	(%rdi,%rdx), %edi
	movslq	%edi, %rdi
	movsd	(%r8,%rdi,8), %xmm6
	movapd	%xmm0, %xmm7
	mulsd	%xmm6, %xmm7
	addsd	%xmm5, %xmm7
	movl	52(%rsp), %edi
	movl	20(%rsp), %r10d
	leal	(%r10,%rdi), %r10d
	movslq	%r10d, %r10
	movsd	(%rcx,%r10,8), %xmm5
	movapd	%xmm7, %xmm8
	mulsd	%xmm5, %xmm8
	mulsd	%xmm0, %xmm4
	mulsd	%xmm1, %xmm6
	subsd	%xmm4, %xmm6
	movl	32(%rsp), %r10d
	leal	(%r10,%rdi), %edi
	movslq	%edi, %rdi
	movsd	(%rcx,%rdi,8), %xmm4
	movapd	%xmm6, %xmm9
	mulsd	%xmm4, %xmm9
	addsd	%xmm8, %xmm9
	addsd	%xmm9, %xmm9
	movl	44(%rsp), %r10d
	movl	12(%rsp), %edi
	leal	(%rdi,%r10), %edi
	imull	%esi, %edi
	movl	%edi, %r10d
	shrl	$31, %r10d
	addl	%edi, %r10d
	movl	%r10d, %edi
	andl	$4294967294, %edi
	movslq	%edi, %r11
	addsd	(%rax,%r11,8), %xmm9
	movsd	%xmm9, (%rax,%r11,8)
	orl	$1, %edi
	movslq	%edi, %rdi
	movq	$0, (%rax,%rdi,8)
	movapd	%xmm0, %xmm8
	mulsd	%xmm5, %xmm8
	movapd	%xmm1, %xmm9
	mulsd	%xmm4, %xmm9
	addsd	%xmm8, %xmm9
	mulsd	%xmm1, %xmm5
	mulsd	%xmm3, %xmm4
	addsd	%xmm5, %xmm4
	sarl	%r10d
	leal	1(%rsi), %edi
	cmpl	48(%rsp), %edi
	jge	.LBB132_15	# bb37
.LBB132_13:	# bb.nph91
	movl	4(%rsp), %edi
	leal	(%rdi,%rdx), %edi
	movl	52(%rsp), %r11d
	movl	(%rsp), %ebx
	leal	(%rbx,%r11), %r11d
	movl	44(%rsp), %r14d
	movl	8(%rsp), %ebx
	leal	(%rbx,%r14), %ebx
	movl	120(%rsp), %r14d
	leal	(%r14,%r14), %r14d
	leal	(%r9,%r9), %r15d
	addl	%r10d, %r10d
	xorl	%r12d, %r12d
	.align	16
.LBB132_14:	# bb35
	movslq	%edi, %r13
	movsd	(%r8,%r13,8), %xmm5
	movapd	%xmm9, %xmm8
	mulsd	%xmm5, %xmm8
	leal	1(%rdi), %r13d
	movslq	%r13d, %r13
	movsd	(%r8,%r13,8), %xmm10
	movapd	%xmm4, %xmm11
	mulsd	%xmm10, %xmm11
	addsd	%xmm8, %xmm11
	movslq	%r11d, %r13
	movsd	(%rcx,%r13,8), %xmm8
	movapd	%xmm6, %xmm12
	mulsd	%xmm8, %xmm12
	leal	1(%r11), %r13d
	movslq	%r13d, %r13
	movsd	(%rcx,%r13,8), %xmm13
	movapd	%xmm7, %xmm14
	mulsd	%xmm13, %xmm14
	addsd	%xmm12, %xmm14
	addsd	%xmm11, %xmm14
	leal	2(%r10), %r13d
	movslq	%r13d, %rbp
	addsd	(%rax,%rbp,8), %xmm14
	movsd	%xmm14, (%rax,%rbp,8)
	mulsd	%xmm4, %xmm5
	mulsd	%xmm9, %xmm10
	subsd	%xmm10, %xmm5
	mulsd	%xmm7, %xmm8
	mulsd	%xmm6, %xmm13
	subsd	%xmm13, %xmm8
	addsd	%xmm5, %xmm8
	mulsd	%xmm2, %xmm8
	addl	$3, %r10d
	movslq	%r10d, %r10
	addsd	(%rax,%r10,8), %xmm8
	movsd	%xmm8, (%rax,%r10,8)
	addl	%r14d, %r11d
	addl	%r15d, %edi
	incl	%r12d
	cmpl	%ebx, %r12d
	movl	%r13d, %r10d
	jne	.LBB132_14	# bb35
.LBB132_15:	# bb37
	addl	24(%rsp), %edx
	movl	52(%rsp), %edi
	addl	16(%rsp), %edi
	movl	%edi, 52(%rsp)
	decl	44(%rsp)
	incl	%esi
	cmpl	48(%rsp), %esi
	jne	.LBB132_12	# bb34
	jmp	.LBB132_29	# return
.LBB132_16:	# bb39
	cmpl	$102, %edi
	sete	%dl
	cmpl	$121, %esi
	sete	%r10b
	andb	%dl, %r10b
	cmpl	$101, %edi
	sete	%dl
	cmpl	$122, %esi
	sete	%sil
	testb	%dl, %sil
	jne	.LBB132_18	# bb47
.LBB132_17:	# bb39
	notb	%r10b
	testb	$1, %r10b
	jne	.LBB132_28	# bb65
.LBB132_18:	# bb47
	testl	%r9d, %r9d
	jg	.LBB132_32	# bb47.bb50_crit_edge
.LBB132_19:	# bb48
	movl	$1, %edx
	subl	48(%rsp), %edx
	imull	%r9d, %edx
.LBB132_20:	# bb50
	cmpl	$0, 120(%rsp)
	jg	.LBB132_33	# bb50.bb64.preheader_crit_edge
.LBB132_21:	# bb51
	movl	$1, %esi
	subl	48(%rsp), %esi
	imull	120(%rsp), %esi
.LBB132_22:	# bb64.preheader
	cmpl	$0, 48(%rsp)
	jle	.LBB132_29	# return
.LBB132_23:	# bb.nph81
	movl	$1, %edi
	subl	48(%rsp), %edi
	movl	%edi, %r10d
	imull	%r9d, %r10d
	movl	%r10d, 40(%rsp)
	movl	120(%rsp), %r10d
	imull	%r10d, %edi
	addl	%esi, %esi
	addl	%edx, %edx
	cvtsi2sd	52(%rsp), %xmm2
	movapd	%xmm0, %xmm3
	xorpd	.LCPI132_0(%rip), %xmm3
	movsd	%xmm3, 32(%rsp)
	leal	(%r10,%r10), %r10d
	movl	%r10d, 52(%rsp)
	leal	(%r9,%r9), %r10d
	movl	%r10d, 44(%rsp)
	xorl	%r10d, %r10d
	.align	16
.LBB132_24:	# bb54
	xorl	%r11d, %r11d
	cmpl	$0, 120(%rsp)
	movl	%edi, %ebx
	cmovg	%r11d, %ebx
	testl	%r9d, %r9d
	cmovle	40(%rsp), %r11d
	movslq	%esi, %r14
	movsd	(%rcx,%r14,8), %xmm3
	movapd	%xmm1, %xmm4
	mulsd	%xmm3, %xmm4
	movapd	%xmm3, %xmm5
	mulsd	32(%rsp), %xmm5
	leal	1(%rsi), %r14d
	movslq	%r14d, %r14
	movsd	(%rcx,%r14,8), %xmm6
	movapd	%xmm1, %xmm7
	mulsd	%xmm6, %xmm7
	addsd	%xmm5, %xmm7
	movapd	%xmm0, %xmm5
	mulsd	%xmm6, %xmm5
	addsd	%xmm4, %xmm5
	movslq	%edx, %r14
	movsd	(%r8,%r14,8), %xmm4
	movapd	%xmm0, %xmm8
	mulsd	%xmm4, %xmm8
	leal	1(%rdx), %r14d
	movslq	%r14d, %r14
	movsd	(%r8,%r14,8), %xmm9
	movapd	%xmm1, %xmm10
	mulsd	%xmm9, %xmm10
	addsd	%xmm8, %xmm10
	mulsd	%xmm1, %xmm4
	mulsd	%xmm0, %xmm9
	subsd	%xmm9, %xmm4
	testl	%r10d, %r10d
	jle	.LBB132_27	# bb63
.LBB132_25:	# bb.nph
	leal	1(%r10), %r14d
	imull	%r10d, %r14d
	movl	%r14d, %r15d
	shrl	$31, %r15d
	addl	%r14d, %r15d
	andl	$4294967294, %r15d
	leal	(%r9,%r9), %r14d
	addl	%r11d, %r11d
	movl	120(%rsp), %r12d
	leal	(%r12,%r12), %r12d
	addl	%ebx, %ebx
	xorl	%r13d, %r13d
	.align	16
.LBB132_26:	# bb61
	movslq	%r11d, %rbp
	movsd	(%r8,%rbp,8), %xmm8
	movapd	%xmm5, %xmm9
	mulsd	%xmm8, %xmm9
	leal	1(%r11), %ebp
	movslq	%ebp, %rbp
	movsd	(%r8,%rbp,8), %xmm11
	movapd	%xmm7, %xmm12
	mulsd	%xmm11, %xmm12
	addsd	%xmm9, %xmm12
	movslq	%ebx, %rbp
	movsd	(%rcx,%rbp,8), %xmm9
	movapd	%xmm4, %xmm13
	mulsd	%xmm9, %xmm13
	leal	1(%rbx), %ebp
	movslq	%ebp, %rbp
	movsd	(%rcx,%rbp,8), %xmm14
	movapd	%xmm10, %xmm15
	mulsd	%xmm14, %xmm15
	addsd	%xmm13, %xmm15
	addsd	%xmm12, %xmm15
	movslq	%r15d, %rbp
	addsd	(%rax,%rbp,8), %xmm15
	movsd	%xmm15, (%rax,%rbp,8)
	mulsd	%xmm7, %xmm8
	mulsd	%xmm5, %xmm11
	subsd	%xmm11, %xmm8
	mulsd	%xmm10, %xmm9
	mulsd	%xmm4, %xmm14
	subsd	%xmm14, %xmm9
	addsd	%xmm8, %xmm9
	mulsd	%xmm2, %xmm9
	leal	1(%r15), %ebp
	movslq	%ebp, %rbp
	addsd	(%rax,%rbp,8), %xmm9
	movsd	%xmm9, (%rax,%rbp,8)
	addl	%r14d, %r11d
	addl	%r12d, %ebx
	addl	$2, %r15d
	incl	%r13d
	cmpl	%r10d, %r13d
	jne	.LBB132_26	# bb61
.LBB132_27:	# bb63
	leal	1(%r10), %r11d
	movl	%r11d, %ebx
	imull	%r10d, %ebx
	movl	%ebx, %r14d
	shrl	$31, %r14d
	addl	%ebx, %r14d
	sarl	%r14d
	addl	%r10d, %r14d
	leal	(%r14,%r14), %r10d
	movslq	%r10d, %r10
	mulsd	%xmm6, %xmm10
	mulsd	%xmm3, %xmm4
	addsd	%xmm10, %xmm4
	addsd	%xmm4, %xmm4
	addsd	(%rax,%r10,8), %xmm4
	movsd	%xmm4, (%rax,%r10,8)
	leal	1(,%r14,2), %r10d
	movslq	%r10d, %r10
	movq	$0, (%rax,%r10,8)
	addl	52(%rsp), %esi
	addl	44(%rsp), %edx
	cmpl	48(%rsp), %r11d
	movl	%r11d, %r10d
	jne	.LBB132_24	# bb54
	jmp	.LBB132_29	# return
.LBB132_28:	# bb65
	xorl	%edi, %edi
	leaq	.str173, %rsi
	leaq	.str1174, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB132_29:	# return
	addq	$56, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB132_30:	# bb27.bb30_crit_edge
	movl	$0, 40(%rsp)
	jmp	.LBB132_8	# bb30
.LBB132_31:	# bb30.bb38.preheader_crit_edge
	movl	$0, 32(%rsp)
	jmp	.LBB132_10	# bb38.preheader
.LBB132_32:	# bb47.bb50_crit_edge
	xorl	%edx, %edx
	jmp	.LBB132_20	# bb50
.LBB132_33:	# bb50.bb64.preheader_crit_edge
	xorl	%esi, %esi
	jmp	.LBB132_22	# bb64.preheader
	.size	cblas_zhpr2, .-cblas_zhpr2
.Leh_func_end92:


	.align	16
	.globl	cblas_zhpr
	.type	cblas_zhpr,@function
cblas_zhpr:
.Leh_func_begin93:
.Llabel93:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$24, %rsp
	cmpl	$102, %edi
	movl	$4294967295, %eax
	movl	$1, %r10d
	cmove	%eax, %r10d
	pxor	%xmm1, %xmm1
	ucomisd	%xmm1, %xmm0
	setnp	%al
	sete	%r11b
	testb	%al, %r11b
	jne	.LBB133_24	# return
.LBB133_1:	# bb13
	cmpl	$121, %esi
	jne	.LBB133_3	# bb16
.LBB133_2:	# bb13
	cmpl	$101, %edi
	je	.LBB133_5	# bb20
.LBB133_3:	# bb16
	cmpl	$122, %esi
	jne	.LBB133_13	# bb29
.LBB133_4:	# bb16
	cmpl	$102, %edi
	jne	.LBB133_13	# bb29
.LBB133_5:	# bb20
	testl	%r8d, %r8d
	jg	.LBB133_25	# bb20.bb28.preheader_crit_edge
.LBB133_6:	# bb21
	movl	$1, %eax
	subl	%edx, %eax
	imull	%r8d, %eax
.LBB133_7:	# bb28.preheader
	testl	%edx, %edx
	jle	.LBB133_24	# return
.LBB133_8:	# bb.nph73
	leal	(%r8,%rax), %esi
	addl	%esi, %esi
	movl	%esi, 16(%rsp)
	cvtsi2sd	%r10d, %xmm1
	mulsd	%xmm0, %xmm1
	negl	%r10d
	cvtsi2sd	%r10d, %xmm2
	leal	1(,%rax,2), %r10d
	movl	%r10d, 20(%rsp)
	addl	%eax, %eax
	leal	(%r8,%r8), %r10d
	movl	%r10d, 12(%rsp)
	leal	1(,%rdx,2), %r10d
	movl	%r10d, 8(%rsp)
	leal	-1(%rdx), %r10d
	movl	%r10d, 4(%rsp)
	xorl	%r10d, %r10d
	movl	%r10d, %esi
	movl	%r10d, %edi
	.align	16
.LBB133_9:	# bb24
	movl	20(%rsp), %r11d
	leal	(%r11,%r10), %r11d
	movslq	%r11d, %r11
	movsd	(%rcx,%r11,8), %xmm3
	movapd	%xmm1, %xmm4
	mulsd	%xmm3, %xmm4
	mulsd	%xmm2, %xmm3
	mulsd	%xmm4, %xmm3
	leal	(%rax,%r10), %r11d
	movslq	%r11d, %r11
	movsd	(%rcx,%r11,8), %xmm5
	movapd	%xmm5, %xmm6
	mulsd	%xmm0, %xmm6
	mulsd	%xmm6, %xmm5
	subsd	%xmm3, %xmm5
	movl	8(%rsp), %r11d
	leal	(%r11,%rsi), %r11d
	imull	%edi, %r11d
	movl	%r11d, %ebx
	shrl	$31, %ebx
	addl	%r11d, %ebx
	movl	%ebx, %r11d
	andl	$4294967294, %r11d
	movslq	%r11d, %r14
	addsd	(%r9,%r14,8), %xmm5
	movsd	%xmm5, (%r9,%r14,8)
	orl	$1, %r11d
	movslq	%r11d, %r11
	movq	$0, (%r9,%r11,8)
	sarl	%ebx
	leal	1(%rdi), %r11d
	cmpl	%edx, %r11d
	jge	.LBB133_12	# bb27
.LBB133_10:	# bb.nph70
	movl	16(%rsp), %r11d
	leal	(%r11,%r10), %r11d
	movl	4(%rsp), %r14d
	leal	(%r14,%rsi), %r14d
	leal	(%r8,%r8), %r15d
	addl	%ebx, %ebx
	xorl	%r12d, %r12d
	.align	16
.LBB133_11:	# bb25
	movslq	%r11d, %r13
	leal	1(%r11), %ebp
	movslq	%ebp, %rbp
	movapd	%xmm2, %xmm3
	mulsd	(%rcx,%rbp,8), %xmm3
	movapd	%xmm3, %xmm5
	mulsd	%xmm4, %xmm5
	movsd	(%rcx,%r13,8), %xmm7
	movapd	%xmm7, %xmm8
	mulsd	%xmm6, %xmm8
	subsd	%xmm5, %xmm8
	leal	2(%rbx), %r13d
	movslq	%r13d, %rbp
	addsd	(%r9,%rbp,8), %xmm8
	movsd	%xmm8, (%r9,%rbp,8)
	mulsd	%xmm6, %xmm3
	mulsd	%xmm4, %xmm7
	addsd	%xmm3, %xmm7
	addl	$3, %ebx
	movslq	%ebx, %rbx
	addsd	(%r9,%rbx,8), %xmm7
	movsd	%xmm7, (%r9,%rbx,8)
	addl	%r15d, %r11d
	incl	%r12d
	cmpl	%r14d, %r12d
	movl	%r13d, %ebx
	jne	.LBB133_11	# bb25
.LBB133_12:	# bb27
	addl	12(%rsp), %r10d
	decl	%esi
	incl	%edi
	cmpl	%edx, %edi
	jne	.LBB133_9	# bb24
	jmp	.LBB133_24	# return
.LBB133_13:	# bb29
	cmpl	$102, %edi
	sete	%al
	cmpl	$121, %esi
	sete	%r11b
	andb	%al, %r11b
	cmpl	$101, %edi
	sete	%al
	cmpl	$122, %esi
	sete	%sil
	testb	%al, %sil
	jne	.LBB133_15	# bb37
.LBB133_14:	# bb29
	notb	%r11b
	testb	$1, %r11b
	jne	.LBB133_23	# bb49
.LBB133_15:	# bb37
	testl	%r8d, %r8d
	jg	.LBB133_26	# bb37.bb48.preheader_crit_edge
.LBB133_16:	# bb38
	movl	$1, %eax
	subl	%edx, %eax
	imull	%r8d, %eax
.LBB133_17:	# bb48.preheader
	testl	%edx, %edx
	jle	.LBB133_24	# return
.LBB133_18:	# bb.nph
	cvtsi2sd	%r10d, %xmm1
	mulsd	%xmm0, %xmm1
	movl	$1, %esi
	subl	%edx, %esi
	imull	%r8d, %esi
	negl	%r10d
	cvtsi2sd	%r10d, %xmm2
	addl	%eax, %eax
	leal	(%r8,%r8), %edi
	movl	%edi, 20(%rsp)
	xorl	%edi, %edi
	.align	16
.LBB133_19:	# bb41
	xorl	%r10d, %r10d
	testl	%edi, %edi
	movl	%edi, %r11d
	cmovs	%r10d, %r11d
	movslq	%eax, %rbx
	movapd	%xmm0, %xmm3
	mulsd	(%rcx,%rbx,8), %xmm3
	testl	%r8d, %r8d
	movl	%esi, %ebx
	cmovg	%r10d, %ebx
	addl	%ebx, %ebx
	leal	1(%rax), %r14d
	movslq	%r14d, %r14
	movapd	%xmm1, %xmm4
	mulsd	(%rcx,%r14,8), %xmm4
	leal	1(%rdi), %r14d
	imull	%edi, %r14d
	movl	%r14d, %r15d
	shrl	$31, %r15d
	addl	%r14d, %r15d
	movl	%r15d, %r14d
	andl	$4294967294, %r14d
	sarl	%r15d
	leal	(%r8,%r8), %r12d
	jmp	.LBB133_21	# bb46
.LBB133_20:	# bb45
	movapd	%xmm6, %xmm7
	mulsd	%xmm4, %xmm7
	movapd	%xmm5, %xmm8
	mulsd	%xmm3, %xmm8
	subsd	%xmm7, %xmm8
	movslq	%r14d, %r13
	addsd	(%r9,%r13,8), %xmm8
	movsd	%xmm8, (%r9,%r13,8)
	mulsd	%xmm4, %xmm5
	mulsd	%xmm3, %xmm6
	addsd	%xmm5, %xmm6
	leal	1(%r14), %r13d
	movslq	%r13d, %r13
	addsd	(%r9,%r13,8), %xmm6
	movsd	%xmm6, (%r9,%r13,8)
	addl	%r12d, %ebx
	addl	$2, %r14d
	incl	%r10d
.LBB133_21:	# bb46
	movslq	%ebx, %r13
	leal	1(%rbx), %ebp
	movslq	%ebp, %rbp
	movapd	%xmm2, %xmm6
	mulsd	(%rcx,%rbp,8), %xmm6
	cmpl	%r11d, %r10d
	movsd	(%rcx,%r13,8), %xmm5
	jne	.LBB133_20	# bb45
.LBB133_22:	# bb47
	mulsd	%xmm4, %xmm6
	mulsd	%xmm3, %xmm5
	subsd	%xmm6, %xmm5
	addl	%edi, %r15d
	leal	(%r15,%r15), %r10d
	movslq	%r10d, %r10
	addsd	(%r9,%r10,8), %xmm5
	movsd	%xmm5, (%r9,%r10,8)
	leal	1(,%r15,2), %r10d
	movslq	%r10d, %r10
	movq	$0, (%r9,%r10,8)
	addl	20(%rsp), %eax
	incl	%edi
	cmpl	%edx, %edi
	jne	.LBB133_19	# bb41
	jmp	.LBB133_24	# return
.LBB133_23:	# bb49
	xorl	%edi, %edi
	leaq	.str175, %rsi
	leaq	.str1176, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB133_24:	# return
	addq	$24, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB133_25:	# bb20.bb28.preheader_crit_edge
	xorl	%eax, %eax
	jmp	.LBB133_7	# bb28.preheader
.LBB133_26:	# bb37.bb48.preheader_crit_edge
	xorl	%eax, %eax
	jmp	.LBB133_17	# bb48.preheader
	.size	cblas_zhpr, .-cblas_zhpr
.Leh_func_end93:


	.align	16
	.globl	cblas_zscal
	.type	cblas_zscal,@function
cblas_zscal:
	testl	%ecx, %ecx
	movsd	8(%rsi), %xmm0
	movsd	(%rsi), %xmm1
	jle	.LBB134_4	# return
.LBB134_1:	# entry
	testl	%edi, %edi
	jle	.LBB134_4	# return
.LBB134_2:	# bb.nph
	addl	%ecx, %ecx
	xorl	%eax, %eax
	movl	%eax, %esi
	.align	16
.LBB134_3:	# bb1
	movslq	%eax, %r8
	movsd	(%rdx,%r8,8), %xmm2
	movapd	%xmm2, %xmm3
	mulsd	%xmm1, %xmm3
	leal	1(%rax), %r9d
	movslq	%r9d, %r9
	movsd	(%rdx,%r9,8), %xmm4
	movapd	%xmm4, %xmm5
	mulsd	%xmm0, %xmm5
	subsd	%xmm5, %xmm3
	movsd	%xmm3, (%rdx,%r8,8)
	mulsd	%xmm0, %xmm2
	mulsd	%xmm1, %xmm4
	addsd	%xmm2, %xmm4
	movsd	%xmm4, (%rdx,%r9,8)
	addl	%ecx, %eax
	incl	%esi
	cmpl	%edi, %esi
	jne	.LBB134_3	# bb1
.LBB134_4:	# return
	ret
	.size	cblas_zscal, .-cblas_zscal


	.align	16
	.globl	cblas_zswap
	.type	cblas_zswap,@function
cblas_zswap:
	pushq	%r14
	pushq	%rbx
	testl	%edx, %edx
	jg	.LBB135_8	# entry.bb2_crit_edge
.LBB135_1:	# bb
	movl	$1, %eax
	subl	%edi, %eax
	imull	%edx, %eax
.LBB135_2:	# bb2
	testl	%r8d, %r8d
	jg	.LBB135_9	# bb2.bb7.preheader_crit_edge
.LBB135_3:	# bb3
	movl	$1, %r9d
	subl	%edi, %r9d
	imull	%r8d, %r9d
.LBB135_4:	# bb7.preheader
	testl	%edi, %edi
	jle	.LBB135_7	# return
.LBB135_5:	# bb.nph
	addl	%r8d, %r8d
	addl	%r9d, %r9d
	addl	%edx, %edx
	addl	%eax, %eax
	xorl	%r10d, %r10d
	.align	16
.LBB135_6:	# bb6
	movslq	%r9d, %r11
	movsd	(%rcx,%r11,8), %xmm0
	movslq	%eax, %rbx
	movsd	(%rsi,%rbx,8), %xmm1
	leal	1(%rax), %r14d
	movslq	%r14d, %r14
	movsd	(%rsi,%r14,8), %xmm2
	movsd	%xmm0, (%rsi,%rbx,8)
	leal	1(%r9), %ebx
	movslq	%ebx, %rbx
	movsd	(%rcx,%rbx,8), %xmm0
	movsd	%xmm0, (%rsi,%r14,8)
	movsd	%xmm1, (%rcx,%r11,8)
	movsd	%xmm2, (%rcx,%rbx,8)
	addl	%r8d, %r9d
	addl	%edx, %eax
	incl	%r10d
	cmpl	%edi, %r10d
	jne	.LBB135_6	# bb6
.LBB135_7:	# return
	popq	%rbx
	popq	%r14
	ret
.LBB135_8:	# entry.bb2_crit_edge
	xorl	%eax, %eax
	jmp	.LBB135_2	# bb2
.LBB135_9:	# bb2.bb7.preheader_crit_edge
	xorl	%r9d, %r9d
	jmp	.LBB135_4	# bb7.preheader
	.size	cblas_zswap, .-cblas_zswap


	.section	.rodata.cst8,"aM",@progbits,8
	.align	8
.LCPI136_0:					
	.quad	4607182418800017408	# double value: 1.000000e+00
	.text
	.align	16
	.globl	cblas_zsymm
	.type	cblas_zsymm,@function
cblas_zsymm:
.Leh_func_begin94:
.Llabel94:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$104, %rsp
	movl	%ecx, 76(%rsp)
	movsd	(%r9), %xmm0
	pxor	%xmm1, %xmm1
	ucomisd	%xmm1, %xmm0
	setp	%al
	setne	%cl
	setnp	%r10b
	sete	%r11b
	andb	%r10b, %r11b
	movsd	8(%r9), %xmm2
	ucomisd	%xmm1, %xmm2
	setp	%r9b
	setnp	%r10b
	sete	%bl
	setne	%r14b
	andb	%r10b, %bl
	andb	%r11b, %bl
	movb	%bl, 96(%rsp)
	orb	%al, %cl
	orb	%r9b, %r14b
	orb	%cl, %r14b
	testb	%r14b, %r14b
	movq	192(%rsp), %rax
	movsd	8(%rax), %xmm1
	movsd	(%rax), %xmm3
	movq	200(%rsp), %rax
	jne	.LBB136_3	# bb33
.LBB136_1:	# entry
	ucomisd	.LCPI136_0(%rip), %xmm3
	jne	.LBB136_3	# bb33
	jp	.LBB136_3	# bb33
.LBB136_2:	# entry
	pxor	%xmm4, %xmm4
	ucomisd	%xmm4, %xmm1
	setnp	%cl
	sete	%r9b
	testb	%cl, %r9b
	jne	.LBB136_66	# return
.LBB136_3:	# bb33
	cmpl	$101, %edi
	je	.LBB136_67	# bb33.bb42_crit_edge
.LBB136_4:	# bb35
	cmpl	$141, %esi
	movl	$142, %ecx
	movl	$141, %esi
	cmove	%ecx, %esi
	cmpl	$121, %edx
	movl	$122, %ecx
	movl	$121, %edx
	cmove	%ecx, %edx
	movl	76(%rsp), %ecx
	movl	%ecx, 100(%rsp)
	movl	%r8d, 76(%rsp)
.LBB136_5:	# bb42
	pxor	%xmm4, %xmm4
	ucomisd	%xmm4, %xmm1
	jne	.LBB136_13	# bb50
	jp	.LBB136_13	# bb50
.LBB136_6:	# bb42
	pxor	%xmm4, %xmm4
	ucomisd	%xmm4, %xmm3
	jne	.LBB136_13	# bb50
	jp	.LBB136_13	# bb50
.LBB136_7:	# bb49.preheader
	cmpl	$0, 76(%rsp)
	jle	.LBB136_20	# bb58
.LBB136_8:	# bb.nph124
	cmpl	$0, 100(%rsp)
	jle	.LBB136_20	# bb58
.LBB136_9:	# bb47.preheader.preheader
	movl	208(%rsp), %ecx
	leal	(%rcx,%rcx), %edi
	xorl	%ecx, %ecx
	movl	%ecx, %r8d
	jmp	.LBB136_12	# bb47.preheader
	.align	16
.LBB136_10:	# bb46
	movslq	%r9d, %r11
	movq	$0, (%rax,%r11,8)
	leal	1(%r9), %r11d
	movslq	%r11d, %r11
	movq	$0, (%rax,%r11,8)
	addl	$2, %r9d
	incl	%r10d
	cmpl	100(%rsp), %r10d
	jne	.LBB136_10	# bb46
.LBB136_11:	# bb48
	addl	%edi, %ecx
	incl	%r8d
	cmpl	76(%rsp), %r8d
	je	.LBB136_20	# bb58
.LBB136_12:	# bb47.preheader
	xorl	%r10d, %r10d
	movl	%ecx, %r9d
	jmp	.LBB136_10	# bb46
.LBB136_13:	# bb50
	pxor	%xmm4, %xmm4
	ucomisd	%xmm4, %xmm1
	setnp	%cl
	sete	%dil
	andb	%cl, %dil
	ucomisd	.LCPI136_0(%rip), %xmm3
	setnp	%cl
	sete	%r8b
	andb	%cl, %r8b
	testb	%r8b, %dil
	jne	.LBB136_20	# bb58
.LBB136_14:	# bb50
	cmpl	$0, 76(%rsp)
	jle	.LBB136_20	# bb58
.LBB136_15:	# bb.nph175
	cmpl	$0, 100(%rsp)
	jle	.LBB136_20	# bb58
.LBB136_16:	# bb55.preheader.preheader
	movl	208(%rsp), %ecx
	leal	(%rcx,%rcx), %ecx
	xorl	%r8d, %r8d
	movl	%r8d, %edi
	.align	16
.LBB136_17:	# bb55.preheader
	xorl	%r9d, %r9d
	movl	%r8d, %r10d
	.align	16
.LBB136_18:	# bb54
	movslq	%r10d, %r11
	movsd	(%rax,%r11,8), %xmm4
	movapd	%xmm3, %xmm5
	mulsd	%xmm4, %xmm5
	leal	1(%r10), %ebx
	movslq	%ebx, %rbx
	movsd	(%rax,%rbx,8), %xmm6
	movapd	%xmm1, %xmm7
	mulsd	%xmm6, %xmm7
	subsd	%xmm7, %xmm5
	movsd	%xmm5, (%rax,%r11,8)
	mulsd	%xmm1, %xmm4
	mulsd	%xmm3, %xmm6
	addsd	%xmm4, %xmm6
	movsd	%xmm6, (%rax,%rbx,8)
	addl	$2, %r10d
	incl	%r9d
	cmpl	100(%rsp), %r9d
	jne	.LBB136_18	# bb54
.LBB136_19:	# bb56
	addl	%ecx, %r8d
	incl	%edi
	cmpl	76(%rsp), %edi
	jne	.LBB136_17	# bb55.preheader
.LBB136_20:	# bb58
	testb	$1, 96(%rsp)
	jne	.LBB136_66	# return
.LBB136_21:	# bb60
	cmpl	$121, %edx
	jne	.LBB136_32	# bb72
.LBB136_22:	# bb60
	cmpl	$141, %esi
	jne	.LBB136_32	# bb72
.LBB136_23:	# bb71.preheader
	cmpl	$0, 76(%rsp)
	jle	.LBB136_66	# return
.LBB136_24:	# bb.nph171
	cmpl	$0, 100(%rsp)
	jle	.LBB136_66	# return
.LBB136_25:	# bb69.preheader.preheader
	movl	168(%rsp), %ecx
	leal	2(,%rcx,2), %ecx
	movl	%ecx, 4(%rsp)
	movl	184(%rsp), %ecx
	leal	(%rcx,%rcx), %ecx
	movl	%ecx, 20(%rsp)
	movl	208(%rsp), %ecx
	leal	(%rcx,%rcx), %ecx
	movl	%ecx, 16(%rsp)
	movl	76(%rsp), %ecx
	leal	-1(%rcx), %ecx
	xorl	%edx, %edx
	movl	%edx, 64(%rsp)
	movl	%edx, 92(%rsp)
	movl	%edx, 80(%rsp)
	movl	%edx, 24(%rsp)
	jmp	.LBB136_31	# bb69.preheader
	.align	16
.LBB136_26:	# bb65
	movl	92(%rsp), %esi
	leal	(%rsi,%rdx), %esi
	movslq	%esi, %rsi
	movq	176(%rsp), %rdi
	movsd	(%rdi,%rsi,8), %xmm1
	movapd	%xmm2, %xmm3
	mulsd	%xmm1, %xmm3
	movl	32(%rsp), %esi
	leal	(%rsi,%rdx), %esi
	movslq	%esi, %rsi
	movsd	(%rdi,%rsi,8), %xmm4
	movapd	%xmm0, %xmm5
	mulsd	%xmm4, %xmm5
	addsd	%xmm3, %xmm5
	movq	160(%rsp), %rsi
	movq	40(%rsp), %rdi
	movsd	(%rsi,%rdi,8), %xmm3
	movapd	%xmm5, %xmm6
	mulsd	%xmm3, %xmm6
	mulsd	%xmm2, %xmm4
	mulsd	%xmm0, %xmm1
	subsd	%xmm4, %xmm1
	movq	48(%rsp), %rdi
	movsd	(%rsi,%rdi,8), %xmm4
	movapd	%xmm1, %xmm7
	mulsd	%xmm4, %xmm7
	subsd	%xmm6, %xmm7
	movl	80(%rsp), %esi
	leal	(%rsi,%rdx), %esi
	movslq	%esi, %rsi
	addsd	(%rax,%rsi,8), %xmm7
	movsd	%xmm7, (%rax,%rsi,8)
	mulsd	%xmm5, %xmm4
	mulsd	%xmm1, %xmm3
	addsd	%xmm4, %xmm3
	movl	36(%rsp), %edi
	leal	(%rdi,%rdx), %edi
	movslq	%edi, %rdi
	addsd	(%rax,%rdi,8), %xmm3
	movsd	%xmm3, (%rax,%rdi,8)
	movl	28(%rsp), %r8d
	cmpl	76(%rsp), %r8d
	jge	.LBB136_68	# bb65.bb68_crit_edge
.LBB136_27:	# bb.nph165
	movl	60(%rsp), %r8d
	leal	(%r8,%rdx), %r8d
	movl	56(%rsp), %r9d
	leal	(%r9,%rdx), %r9d
	movl	184(%rsp), %r10d
	leal	(%r10,%r10), %r10d
	movl	208(%rsp), %r11d
	leal	(%r11,%r11), %r11d
	pxor	%xmm3, %xmm3
	xorl	%ebx, %ebx
	movl	64(%rsp), %r14d
	movapd	%xmm3, %xmm4
	.align	16
.LBB136_28:	# bb66
	leal	3(%r14), %r15d
	movslq	%r15d, %r15
	movq	160(%rsp), %r12
	movsd	(%r12,%r15,8), %xmm6
	movapd	%xmm6, %xmm7
	mulsd	%xmm5, %xmm7
	addl	$2, %r14d
	movslq	%r14d, %r15
	movsd	(%r12,%r15,8), %xmm8
	movapd	%xmm8, %xmm9
	mulsd	%xmm1, %xmm9
	subsd	%xmm7, %xmm9
	movslq	%r9d, %r15
	addsd	(%rax,%r15,8), %xmm9
	movslq	%r8d, %r12
	movq	176(%rsp), %r13
	movsd	(%r13,%r12,8), %xmm7
	leal	1(%r8), %r12d
	movslq	%r12d, %r12
	movsd	(%r13,%r12,8), %xmm10
	movsd	%xmm9, (%rax,%r15,8)
	movapd	%xmm6, %xmm9
	mulsd	%xmm1, %xmm9
	movapd	%xmm8, %xmm11
	mulsd	%xmm5, %xmm11
	addsd	%xmm9, %xmm11
	leal	1(%r9), %r15d
	movslq	%r15d, %r15
	addsd	(%rax,%r15,8), %xmm11
	movsd	%xmm11, (%rax,%r15,8)
	movapd	%xmm6, %xmm9
	mulsd	%xmm7, %xmm9
	movapd	%xmm8, %xmm11
	mulsd	%xmm10, %xmm11
	addsd	%xmm9, %xmm11
	addsd	%xmm11, %xmm4
	mulsd	%xmm10, %xmm6
	mulsd	%xmm7, %xmm8
	subsd	%xmm6, %xmm8
	addsd	%xmm8, %xmm3
	addl	%r10d, %r8d
	addl	%r11d, %r9d
	incl	%ebx
	cmpl	%ecx, %ebx
	jne	.LBB136_28	# bb66
.LBB136_29:	# bb68
	movapd	%xmm2, %xmm1
	mulsd	%xmm4, %xmm1
	movapd	%xmm0, %xmm5
	mulsd	%xmm3, %xmm5
	subsd	%xmm1, %xmm5
	addsd	(%rax,%rsi,8), %xmm5
	movsd	%xmm5, (%rax,%rsi,8)
	mulsd	%xmm2, %xmm3
	mulsd	%xmm0, %xmm4
	addsd	%xmm3, %xmm4
	addsd	(%rax,%rdi,8), %xmm4
	movsd	%xmm4, (%rax,%rdi,8)
	addl	$2, %edx
	movl	96(%rsp), %esi
	incl	%esi
	movl	%esi, 96(%rsp)
	cmpl	100(%rsp), %esi
	jne	.LBB136_26	# bb65
.LBB136_30:	# bb70
	movl	64(%rsp), %edx
	addl	4(%rsp), %edx
	movl	%edx, 64(%rsp)
	movl	92(%rsp), %edx
	addl	20(%rsp), %edx
	movl	%edx, 92(%rsp)
	movl	80(%rsp), %edx
	addl	16(%rsp), %edx
	movl	%edx, 80(%rsp)
	decl	%ecx
	movl	24(%rsp), %edx
	incl	%edx
	movl	%edx, 24(%rsp)
	cmpl	76(%rsp), %edx
	je	.LBB136_66	# return
.LBB136_31:	# bb69.preheader
	movl	92(%rsp), %esi
	movl	20(%rsp), %edx
	leal	(%rdx,%rsi), %edx
	movl	%edx, 60(%rsp)
	movl	80(%rsp), %edi
	movl	16(%rsp), %edx
	leal	(%rdx,%rdi), %edx
	movl	%edx, 56(%rsp)
	movl	64(%rsp), %edx
	movslq	%edx, %r8
	movq	%r8, 48(%rsp)
	leal	1(%rdx), %edx
	movslq	%edx, %rdx
	movq	%rdx, 40(%rsp)
	leal	1(%rdi), %edx
	movl	%edx, 36(%rsp)
	leal	1(%rsi), %edx
	movl	%edx, 32(%rsp)
	movl	24(%rsp), %edx
	leal	1(%rdx), %edx
	movl	%edx, 28(%rsp)
	xorl	%edx, %edx
	movl	%edx, 96(%rsp)
	jmp	.LBB136_26	# bb65
.LBB136_32:	# bb72
	cmpl	$122, %edx
	jne	.LBB136_43	# bb85
.LBB136_33:	# bb72
	cmpl	$141, %esi
	jne	.LBB136_43	# bb85
.LBB136_34:	# bb84.preheader
	cmpl	$0, 76(%rsp)
	jle	.LBB136_66	# return
.LBB136_35:	# bb.nph159
	cmpl	$0, 100(%rsp)
	jle	.LBB136_66	# return
.LBB136_36:	# bb82.preheader.preheader
	movl	168(%rsp), %ecx
	leal	2(,%rcx,2), %esi
	movl	%esi, 48(%rsp)
	addl	%ecx, %ecx
	movl	%ecx, 16(%rsp)
	movl	184(%rsp), %ecx
	leal	(%rcx,%rcx), %ecx
	movl	%ecx, 40(%rsp)
	movl	208(%rsp), %ecx
	leal	(%rcx,%rcx), %ecx
	movl	%ecx, 36(%rsp)
	xorl	%ecx, %ecx
	movl	%ecx, 80(%rsp)
	movl	%ecx, 96(%rsp)
	movl	%ecx, 92(%rsp)
	movl	%ecx, %esi
	jmp	.LBB136_42	# bb82.preheader
	.align	16
.LBB136_37:	# bb78
	movl	96(%rsp), %r8d
	leal	(%r8,%rcx), %r8d
	movslq	%r8d, %r8
	movq	176(%rsp), %r9
	movsd	(%r9,%r8,8), %xmm1
	movapd	%xmm2, %xmm3
	mulsd	%xmm1, %xmm3
	movl	56(%rsp), %r8d
	leal	(%r8,%rcx), %r8d
	movslq	%r8d, %r8
	movsd	(%r9,%r8,8), %xmm4
	movapd	%xmm0, %xmm5
	mulsd	%xmm4, %xmm5
	addsd	%xmm3, %xmm5
	mulsd	%xmm2, %xmm4
	mulsd	%xmm0, %xmm1
	subsd	%xmm4, %xmm1
	testl	%esi, %esi
	jle	.LBB136_69	# bb78.bb81_crit_edge
.LBB136_38:	# bb79.preheader
	movl	184(%rsp), %r8d
	leal	(%r8,%r8), %r8d
	movl	208(%rsp), %r9d
	leal	(%r9,%r9), %r9d
	pxor	%xmm3, %xmm3
	xorl	%r10d, %r10d
	movl	%ecx, %r11d
	movl	%ecx, %ebx
	movl	80(%rsp), %r14d
	movapd	%xmm3, %xmm4
	.align	16
.LBB136_39:	# bb79
	movslq	%r14d, %r15
	movq	160(%rsp), %r12
	movsd	(%r12,%r15,8), %xmm6
	movapd	%xmm6, %xmm7
	mulsd	%xmm1, %xmm7
	leal	1(%r14), %r15d
	movslq	%r15d, %r15
	movsd	(%r12,%r15,8), %xmm8
	movapd	%xmm8, %xmm9
	mulsd	%xmm5, %xmm9
	subsd	%xmm9, %xmm7
	movslq	%ebx, %r15
	addsd	(%rax,%r15,8), %xmm7
	movslq	%r11d, %r12
	movq	176(%rsp), %r13
	movsd	(%r13,%r12,8), %xmm9
	leal	1(%r11), %r12d
	movslq	%r12d, %r12
	movsd	(%r13,%r12,8), %xmm10
	movsd	%xmm7, (%rax,%r15,8)
	movapd	%xmm6, %xmm7
	mulsd	%xmm5, %xmm7
	movapd	%xmm8, %xmm11
	mulsd	%xmm1, %xmm11
	addsd	%xmm7, %xmm11
	leal	1(%rbx), %r15d
	movslq	%r15d, %r15
	addsd	(%rax,%r15,8), %xmm11
	movsd	%xmm11, (%rax,%r15,8)
	movapd	%xmm8, %xmm7
	mulsd	%xmm9, %xmm7
	movapd	%xmm6, %xmm11
	mulsd	%xmm10, %xmm11
	addsd	%xmm7, %xmm11
	addsd	%xmm11, %xmm4
	mulsd	%xmm9, %xmm6
	mulsd	%xmm10, %xmm8
	subsd	%xmm8, %xmm6
	addsd	%xmm6, %xmm3
	addl	%r8d, %r11d
	addl	%r9d, %ebx
	addl	$2, %r14d
	incl	%r10d
	cmpl	%esi, %r10d
	jne	.LBB136_39	# bb79
.LBB136_40:	# bb81
	movq	160(%rsp), %r9
	movq	64(%rsp), %r8
	movsd	(%r9,%r8,8), %xmm6
	movapd	%xmm5, %xmm7
	mulsd	%xmm6, %xmm7
	movsd	(%r9,%rdx,8), %xmm8
	movapd	%xmm1, %xmm9
	mulsd	%xmm8, %xmm9
	subsd	%xmm7, %xmm9
	movl	92(%rsp), %r8d
	leal	(%r8,%rcx), %r8d
	movslq	%r8d, %r8
	addsd	(%rax,%r8,8), %xmm9
	movsd	%xmm9, (%rax,%r8,8)
	mulsd	%xmm8, %xmm5
	mulsd	%xmm6, %xmm1
	addsd	%xmm5, %xmm1
	movl	60(%rsp), %r9d
	leal	(%r9,%rcx), %r9d
	movslq	%r9d, %r9
	addsd	(%rax,%r9,8), %xmm1
	movsd	%xmm1, (%rax,%r9,8)
	movapd	%xmm2, %xmm1
	mulsd	%xmm4, %xmm1
	movapd	%xmm0, %xmm5
	mulsd	%xmm3, %xmm5
	subsd	%xmm1, %xmm5
	addsd	(%rax,%r8,8), %xmm5
	movsd	%xmm5, (%rax,%r8,8)
	mulsd	%xmm2, %xmm3
	mulsd	%xmm0, %xmm4
	addsd	%xmm3, %xmm4
	addsd	(%rax,%r9,8), %xmm4
	movsd	%xmm4, (%rax,%r9,8)
	addl	$2, %ecx
	incl	%edi
	cmpl	100(%rsp), %edi
	jne	.LBB136_37	# bb78
.LBB136_41:	# bb83
	movl	%edx, %ecx
	addl	48(%rsp), %ecx
	movl	80(%rsp), %edi
	addl	16(%rsp), %edi
	movl	%edi, 80(%rsp)
	movl	96(%rsp), %edi
	addl	40(%rsp), %edi
	movl	%edi, 96(%rsp)
	movl	92(%rsp), %edi
	addl	36(%rsp), %edi
	movl	%edi, 92(%rsp)
	incl	%esi
	cmpl	76(%rsp), %esi
	je	.LBB136_66	# return
.LBB136_42:	# bb82.preheader
	movslq	%ecx, %rdx
	incl	%ecx
	movslq	%ecx, %rcx
	movq	%rcx, 64(%rsp)
	movl	92(%rsp), %ecx
	leal	1(%rcx), %ecx
	movl	%ecx, 60(%rsp)
	movl	96(%rsp), %ecx
	leal	1(%rcx), %ecx
	movl	%ecx, 56(%rsp)
	xorl	%ecx, %ecx
	movl	%ecx, %edi
	jmp	.LBB136_37	# bb78
.LBB136_43:	# bb85
	cmpl	$121, %edx
	jne	.LBB136_54	# bb98
.LBB136_44:	# bb85
	cmpl	$142, %esi
	jne	.LBB136_54	# bb98
.LBB136_45:	# bb97.preheader
	cmpl	$0, 76(%rsp)
	jle	.LBB136_66	# return
.LBB136_46:	# bb.nph147
	cmpl	$0, 100(%rsp)
	jle	.LBB136_66	# return
.LBB136_47:	# bb95.preheader.preheader
	movl	184(%rsp), %ecx
	addl	%ecx, %ecx
	movl	%ecx, 12(%rsp)
	movl	208(%rsp), %ecx
	addl	%ecx, %ecx
	movl	%ecx, 8(%rsp)
	xorl	%ecx, %ecx
	movl	%ecx, 60(%rsp)
	movl	%ecx, 64(%rsp)
	movl	%ecx, 20(%rsp)
	jmp	.LBB136_53	# bb95.preheader
	.align	16
.LBB136_48:	# bb91
	movl	60(%rsp), %esi
	leal	(%rsi,%rcx), %esi
	movslq	%esi, %rsi
	movq	176(%rsp), %rdi
	movsd	(%rdi,%rsi,8), %xmm1
	movapd	%xmm2, %xmm3
	mulsd	%xmm1, %xmm3
	movl	24(%rsp), %esi
	leal	(%rsi,%rcx), %esi
	movslq	%esi, %rsi
	movsd	(%rdi,%rsi,8), %xmm4
	movapd	%xmm0, %xmm5
	mulsd	%xmm4, %xmm5
	addsd	%xmm3, %xmm5
	movl	96(%rsp), %esi
	movslq	%esi, %rdi
	leal	1(%rsi), %esi
	movslq	%esi, %rsi
	movq	160(%rsp), %r8
	movsd	(%r8,%rsi,8), %xmm3
	movapd	%xmm5, %xmm6
	mulsd	%xmm3, %xmm6
	mulsd	%xmm2, %xmm4
	mulsd	%xmm0, %xmm1
	subsd	%xmm4, %xmm1
	movsd	(%r8,%rdi,8), %xmm4
	movapd	%xmm1, %xmm7
	mulsd	%xmm4, %xmm7
	subsd	%xmm6, %xmm7
	movl	64(%rsp), %esi
	leal	(%rsi,%rcx), %esi
	movslq	%esi, %rsi
	movq	%rsi, 80(%rsp)
	addsd	(%rax,%rsi,8), %xmm7
	movsd	%xmm7, (%rax,%rsi,8)
	mulsd	%xmm5, %xmm4
	mulsd	%xmm1, %xmm3
	addsd	%xmm4, %xmm3
	movl	28(%rsp), %esi
	leal	(%rsi,%rcx), %esi
	movslq	%esi, %rsi
	addsd	(%rax,%rsi,8), %xmm3
	movsd	%xmm3, (%rax,%rsi,8)
	movl	92(%rsp), %edi
	leal	1(%rdi), %edi
	cmpl	100(%rsp), %edi
	jge	.LBB136_70	# bb91.bb94_crit_edge
.LBB136_49:	# bb.nph141
	movl	48(%rsp), %edi
	leal	(%rdi,%rcx), %edi
	movl	40(%rsp), %r8d
	leal	(%r8,%rcx), %r8d
	movl	36(%rsp), %r9d
	leal	(%r9,%rcx), %r9d
	movl	32(%rsp), %r10d
	leal	(%r10,%rcx), %r10d
	movl	96(%rsp), %r11d
	leal	3(%r11), %ebx
	leal	2(%r11), %r11d
	pxor	%xmm3, %xmm3
	xorl	%r14d, %r14d
	movl	%r14d, %r15d
	movapd	%xmm3, %xmm4
	.align	16
.LBB136_50:	# bb92
	leal	(%rbx,%r14), %r12d
	movslq	%r12d, %r12
	movq	160(%rsp), %r13
	movsd	(%r13,%r12,8), %xmm6
	movapd	%xmm5, %xmm7
	mulsd	%xmm6, %xmm7
	leal	(%r11,%r14), %r12d
	movslq	%r12d, %r12
	movsd	(%r13,%r12,8), %xmm8
	movapd	%xmm1, %xmm9
	mulsd	%xmm8, %xmm9
	subsd	%xmm7, %xmm9
	leal	(%r8,%r14), %r12d
	movslq	%r12d, %r12
	addsd	(%rax,%r12,8), %xmm9
	leal	(%r9,%r14), %r13d
	movslq	%r13d, %r13
	movq	176(%rsp), %rbp
	movsd	(%rbp,%r13,8), %xmm7
	leal	(%r10,%r14), %r13d
	movslq	%r13d, %r13
	movsd	(%rbp,%r13,8), %xmm10
	movsd	%xmm9, (%rax,%r12,8)
	movapd	%xmm5, %xmm9
	mulsd	%xmm8, %xmm9
	movapd	%xmm1, %xmm11
	mulsd	%xmm6, %xmm11
	addsd	%xmm9, %xmm11
	leal	(%rdi,%r14), %r12d
	movslq	%r12d, %r12
	addsd	(%rax,%r12,8), %xmm11
	movsd	%xmm11, (%rax,%r12,8)
	movapd	%xmm7, %xmm9
	mulsd	%xmm8, %xmm9
	movapd	%xmm10, %xmm11
	mulsd	%xmm6, %xmm11
	addsd	%xmm9, %xmm11
	addsd	%xmm11, %xmm4
	mulsd	%xmm6, %xmm7
	mulsd	%xmm8, %xmm10
	subsd	%xmm7, %xmm10
	addsd	%xmm10, %xmm3
	addl	$2, %r14d
	incl	%r15d
	cmpl	%edx, %r15d
	jne	.LBB136_50	# bb92
.LBB136_51:	# bb94
	movapd	%xmm2, %xmm1
	mulsd	%xmm4, %xmm1
	movapd	%xmm0, %xmm5
	mulsd	%xmm3, %xmm5
	subsd	%xmm1, %xmm5
	movq	80(%rsp), %rdi
	addsd	(%rax,%rdi,8), %xmm5
	movsd	%xmm5, (%rax,%rdi,8)
	mulsd	%xmm2, %xmm3
	mulsd	%xmm0, %xmm4
	addsd	%xmm3, %xmm4
	addsd	(%rax,%rsi,8), %xmm4
	movsd	%xmm4, (%rax,%rsi,8)
	movl	96(%rsp), %esi
	addl	56(%rsp), %esi
	movl	%esi, 96(%rsp)
	addl	$2, %ecx
	decl	%edx
	movl	92(%rsp), %esi
	incl	%esi
	movl	%esi, 92(%rsp)
	cmpl	100(%rsp), %esi
	jne	.LBB136_48	# bb91
.LBB136_52:	# bb96
	movl	60(%rsp), %ecx
	addl	12(%rsp), %ecx
	movl	%ecx, 60(%rsp)
	movl	64(%rsp), %ecx
	addl	8(%rsp), %ecx
	movl	%ecx, 64(%rsp)
	movl	20(%rsp), %ecx
	incl	%ecx
	movl	%ecx, 20(%rsp)
	cmpl	76(%rsp), %ecx
	je	.LBB136_66	# return
.LBB136_53:	# bb95.preheader
	movl	168(%rsp), %ecx
	leal	2(,%rcx,2), %ecx
	movl	%ecx, 56(%rsp)
	movl	64(%rsp), %ecx
	leal	3(%rcx), %edx
	movl	%edx, 48(%rsp)
	leal	2(%rcx), %edx
	movl	%edx, 40(%rsp)
	movl	60(%rsp), %edx
	leal	3(%rdx), %esi
	movl	%esi, 36(%rsp)
	leal	2(%rdx), %esi
	movl	%esi, 32(%rsp)
	leal	1(%rcx), %ecx
	movl	%ecx, 28(%rsp)
	leal	1(%rdx), %ecx
	movl	%ecx, 24(%rsp)
	movl	100(%rsp), %ecx
	leal	-1(%rcx), %edx
	xorl	%esi, %esi
	movl	%esi, 96(%rsp)
	movl	%esi, %ecx
	movl	%esi, 92(%rsp)
	jmp	.LBB136_48	# bb91
.LBB136_54:	# bb98
	cmpl	$122, %edx
	jne	.LBB136_65	# bb111
.LBB136_55:	# bb98
	cmpl	$142, %esi
	jne	.LBB136_65	# bb111
.LBB136_56:	# bb110.preheader
	cmpl	$0, 76(%rsp)
	jle	.LBB136_66	# return
.LBB136_57:	# bb.nph135
	cmpl	$0, 100(%rsp)
	jle	.LBB136_66	# return
.LBB136_58:	# bb108.preheader.preheader
	movl	184(%rsp), %ecx
	addl	%ecx, %ecx
	movl	%ecx, 12(%rsp)
	movl	208(%rsp), %ecx
	addl	%ecx, %ecx
	movl	%ecx, 8(%rsp)
	xorl	%edx, %edx
	movl	%edx, %esi
	movl	%edx, 80(%rsp)
	jmp	.LBB136_64	# bb108.preheader
	.align	16
.LBB136_59:	# bb104
	leal	(%rdx,%r10), %ebx
	movslq	%ebx, %rbx
	movq	176(%rsp), %r14
	movsd	(%r14,%rbx,8), %xmm1
	movapd	%xmm2, %xmm3
	mulsd	%xmm1, %xmm3
	leal	(%r8,%r10), %ebx
	movslq	%ebx, %rbx
	movsd	(%r14,%rbx,8), %xmm4
	movapd	%xmm0, %xmm5
	mulsd	%xmm4, %xmm5
	addsd	%xmm3, %xmm5
	mulsd	%xmm2, %xmm4
	mulsd	%xmm0, %xmm1
	subsd	%xmm4, %xmm1
	testl	%r11d, %r11d
	jle	.LBB136_71	# bb104.bb107_crit_edge
.LBB136_60:	# bb.nph128
	leal	1(%rdi), %ebx
	pxor	%xmm3, %xmm3
	xorl	%r14d, %r14d
	movl	%r14d, %r15d
	movapd	%xmm3, %xmm4
	.align	16
.LBB136_61:	# bb105
	leal	(%rbx,%r14), %r12d
	movslq	%r12d, %r12
	movq	160(%rsp), %r13
	movsd	(%r13,%r12,8), %xmm6
	movapd	%xmm5, %xmm7
	mulsd	%xmm6, %xmm7
	leal	(%rdi,%r14), %r12d
	movslq	%r12d, %r12
	movsd	(%r13,%r12,8), %xmm8
	movapd	%xmm1, %xmm9
	mulsd	%xmm8, %xmm9
	subsd	%xmm7, %xmm9
	leal	(%rsi,%r14), %r12d
	movslq	%r12d, %r12
	addsd	(%rax,%r12,8), %xmm9
	leal	(%r8,%r14), %r13d
	movslq	%r13d, %r13
	movq	176(%rsp), %rbp
	movsd	(%rbp,%r13,8), %xmm7
	leal	(%rdx,%r14), %r13d
	movslq	%r13d, %r13
	movsd	(%rbp,%r13,8), %xmm10
	movsd	%xmm9, (%rax,%r12,8)
	movapd	%xmm5, %xmm9
	mulsd	%xmm8, %xmm9
	movapd	%xmm1, %xmm11
	mulsd	%xmm6, %xmm11
	addsd	%xmm9, %xmm11
	leal	(%rcx,%r14), %r12d
	movslq	%r12d, %r12
	addsd	(%rax,%r12,8), %xmm11
	movsd	%xmm11, (%rax,%r12,8)
	movapd	%xmm7, %xmm9
	mulsd	%xmm8, %xmm9
	movapd	%xmm10, %xmm11
	mulsd	%xmm6, %xmm11
	addsd	%xmm9, %xmm11
	addsd	%xmm11, %xmm4
	mulsd	%xmm6, %xmm7
	mulsd	%xmm8, %xmm10
	subsd	%xmm7, %xmm10
	addsd	%xmm10, %xmm3
	addl	$2, %r14d
	incl	%r15d
	cmpl	%r11d, %r15d
	jne	.LBB136_61	# bb105
.LBB136_62:	# bb107
	movslq	%r9d, %rbx
	movq	160(%rsp), %r14
	movsd	(%r14,%rbx,8), %xmm6
	movapd	%xmm1, %xmm7
	mulsd	%xmm6, %xmm7
	leal	1(%r9), %ebx
	movslq	%ebx, %rbx
	movsd	(%r14,%rbx,8), %xmm8
	movapd	%xmm5, %xmm9
	mulsd	%xmm8, %xmm9
	subsd	%xmm9, %xmm7
	leal	(%rsi,%r10), %ebx
	movslq	%ebx, %rbx
	addsd	(%rax,%rbx,8), %xmm7
	movsd	%xmm7, (%rax,%rbx,8)
	mulsd	%xmm6, %xmm5
	mulsd	%xmm8, %xmm1
	addsd	%xmm5, %xmm1
	leal	(%rcx,%r10), %r14d
	movslq	%r14d, %r14
	addsd	(%rax,%r14,8), %xmm1
	movsd	%xmm1, (%rax,%r14,8)
	movapd	%xmm2, %xmm1
	mulsd	%xmm4, %xmm1
	movapd	%xmm0, %xmm5
	mulsd	%xmm3, %xmm5
	subsd	%xmm1, %xmm5
	addsd	(%rax,%rbx,8), %xmm5
	movsd	%xmm5, (%rax,%rbx,8)
	mulsd	%xmm2, %xmm3
	mulsd	%xmm0, %xmm4
	addsd	%xmm3, %xmm4
	addsd	(%rax,%r14,8), %xmm4
	movsd	%xmm4, (%rax,%r14,8)
	addl	96(%rsp), %r9d
	addl	92(%rsp), %edi
	addl	$2, %r10d
	incl	%r11d
	cmpl	100(%rsp), %r11d
	jne	.LBB136_59	# bb104
.LBB136_63:	# bb109
	addl	12(%rsp), %edx
	addl	8(%rsp), %esi
	movl	80(%rsp), %ecx
	incl	%ecx
	movl	%ecx, 80(%rsp)
	cmpl	76(%rsp), %ecx
	je	.LBB136_66	# return
.LBB136_64:	# bb108.preheader
	leal	1(%rdx), %r8d
	leal	1(%rsi), %ecx
	movl	168(%rsp), %edi
	leal	2(,%rdi,2), %r9d
	movl	%r9d, 96(%rsp)
	leal	(%rdi,%rdi), %edi
	movl	%edi, 92(%rsp)
	xorl	%r9d, %r9d
	movl	%r9d, %edi
	movl	%r9d, %r10d
	movl	%r9d, %r11d
	jmp	.LBB136_59	# bb104
.LBB136_65:	# bb111
	xorl	%edi, %edi
	leaq	.str177, %rsi
	leaq	.str1178, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB136_66:	# return
	addq	$104, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB136_67:	# bb33.bb42_crit_edge
	movl	%r8d, 100(%rsp)
	jmp	.LBB136_5	# bb42
.LBB136_68:	# bb65.bb68_crit_edge
	pxor	%xmm3, %xmm3
	movapd	%xmm3, %xmm4
	jmp	.LBB136_29	# bb68
.LBB136_69:	# bb78.bb81_crit_edge
	pxor	%xmm3, %xmm3
	movapd	%xmm3, %xmm4
	jmp	.LBB136_40	# bb81
.LBB136_70:	# bb91.bb94_crit_edge
	pxor	%xmm3, %xmm3
	movapd	%xmm3, %xmm4
	jmp	.LBB136_51	# bb94
.LBB136_71:	# bb104.bb107_crit_edge
	pxor	%xmm3, %xmm3
	movapd	%xmm3, %xmm4
	jmp	.LBB136_62	# bb107
	.size	cblas_zsymm, .-cblas_zsymm
.Leh_func_end94:


	.section	.rodata.cst8,"aM",@progbits,8
	.align	8
.LCPI137_0:					
	.quad	4607182418800017408	# double value: 1.000000e+00
	.text
	.align	16
	.globl	cblas_zsyr2k
	.type	cblas_zsyr2k,@function
cblas_zsyr2k:
.Leh_func_begin95:
.Llabel95:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$56, %rsp
	movl	%r8d, 52(%rsp)
	movl	%edx, 48(%rsp)
	movsd	(%r9), %xmm0
	pxor	%xmm1, %xmm1
	ucomisd	%xmm1, %xmm0
	setp	%al
	setne	%dl
	setnp	%r8b
	sete	%r10b
	andb	%r8b, %r10b
	movsd	8(%r9), %xmm2
	ucomisd	%xmm1, %xmm2
	setp	%r8b
	setnp	%r9b
	sete	%r11b
	setne	%bl
	andb	%r9b, %r11b
	andb	%r10b, %r11b
	movb	%r11b, 44(%rsp)
	orb	%al, %dl
	orb	%r8b, %bl
	orb	%dl, %bl
	testb	%bl, %bl
	movq	144(%rsp), %rax
	movsd	8(%rax), %xmm1
	movsd	(%rax), %xmm3
	movq	152(%rsp), %rax
	movq	128(%rsp), %rdx
	movq	112(%rsp), %r8
	jne	.LBB137_3	# bb27
.LBB137_1:	# entry
	ucomisd	.LCPI137_0(%rip), %xmm3
	jne	.LBB137_3	# bb27
	jp	.LBB137_3	# bb27
.LBB137_2:	# entry
	pxor	%xmm4, %xmm4
	ucomisd	%xmm4, %xmm1
	setnp	%r9b
	sete	%r10b
	testb	%r9b, %r10b
	jne	.LBB137_83	# return
.LBB137_3:	# bb27
	cmpl	$101, %edi
	je	.LBB137_5	# bb36
.LBB137_4:	# bb29
	cmpl	$111, 48(%rsp)
	movl	$112, %edi
	movl	$111, %r9d
	cmove	%edi, %r9d
	movl	%r9d, 48(%rsp)
	cmpl	$121, %esi
	movl	$122, %edi
	movl	$121, %esi
	cmove	%edi, %esi
.LBB137_5:	# bb36
	pxor	%xmm4, %xmm4
	ucomisd	%xmm4, %xmm1
	jne	.LBB137_25	# bb51
	jp	.LBB137_25	# bb51
.LBB137_6:	# bb36
	pxor	%xmm4, %xmm4
	ucomisd	%xmm4, %xmm3
	jne	.LBB137_25	# bb51
	jp	.LBB137_25	# bb51
.LBB137_7:	# bb38
	cmpl	$121, %esi
	je	.LBB137_17	# bb44.preheader
.LBB137_8:	# bb50.preheader
	testl	%ecx, %ecx
	jle	.LBB137_12	# bb66
.LBB137_9:	# bb.nph134
	movl	160(%rsp), %edi
	leal	(%rdi,%rdi), %ebx
	xorl	%r10d, %r10d
	movl	%r10d, %r11d
	jmp	.LBB137_23	# bb48.preheader
	.align	16
.LBB137_10:	# bb41
	movslq	%ebx, %r15
	movq	$0, (%rax,%r15,8)
	leal	1(%rbx), %r15d
	movslq	%r15d, %r15
	movq	$0, (%rax,%r15,8)
	addl	$2, %ebx
	incl	%edi
	cmpl	%r10d, %edi
	jne	.LBB137_10	# bb41
.LBB137_11:	# bb43
	addl	%r11d, %r14d
	decl	%r10d
	incl	%r9d
	cmpl	%ecx, %r9d
	jne	.LBB137_19	# bb42.preheader
.LBB137_12:	# bb66
	testb	$1, 44(%rsp)
	jne	.LBB137_83	# return
.LBB137_13:	# bb68
	cmpl	$121, %esi
	jne	.LBB137_48	# bb80
.LBB137_14:	# bb68
	cmpl	$111, 48(%rsp)
	jne	.LBB137_48	# bb80
.LBB137_15:	# bb79.preheader
	testl	%ecx, %ecx
	jle	.LBB137_83	# return
.LBB137_16:	# bb.nph174
	movl	160(%rsp), %esi
	leal	2(,%rsi,2), %esi
	movl	%esi, 16(%rsp)
	movl	136(%rsp), %esi
	leal	(%rsi,%rsi), %esi
	movl	%esi, 12(%rsp)
	movl	120(%rsp), %esi
	leal	(%rsi,%rsi), %esi
	movl	%esi, 8(%rsp)
	xorl	%r9d, %r9d
	movl	%r9d, 24(%rsp)
	movl	%r9d, %esi
	movl	%r9d, %edi
	movl	%ecx, 40(%rsp)
	movl	%r9d, 28(%rsp)
	jmp	.LBB137_44	# bb77.preheader
.LBB137_17:	# bb44.preheader
	testl	%ecx, %ecx
	jle	.LBB137_12	# bb66
.LBB137_18:	# bb.nph138
	movl	160(%rsp), %edi
	leal	2(,%rdi,2), %r11d
	xorl	%r14d, %r14d
	movl	%ecx, %r10d
	movl	%r14d, %r9d
	.align	16
.LBB137_19:	# bb42.preheader
	cmpl	%ecx, %r9d
	jge	.LBB137_11	# bb43
.LBB137_20:	# bb42.preheader.bb41_crit_edge
	xorl	%edi, %edi
	movl	%r14d, %ebx
	jmp	.LBB137_10	# bb41
	.align	16
.LBB137_21:	# bb47
	movslq	%edi, %r14
	movq	$0, (%rax,%r14,8)
	leal	1(%rdi), %r14d
	movslq	%r14d, %r14
	movq	$0, (%rax,%r14,8)
	addl	$2, %edi
	incl	%r9d
	cmpl	%r11d, %r9d
	jle	.LBB137_21	# bb47
.LBB137_22:	# bb49
	addl	%ebx, %r10d
	incl	%r11d
	cmpl	%ecx, %r11d
	je	.LBB137_12	# bb66
.LBB137_23:	# bb48.preheader
	testl	%r11d, %r11d
	js	.LBB137_22	# bb49
.LBB137_24:	# bb48.preheader.bb47_crit_edge
	xorl	%r9d, %r9d
	movl	%r10d, %edi
	jmp	.LBB137_21	# bb47
.LBB137_25:	# bb51
	pxor	%xmm4, %xmm4
	ucomisd	%xmm4, %xmm1
	jne	.LBB137_27	# bb53
	jp	.LBB137_27	# bb53
.LBB137_26:	# bb51
	ucomisd	.LCPI137_0(%rip), %xmm3
	setnp	%dil
	sete	%r9b
	testb	%dil, %r9b
	jne	.LBB137_12	# bb66
.LBB137_27:	# bb53
	cmpl	$121, %esi
	je	.LBB137_32	# bb59.preheader
.LBB137_28:	# bb65.preheader
	testl	%ecx, %ecx
	jle	.LBB137_12	# bb66
.LBB137_29:	# bb.nph142
	movl	160(%rsp), %edi
	leal	(%rdi,%rdi), %r10d
	xorl	%ebx, %ebx
	movl	%ebx, %edi
	jmp	.LBB137_38	# bb63.preheader
	.align	16
.LBB137_30:	# bb56
	movslq	%ebx, %r15
	movsd	(%rax,%r15,8), %xmm4
	movapd	%xmm3, %xmm5
	mulsd	%xmm4, %xmm5
	leal	1(%rbx), %r12d
	movslq	%r12d, %r12
	movsd	(%rax,%r12,8), %xmm6
	movapd	%xmm1, %xmm7
	mulsd	%xmm6, %xmm7
	subsd	%xmm7, %xmm5
	movsd	%xmm5, (%rax,%r15,8)
	mulsd	%xmm1, %xmm4
	mulsd	%xmm3, %xmm6
	addsd	%xmm4, %xmm6
	movsd	%xmm6, (%rax,%r12,8)
	addl	$2, %ebx
	incl	%r14d
	cmpl	%r10d, %r14d
	jne	.LBB137_30	# bb56
.LBB137_31:	# bb58
	addl	%r11d, %r9d
	decl	%r10d
	incl	%edi
	cmpl	%ecx, %edi
	je	.LBB137_12	# bb66
	jmp	.LBB137_34	# bb57.preheader
.LBB137_32:	# bb59.preheader
	testl	%ecx, %ecx
	jle	.LBB137_12	# bb66
.LBB137_33:	# bb.nph178
	movl	160(%rsp), %edi
	leal	2(,%rdi,2), %r11d
	xorl	%r9d, %r9d
	movl	%ecx, %r10d
	movl	%r9d, %edi
	.align	16
.LBB137_34:	# bb57.preheader
	cmpl	%ecx, %edi
	jge	.LBB137_31	# bb58
.LBB137_35:	# bb57.preheader.bb56_crit_edge
	xorl	%r14d, %r14d
	movl	%r9d, %ebx
	jmp	.LBB137_30	# bb56
	.align	16
.LBB137_36:	# bb62
	movslq	%r11d, %r14
	movsd	(%rax,%r14,8), %xmm4
	movapd	%xmm3, %xmm5
	mulsd	%xmm4, %xmm5
	leal	1(%r11), %r15d
	movslq	%r15d, %r15
	movsd	(%rax,%r15,8), %xmm6
	movapd	%xmm1, %xmm7
	mulsd	%xmm6, %xmm7
	subsd	%xmm7, %xmm5
	movsd	%xmm5, (%rax,%r14,8)
	mulsd	%xmm1, %xmm4
	mulsd	%xmm3, %xmm6
	addsd	%xmm4, %xmm6
	movsd	%xmm6, (%rax,%r15,8)
	addl	$2, %r11d
	incl	%r9d
	cmpl	%edi, %r9d
	jle	.LBB137_36	# bb62
.LBB137_37:	# bb64
	addl	%r10d, %ebx
	incl	%edi
	cmpl	%ecx, %edi
	je	.LBB137_12	# bb66
.LBB137_38:	# bb63.preheader
	testl	%edi, %edi
	js	.LBB137_37	# bb64
.LBB137_39:	# bb63.preheader.bb62_crit_edge
	xorl	%r9d, %r9d
	movl	%ebx, %r11d
	jmp	.LBB137_36	# bb62
.LBB137_40:	# bb.nph168
	leal	1(%r9), %r14d
	leal	1(%rbx), %r15d
	pxor	%xmm1, %xmm1
	xorl	%r12d, %r12d
	movl	%r12d, %r13d
	movapd	%xmm1, %xmm3
	.align	16
.LBB137_41:	# bb74
	leal	(%rbx,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r8,%rbp,8), %xmm4
	leal	(%r11,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%rdx,%rbp,8), %xmm5
	movapd	%xmm5, %xmm6
	mulsd	%xmm4, %xmm6
	leal	(%r15,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r8,%rbp,8), %xmm7
	leal	(%rsi,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%rdx,%rbp,8), %xmm8
	movapd	%xmm8, %xmm9
	mulsd	%xmm7, %xmm9
	addsd	%xmm6, %xmm9
	leal	(%r9,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%rdx,%rbp,8), %xmm6
	leal	(%r10,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r8,%rbp,8), %xmm10
	movapd	%xmm10, %xmm11
	mulsd	%xmm6, %xmm11
	leal	(%r14,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%rdx,%rbp,8), %xmm12
	leal	(%rdi,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r8,%rbp,8), %xmm13
	movapd	%xmm13, %xmm14
	mulsd	%xmm12, %xmm14
	addsd	%xmm11, %xmm14
	addsd	%xmm9, %xmm14
	addsd	%xmm14, %xmm3
	mulsd	%xmm7, %xmm5
	mulsd	%xmm4, %xmm8
	subsd	%xmm5, %xmm8
	mulsd	%xmm12, %xmm10
	mulsd	%xmm6, %xmm13
	subsd	%xmm10, %xmm13
	addsd	%xmm8, %xmm13
	addsd	%xmm13, %xmm1
	addl	$2, %r12d
	incl	%r13d
	cmpl	52(%rsp), %r13d
	jne	.LBB137_41	# bb74
.LBB137_42:	# bb76
	movapd	%xmm2, %xmm4
	mulsd	%xmm3, %xmm4
	movapd	%xmm0, %xmm5
	mulsd	%xmm1, %xmm5
	subsd	%xmm4, %xmm5
	movl	48(%rsp), %r14d
	movslq	%r14d, %r15
	addsd	(%rax,%r15,8), %xmm5
	movsd	%xmm5, (%rax,%r15,8)
	mulsd	%xmm2, %xmm1
	mulsd	%xmm0, %xmm3
	addsd	%xmm1, %xmm3
	leal	1(%r14), %r15d
	movslq	%r15d, %r15
	addsd	(%rax,%r15,8), %xmm3
	movsd	%xmm3, (%rax,%r15,8)
	addl	36(%rsp), %ebx
	addl	32(%rsp), %r9d
	addl	$2, %r14d
	movl	%r14d, 48(%rsp)
	movl	44(%rsp), %r14d
	incl	%r14d
	movl	%r14d, 44(%rsp)
	cmpl	40(%rsp), %r14d
	jne	.LBB137_46	# bb75.preheader
.LBB137_43:	# bb78
	movl	24(%rsp), %r9d
	addl	16(%rsp), %r9d
	movl	%r9d, 24(%rsp)
	addl	12(%rsp), %esi
	addl	8(%rsp), %edi
	decl	40(%rsp)
	movl	28(%rsp), %r9d
	incl	%r9d
	movl	%r9d, 28(%rsp)
	cmpl	%ecx, %r9d
	je	.LBB137_83	# return
.LBB137_44:	# bb77.preheader
	cmpl	%ecx, 28(%rsp)
	jge	.LBB137_43	# bb78
.LBB137_45:	# bb.nph172
	leal	1(%rsi), %r11d
	leal	1(%rdi), %r10d
	movl	120(%rsp), %r9d
	leal	(%r9,%r9), %r9d
	movl	%r9d, 36(%rsp)
	movl	136(%rsp), %r9d
	leal	(%r9,%r9), %r9d
	movl	%r9d, 32(%rsp)
	movl	$0, 44(%rsp)
	movl	%edi, %ebx
	movl	%esi, %r9d
	movl	24(%rsp), %r14d
	movl	%r14d, 48(%rsp)
	.align	16
.LBB137_46:	# bb75.preheader
	cmpl	$0, 52(%rsp)
	jg	.LBB137_40	# bb.nph168
.LBB137_47:	# bb75.preheader.bb76_crit_edge
	pxor	%xmm1, %xmm1
	movapd	%xmm1, %xmm3
	jmp	.LBB137_42	# bb76
.LBB137_48:	# bb80
	cmpl	$121, %esi
	jne	.LBB137_59	# bb93
.LBB137_49:	# bb80
	cmpl	$112, 48(%rsp)
	jne	.LBB137_59	# bb93
.LBB137_50:	# bb92.preheader
	cmpl	$0, 52(%rsp)
	jle	.LBB137_83	# return
.LBB137_51:	# bb.nph164
	testl	%ecx, %ecx
	jle	.LBB137_83	# return
.LBB137_52:	# bb90.preheader.preheader
	movl	120(%rsp), %esi
	leal	(%rsi,%rsi), %esi
	movl	%esi, 16(%rsp)
	movl	136(%rsp), %esi
	addl	%esi, %esi
	movl	%esi, 20(%rsp)
	xorl	%esi, %esi
	movl	%esi, 40(%rsp)
	movl	%esi, 44(%rsp)
	movl	%esi, 24(%rsp)
	jmp	.LBB137_58	# bb90.preheader
	.align	16
.LBB137_53:	# bb86
	movl	44(%rsp), %r10d
	leal	(%r10,%r9), %r10d
	movslq	%r10d, %r10
	movsd	(%rdx,%r10,8), %xmm1
	movapd	%xmm2, %xmm3
	mulsd	%xmm1, %xmm3
	movl	36(%rsp), %r10d
	leal	(%r10,%r9), %r10d
	movslq	%r10d, %r10
	movsd	(%rdx,%r10,8), %xmm4
	movapd	%xmm0, %xmm5
	mulsd	%xmm4, %xmm5
	addsd	%xmm3, %xmm5
	mulsd	%xmm2, %xmm4
	mulsd	%xmm0, %xmm1
	subsd	%xmm4, %xmm1
	movl	40(%rsp), %r10d
	leal	(%r10,%r9), %r10d
	movslq	%r10d, %r10
	movsd	(%r8,%r10,8), %xmm3
	movapd	%xmm2, %xmm4
	mulsd	%xmm3, %xmm4
	movl	32(%rsp), %r10d
	leal	(%r10,%r9), %r10d
	movslq	%r10d, %r10
	movsd	(%r8,%r10,8), %xmm6
	movapd	%xmm0, %xmm7
	mulsd	%xmm6, %xmm7
	addsd	%xmm4, %xmm7
	mulsd	%xmm2, %xmm6
	mulsd	%xmm0, %xmm3
	subsd	%xmm6, %xmm3
	cmpl	%ecx, 48(%rsp)
	jge	.LBB137_56	# bb89
.LBB137_54:	# bb.nph160
	movl	36(%rsp), %r10d
	leal	(%r10,%r9), %r10d
	movl	44(%rsp), %r11d
	leal	(%r11,%r9), %r11d
	movl	32(%rsp), %ebx
	leal	(%rbx,%r9), %ebx
	movl	40(%rsp), %r14d
	leal	(%r14,%r9), %r14d
	leal	1(%rdi), %r15d
	xorl	%r12d, %r12d
	movl	%r12d, %r13d
	.align	16
.LBB137_55:	# bb87
	leal	(%rbx,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r8,%rbp,8), %xmm4
	movapd	%xmm5, %xmm6
	mulsd	%xmm4, %xmm6
	leal	(%r14,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r8,%rbp,8), %xmm8
	movapd	%xmm1, %xmm9
	mulsd	%xmm8, %xmm9
	subsd	%xmm6, %xmm9
	leal	(%r10,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%rdx,%rbp,8), %xmm6
	movapd	%xmm7, %xmm10
	mulsd	%xmm6, %xmm10
	leal	(%r11,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%rdx,%rbp,8), %xmm11
	movapd	%xmm3, %xmm12
	mulsd	%xmm11, %xmm12
	subsd	%xmm10, %xmm12
	addsd	%xmm9, %xmm12
	leal	(%rdi,%r12), %ebp
	movslq	%ebp, %rbp
	addsd	(%rax,%rbp,8), %xmm12
	movsd	%xmm12, (%rax,%rbp,8)
	mulsd	%xmm5, %xmm8
	mulsd	%xmm1, %xmm4
	addsd	%xmm8, %xmm4
	mulsd	%xmm7, %xmm11
	mulsd	%xmm3, %xmm6
	addsd	%xmm11, %xmm6
	addsd	%xmm4, %xmm6
	leal	(%r15,%r12), %ebp
	movslq	%ebp, %rbp
	addsd	(%rax,%rbp,8), %xmm6
	movsd	%xmm6, (%rax,%rbp,8)
	addl	$2, %r12d
	incl	%r13d
	cmpl	%esi, %r13d
	jne	.LBB137_55	# bb87
.LBB137_56:	# bb89
	addl	28(%rsp), %edi
	addl	$2, %r9d
	decl	%esi
	movl	48(%rsp), %r10d
	incl	%r10d
	movl	%r10d, 48(%rsp)
	cmpl	%ecx, %r10d
	jne	.LBB137_53	# bb86
.LBB137_57:	# bb91
	movl	40(%rsp), %esi
	addl	16(%rsp), %esi
	movl	%esi, 40(%rsp)
	movl	44(%rsp), %esi
	addl	20(%rsp), %esi
	movl	%esi, 44(%rsp)
	movl	24(%rsp), %esi
	incl	%esi
	movl	%esi, 24(%rsp)
	cmpl	52(%rsp), %esi
	je	.LBB137_83	# return
.LBB137_58:	# bb90.preheader
	movl	120(%rsp), %esi
	leal	2(,%rsi,2), %esi
	movl	%esi, 28(%rsp)
	movl	44(%rsp), %esi
	leal	1(%rsi), %esi
	movl	%esi, 36(%rsp)
	movl	40(%rsp), %esi
	leal	1(%rsi), %esi
	movl	%esi, 32(%rsp)
	xorl	%edi, %edi
	movl	%edi, %r9d
	movl	%ecx, %esi
	movl	%edi, 48(%rsp)
	jmp	.LBB137_53	# bb86
.LBB137_59:	# bb93
	cmpl	$122, %esi
	jne	.LBB137_71	# bb106
.LBB137_60:	# bb93
	cmpl	$111, 48(%rsp)
	jne	.LBB137_71	# bb106
.LBB137_61:	# bb105.preheader
	testl	%ecx, %ecx
	jle	.LBB137_83	# return
.LBB137_62:	# bb.nph158
	movl	120(%rsp), %esi
	leal	(%rsi,%rsi), %esi
	movl	%esi, 24(%rsp)
	movl	136(%rsp), %esi
	leal	(%rsi,%rsi), %esi
	movl	%esi, 16(%rsp)
	movl	160(%rsp), %esi
	addl	%esi, %esi
	movl	%esi, 12(%rsp)
	xorl	%r11d, %r11d
	movl	%r11d, %esi
	movl	%r11d, 28(%rsp)
	movl	%r11d, 40(%rsp)
	jmp	.LBB137_67	# bb103.preheader
.LBB137_63:	# bb.nph152
	leal	1(%r10), %r14d
	leal	1(%rbx), %r15d
	pxor	%xmm1, %xmm1
	xorl	%r12d, %r12d
	movl	%r12d, %r13d
	movapd	%xmm1, %xmm3
	.align	16
.LBB137_64:	# bb100
	leal	(%rbx,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r8,%rbp,8), %xmm4
	leal	(%rdi,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%rdx,%rbp,8), %xmm5
	movapd	%xmm5, %xmm6
	mulsd	%xmm4, %xmm6
	leal	(%r15,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r8,%rbp,8), %xmm7
	leal	(%rsi,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%rdx,%rbp,8), %xmm8
	movapd	%xmm8, %xmm9
	mulsd	%xmm7, %xmm9
	addsd	%xmm6, %xmm9
	leal	(%r10,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%rdx,%rbp,8), %xmm6
	leal	(%r9,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r8,%rbp,8), %xmm10
	movapd	%xmm10, %xmm11
	mulsd	%xmm6, %xmm11
	leal	(%r14,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%rdx,%rbp,8), %xmm12
	leal	(%r11,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r8,%rbp,8), %xmm13
	movapd	%xmm13, %xmm14
	mulsd	%xmm12, %xmm14
	addsd	%xmm11, %xmm14
	addsd	%xmm9, %xmm14
	addsd	%xmm14, %xmm3
	mulsd	%xmm7, %xmm5
	mulsd	%xmm4, %xmm8
	subsd	%xmm5, %xmm8
	mulsd	%xmm12, %xmm10
	mulsd	%xmm6, %xmm13
	subsd	%xmm10, %xmm13
	addsd	%xmm8, %xmm13
	addsd	%xmm13, %xmm1
	addl	$2, %r12d
	incl	%r13d
	cmpl	52(%rsp), %r13d
	jne	.LBB137_64	# bb100
.LBB137_65:	# bb102
	movapd	%xmm2, %xmm4
	mulsd	%xmm3, %xmm4
	movapd	%xmm0, %xmm5
	mulsd	%xmm1, %xmm5
	subsd	%xmm4, %xmm5
	movl	44(%rsp), %r14d
	movslq	%r14d, %r15
	addsd	(%rax,%r15,8), %xmm5
	movsd	%xmm5, (%rax,%r15,8)
	mulsd	%xmm2, %xmm1
	mulsd	%xmm0, %xmm3
	addsd	%xmm1, %xmm3
	leal	1(%r14), %r15d
	movslq	%r15d, %r15
	addsd	(%rax,%r15,8), %xmm3
	movsd	%xmm3, (%rax,%r15,8)
	addl	36(%rsp), %ebx
	addl	32(%rsp), %r10d
	addl	$2, %r14d
	movl	%r14d, 44(%rsp)
	movl	48(%rsp), %r14d
	incl	%r14d
	movl	%r14d, 48(%rsp)
	cmpl	40(%rsp), %r14d
	jle	.LBB137_69	# bb101.preheader
.LBB137_66:	# bb104
	addl	24(%rsp), %r11d
	addl	16(%rsp), %esi
	movl	28(%rsp), %edi
	addl	12(%rsp), %edi
	movl	%edi, 28(%rsp)
	movl	40(%rsp), %edi
	incl	%edi
	movl	%edi, 40(%rsp)
	cmpl	%ecx, %edi
	je	.LBB137_83	# return
.LBB137_67:	# bb103.preheader
	cmpl	$0, 40(%rsp)
	js	.LBB137_66	# bb104
.LBB137_68:	# bb.nph156
	leal	1(%r11), %r9d
	leal	1(%rsi), %edi
	movl	120(%rsp), %r10d
	leal	(%r10,%r10), %r10d
	movl	%r10d, 36(%rsp)
	movl	136(%rsp), %r10d
	leal	(%r10,%r10), %r10d
	movl	%r10d, 32(%rsp)
	xorl	%ebx, %ebx
	movl	%ebx, %r10d
	movl	28(%rsp), %r14d
	movl	%r14d, 44(%rsp)
	movl	%ebx, 48(%rsp)
	.align	16
.LBB137_69:	# bb101.preheader
	cmpl	$0, 52(%rsp)
	jg	.LBB137_63	# bb.nph152
.LBB137_70:	# bb101.preheader.bb102_crit_edge
	pxor	%xmm1, %xmm1
	movapd	%xmm1, %xmm3
	jmp	.LBB137_65	# bb102
.LBB137_71:	# bb106
	cmpl	$122, %esi
	jne	.LBB137_82	# bb119
.LBB137_72:	# bb106
	cmpl	$112, 48(%rsp)
	jne	.LBB137_82	# bb119
.LBB137_73:	# bb118.preheader
	cmpl	$0, 52(%rsp)
	jle	.LBB137_83	# return
.LBB137_74:	# bb.nph148
	testl	%ecx, %ecx
	jle	.LBB137_83	# return
.LBB137_75:	# bb116.preheader.preheader
	movl	120(%rsp), %esi
	leal	(%rsi,%rsi), %esi
	movl	%esi, 40(%rsp)
	movl	136(%rsp), %esi
	addl	%esi, %esi
	movl	%esi, 20(%rsp)
	xorl	%r10d, %r10d
	movl	%r10d, %edi
	movl	%r10d, 44(%rsp)
	jmp	.LBB137_81	# bb116.preheader
	.align	16
.LBB137_76:	# bb112
	leal	(%rdi,%r9), %r15d
	movslq	%r15d, %r15
	movsd	(%rdx,%r15,8), %xmm1
	movapd	%xmm2, %xmm3
	mulsd	%xmm1, %xmm3
	leal	(%r11,%r9), %r15d
	movslq	%r15d, %r15
	movsd	(%rdx,%r15,8), %xmm4
	movapd	%xmm0, %xmm5
	mulsd	%xmm4, %xmm5
	addsd	%xmm3, %xmm5
	mulsd	%xmm2, %xmm4
	mulsd	%xmm0, %xmm1
	subsd	%xmm4, %xmm1
	leal	(%r10,%r9), %r15d
	movslq	%r15d, %r15
	movsd	(%r8,%r15,8), %xmm3
	movapd	%xmm2, %xmm4
	mulsd	%xmm3, %xmm4
	leal	(%rbx,%r9), %r15d
	movslq	%r15d, %r15
	movsd	(%r8,%r15,8), %xmm6
	movapd	%xmm0, %xmm7
	mulsd	%xmm6, %xmm7
	addsd	%xmm4, %xmm7
	mulsd	%xmm2, %xmm6
	mulsd	%xmm0, %xmm3
	subsd	%xmm6, %xmm3
	testl	%esi, %esi
	js	.LBB137_79	# bb115
.LBB137_77:	# bb.nph144
	leal	1(%r14), %r15d
	xorl	%r12d, %r12d
	movl	%r12d, %r13d
	.align	16
.LBB137_78:	# bb113
	leal	(%rbx,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r8,%rbp,8), %xmm4
	movapd	%xmm5, %xmm6
	mulsd	%xmm4, %xmm6
	leal	(%r10,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r8,%rbp,8), %xmm8
	movapd	%xmm1, %xmm9
	mulsd	%xmm8, %xmm9
	subsd	%xmm6, %xmm9
	leal	(%r11,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%rdx,%rbp,8), %xmm6
	movapd	%xmm7, %xmm10
	mulsd	%xmm6, %xmm10
	leal	(%rdi,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%rdx,%rbp,8), %xmm11
	movapd	%xmm3, %xmm12
	mulsd	%xmm11, %xmm12
	subsd	%xmm10, %xmm12
	addsd	%xmm9, %xmm12
	leal	(%r14,%r12), %ebp
	movslq	%ebp, %rbp
	addsd	(%rax,%rbp,8), %xmm12
	movsd	%xmm12, (%rax,%rbp,8)
	mulsd	%xmm5, %xmm8
	mulsd	%xmm1, %xmm4
	addsd	%xmm8, %xmm4
	mulsd	%xmm7, %xmm11
	mulsd	%xmm3, %xmm6
	addsd	%xmm11, %xmm6
	addsd	%xmm4, %xmm6
	leal	(%r15,%r12), %ebp
	movslq	%ebp, %rbp
	addsd	(%rax,%rbp,8), %xmm6
	movsd	%xmm6, (%rax,%rbp,8)
	addl	$2, %r12d
	incl	%r13d
	cmpl	%esi, %r13d
	jle	.LBB137_78	# bb113
.LBB137_79:	# bb115
	addl	48(%rsp), %r14d
	addl	$2, %r9d
	incl	%esi
	cmpl	%ecx, %esi
	jne	.LBB137_76	# bb112
.LBB137_80:	# bb117
	addl	40(%rsp), %r10d
	addl	20(%rsp), %edi
	movl	44(%rsp), %esi
	incl	%esi
	movl	%esi, 44(%rsp)
	cmpl	52(%rsp), %esi
	je	.LBB137_83	# return
.LBB137_81:	# bb116.preheader
	leal	1(%r10), %ebx
	leal	1(%rdi), %r11d
	movl	120(%rsp), %esi
	leal	(%rsi,%rsi), %esi
	movl	%esi, 48(%rsp)
	xorl	%r14d, %r14d
	movl	%r14d, %r9d
	movl	%r14d, %esi
	jmp	.LBB137_76	# bb112
.LBB137_82:	# bb119
	xorl	%edi, %edi
	leaq	.str179, %rsi
	leaq	.str1180, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB137_83:	# return
	addq	$56, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
	.size	cblas_zsyr2k, .-cblas_zsyr2k
.Leh_func_end95:


	.section	.rodata.cst8,"aM",@progbits,8
	.align	8
.LCPI138_0:					
	.quad	4607182418800017408	# double value: 1.000000e+00
	.text
	.align	16
	.globl	cblas_zsyrk
	.type	cblas_zsyrk,@function
cblas_zsyrk:
.Leh_func_begin96:
.Llabel96:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$24, %rsp
	movsd	(%r9), %xmm0
	pxor	%xmm1, %xmm1
	ucomisd	%xmm1, %xmm0
	setp	%al
	setne	%r10b
	setnp	%r11b
	sete	%bl
	andb	%r11b, %bl
	movsd	8(%r9), %xmm2
	ucomisd	%xmm1, %xmm2
	setp	%r9b
	setnp	%r11b
	sete	%r14b
	setne	%r15b
	andb	%r11b, %r14b
	andb	%bl, %r14b
	movb	%r14b, 12(%rsp)
	orb	%al, %r10b
	orb	%r9b, %r15b
	orb	%r10b, %r15b
	testb	%r15b, %r15b
	movq	96(%rsp), %rax
	movsd	8(%rax), %xmm1
	movsd	(%rax), %xmm3
	movq	104(%rsp), %rax
	movq	80(%rsp), %r9
	jne	.LBB138_3	# bb19
.LBB138_1:	# entry
	ucomisd	.LCPI138_0(%rip), %xmm3
	jne	.LBB138_3	# bb19
	jp	.LBB138_3	# bb19
.LBB138_2:	# entry
	pxor	%xmm4, %xmm4
	ucomisd	%xmm4, %xmm1
	setnp	%r10b
	sete	%r11b
	testb	%r10b, %r11b
	jne	.LBB138_85	# return
.LBB138_3:	# bb19
	cmpl	$101, %edi
	je	.LBB138_86	# bb20
.LBB138_4:	# bb24
	cmpl	$111, %edx
	movl	$112, %edx
	movl	$111, %edi
	cmove	%edx, %edi
	movl	%edi, 20(%rsp)
	cmpl	$121, %esi
	movl	$122, %edx
	movl	$121, %esi
	cmove	%edx, %esi
.LBB138_5:	# bb31
	pxor	%xmm4, %xmm4
	ucomisd	%xmm4, %xmm1
	jne	.LBB138_25	# bb46
	jp	.LBB138_25	# bb46
.LBB138_6:	# bb31
	pxor	%xmm4, %xmm4
	ucomisd	%xmm4, %xmm3
	jne	.LBB138_25	# bb46
	jp	.LBB138_25	# bb46
.LBB138_7:	# bb33
	cmpl	$121, %esi
	je	.LBB138_17	# bb39.preheader
.LBB138_8:	# bb45.preheader
	testl	%ecx, %ecx
	jle	.LBB138_12	# bb61
.LBB138_9:	# bb.nph129
	movl	112(%rsp), %edx
	leal	(%rdx,%rdx), %ebx
	xorl	%r11d, %r11d
	movl	%r11d, %r10d
	jmp	.LBB138_23	# bb43.preheader
	.align	16
.LBB138_10:	# bb36
	movslq	%r10d, %r15
	movq	$0, (%rax,%r15,8)
	leal	1(%r10), %r15d
	movslq	%r15d, %r15
	movq	$0, (%rax,%r15,8)
	addl	$2, %r10d
	incl	%ebx
	cmpl	%r14d, %ebx
	jne	.LBB138_10	# bb36
.LBB138_11:	# bb38
	addl	%edx, %r11d
	decl	%r14d
	incl	%edi
	cmpl	%ecx, %edi
	jne	.LBB138_19	# bb37.preheader
.LBB138_12:	# bb61
	testb	$1, 12(%rsp)
	jne	.LBB138_85	# return
.LBB138_13:	# bb63
	cmpl	$121, %esi
	jne	.LBB138_48	# bb75
.LBB138_14:	# bb63
	cmpl	$111, 20(%rsp)
	jne	.LBB138_48	# bb75
.LBB138_15:	# bb74.preheader
	testl	%ecx, %ecx
	jle	.LBB138_85	# return
.LBB138_16:	# bb.nph177
	movl	88(%rsp), %esi
	leal	(%rsi,%rsi), %esi
	movl	%esi, 8(%rsp)
	movl	112(%rsp), %esi
	leal	2(,%rsi,2), %esi
	movl	%esi, 4(%rsp)
	xorl	%edx, %edx
	movl	%edx, 20(%rsp)
	movl	%edx, 16(%rsp)
	movl	%ecx, %esi
	movl	%edx, 12(%rsp)
	jmp	.LBB138_44	# bb72.preheader
.LBB138_17:	# bb39.preheader
	testl	%ecx, %ecx
	jle	.LBB138_12	# bb61
.LBB138_18:	# bb.nph133
	movl	112(%rsp), %edx
	leal	2(,%rdx,2), %edx
	xorl	%r11d, %r11d
	movl	%ecx, %r14d
	movl	%r11d, %edi
	.align	16
.LBB138_19:	# bb37.preheader
	cmpl	%ecx, %edi
	jge	.LBB138_11	# bb38
.LBB138_20:	# bb37.preheader.bb36_crit_edge
	xorl	%ebx, %ebx
	movl	%r11d, %r10d
	jmp	.LBB138_10	# bb36
	.align	16
.LBB138_21:	# bb42
	movslq	%edx, %r14
	movq	$0, (%rax,%r14,8)
	leal	1(%rdx), %r14d
	movslq	%r14d, %r14
	movq	$0, (%rax,%r14,8)
	addl	$2, %edx
	incl	%edi
	cmpl	%r10d, %edi
	jle	.LBB138_21	# bb42
.LBB138_22:	# bb44
	addl	%ebx, %r11d
	incl	%r10d
	cmpl	%ecx, %r10d
	je	.LBB138_12	# bb61
.LBB138_23:	# bb43.preheader
	testl	%r10d, %r10d
	js	.LBB138_22	# bb44
.LBB138_24:	# bb43.preheader.bb42_crit_edge
	xorl	%edi, %edi
	movl	%r11d, %edx
	jmp	.LBB138_21	# bb42
.LBB138_25:	# bb46
	pxor	%xmm4, %xmm4
	ucomisd	%xmm4, %xmm1
	jne	.LBB138_27	# bb48
	jp	.LBB138_27	# bb48
.LBB138_26:	# bb46
	ucomisd	.LCPI138_0(%rip), %xmm3
	setnp	%dl
	sete	%dil
	testb	%dl, %dil
	jne	.LBB138_12	# bb61
.LBB138_27:	# bb48
	cmpl	$121, %esi
	je	.LBB138_32	# bb54.preheader
.LBB138_28:	# bb60.preheader
	testl	%ecx, %ecx
	jle	.LBB138_12	# bb61
.LBB138_29:	# bb.nph137
	movl	112(%rsp), %edx
	leal	(%rdx,%rdx), %r10d
	xorl	%r11d, %r11d
	movl	%r11d, %edi
	jmp	.LBB138_38	# bb58.preheader
	.align	16
.LBB138_30:	# bb51
	movslq	%ebx, %r15
	movsd	(%rax,%r15,8), %xmm4
	movapd	%xmm3, %xmm5
	mulsd	%xmm4, %xmm5
	leal	1(%rbx), %r12d
	movslq	%r12d, %r12
	movsd	(%rax,%r12,8), %xmm6
	movapd	%xmm1, %xmm7
	mulsd	%xmm6, %xmm7
	subsd	%xmm7, %xmm5
	movsd	%xmm5, (%rax,%r15,8)
	mulsd	%xmm1, %xmm4
	mulsd	%xmm3, %xmm6
	addsd	%xmm4, %xmm6
	movsd	%xmm6, (%rax,%r12,8)
	addl	$2, %ebx
	incl	%r14d
	cmpl	%r11d, %r14d
	jne	.LBB138_30	# bb51
.LBB138_31:	# bb53
	addl	%edi, %edx
	decl	%r11d
	incl	%r10d
	cmpl	%ecx, %r10d
	je	.LBB138_12	# bb61
	jmp	.LBB138_34	# bb52.preheader
.LBB138_32:	# bb54.preheader
	testl	%ecx, %ecx
	jle	.LBB138_12	# bb61
.LBB138_33:	# bb.nph181
	movl	112(%rsp), %edx
	leal	2(,%rdx,2), %edi
	xorl	%edx, %edx
	movl	%ecx, %r11d
	movl	%edx, %r10d
	.align	16
.LBB138_34:	# bb52.preheader
	cmpl	%ecx, %r10d
	jge	.LBB138_31	# bb53
.LBB138_35:	# bb52.preheader.bb51_crit_edge
	xorl	%r14d, %r14d
	movl	%edx, %ebx
	jmp	.LBB138_30	# bb51
	.align	16
.LBB138_36:	# bb57
	movslq	%edx, %r14
	movsd	(%rax,%r14,8), %xmm4
	movapd	%xmm3, %xmm5
	mulsd	%xmm4, %xmm5
	leal	1(%rdx), %r15d
	movslq	%r15d, %r15
	movsd	(%rax,%r15,8), %xmm6
	movapd	%xmm1, %xmm7
	mulsd	%xmm6, %xmm7
	subsd	%xmm7, %xmm5
	movsd	%xmm5, (%rax,%r14,8)
	mulsd	%xmm1, %xmm4
	mulsd	%xmm3, %xmm6
	addsd	%xmm4, %xmm6
	movsd	%xmm6, (%rax,%r15,8)
	addl	$2, %edx
	incl	%ebx
	cmpl	%edi, %ebx
	jle	.LBB138_36	# bb57
.LBB138_37:	# bb59
	addl	%r10d, %r11d
	incl	%edi
	cmpl	%ecx, %edi
	je	.LBB138_12	# bb61
.LBB138_38:	# bb58.preheader
	testl	%edi, %edi
	js	.LBB138_37	# bb59
.LBB138_39:	# bb58.preheader.bb57_crit_edge
	xorl	%ebx, %ebx
	movl	%r11d, %edx
	jmp	.LBB138_36	# bb57
.LBB138_40:	# bb.nph171
	leal	1(%rdi), %ebx
	pxor	%xmm1, %xmm1
	xorl	%r14d, %r14d
	movl	20(%rsp), %r15d
	movapd	%xmm1, %xmm3
	.align	16
.LBB138_41:	# bb69
	leal	(%rdi,%r15), %r12d
	movslq	%r12d, %r12
	movsd	(%r9,%r12,8), %xmm4
	leal	(%rbx,%r15), %r12d
	movslq	%r15d, %r13
	leal	1(%r15), %ebp
	movslq	%ebp, %rbp
	movsd	(%r9,%rbp,8), %xmm5
	movapd	%xmm5, %xmm6
	mulsd	%xmm4, %xmm6
	movsd	(%r9,%r13,8), %xmm7
	movslq	%r12d, %r12
	movsd	(%r9,%r12,8), %xmm8
	movapd	%xmm7, %xmm9
	mulsd	%xmm8, %xmm9
	addsd	%xmm6, %xmm9
	addsd	%xmm9, %xmm3
	mulsd	%xmm8, %xmm5
	mulsd	%xmm4, %xmm7
	subsd	%xmm5, %xmm7
	addsd	%xmm7, %xmm1
	addl	$2, %r15d
	incl	%r14d
	cmpl	%r8d, %r14d
	jne	.LBB138_41	# bb69
.LBB138_42:	# bb71
	movapd	%xmm2, %xmm4
	mulsd	%xmm3, %xmm4
	movapd	%xmm0, %xmm5
	mulsd	%xmm1, %xmm5
	subsd	%xmm4, %xmm5
	movslq	%r10d, %rbx
	addsd	(%rax,%rbx,8), %xmm5
	movsd	%xmm5, (%rax,%rbx,8)
	mulsd	%xmm2, %xmm1
	mulsd	%xmm0, %xmm3
	addsd	%xmm1, %xmm3
	leal	1(%r10), %ebx
	movslq	%ebx, %rbx
	addsd	(%rax,%rbx,8), %xmm3
	movsd	%xmm3, (%rax,%rbx,8)
	addl	%edx, %edi
	addl	$2, %r10d
	incl	%r11d
	cmpl	%esi, %r11d
	jne	.LBB138_46	# bb70.preheader
.LBB138_43:	# bb73
	movl	20(%rsp), %edx
	addl	8(%rsp), %edx
	movl	%edx, 20(%rsp)
	movl	16(%rsp), %edx
	addl	4(%rsp), %edx
	movl	%edx, 16(%rsp)
	decl	%esi
	movl	12(%rsp), %edx
	incl	%edx
	movl	%edx, 12(%rsp)
	cmpl	%ecx, %edx
	je	.LBB138_85	# return
.LBB138_44:	# bb72.preheader
	cmpl	%ecx, 12(%rsp)
	jge	.LBB138_43	# bb73
.LBB138_45:	# bb.nph175
	movl	88(%rsp), %edx
	leal	(%rdx,%rdx), %edx
	xorl	%edi, %edi
	movl	16(%rsp), %r10d
	movl	%edi, %r11d
	.align	16
.LBB138_46:	# bb70.preheader
	testl	%r8d, %r8d
	jg	.LBB138_40	# bb.nph171
.LBB138_47:	# bb70.preheader.bb71_crit_edge
	pxor	%xmm1, %xmm1
	movapd	%xmm1, %xmm3
	jmp	.LBB138_42	# bb71
.LBB138_48:	# bb75
	cmpl	$121, %esi
	jne	.LBB138_60	# bb88
.LBB138_49:	# bb75
	cmpl	$112, 20(%rsp)
	jne	.LBB138_60	# bb88
.LBB138_50:	# bb87.preheader
	testl	%ecx, %ecx
	jle	.LBB138_85	# return
.LBB138_51:	# bb.nph167
	movl	112(%rsp), %edx
	leal	2(,%rdx,2), %edx
	movl	%edx, 8(%rsp)
	xorl	%edx, %edx
	movl	%edx, 20(%rsp)
	movl	%ecx, 16(%rsp)
	movl	%edx, 12(%rsp)
	jmp	.LBB138_56	# bb85.preheader
.LBB138_52:	# bb82.preheader
	leal	1(%rbx), %r15d
	movl	88(%rsp), %esi
	leal	(%rsi,%rsi), %esi
	pxor	%xmm3, %xmm3
	xorl	%r10d, %r10d
	movl	%edi, %r11d
	movapd	%xmm3, %xmm1
	.align	16
.LBB138_53:	# bb82
	leal	(%rbx,%r11), %r12d
	movslq	%r12d, %r12
	movsd	(%r9,%r12,8), %xmm4
	leal	(%r15,%r11), %r12d
	movslq	%r11d, %r13
	leal	1(%r11), %ebp
	movslq	%ebp, %rbp
	movsd	(%r9,%rbp,8), %xmm5
	movapd	%xmm5, %xmm6
	mulsd	%xmm4, %xmm6
	movsd	(%r9,%r13,8), %xmm7
	movslq	%r12d, %r12
	movsd	(%r9,%r12,8), %xmm8
	movapd	%xmm7, %xmm9
	mulsd	%xmm8, %xmm9
	addsd	%xmm6, %xmm9
	addsd	%xmm9, %xmm1
	mulsd	%xmm8, %xmm5
	mulsd	%xmm4, %xmm7
	subsd	%xmm5, %xmm7
	addsd	%xmm7, %xmm3
	addl	%esi, %r11d
	incl	%r10d
	cmpl	%r8d, %r10d
	jne	.LBB138_53	# bb82
.LBB138_54:	# bb84
	movapd	%xmm2, %xmm4
	mulsd	%xmm1, %xmm4
	movapd	%xmm0, %xmm5
	mulsd	%xmm3, %xmm5
	subsd	%xmm4, %xmm5
	movl	20(%rsp), %esi
	leal	(%rsi,%rbx), %esi
	movslq	%esi, %rsi
	addsd	(%rax,%rsi,8), %xmm5
	movsd	%xmm5, (%rax,%rsi,8)
	mulsd	%xmm2, %xmm3
	mulsd	%xmm0, %xmm1
	addsd	%xmm3, %xmm1
	leal	(%r14,%rbx), %esi
	movslq	%esi, %rsi
	addsd	(%rax,%rsi,8), %xmm1
	movsd	%xmm1, (%rax,%rsi,8)
	addl	$2, %ebx
	incl	%edx
	cmpl	16(%rsp), %edx
	jne	.LBB138_58	# bb83.preheader
.LBB138_55:	# bb86
	movl	20(%rsp), %edx
	addl	8(%rsp), %edx
	movl	%edx, 20(%rsp)
	decl	16(%rsp)
	movl	12(%rsp), %edx
	incl	%edx
	movl	%edx, 12(%rsp)
	cmpl	%ecx, %edx
	je	.LBB138_85	# return
.LBB138_56:	# bb85.preheader
	cmpl	%ecx, 12(%rsp)
	jge	.LBB138_55	# bb86
.LBB138_57:	# bb.nph165
	movl	12(%rsp), %edx
	leal	(%rdx,%rdx), %edi
	movl	20(%rsp), %edx
	leal	1(%rdx), %r14d
	xorl	%ebx, %ebx
	movl	%ebx, %edx
	.align	16
.LBB138_58:	# bb83.preheader
	testl	%r8d, %r8d
	jg	.LBB138_52	# bb82.preheader
.LBB138_59:	# bb83.preheader.bb84_crit_edge
	pxor	%xmm3, %xmm3
	movapd	%xmm3, %xmm1
	jmp	.LBB138_54	# bb84
.LBB138_60:	# bb88
	cmpl	$122, %esi
	jne	.LBB138_72	# bb101
.LBB138_61:	# bb88
	cmpl	$111, 20(%rsp)
	jne	.LBB138_72	# bb101
.LBB138_62:	# bb100.preheader
	testl	%ecx, %ecx
	jle	.LBB138_85	# return
.LBB138_63:	# bb.nph157
	movl	88(%rsp), %ebx
	leal	(%rbx,%rbx), %ebx
	movl	%ebx, 12(%rsp)
	movl	112(%rsp), %ebx
	addl	%ebx, %ebx
	movl	%ebx, 16(%rsp)
	xorl	%r14d, %r14d
	movl	%r14d, 20(%rsp)
	movl	%r14d, %ebx
	jmp	.LBB138_68	# bb98.preheader
.LBB138_64:	# bb.nph151
	leal	1(%rsi), %r15d
	pxor	%xmm1, %xmm1
	xorl	%r12d, %r12d
	movl	%r12d, %r13d
	movapd	%xmm1, %xmm3
	.align	16
.LBB138_65:	# bb95
	leal	(%rsi,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r9,%rbp,8), %xmm4
	leal	(%r11,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r9,%rbp,8), %xmm5
	movapd	%xmm5, %xmm6
	mulsd	%xmm4, %xmm6
	leal	(%r15,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r9,%rbp,8), %xmm7
	leal	(%r14,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r9,%rbp,8), %xmm8
	movapd	%xmm8, %xmm9
	mulsd	%xmm7, %xmm9
	addsd	%xmm6, %xmm9
	addsd	%xmm9, %xmm3
	mulsd	%xmm7, %xmm5
	mulsd	%xmm4, %xmm8
	subsd	%xmm5, %xmm8
	addsd	%xmm8, %xmm1
	addl	$2, %r12d
	incl	%r13d
	cmpl	%r8d, %r13d
	jne	.LBB138_65	# bb95
.LBB138_66:	# bb97
	movapd	%xmm2, %xmm4
	mulsd	%xmm3, %xmm4
	movapd	%xmm0, %xmm5
	mulsd	%xmm1, %xmm5
	subsd	%xmm4, %xmm5
	movslq	%r10d, %r15
	addsd	(%rax,%r15,8), %xmm5
	movsd	%xmm5, (%rax,%r15,8)
	mulsd	%xmm2, %xmm1
	mulsd	%xmm0, %xmm3
	addsd	%xmm1, %xmm3
	leal	1(%r10), %r15d
	movslq	%r15d, %r15
	addsd	(%rax,%r15,8), %xmm3
	movsd	%xmm3, (%rax,%r15,8)
	addl	%edx, %esi
	addl	$2, %r10d
	incl	%edi
	cmpl	%ebx, %edi
	jle	.LBB138_70	# bb96.preheader
.LBB138_67:	# bb99
	addl	12(%rsp), %r14d
	movl	20(%rsp), %edx
	addl	16(%rsp), %edx
	movl	%edx, 20(%rsp)
	incl	%ebx
	cmpl	%ecx, %ebx
	je	.LBB138_85	# return
.LBB138_68:	# bb98.preheader
	testl	%ebx, %ebx
	js	.LBB138_67	# bb99
.LBB138_69:	# bb.nph155
	leal	1(%r14), %r11d
	movl	88(%rsp), %edx
	leal	(%rdx,%rdx), %edx
	xorl	%esi, %esi
	movl	20(%rsp), %r10d
	movl	%esi, %edi
	.align	16
.LBB138_70:	# bb96.preheader
	testl	%r8d, %r8d
	jg	.LBB138_64	# bb.nph151
.LBB138_71:	# bb96.preheader.bb97_crit_edge
	pxor	%xmm1, %xmm1
	movapd	%xmm1, %xmm3
	jmp	.LBB138_66	# bb97
.LBB138_72:	# bb101
	cmpl	$122, %esi
	jne	.LBB138_84	# bb114
.LBB138_73:	# bb101
	cmpl	$112, 20(%rsp)
	jne	.LBB138_84	# bb114
.LBB138_74:	# bb113.preheader
	testl	%ecx, %ecx
	jle	.LBB138_85	# return
.LBB138_75:	# bb.nph147
	movl	112(%rsp), %esi
	addl	%esi, %esi
	movl	%esi, 16(%rsp)
	xorl	%r10d, %r10d
	movl	%r10d, 20(%rsp)
	movl	%r10d, %esi
	jmp	.LBB138_80	# bb111.preheader
.LBB138_76:	# bb108.preheader
	leal	1(%r13), %edi
	movl	88(%rsp), %edx
	leal	(%rdx,%rdx), %edx
	pxor	%xmm3, %xmm3
	xorl	%r12d, %r12d
	movl	%r12d, %r15d
	movapd	%xmm3, %xmm1
	.align	16
.LBB138_77:	# bb108
	leal	(%r13,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r9,%rbp,8), %xmm4
	leal	(%r11,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r9,%rbp,8), %xmm5
	movapd	%xmm5, %xmm6
	mulsd	%xmm4, %xmm6
	leal	(%rdi,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r9,%rbp,8), %xmm7
	leal	(%rsi,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r9,%rbp,8), %xmm8
	movapd	%xmm8, %xmm9
	mulsd	%xmm7, %xmm9
	addsd	%xmm6, %xmm9
	addsd	%xmm9, %xmm1
	mulsd	%xmm7, %xmm5
	mulsd	%xmm4, %xmm8
	subsd	%xmm5, %xmm8
	addsd	%xmm8, %xmm3
	addl	%edx, %r12d
	incl	%r15d
	cmpl	%r8d, %r15d
	jne	.LBB138_77	# bb108
.LBB138_78:	# bb110
	movapd	%xmm2, %xmm4
	mulsd	%xmm1, %xmm4
	movapd	%xmm0, %xmm5
	mulsd	%xmm3, %xmm5
	subsd	%xmm4, %xmm5
	movl	20(%rsp), %edx
	leal	(%rdx,%r13), %edx
	movslq	%edx, %rdx
	addsd	(%rax,%rdx,8), %xmm5
	movsd	%xmm5, (%rax,%rdx,8)
	mulsd	%xmm2, %xmm3
	mulsd	%xmm0, %xmm1
	addsd	%xmm3, %xmm1
	leal	(%rbx,%r13), %edx
	movslq	%edx, %rdx
	addsd	(%rax,%rdx,8), %xmm1
	movsd	%xmm1, (%rax,%rdx,8)
	addl	$2, %r13d
	incl	%r14d
	cmpl	%r10d, %r14d
	jle	.LBB138_82	# bb109.preheader
.LBB138_79:	# bb112
	movl	20(%rsp), %edx
	addl	16(%rsp), %edx
	movl	%edx, 20(%rsp)
	addl	$2, %esi
	incl	%r10d
	cmpl	%ecx, %r10d
	je	.LBB138_85	# return
.LBB138_80:	# bb111.preheader
	testl	%r10d, %r10d
	js	.LBB138_79	# bb112
.LBB138_81:	# bb.nph145
	leal	1(%rsi), %r11d
	movl	20(%rsp), %edx
	leal	1(%rdx), %ebx
	xorl	%r13d, %r13d
	movl	%r13d, %r14d
	.align	16
.LBB138_82:	# bb109.preheader
	testl	%r8d, %r8d
	jg	.LBB138_76	# bb108.preheader
.LBB138_83:	# bb109.preheader.bb110_crit_edge
	pxor	%xmm3, %xmm3
	movapd	%xmm3, %xmm1
	jmp	.LBB138_78	# bb110
.LBB138_84:	# bb114
	xorl	%edi, %edi
	leaq	.str181, %rsi
	leaq	.str1182, %rdx
	xorb	%al, %al
	call	cblas_xerbla
.LBB138_85:	# return
	addq	$24, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB138_86:	# bb20
	cmpl	$111, %edx
	movl	$111, %edx
	movl	$112, %edi
	cmove	%edx, %edi
	movl	%edi, 20(%rsp)
	jmp	.LBB138_5	# bb31
	.size	cblas_zsyrk, .-cblas_zsyrk
.Leh_func_end96:


	.align	16
	.globl	cblas_ztbmv
	.type	cblas_ztbmv,@function
cblas_ztbmv:
.Leh_func_begin97:
.Llabel97:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$56, %rsp
	cmpl	$113, %edx
	movl	$4294967295, %eax
	movl	$1, %r10d
	cmove	%eax, %r10d
	movl	%r10d, 44(%rsp)
	movl	$112, %eax
	cmovne	%edx, %eax
	cmpl	$121, %esi
	sete	%dl
	setne	%r10b
	cmpl	$101, %edi
	sete	%r11b
	setne	%bl
	andb	%dl, %r11b
	orb	%r10b, %bl
	testb	%bl, %bl
	movl	136(%rsp), %edx
	movq	128(%rsp), %r10
	movq	112(%rsp), %rbx
	movl	%r9d, 52(%rsp)
	movl	%ecx, 48(%rsp)
	jne	.LBB139_2	# bb58
.LBB139_1:	# entry
	cmpl	$111, %eax
	je	.LBB139_4	# bb66
.LBB139_2:	# bb58
	cmpl	$122, %esi
	sete	%cl
	setne	%r9b
	cmpl	$102, %edi
	sete	%r14b
	setne	%r15b
	andb	%cl, %r14b
	orb	%r9b, %r15b
	testb	%r15b, %r15b
	jne	.LBB139_14	# bb81
.LBB139_3:	# bb58
	cmpl	$112, %eax
	jne	.LBB139_14	# bb81
.LBB139_4:	# bb66
	testl	%edx, %edx
	jg	.LBB139_55	# bb66.bb80.preheader_crit_edge
.LBB139_5:	# bb67
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
.LBB139_6:	# bb80.preheader
	testl	%r8d, %r8d
	jle	.LBB139_29	# bb118.thread
.LBB139_7:	# bb.nph275
	movl	$1, %ecx
	subl	%r8d, %ecx
	imull	%edx, %ecx
	movl	%ecx, 28(%rsp)
	addl	%eax, %eax
	movl	120(%rsp), %ecx
	addl	%ecx, %ecx
	movl	%ecx, 36(%rsp)
	movl	52(%rsp), %ecx
	leal	1(%rcx), %ecx
	movl	%ecx, 40(%rsp)
	cvtsi2sd	44(%rsp), %xmm0
	leal	(%rdx,%rdx), %ecx
	movl	%ecx, 32(%rsp)
	movl	$4294967294, 44(%rsp)
	xorl	%ecx, %ecx
	movl	%edx, %esi
	movl	%ecx, %edi
	movl	%ecx, %r9d
	.align	16
.LBB139_8:	# bb70
	testl	%edx, %edx
	movl	$0, %r11d
	cmovle	28(%rsp), %r11d
	movl	40(%rsp), %r14d
	leal	(%r14,%r9), %r14d
	cmpl	%r8d, %r14d
	cmovg	%r8d, %r14d
	leal	1(%r9), %r15d
	cmpl	%r14d, %r15d
	jge	.LBB139_56	# bb70.bb76_crit_edge
.LBB139_9:	# bb.nph269
	movl	$4294967295, %r14d
	subl	%r8d, %r14d
	movl	%edi, %r15d
	negl	%r15d
	subl	52(%rsp), %r15d
	addl	$4294967294, %r15d
	cmpl	%r15d, %r14d
	cmovg	%r14d, %r15d
	movl	44(%rsp), %r14d
	subl	%r15d, %r14d
	pxor	%xmm1, %xmm1
	xorl	%r15d, %r15d
	movl	%ecx, %r12d
	movl	%esi, %r13d
	movapd	%xmm1, %xmm2
	.align	16
.LBB139_10:	# bb74
	leal	3(%r12), %ebp
	movslq	%ebp, %rbp
	movapd	%xmm0, %xmm3
	mulsd	(%rbx,%rbp,8), %xmm3
	addl	%r11d, %r13d
	leal	1(,%r13,2), %r11d
	leal	(%r13,%r13), %ebp
	movslq	%ebp, %rbp
	movsd	(%r10,%rbp,8), %xmm4
	movapd	%xmm3, %xmm5
	mulsd	%xmm4, %xmm5
	addl	$2, %r12d
	movslq	%r12d, %rbp
	movsd	(%rbx,%rbp,8), %xmm6
	movslq	%r11d, %r11
	movsd	(%r10,%r11,8), %xmm7
	movapd	%xmm6, %xmm8
	mulsd	%xmm7, %xmm8
	addsd	%xmm5, %xmm8
	addsd	%xmm8, %xmm2
	mulsd	%xmm7, %xmm3
	mulsd	%xmm4, %xmm6
	subsd	%xmm3, %xmm6
	addsd	%xmm6, %xmm1
	incl	%r15d
	cmpl	%r14d, %r15d
	movl	%edx, %r11d
	jne	.LBB139_10	# bb74
.LBB139_11:	# bb76
	movslq	%eax, %r11
	movsd	(%r10,%r11,8), %xmm3
	cmpl	$131, 48(%rsp)
	je	.LBB139_57	# bb77
.LBB139_12:	# bb78
	addsd	%xmm1, %xmm3
	movsd	%xmm3, (%r10,%r11,8)
	leal	1(%rax), %r11d
	movslq	%r11d, %r11
	addsd	(%r10,%r11,8), %xmm2
	movsd	%xmm2, (%r10,%r11,8)
.LBB139_13:	# bb79
	addl	32(%rsp), %eax
	addl	%edx, %esi
	addl	36(%rsp), %ecx
	incl	%edi
	decl	44(%rsp)
	incl	%r9d
	cmpl	%r8d, %r9d
	jne	.LBB139_8	# bb70
	jmp	.LBB139_29	# bb118.thread
.LBB139_14:	# bb81
	cmpl	$122, %esi
	sete	%cl
	setne	%r9b
	cmpl	$101, %edi
	sete	%r15b
	setne	%r12b
	andb	%cl, %r15b
	orb	%r9b, %r12b
	testb	%r12b, %r12b
	jne	.LBB139_16	# bb89
.LBB139_15:	# bb81
	cmpl	$111, %eax
	je	.LBB139_18	# bb97
.LBB139_16:	# bb89
	cmpl	$121, %esi
	sete	%cl
	setne	%sil
	cmpl	$102, %edi
	sete	%dil
	setne	%r9b
	andb	%cl, %dil
	orb	%sil, %r9b
	testb	%r9b, %r9b
	jne	.LBB139_30	# bb120
.LBB139_17:	# bb89
	cmpl	$112, %eax
	jne	.LBB139_30	# bb120
.LBB139_18:	# bb97
	testl	%edx, %edx
	jg	.LBB139_58	# bb97.bb100_crit_edge
.LBB139_19:	# bb98
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
.LBB139_20:	# bb100
	leal	-1(%r8), %ecx
	movl	120(%rsp), %esi
	movl	%esi, %edi
	imull	%ecx, %edi
	movl	$4294967295, %r11d
	movl	52(%rsp), %r9d
	subl	%r9d, %r11d
	movl	%r11d, 40(%rsp)
	leal	(%r9,%rdi), %r11d
	subl	%r8d, %r9d
	leal	1(%r9,%rdi), %edi
	addl	%r11d, %r11d
	imull	%edx, %ecx
	addl	%eax, %ecx
	addl	%ecx, %ecx
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
	movl	$1, %r9d
	subl	%esi, %r9d
	movl	%r9d, 32(%rsp)
	cvtsi2sd	44(%rsp), %xmm0
	addl	%esi, %esi
	movl	%esi, 36(%rsp)
	leal	(%rdx,%rdx), %esi
	movl	%esi, 44(%rsp)
	jmp	.LBB139_27	# bb114
.LBB139_21:	# bb101
	xorl	%r9d, %r9d
	testl	%edx, %edx
	movl	%eax, %r14d
	cmovg	%r9d, %r14d
	movl	40(%rsp), %r15d
	leal	(%r15,%r8), %r15d
	cmpl	52(%rsp), %esi
	cmovl	%r9d, %r15d
	movl	%r15d, %r9d
	imull	%edx, %r9d
	cmpl	%esi, %r15d
	jge	.LBB139_59	# bb101.bb110_crit_edge
.LBB139_22:	# bb.nph257
	movl	%r8d, %esi
	subl	%r15d, %esi
	addl	%edi, %r15d
	addl	%r15d, %r15d
	decl	%esi
	pxor	%xmm1, %xmm1
	xorl	%r12d, %r12d
	movapd	%xmm1, %xmm2
	.align	16
.LBB139_23:	# bb108
	movslq	%r15d, %r13
	leal	1(%r15), %ebp
	movslq	%ebp, %rbp
	movapd	%xmm0, %xmm3
	mulsd	(%rbx,%rbp,8), %xmm3
	addl	%r14d, %r9d
	leal	1(,%r9,2), %r14d
	leal	(%r9,%r9), %ebp
	movslq	%ebp, %rbp
	movsd	(%r10,%rbp,8), %xmm4
	movapd	%xmm3, %xmm5
	mulsd	%xmm4, %xmm5
	movsd	(%rbx,%r13,8), %xmm6
	movslq	%r14d, %r14
	movsd	(%r10,%r14,8), %xmm7
	movapd	%xmm6, %xmm8
	mulsd	%xmm7, %xmm8
	addsd	%xmm5, %xmm8
	addsd	%xmm8, %xmm2
	mulsd	%xmm7, %xmm3
	mulsd	%xmm4, %xmm6
	subsd	%xmm3, %xmm6
	addsd	%xmm6, %xmm1
	addl	$2, %r15d
	incl	%r12d
	cmpl	%esi, %r12d
	movl	%edx, %r14d
	jne	.LBB139_23	# bb108
.LBB139_24:	# bb110
	movslq	%ecx, %rsi
	movsd	(%r10,%rsi,8), %xmm3
	cmpl	$131, 48(%rsp)
	je	.LBB139_60	# bb111
.LBB139_25:	# bb112
	addsd	%xmm1, %xmm3
	movsd	%xmm3, (%r10,%rsi,8)
	leal	1(%rcx), %esi
	movslq	%esi, %rsi
	addsd	(%r10,%rsi,8), %xmm2
	movsd	%xmm2, (%r10,%rsi,8)
.LBB139_26:	# bb113
	subl	36(%rsp), %r11d
	subl	44(%rsp), %ecx
	addl	32(%rsp), %edi
	decl	%r8d
.LBB139_27:	# bb114
	testl	%r8d, %r8d
	jle	.LBB139_29	# bb118.thread
.LBB139_28:	# bb115
	leal	-1(%r8), %esi
	testl	%r8d, %r8d
	jne	.LBB139_21	# bb101
.LBB139_29:	# bb118.thread
	addq	$56, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB139_30:	# bb120
	cmpl	$111, %eax
	sete	%cl
	andb	%cl, %r14b
	cmpl	$112, %eax
	sete	%cl
	testb	%cl, %r11b
	jne	.LBB139_32	# bb136
.LBB139_31:	# bb120
	notb	%r14b
	testb	$1, %r14b
	jne	.LBB139_43	# bb159
.LBB139_32:	# bb136
	testl	%edx, %edx
	jg	.LBB139_61	# bb136.bb139_crit_edge
.LBB139_33:	# bb137
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
.LBB139_34:	# bb139
	movl	$4294967295, %ecx
	subl	52(%rsp), %ecx
	movl	%ecx, 32(%rsp)
	leal	-1(%r8), %ecx
	movl	120(%rsp), %esi
	movl	%esi, %edi
	imull	%ecx, %edi
	addl	%edi, %edi
	imull	%edx, %ecx
	addl	%eax, %ecx
	addl	%ecx, %ecx
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
	cvtsi2sd	44(%rsp), %xmm0
	leal	(%rsi,%rsi), %esi
	movl	%esi, 44(%rsp)
	leal	(%rdx,%rdx), %esi
	movl	%esi, 40(%rsp)
	jmp	.LBB139_41	# bb153
.LBB139_35:	# bb140
	xorl	%r9d, %r9d
	testl	%edx, %edx
	movl	%eax, %r11d
	cmovg	%r9d, %r11d
	movl	32(%rsp), %r14d
	leal	(%r14,%r8), %r14d
	cmpl	52(%rsp), %esi
	cmovl	%r9d, %r14d
	movl	%r14d, %r9d
	imull	%edx, %r9d
	cmpl	%esi, %r14d
	jge	.LBB139_62	# bb140.bb149_crit_edge
.LBB139_36:	# bb.nph245
	movl	120(%rsp), %esi
	leal	-1(%rsi), %r15d
	imull	%r14d, %r15d
	leal	-1(%r8,%r15), %r15d
	addl	%r15d, %r15d
	leal	-1(%r8), %r12d
	subl	%r14d, %r12d
	leal	-2(,%rsi,2), %esi
	pxor	%xmm1, %xmm1
	xorl	%r14d, %r14d
	movapd	%xmm1, %xmm2
	.align	16
.LBB139_37:	# bb147
	movslq	%r15d, %r13
	leal	1(%r15), %ebp
	movslq	%ebp, %rbp
	movapd	%xmm0, %xmm3
	mulsd	(%rbx,%rbp,8), %xmm3
	addl	%r11d, %r9d
	leal	1(,%r9,2), %r11d
	leal	(%r9,%r9), %ebp
	movslq	%ebp, %rbp
	movsd	(%r10,%rbp,8), %xmm4
	movapd	%xmm3, %xmm5
	mulsd	%xmm4, %xmm5
	movsd	(%rbx,%r13,8), %xmm6
	movslq	%r11d, %r11
	movsd	(%r10,%r11,8), %xmm7
	movapd	%xmm6, %xmm8
	mulsd	%xmm7, %xmm8
	addsd	%xmm5, %xmm8
	addsd	%xmm8, %xmm2
	mulsd	%xmm7, %xmm3
	mulsd	%xmm4, %xmm6
	subsd	%xmm3, %xmm6
	addsd	%xmm6, %xmm1
	addl	%esi, %r15d
	incl	%r14d
	cmpl	%r12d, %r14d
	movl	%edx, %r11d
	jne	.LBB139_37	# bb147
.LBB139_38:	# bb149
	movslq	%ecx, %rsi
	movsd	(%r10,%rsi,8), %xmm3
	cmpl	$131, 48(%rsp)
	je	.LBB139_63	# bb150
.LBB139_39:	# bb151
	addsd	%xmm1, %xmm3
	movsd	%xmm3, (%r10,%rsi,8)
	leal	1(%rcx), %esi
	movslq	%esi, %rsi
	addsd	(%r10,%rsi,8), %xmm2
	movsd	%xmm2, (%r10,%rsi,8)
.LBB139_40:	# bb152
	subl	44(%rsp), %edi
	subl	40(%rsp), %ecx
	decl	%r8d
.LBB139_41:	# bb153
	testl	%r8d, %r8d
	jle	.LBB139_29	# bb118.thread
.LBB139_42:	# bb154
	leal	-1(%r8), %esi
	testl	%r8d, %r8d
	jne	.LBB139_35	# bb140
	jmp	.LBB139_29	# bb118.thread
.LBB139_43:	# bb159
	cmpl	$111, %eax
	sete	%cl
	andb	%cl, %dil
	cmpl	$112, %eax
	sete	%al
	testb	%al, %r15b
	jne	.LBB139_45	# bb175
.LBB139_44:	# bb159
	notb	%dil
	testb	$1, %dil
	jne	.LBB139_67	# bb191
.LBB139_45:	# bb175
	testl	%edx, %edx
	jg	.LBB139_64	# bb175.bb190.preheader_crit_edge
.LBB139_46:	# bb176
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
.LBB139_47:	# bb190.preheader
	testl	%r8d, %r8d
	jle	.LBB139_29	# bb118.thread
.LBB139_48:	# bb.nph236
	movl	52(%rsp), %ecx
	leal	1(,%rcx,2), %esi
	movl	%esi, 28(%rsp)
	movl	120(%rsp), %esi
	leal	(%rcx,%rsi), %edi
	leal	-2(,%rdi,2), %edi
	movl	%edi, 24(%rsp)
	movl	$1, %edi
	subl	%r8d, %edi
	imull	%edx, %edi
	movl	%edi, 4(%rsp)
	addl	%eax, %eax
	leal	(%rsi,%rsi), %esi
	movl	%esi, 20(%rsp)
	cvtsi2sd	44(%rsp), %xmm0
	leal	(%rcx,%rcx), %esi
	movl	%esi, 16(%rsp)
	leal	1(%rcx), %ecx
	movl	%ecx, 12(%rsp)
	leal	(%rdx,%rdx), %ecx
	movl	%ecx, 8(%rsp)
	movl	$4294967294, 32(%rsp)
	xorl	%ecx, %ecx
	movl	%edx, 44(%rsp)
	movl	%ecx, 40(%rsp)
	movl	%ecx, %esi
	.align	16
.LBB139_49:	# bb179
	testl	%edx, %edx
	movl	$0, %edi
	cmovle	4(%rsp), %edi
	movl	12(%rsp), %r9d
	leal	(%r9,%rsi), %r9d
	cmpl	%r8d, %r9d
	cmovg	%r8d, %r9d
	leal	1(%rsi), %r11d
	cmpl	%r9d, %r11d
	jge	.LBB139_65	# bb179.bb186_crit_edge
.LBB139_50:	# bb.nph
	movl	$4294967295, %r9d
	subl	%r8d, %r9d
	movl	40(%rsp), %r11d
	negl	%r11d
	subl	52(%rsp), %r11d
	addl	$4294967294, %r11d
	cmpl	%r11d, %r9d
	cmovg	%r9d, %r11d
	movl	32(%rsp), %r9d
	subl	%r11d, %r9d
	movl	24(%rsp), %r11d
	leal	(%r11,%rcx), %r11d
	movl	120(%rsp), %r14d
	leal	-2(,%r14,2), %r14d
	pxor	%xmm1, %xmm1
	xorl	%r15d, %r15d
	movl	44(%rsp), %r12d
	movapd	%xmm1, %xmm2
	.align	16
.LBB139_51:	# bb184
	movslq	%r11d, %r13
	leal	1(%r11), %ebp
	movslq	%ebp, %rbp
	movapd	%xmm0, %xmm3
	mulsd	(%rbx,%rbp,8), %xmm3
	addl	%edi, %r12d
	leal	1(,%r12,2), %edi
	leal	(%r12,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r10,%rbp,8), %xmm4
	movapd	%xmm3, %xmm5
	mulsd	%xmm4, %xmm5
	movsd	(%rbx,%r13,8), %xmm6
	movslq	%edi, %rdi
	movsd	(%r10,%rdi,8), %xmm7
	movapd	%xmm6, %xmm8
	mulsd	%xmm7, %xmm8
	addsd	%xmm5, %xmm8
	addsd	%xmm8, %xmm1
	mulsd	%xmm7, %xmm3
	mulsd	%xmm4, %xmm6
	subsd	%xmm3, %xmm6
	addsd	%xmm6, %xmm2
	addl	%r14d, %r11d
	incl	%r15d
	cmpl	%r9d, %r15d
	movl	%edx, %edi
	jne	.LBB139_51	# bb184
.LBB139_52:	# bb186
	movslq	%eax, %rdi
	movsd	(%r10,%rdi,8), %xmm3
	cmpl	$131, 48(%rsp)
	je	.LBB139_66	# bb187
.LBB139_53:	# bb188
	addsd	%xmm2, %xmm3
	movsd	%xmm3, (%r10,%rdi,8)
	leal	1(%rax), %edi
	movslq	%edi, %rdi
	addsd	(%r10,%rdi,8), %xmm1
	movsd	%xmm1, (%r10,%rdi,8)
.LBB139_54:	# bb189
	addl	8(%rsp), %eax
	addl	%edx, 44(%rsp)
	addl	20(%rsp), %ecx
	incl	40(%rsp)
	decl	32(%rsp)
	incl	%esi
	cmpl	%r8d, %esi
	je	.LBB139_29	# bb118.thread
	jmp	.LBB139_49	# bb179
.LBB139_55:	# bb66.bb80.preheader_crit_edge
	xorl	%eax, %eax
	jmp	.LBB139_6	# bb80.preheader
.LBB139_56:	# bb70.bb76_crit_edge
	pxor	%xmm1, %xmm1
	movapd	%xmm1, %xmm2
	jmp	.LBB139_11	# bb76
.LBB139_57:	# bb77
	movslq	%ecx, %r14
	leal	1(%rcx), %r15d
	movslq	%r15d, %r15
	movapd	%xmm0, %xmm4
	mulsd	(%rbx,%r15,8), %xmm4
	leal	1(%rax), %r15d
	movslq	%r15d, %r15
	movsd	(%r10,%r15,8), %xmm5
	movapd	%xmm4, %xmm6
	mulsd	%xmm5, %xmm6
	movsd	(%rbx,%r14,8), %xmm7
	movapd	%xmm7, %xmm8
	mulsd	%xmm3, %xmm8
	subsd	%xmm6, %xmm8
	addsd	%xmm1, %xmm8
	movsd	%xmm8, (%r10,%r11,8)
	mulsd	%xmm5, %xmm7
	mulsd	%xmm3, %xmm4
	addsd	%xmm7, %xmm4
	addsd	%xmm2, %xmm4
	movsd	%xmm4, (%r10,%r15,8)
	jmp	.LBB139_13	# bb79
.LBB139_58:	# bb97.bb100_crit_edge
	xorl	%eax, %eax
	jmp	.LBB139_20	# bb100
.LBB139_59:	# bb101.bb110_crit_edge
	pxor	%xmm1, %xmm1
	movapd	%xmm1, %xmm2
	jmp	.LBB139_24	# bb110
.LBB139_60:	# bb111
	movslq	%r11d, %r9
	leal	1(%r11), %r14d
	movslq	%r14d, %r14
	movapd	%xmm0, %xmm4
	mulsd	(%rbx,%r14,8), %xmm4
	leal	1(%rcx), %r14d
	movslq	%r14d, %r14
	movsd	(%r10,%r14,8), %xmm5
	movapd	%xmm4, %xmm6
	mulsd	%xmm5, %xmm6
	movsd	(%rbx,%r9,8), %xmm7
	movapd	%xmm7, %xmm8
	mulsd	%xmm3, %xmm8
	subsd	%xmm6, %xmm8
	addsd	%xmm1, %xmm8
	movsd	%xmm8, (%r10,%rsi,8)
	mulsd	%xmm5, %xmm7
	mulsd	%xmm3, %xmm4
	addsd	%xmm7, %xmm4
	addsd	%xmm2, %xmm4
	movsd	%xmm4, (%r10,%r14,8)
	jmp	.LBB139_26	# bb113
.LBB139_61:	# bb136.bb139_crit_edge
	xorl	%eax, %eax
	jmp	.LBB139_34	# bb139
.LBB139_62:	# bb140.bb149_crit_edge
	pxor	%xmm1, %xmm1
	movapd	%xmm1, %xmm2
	jmp	.LBB139_38	# bb149
.LBB139_63:	# bb150
	movslq	%edi, %r9
	leal	1(%rdi), %r11d
	movslq	%r11d, %r11
	movapd	%xmm0, %xmm4
	mulsd	(%rbx,%r11,8), %xmm4
	leal	1(%rcx), %r11d
	movslq	%r11d, %r11
	movsd	(%r10,%r11,8), %xmm5
	movapd	%xmm4, %xmm6
	mulsd	%xmm5, %xmm6
	movsd	(%rbx,%r9,8), %xmm7
	movapd	%xmm7, %xmm8
	mulsd	%xmm3, %xmm8
	subsd	%xmm6, %xmm8
	addsd	%xmm1, %xmm8
	movsd	%xmm8, (%r10,%rsi,8)
	mulsd	%xmm5, %xmm7
	mulsd	%xmm3, %xmm4
	addsd	%xmm7, %xmm4
	addsd	%xmm2, %xmm4
	movsd	%xmm4, (%r10,%r11,8)
	jmp	.LBB139_40	# bb152
.LBB139_64:	# bb175.bb190.preheader_crit_edge
	xorl	%eax, %eax
	jmp	.LBB139_47	# bb190.preheader
.LBB139_65:	# bb179.bb186_crit_edge
	pxor	%xmm1, %xmm1
	movapd	%xmm1, %xmm2
	jmp	.LBB139_52	# bb186
.LBB139_66:	# bb187
	leal	1(%rax), %r9d
	movslq	%r9d, %r9
	movsd	(%r10,%r9,8), %xmm4
	movl	28(%rsp), %r11d
	leal	(%r11,%rcx), %r11d
	movslq	%r11d, %r11
	movapd	%xmm0, %xmm5
	mulsd	(%rbx,%r11,8), %xmm5
	movapd	%xmm5, %xmm6
	mulsd	%xmm4, %xmm6
	movl	16(%rsp), %r11d
	leal	(%r11,%rcx), %r11d
	movslq	%r11d, %r11
	movsd	(%rbx,%r11,8), %xmm7
	movapd	%xmm7, %xmm8
	mulsd	%xmm3, %xmm8
	subsd	%xmm6, %xmm8
	addsd	%xmm2, %xmm8
	movsd	%xmm8, (%r10,%rdi,8)
	mulsd	%xmm4, %xmm7
	mulsd	%xmm3, %xmm5
	addsd	%xmm7, %xmm5
	addsd	%xmm1, %xmm5
	movsd	%xmm5, (%r10,%r9,8)
	jmp	.LBB139_54	# bb189
.LBB139_67:	# bb191
	xorl	%edi, %edi
	leaq	.str183, %rsi
	leaq	.str1184, %rdx
	xorb	%al, %al
	call	cblas_xerbla
	jmp	.LBB139_29	# bb118.thread
	.size	cblas_ztbmv, .-cblas_ztbmv
.Leh_func_end97:


	.align	16
	.globl	cblas_ztbsv
	.type	cblas_ztbsv,@function
cblas_ztbsv:
.Leh_func_begin98:
.Llabel98:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$120, %rsp
	cmpl	$113, %edx
	movl	$4294967295, %eax
	movl	$1, %r10d
	cmove	%eax, %r10d
	movl	$112, %eax
	cmovne	%edx, %eax
	testl	%r8d, %r8d
	movl	200(%rsp), %ebx
	movq	192(%rsp), %r14
	movq	176(%rsp), %r15
	movl	%r9d, 80(%rsp)
	movl	%r8d, 84(%rsp)
	movl	%ecx, 76(%rsp)
	je	.LBB140_16	# bb84.thread
.LBB140_1:	# bb51
	cmpl	$121, %esi
	sete	%cl
	setne	%dl
	cmpl	$101, %edi
	sete	%r8b
	setne	%r9b
	andb	%cl, %r8b
	orb	%dl, %r9b
	testb	%r9b, %r9b
	jne	.LBB140_3	# bb58
.LBB140_2:	# bb51
	cmpl	$111, %eax
	je	.LBB140_5	# bb66
.LBB140_3:	# bb58
	cmpl	$122, %esi
	sete	%cl
	setne	%dl
	cmpl	$102, %edi
	sete	%r9b
	setne	%r11b
	andb	%cl, %r9b
	orb	%dl, %r11b
	testb	%r11b, %r11b
	jne	.LBB140_17	# bb86
.LBB140_4:	# bb58
	cmpl	$112, %eax
	jne	.LBB140_17	# bb86
.LBB140_5:	# bb66
	testl	%ebx, %ebx
	jg	.LBB140_57	# bb66.bb69_crit_edge
.LBB140_6:	# bb67
	movl	$1, %eax
	subl	84(%rsp), %eax
	imull	%ebx, %eax
.LBB140_7:	# bb69
	movl	84(%rsp), %ecx
	leal	-1(%rcx), %edx
	movl	184(%rsp), %esi
	movl	%esi, %edi
	imull	%edx, %edi
	addl	%edi, %edi
	movl	%edi, 72(%rsp)
	imull	%ebx, %edx
	addl	%eax, %edx
	addl	%edx, %edx
	movl	%edx, 64(%rsp)
	movl	$1, %eax
	subl	%ecx, %eax
	imull	%ebx, %eax
	movl	%eax, 24(%rsp)
	movl	%ecx, %eax
	imull	%ebx, %eax
	movl	%eax, 52(%rsp)
	cvtsi2sd	%r10d, %xmm0
	movsd	%xmm0, 96(%rsp)
	leal	(%rbx,%rbx), %eax
	movl	%eax, 28(%rsp)
	addl	%esi, %esi
	movl	%esi, 44(%rsp)
	xorl	%eax, %eax
	movl	%eax, 32(%rsp)
	movl	%ecx, %r12d
	movl	%eax, 48(%rsp)
	jmp	.LBB140_14	# bb80
.LBB140_8:	# bb70
	testl	%ebx, %ebx
	movl	$0, %eax
	cmovle	24(%rsp), %eax
	movl	80(%rsp), %r10d
	leal	(%r10,%r12), %r10d
	movl	84(%rsp), %ecx
	cmpl	%ecx, %r10d
	cmovg	%ecx, %r10d
	movl	64(%rsp), %ecx
	movslq	%ecx, %r13
	leal	1(%rcx), %ecx
	cmpl	%r10d, %r12d
	movsd	(%r14,%r13,8), %xmm0
	movsd	%xmm0, 112(%rsp)
	movslq	%ecx, %rbp
	movsd	(%r14,%rbp,8), %xmm0
	movsd	%xmm0, 104(%rsp)
	jge	.LBB140_11	# bb76
.LBB140_9:	# bb.nph264
	movl	$4294967295, %r10d
	movl	84(%rsp), %ecx
	subl	%ecx, %r10d
	movl	80(%rsp), %edx
	leal	1(%rdx,%rcx), %edx
	movl	32(%rsp), %esi
	subl	%edx, %esi
	cmpl	%esi, %r10d
	cmovg	%r10d, %esi
	addl	%ecx, %esi
	movl	48(%rsp), %r10d
	subl	%esi, %r10d
	decl	%r10d
	xorl	%ecx, %ecx
	movl	72(%rsp), %edx
	movl	52(%rsp), %esi
	.align	16
.LBB140_10:	# bb74
	leal	3(%rdx), %edi
	movslq	%edi, %rdi
	movsd	96(%rsp), %xmm0
	mulsd	(%r15,%rdi,8), %xmm0
	addl	%eax, %esi
	leal	1(,%rsi,2), %eax
	leal	(%rsi,%rsi), %edi
	movslq	%edi, %rdi
	movsd	(%r14,%rdi,8), %xmm1
	movapd	%xmm0, %xmm2
	mulsd	%xmm1, %xmm2
	addl	$2, %edx
	movslq	%edx, %rdi
	movsd	(%r15,%rdi,8), %xmm3
	movslq	%eax, %rax
	movsd	(%r14,%rax,8), %xmm4
	movapd	%xmm3, %xmm5
	mulsd	%xmm4, %xmm5
	addsd	%xmm2, %xmm5
	movsd	104(%rsp), %xmm2
	subsd	%xmm5, %xmm2
	movsd	%xmm2, 104(%rsp)
	mulsd	%xmm4, %xmm0
	mulsd	%xmm1, %xmm3
	subsd	%xmm0, %xmm3
	movsd	112(%rsp), %xmm0
	subsd	%xmm3, %xmm0
	movsd	%xmm0, 112(%rsp)
	incl	%ecx
	cmpl	%r10d, %ecx
	movl	%ebx, %eax
	jne	.LBB140_10	# bb74
.LBB140_11:	# bb76
	cmpl	$131, 76(%rsp)
	je	.LBB140_58	# bb77
.LBB140_12:	# bb78
	movsd	112(%rsp), %xmm0
	movsd	%xmm0, (%r14,%r13,8)
	movsd	104(%rsp), %xmm0
	movsd	%xmm0, 56(%rsp)
.LBB140_13:	# bb79
	movsd	56(%rsp), %xmm0
	movsd	%xmm0, (%r14,%rbp,8)
	subl	%ebx, 52(%rsp)
	movl	64(%rsp), %eax
	subl	28(%rsp), %eax
	movl	%eax, 64(%rsp)
	movl	72(%rsp), %eax
	subl	44(%rsp), %eax
	movl	%eax, 72(%rsp)
	decl	%r12d
	incl	32(%rsp)
	incl	48(%rsp)
.LBB140_14:	# bb80
	testl	%r12d, %r12d
	jle	.LBB140_16	# bb84.thread
.LBB140_15:	# bb81
	testl	%r12d, %r12d
	jne	.LBB140_8	# bb70
.LBB140_16:	# bb84.thread
	addq	$120, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB140_17:	# bb86
	cmpl	$122, %esi
	sete	%cl
	setne	%dl
	cmpl	$101, %edi
	sete	%r11b
	setne	%r12b
	andb	%cl, %r11b
	orb	%dl, %r12b
	testb	%r12b, %r12b
	jne	.LBB140_19	# bb94
.LBB140_18:	# bb86
	cmpl	$111, %eax
	je	.LBB140_21	# bb102
.LBB140_19:	# bb94
	cmpl	$121, %esi
	sete	%cl
	setne	%dl
	cmpl	$102, %edi
	sete	%sil
	setne	%dil
	andb	%cl, %sil
	orb	%dl, %dil
	testb	%dil, %dil
	jne	.LBB140_31	# bb120
.LBB140_20:	# bb94
	cmpl	$112, %eax
	jne	.LBB140_31	# bb120
.LBB140_21:	# bb102
	testl	%ebx, %ebx
	jg	.LBB140_59	# bb102.bb119.preheader_crit_edge
.LBB140_22:	# bb103
	movl	$1, %eax
	subl	84(%rsp), %eax
	imull	%ebx, %eax
.LBB140_23:	# bb119.preheader
	cmpl	$0, 84(%rsp)
	jle	.LBB140_16	# bb84.thread
.LBB140_24:	# bb.nph256
	movl	$1, %ecx
	subl	84(%rsp), %ecx
	imull	%ebx, %ecx
	movl	%ecx, 28(%rsp)
	addl	%eax, %eax
	movl	184(%rsp), %ecx
	leal	(%rcx,%rcx), %edx
	movl	%edx, 52(%rsp)
	decl	%ecx
	movl	%ecx, 44(%rsp)
	movl	80(%rsp), %ecx
	leal	(%rcx,%rcx), %edx
	movl	%edx, 64(%rsp)
	movl	%ecx, %edx
	negl	%edx
	movl	%edx, 32(%rsp)
	cvtsi2sd	%r10d, %xmm0
	movsd	%xmm0, 96(%rsp)
	leal	(%rbx,%rbx), %r10d
	movl	%r10d, 48(%rsp)
	xorl	%r12d, %r12d
	movl	%ecx, 72(%rsp)
	.align	16
.LBB140_25:	# bb106
	xorl	%r10d, %r10d
	testl	%ebx, %ebx
	movl	28(%rsp), %ecx
	cmovg	%r10d, %ecx
	movl	32(%rsp), %edx
	leal	(%rdx,%r12), %edx
	cmpl	80(%rsp), %r12d
	cmovl	%r10d, %edx
	movl	%edx, %r10d
	imull	%ebx, %r10d
	movslq	%eax, %r13
	incl	%eax
	cmpl	%r12d, %edx
	movsd	(%r14,%r13,8), %xmm0
	movsd	%xmm0, 112(%rsp)
	movslq	%eax, %rbp
	movsd	(%r14,%rbp,8), %xmm0
	movsd	%xmm0, 104(%rsp)
	jge	.LBB140_28	# bb115
.LBB140_26:	# bb.nph251
	movl	%r12d, %eax
	subl	%edx, %eax
	addl	72(%rsp), %edx
	addl	%edx, %edx
	xorl	%esi, %esi
	.align	16
.LBB140_27:	# bb113
	movslq	%edx, %rdi
	leal	1(%rdx), %r8d
	movslq	%r8d, %r8
	movsd	96(%rsp), %xmm0
	mulsd	(%r15,%r8,8), %xmm0
	addl	%ecx, %r10d
	leal	1(,%r10,2), %ecx
	leal	(%r10,%r10), %r8d
	movslq	%r8d, %r8
	movsd	(%r14,%r8,8), %xmm1
	movapd	%xmm0, %xmm2
	mulsd	%xmm1, %xmm2
	movsd	(%r15,%rdi,8), %xmm3
	movslq	%ecx, %rcx
	movsd	(%r14,%rcx,8), %xmm4
	movapd	%xmm3, %xmm5
	mulsd	%xmm4, %xmm5
	addsd	%xmm2, %xmm5
	movsd	104(%rsp), %xmm2
	subsd	%xmm5, %xmm2
	movsd	%xmm2, 104(%rsp)
	mulsd	%xmm4, %xmm0
	mulsd	%xmm1, %xmm3
	subsd	%xmm0, %xmm3
	movsd	112(%rsp), %xmm0
	subsd	%xmm3, %xmm0
	movsd	%xmm0, 112(%rsp)
	addl	$2, %edx
	incl	%esi
	cmpl	%eax, %esi
	movl	%ebx, %ecx
	jne	.LBB140_27	# bb113
.LBB140_28:	# bb115
	cmpl	$131, 76(%rsp)
	je	.LBB140_60	# bb116
.LBB140_29:	# bb117
	movsd	112(%rsp), %xmm0
	movsd	%xmm0, (%r14,%r13,8)
	movsd	104(%rsp), %xmm0
	movsd	%xmm0, 56(%rsp)
.LBB140_30:	# bb118
	movsd	56(%rsp), %xmm0
	movsd	%xmm0, (%r14,%rbp,8)
	movl	%r13d, %eax
	addl	48(%rsp), %eax
	movl	64(%rsp), %r10d
	addl	52(%rsp), %r10d
	movl	%r10d, 64(%rsp)
	movl	72(%rsp), %r10d
	addl	44(%rsp), %r10d
	movl	%r10d, 72(%rsp)
	incl	%r12d
	cmpl	84(%rsp), %r12d
	je	.LBB140_16	# bb84.thread
	jmp	.LBB140_25	# bb106
.LBB140_31:	# bb120
	cmpl	$111, %eax
	sete	%cl
	andb	%cl, %r9b
	cmpl	$112, %eax
	sete	%cl
	testb	%cl, %r8b
	jne	.LBB140_33	# bb136
.LBB140_32:	# bb120
	notb	%r9b
	testb	$1, %r9b
	jne	.LBB140_43	# bb154
.LBB140_33:	# bb136
	testl	%ebx, %ebx
	jg	.LBB140_61	# bb136.bb153.preheader_crit_edge
.LBB140_34:	# bb137
	movl	$1, %eax
	subl	84(%rsp), %eax
	imull	%ebx, %eax
.LBB140_35:	# bb153.preheader
	cmpl	$0, 84(%rsp)
	jle	.LBB140_16	# bb84.thread
.LBB140_36:	# bb.nph243
	movl	$1, %ecx
	subl	84(%rsp), %ecx
	imull	%ebx, %ecx
	movl	%ecx, 32(%rsp)
	addl	%eax, %eax
	movl	184(%rsp), %ecx
	leal	(%rcx,%rcx), %ecx
	movl	%ecx, 56(%rsp)
	movl	80(%rsp), %ecx
	negl	%ecx
	movl	%ecx, 48(%rsp)
	cvtsi2sd	%r10d, %xmm0
	movsd	%xmm0, 96(%rsp)
	leal	(%rbx,%rbx), %r10d
	movl	%r10d, 52(%rsp)
	xorl	%r10d, %r10d
	movl	%r10d, 72(%rsp)
	movl	%r10d, %r12d
	.align	16
.LBB140_37:	# bb140
	xorl	%r10d, %r10d
	testl	%ebx, %ebx
	movl	32(%rsp), %ecx
	cmovg	%r10d, %ecx
	movl	48(%rsp), %edx
	leal	(%rdx,%r12), %edx
	cmpl	80(%rsp), %r12d
	cmovl	%r10d, %edx
	movl	%edx, %r10d
	imull	%ebx, %r10d
	movslq	%eax, %r13
	incl	%eax
	cmpl	%r12d, %edx
	movsd	(%r14,%r13,8), %xmm0
	movsd	%xmm0, 112(%rsp)
	movslq	%eax, %rbp
	movsd	(%r14,%rbp,8), %xmm0
	movsd	%xmm0, 104(%rsp)
	jge	.LBB140_40	# bb149
.LBB140_38:	# bb.nph238
	movl	184(%rsp), %eax
	leal	-1(%rax), %esi
	imull	%edx, %esi
	addl	%r12d, %esi
	addl	%esi, %esi
	movl	%r12d, %edi
	subl	%edx, %edi
	leal	-2(,%rax,2), %eax
	xorl	%edx, %edx
	.align	16
.LBB140_39:	# bb147
	movslq	%esi, %r8
	leal	1(%rsi), %r9d
	movslq	%r9d, %r9
	movsd	96(%rsp), %xmm0
	mulsd	(%r15,%r9,8), %xmm0
	addl	%ecx, %r10d
	leal	1(,%r10,2), %ecx
	leal	(%r10,%r10), %r9d
	movslq	%r9d, %r9
	movsd	(%r14,%r9,8), %xmm1
	movapd	%xmm0, %xmm2
	mulsd	%xmm1, %xmm2
	movsd	(%r15,%r8,8), %xmm3
	movslq	%ecx, %rcx
	movsd	(%r14,%rcx,8), %xmm4
	movapd	%xmm3, %xmm5
	mulsd	%xmm4, %xmm5
	addsd	%xmm2, %xmm5
	movsd	104(%rsp), %xmm2
	subsd	%xmm5, %xmm2
	movsd	%xmm2, 104(%rsp)
	mulsd	%xmm4, %xmm0
	mulsd	%xmm1, %xmm3
	subsd	%xmm0, %xmm3
	movsd	112(%rsp), %xmm0
	subsd	%xmm3, %xmm0
	movsd	%xmm0, 112(%rsp)
	addl	%eax, %esi
	incl	%edx
	cmpl	%edi, %edx
	movl	%ebx, %ecx
	jne	.LBB140_39	# bb147
.LBB140_40:	# bb149
	cmpl	$131, 76(%rsp)
	je	.LBB140_62	# bb150
.LBB140_41:	# bb151
	movsd	112(%rsp), %xmm0
	movsd	%xmm0, (%r14,%r13,8)
	movsd	104(%rsp), %xmm0
	movsd	%xmm0, 64(%rsp)
.LBB140_42:	# bb152
	movsd	64(%rsp), %xmm0
	movsd	%xmm0, (%r14,%rbp,8)
	movl	%r13d, %eax
	addl	52(%rsp), %eax
	movl	72(%rsp), %r10d
	addl	56(%rsp), %r10d
	movl	%r10d, 72(%rsp)
	incl	%r12d
	cmpl	84(%rsp), %r12d
	je	.LBB140_16	# bb84.thread
	jmp	.LBB140_37	# bb140
.LBB140_43:	# bb154
	cmpl	$111, %eax
	sete	%cl
	andb	%cl, %sil
	cmpl	$112, %eax
	sete	%al
	testb	%al, %r11b
	jne	.LBB140_45	# bb170
.LBB140_44:	# bb154
	notb	%sil
	testb	$1, %sil
	jne	.LBB140_56	# bb191
.LBB140_45:	# bb170
	testl	%ebx, %ebx
	jg	.LBB140_63	# bb170.bb173_crit_edge
.LBB140_46:	# bb171
	movl	$1, %eax
	subl	84(%rsp), %eax
	imull	%ebx, %eax
.LBB140_47:	# bb173
	movl	184(%rsp), %ecx
	movl	%ecx, %edx
	movl	84(%rsp), %esi
	imull	%esi, %edx
	movl	80(%rsp), %edi
	addl	%edi, %edx
	leal	-2(,%rdx,2), %edx
	movl	%edx, 28(%rsp)
	leal	-1(%rsi), %edx
	movl	%ebx, %r8d
	imull	%edx, %r8d
	addl	%eax, %r8d
	addl	%r8d, %r8d
	movl	%r8d, 72(%rsp)
	imull	%ecx, %edx
	addl	%edi, %edx
	leal	1(,%rdx,2), %eax
	movl	%eax, 24(%rsp)
	addl	%edx, %edx
	movl	%edx, 12(%rsp)
	movl	$1, %eax
	subl	%esi, %eax
	imull	%ebx, %eax
	movl	%eax, 8(%rsp)
	movl	%esi, %eax
	imull	%ebx, %eax
	movl	%eax, 56(%rsp)
	cvtsi2sd	%r10d, %xmm0
	movsd	%xmm0, 96(%rsp)
	leal	(%rbx,%rbx), %eax
	movl	%eax, 20(%rsp)
	leal	(%rcx,%rcx), %eax
	movl	%eax, 16(%rsp)
	xorl	%r12d, %r12d
	movl	%esi, %r13d
	movl	%r12d, 52(%rsp)
	movl	%r12d, 48(%rsp)
	jmp	.LBB140_54	# bb185
.LBB140_48:	# bb174
	testl	%ebx, %ebx
	movl	$0, %eax
	cmovle	8(%rsp), %eax
	movl	80(%rsp), %ecx
	leal	(%rcx,%r13), %ecx
	movl	84(%rsp), %edx
	cmpl	%edx, %ecx
	cmovg	%edx, %ecx
	movl	72(%rsp), %edx
	movslq	%edx, %rbp
	leal	1(%rdx), %edx
	cmpl	%ecx, %r13d
	movsd	(%r14,%rbp,8), %xmm0
	movsd	%xmm0, 112(%rsp)
	movslq	%edx, %rcx
	movq	%rcx, 32(%rsp)
	movsd	(%r14,%rcx,8), %xmm0
	movsd	%xmm0, 104(%rsp)
	jge	.LBB140_51	# bb181
.LBB140_49:	# bb.nph
	movl	$4294967295, %ecx
	movl	84(%rsp), %edx
	subl	%edx, %ecx
	movl	80(%rsp), %esi
	leal	1(%rsi,%rdx), %esi
	movl	52(%rsp), %edi
	subl	%esi, %edi
	cmpl	%edi, %ecx
	cmovg	%ecx, %edi
	addl	%edx, %edi
	movl	48(%rsp), %ecx
	subl	%edi, %ecx
	decl	%ecx
	movl	28(%rsp), %edx
	leal	(%rdx,%r12), %edx
	movl	184(%rsp), %esi
	leal	-2(,%rsi,2), %esi
	xorl	%edi, %edi
	movl	56(%rsp), %r8d
	.align	16
.LBB140_50:	# bb179
	movslq	%edx, %r9
	leal	1(%rdx), %r10d
	movslq	%r10d, %r10
	movsd	96(%rsp), %xmm0
	mulsd	(%r15,%r10,8), %xmm0
	addl	%eax, %r8d
	leal	1(,%r8,2), %eax
	leal	(%r8,%r8), %r10d
	movslq	%r10d, %r10
	movsd	(%r14,%r10,8), %xmm1
	movapd	%xmm0, %xmm2
	mulsd	%xmm1, %xmm2
	movsd	(%r15,%r9,8), %xmm3
	movslq	%eax, %rax
	movsd	(%r14,%rax,8), %xmm4
	movapd	%xmm3, %xmm5
	mulsd	%xmm4, %xmm5
	addsd	%xmm2, %xmm5
	movsd	104(%rsp), %xmm2
	subsd	%xmm5, %xmm2
	movsd	%xmm2, 104(%rsp)
	mulsd	%xmm4, %xmm0
	mulsd	%xmm1, %xmm3
	subsd	%xmm0, %xmm3
	movsd	112(%rsp), %xmm0
	subsd	%xmm3, %xmm0
	movsd	%xmm0, 112(%rsp)
	addl	%esi, %edx
	incl	%edi
	cmpl	%ecx, %edi
	movl	%ebx, %eax
	jne	.LBB140_50	# bb179
.LBB140_51:	# bb181
	cmpl	$131, 76(%rsp)
	je	.LBB140_64	# bb182
.LBB140_52:	# bb183
	movsd	112(%rsp), %xmm0
	movsd	%xmm0, (%r14,%rbp,8)
	movsd	104(%rsp), %xmm0
	movsd	%xmm0, 64(%rsp)
.LBB140_53:	# bb184
	movsd	64(%rsp), %xmm0
	movq	32(%rsp), %rax
	movsd	%xmm0, (%r14,%rax,8)
	subl	%ebx, 56(%rsp)
	movl	72(%rsp), %eax
	subl	20(%rsp), %eax
	movl	%eax, 72(%rsp)
	subl	16(%rsp), %r12d
	decl	%r13d
	incl	52(%rsp)
	incl	48(%rsp)
.LBB140_54:	# bb185
	testl	%r13d, %r13d
	jle	.LBB140_16	# bb84.thread
.LBB140_55:	# bb186
	testl	%r13d, %r13d
	jne	.LBB140_48	# bb174
	jmp	.LBB140_16	# bb84.thread
.LBB140_56:	# bb191
	xorl	%edi, %edi
	leaq	.str185, %rsi
	leaq	.str1186, %rdx
	xorb	%al, %al
	call	cblas_xerbla
	jmp	.LBB140_16	# bb84.thread
.LBB140_57:	# bb66.bb69_crit_edge
	xorl	%eax, %eax
	jmp	.LBB140_7	# bb69
.LBB140_58:	# bb77
	movl	72(%rsp), %eax
	movslq	%eax, %rcx
	leal	1(%rax), %eax
	movslq	%eax, %rax
	movsd	96(%rsp), %xmm0
	mulsd	(%r15,%rax,8), %xmm0
	movsd	%xmm0, 88(%rsp)
	movsd	(%r15,%rcx,8), %xmm0
	movsd	%xmm0, 56(%rsp)
	movsd	88(%rsp), %xmm1
	call	_ZL6xhypotdd187
	movsd	88(%rsp), %xmm1
	divsd	%xmm0, %xmm1
	movsd	%xmm1, 88(%rsp)
	movsd	104(%rsp), %xmm1
	movapd	%xmm1, %xmm2
	mulsd	88(%rsp), %xmm2
	movsd	56(%rsp), %xmm3
	divsd	%xmm0, %xmm3
	movsd	%xmm3, 56(%rsp)
	movsd	112(%rsp), %xmm3
	movapd	%xmm3, %xmm4
	mulsd	56(%rsp), %xmm4
	addsd	%xmm2, %xmm4
	divsd	%xmm0, %xmm4
	movsd	%xmm4, (%r14,%r13,8)
	movsd	88(%rsp), %xmm2
	mulsd	%xmm3, %xmm2
	movsd	%xmm2, 88(%rsp)
	movsd	56(%rsp), %xmm2
	mulsd	%xmm1, %xmm2
	subsd	88(%rsp), %xmm2
	divsd	%xmm0, %xmm2
	movsd	%xmm2, 56(%rsp)
	jmp	.LBB140_13	# bb79
.LBB140_59:	# bb102.bb119.preheader_crit_edge
	xorl	%eax, %eax
	jmp	.LBB140_23	# bb119.preheader
.LBB140_60:	# bb116
	movl	64(%rsp), %eax
	movslq	%eax, %rcx
	leal	1(%rax), %eax
	movslq	%eax, %rax
	movsd	96(%rsp), %xmm0
	mulsd	(%r15,%rax,8), %xmm0
	movsd	%xmm0, 88(%rsp)
	movsd	(%r15,%rcx,8), %xmm0
	movsd	%xmm0, 56(%rsp)
	movsd	88(%rsp), %xmm1
	call	_ZL6xhypotdd187
	movsd	88(%rsp), %xmm1
	divsd	%xmm0, %xmm1
	movsd	%xmm1, 88(%rsp)
	movsd	104(%rsp), %xmm1
	movapd	%xmm1, %xmm2
	mulsd	88(%rsp), %xmm2
	movsd	56(%rsp), %xmm3
	divsd	%xmm0, %xmm3
	movsd	%xmm3, 56(%rsp)
	movsd	112(%rsp), %xmm3
	movapd	%xmm3, %xmm4
	mulsd	56(%rsp), %xmm4
	addsd	%xmm2, %xmm4
	divsd	%xmm0, %xmm4
	movsd	%xmm4, (%r14,%r13,8)
	movsd	88(%rsp), %xmm2
	mulsd	%xmm3, %xmm2
	movsd	%xmm2, 88(%rsp)
	movsd	56(%rsp), %xmm2
	mulsd	%xmm1, %xmm2
	subsd	88(%rsp), %xmm2
	divsd	%xmm0, %xmm2
	movsd	%xmm2, 56(%rsp)
	jmp	.LBB140_30	# bb118
.LBB140_61:	# bb136.bb153.preheader_crit_edge
	xorl	%eax, %eax
	jmp	.LBB140_35	# bb153.preheader
.LBB140_62:	# bb150
	movl	72(%rsp), %eax
	movslq	%eax, %rcx
	leal	1(%rax), %eax
	movslq	%eax, %rax
	movsd	96(%rsp), %xmm0
	mulsd	(%r15,%rax,8), %xmm0
	movsd	%xmm0, 88(%rsp)
	movsd	(%r15,%rcx,8), %xmm0
	movsd	%xmm0, 64(%rsp)
	movsd	88(%rsp), %xmm1
	call	_ZL6xhypotdd187
	movsd	88(%rsp), %xmm1
	divsd	%xmm0, %xmm1
	movsd	%xmm1, 88(%rsp)
	movsd	104(%rsp), %xmm1
	movapd	%xmm1, %xmm2
	mulsd	88(%rsp), %xmm2
	movsd	64(%rsp), %xmm3
	divsd	%xmm0, %xmm3
	movsd	%xmm3, 64(%rsp)
	movsd	112(%rsp), %xmm3
	movapd	%xmm3, %xmm4
	mulsd	64(%rsp), %xmm4
	addsd	%xmm2, %xmm4
	divsd	%xmm0, %xmm4
	movsd	%xmm4, (%r14,%r13,8)
	movsd	88(%rsp), %xmm2
	mulsd	%xmm3, %xmm2
	movsd	%xmm2, 88(%rsp)
	movsd	64(%rsp), %xmm2
	mulsd	%xmm1, %xmm2
	subsd	88(%rsp), %xmm2
	divsd	%xmm0, %xmm2
	movsd	%xmm2, 64(%rsp)
	jmp	.LBB140_42	# bb152
.LBB140_63:	# bb170.bb173_crit_edge
	xorl	%eax, %eax
	jmp	.LBB140_47	# bb173
.LBB140_64:	# bb182
	movl	24(%rsp), %eax
	leal	(%rax,%r12), %eax
	movslq	%eax, %rax
	movsd	96(%rsp), %xmm0
	mulsd	(%r15,%rax,8), %xmm0
	movsd	%xmm0, 88(%rsp)
	movl	12(%rsp), %eax
	leal	(%rax,%r12), %eax
	movslq	%eax, %rax
	movsd	(%r15,%rax,8), %xmm0
	movsd	%xmm0, 64(%rsp)
	movsd	88(%rsp), %xmm1
	call	_ZL6xhypotdd187
	movsd	88(%rsp), %xmm1
	divsd	%xmm0, %xmm1
	movsd	%xmm1, 88(%rsp)
	movsd	104(%rsp), %xmm1
	movapd	%xmm1, %xmm2
	mulsd	88(%rsp), %xmm2
	movsd	64(%rsp), %xmm3
	divsd	%xmm0, %xmm3
	movsd	%xmm3, 64(%rsp)
	movsd	112(%rsp), %xmm3
	movapd	%xmm3, %xmm4
	mulsd	64(%rsp), %xmm4
	addsd	%xmm2, %xmm4
	divsd	%xmm0, %xmm4
	movsd	%xmm4, (%r14,%rbp,8)
	movsd	88(%rsp), %xmm2
	mulsd	%xmm3, %xmm2
	movsd	%xmm2, 88(%rsp)
	movsd	64(%rsp), %xmm2
	mulsd	%xmm1, %xmm2
	subsd	88(%rsp), %xmm2
	divsd	%xmm0, %xmm2
	movsd	%xmm2, 64(%rsp)
	jmp	.LBB140_53	# bb184
	.size	cblas_ztbsv, .-cblas_ztbsv
.Leh_func_end98:


	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI141_0:					
	.quad	9223372036854775807	# double value: nan
	.quad	9223372036854775807	# double value: nan
	.section	.rodata.cst8,"aM",@progbits,8
	.align	16
.LCPI141_1:					
	.quad	4607182418800017408	# double value: 1.000000e+00
	.text
	.align	16
	.type	_ZL6xhypotdd187,@function
_ZL6xhypotdd187:
	movsd	.LCPI141_0(%rip), %xmm2
	andpd	%xmm2, %xmm0
	andpd	%xmm2, %xmm1
	movapd	%xmm1, %xmm2
	maxsd	%xmm0, %xmm2
	minsd	%xmm1, %xmm0
	pxor	%xmm1, %xmm1
	ucomisd	%xmm1, %xmm0
	setnp	%al
	sete	%cl
	testb	%al, %cl
	jne	.LBB141_2	# bb5
.LBB141_1:	# bb4
	divsd	%xmm2, %xmm0
	mulsd	%xmm0, %xmm0
	addsd	.LCPI141_1(%rip), %xmm0
	sqrtsd	%xmm0, %xmm0
	mulsd	%xmm2, %xmm0
	ret
.LBB141_2:	# bb5
	movapd	%xmm2, %xmm0
	ret
	.size	_ZL6xhypotdd187, .-_ZL6xhypotdd187


	.align	16
	.globl	cblas_ztpmv
	.type	cblas_ztpmv,@function
cblas_ztpmv:
.Leh_func_begin99:
.Llabel99:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$24, %rsp
	cmpl	$113, %edx
	movl	$4294967295, %eax
	movl	$1, %r10d
	cmove	%eax, %r10d
	movl	$112, %eax
	cmovne	%edx, %eax
	cmpl	$121, %esi
	sete	%dl
	setne	%r11b
	cmpl	$101, %edi
	sete	%bl
	setne	%r14b
	andb	%dl, %bl
	orb	%r11b, %r14b
	testb	%r14b, %r14b
	movq	80(%rsp), %rdx
	movl	%ecx, 20(%rsp)
	jne	.LBB142_2	# bb48
.LBB142_1:	# entry
	cmpl	$111, %eax
	je	.LBB142_4	# bb56
.LBB142_2:	# bb48
	cmpl	$122, %esi
	sete	%cl
	setne	%r11b
	cmpl	$102, %edi
	sete	%r14b
	setne	%r15b
	andb	%cl, %r14b
	orb	%r11b, %r15b
	testb	%r15b, %r15b
	jne	.LBB142_14	# bb71
.LBB142_3:	# bb48
	cmpl	$112, %eax
	jne	.LBB142_14	# bb71
.LBB142_4:	# bb56
	cmpl	$0, 88(%rsp)
	jg	.LBB142_55	# bb56.bb70.preheader_crit_edge
.LBB142_5:	# bb57
	movl	$1, %eax
	subl	%r8d, %eax
	imull	88(%rsp), %eax
.LBB142_6:	# bb70.preheader
	testl	%r8d, %r8d
	jle	.LBB142_29	# bb105.thread
.LBB142_7:	# bb.nph257
	movl	$1, %ecx
	subl	%r8d, %ecx
	movl	88(%rsp), %esi
	imull	%esi, %ecx
	movl	%ecx, 4(%rsp)
	addl	%eax, %eax
	leal	1(,%r8,2), %ecx
	movl	%ecx, 16(%rsp)
	leal	-1(%r8), %ecx
	movl	%ecx, 12(%rsp)
	cvtsi2sd	%r10d, %xmm0
	leal	(%rsi,%rsi), %r10d
	movl	%r10d, 8(%rsp)
	xorl	%r10d, %r10d
	movl	%r10d, %ecx
	.align	16
.LBB142_8:	# bb60
	movl	16(%rsp), %edi
	leal	(%rdi,%r10), %edi
	imull	%ecx, %edi
	movl	%edi, %r11d
	shrl	$31, %r11d
	addl	%edi, %r11d
	movl	%r11d, %edi
	andl	$4294967294, %edi
	sarl	%r11d
	movslq	%edi, %rbx
	movsd	(%r9,%rbx,8), %xmm1
	orl	$1, %edi
	movslq	%edi, %rdi
	movapd	%xmm0, %xmm2
	mulsd	(%r9,%rdi,8), %xmm2
	movslq	%eax, %rdi
	movsd	(%rdx,%rdi,8), %xmm3
	incl	%eax
	movslq	%eax, %rax
	movsd	(%rdx,%rax,8), %xmm4
	cmpl	$131, 20(%rsp)
	jne	.LBB142_10	# bb63
.LBB142_9:	# bb61
	movapd	%xmm2, %xmm5
	mulsd	%xmm3, %xmm5
	movapd	%xmm1, %xmm6
	mulsd	%xmm4, %xmm6
	addsd	%xmm5, %xmm6
	mulsd	%xmm4, %xmm2
	mulsd	%xmm3, %xmm1
	subsd	%xmm2, %xmm1
	movapd	%xmm6, %xmm4
	movapd	%xmm1, %xmm3
.LBB142_10:	# bb63
	cmpl	$0, 88(%rsp)
	movl	$0, %ebx
	cmovle	4(%rsp), %ebx
	leal	1(%rcx), %r14d
	cmpl	%r8d, %r14d
	jge	.LBB142_13	# bb69
.LBB142_11:	# bb.nph252
	movl	12(%rsp), %r14d
	leal	(%r14,%r10), %r14d
	addl	%r11d, %r11d
	xorl	%r15d, %r15d
	movl	%esi, %r12d
	.align	16
.LBB142_12:	# bb67
	leal	3(%r11), %r13d
	movslq	%r13d, %r13
	movapd	%xmm0, %xmm1
	mulsd	(%r9,%r13,8), %xmm1
	addl	%ebx, %r12d
	leal	1(,%r12,2), %ebx
	leal	(%r12,%r12), %r13d
	movslq	%r13d, %r13
	movsd	(%rdx,%r13,8), %xmm2
	movapd	%xmm1, %xmm5
	mulsd	%xmm2, %xmm5
	addl	$2, %r11d
	movslq	%r11d, %r13
	movsd	(%r9,%r13,8), %xmm6
	movslq	%ebx, %rbx
	movsd	(%rdx,%rbx,8), %xmm7
	movapd	%xmm6, %xmm8
	mulsd	%xmm7, %xmm8
	addsd	%xmm5, %xmm8
	addsd	%xmm8, %xmm4
	mulsd	%xmm7, %xmm1
	mulsd	%xmm2, %xmm6
	subsd	%xmm1, %xmm6
	addsd	%xmm6, %xmm3
	incl	%r15d
	cmpl	%r14d, %r15d
	movl	88(%rsp), %ebx
	jne	.LBB142_12	# bb67
.LBB142_13:	# bb69
	movsd	%xmm3, (%rdx,%rdi,8)
	movsd	%xmm4, (%rdx,%rax,8)
	movl	%edi, %eax
	addl	8(%rsp), %eax
	addl	88(%rsp), %esi
	decl	%r10d
	incl	%ecx
	cmpl	%r8d, %ecx
	jne	.LBB142_8	# bb60
	jmp	.LBB142_29	# bb105.thread
.LBB142_14:	# bb71
	cmpl	$122, %esi
	sete	%cl
	setne	%r11b
	cmpl	$101, %edi
	sete	%r15b
	setne	%r12b
	andb	%cl, %r15b
	orb	%r11b, %r12b
	testb	%r12b, %r12b
	jne	.LBB142_16	# bb79
.LBB142_15:	# bb71
	cmpl	$111, %eax
	je	.LBB142_18	# bb87
.LBB142_16:	# bb79
	cmpl	$121, %esi
	sete	%cl
	setne	%sil
	cmpl	$102, %edi
	sete	%dil
	setne	%r11b
	andb	%cl, %dil
	orb	%sil, %r11b
	testb	%r11b, %r11b
	jne	.LBB142_30	# bb107
.LBB142_17:	# bb79
	cmpl	$112, %eax
	jne	.LBB142_30	# bb107
.LBB142_18:	# bb87
	cmpl	$0, 88(%rsp)
	jg	.LBB142_56	# bb87.bb90_crit_edge
.LBB142_19:	# bb88
	movl	$1, %eax
	subl	%r8d, %eax
	imull	88(%rsp), %eax
.LBB142_20:	# bb90
	leal	-1(%r8), %ecx
	movl	88(%rsp), %esi
	imull	%esi, %ecx
	addl	%eax, %ecx
	addl	%ecx, %ecx
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%esi, %eax
	cvtsi2sd	%r10d, %xmm0
	leal	(%rsi,%rsi), %r10d
	movl	%r10d, 16(%rsp)
	jmp	.LBB142_27	# bb101
.LBB142_21:	# bb91
	movl	%r10d, %esi
	imull	%r8d, %esi
	movl	%esi, %edi
	shrl	$31, %edi
	addl	%esi, %edi
	sarl	%edi
	leal	(%r8,%rdi), %esi
	leal	-1(,%rsi,2), %r11d
	movslq	%r11d, %r11
	movapd	%xmm0, %xmm1
	mulsd	(%r9,%r11,8), %xmm1
	leal	-2(,%rsi,2), %esi
	movslq	%esi, %rsi
	movsd	(%r9,%rsi,8), %xmm2
	movslq	%ecx, %rsi
	movsd	(%rdx,%rsi,8), %xmm3
	leal	1(%rcx), %r11d
	movslq	%r11d, %r11
	movsd	(%rdx,%r11,8), %xmm4
	cmpl	$131, 20(%rsp)
	jne	.LBB142_23	# bb94
.LBB142_22:	# bb92
	movapd	%xmm1, %xmm5
	mulsd	%xmm3, %xmm5
	movapd	%xmm2, %xmm6
	mulsd	%xmm4, %xmm6
	addsd	%xmm5, %xmm6
	mulsd	%xmm4, %xmm1
	mulsd	%xmm3, %xmm2
	subsd	%xmm1, %xmm2
	movapd	%xmm6, %xmm4
	movapd	%xmm2, %xmm3
.LBB142_23:	# bb94
	cmpl	$0, 88(%rsp)
	movl	$0, %ebx
	cmovle	%eax, %ebx
	testl	%r10d, %r10d
	jle	.LBB142_26	# bb100
.LBB142_24:	# bb.nph240
	movl	88(%rsp), %r10d
	leal	(%r10,%r10), %r10d
	addl	%ebx, %ebx
	addl	%edi, %edi
	leal	-1(%r8), %r14d
	xorl	%r15d, %r15d
	.align	16
.LBB142_25:	# bb98
	movslq	%edi, %r12
	leal	1(%rdi), %r13d
	movslq	%r13d, %r13
	movapd	%xmm0, %xmm1
	mulsd	(%r9,%r13,8), %xmm1
	movslq	%ebx, %r13
	movsd	(%rdx,%r13,8), %xmm2
	movapd	%xmm1, %xmm5
	mulsd	%xmm2, %xmm5
	movsd	(%r9,%r12,8), %xmm6
	leal	1(%rbx), %r12d
	movslq	%r12d, %r12
	movsd	(%rdx,%r12,8), %xmm7
	movapd	%xmm6, %xmm8
	mulsd	%xmm7, %xmm8
	addsd	%xmm5, %xmm8
	addsd	%xmm8, %xmm4
	mulsd	%xmm2, %xmm6
	mulsd	%xmm7, %xmm1
	subsd	%xmm1, %xmm6
	addsd	%xmm6, %xmm3
	addl	%r10d, %ebx
	addl	$2, %edi
	incl	%r15d
	cmpl	%r14d, %r15d
	jne	.LBB142_25	# bb98
.LBB142_26:	# bb100
	movsd	%xmm3, (%rdx,%rsi,8)
	movsd	%xmm4, (%rdx,%r11,8)
	subl	16(%rsp), %ecx
	decl	%r8d
.LBB142_27:	# bb101
	testl	%r8d, %r8d
	jle	.LBB142_29	# bb105.thread
.LBB142_28:	# bb102
	leal	-1(%r8), %r10d
	testl	%r8d, %r8d
	jne	.LBB142_21	# bb91
.LBB142_29:	# bb105.thread
	addq	$24, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB142_30:	# bb107
	cmpl	$111, %eax
	sete	%cl
	andb	%cl, %r14b
	cmpl	$112, %eax
	sete	%cl
	testb	%cl, %bl
	jne	.LBB142_32	# bb123
.LBB142_31:	# bb107
	notb	%r14b
	testb	$1, %r14b
	jne	.LBB142_43	# bb143
.LBB142_32:	# bb123
	cmpl	$0, 88(%rsp)
	jg	.LBB142_57	# bb123.bb126_crit_edge
.LBB142_33:	# bb124
	movl	$1, %eax
	subl	%r8d, %eax
	imull	88(%rsp), %eax
.LBB142_34:	# bb126
	leal	-1(%r8), %ecx
	movl	88(%rsp), %esi
	imull	%esi, %ecx
	addl	%eax, %ecx
	addl	%ecx, %ecx
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%esi, %eax
	movl	%eax, 8(%rsp)
	leal	2(%r8), %eax
	movl	%eax, 16(%rsp)
	cvtsi2sd	%r10d, %xmm0
	leal	(%rsi,%rsi), %eax
	movl	%eax, 12(%rsp)
	movl	%r8d, %eax
	jmp	.LBB142_41	# bb137
.LBB142_35:	# bb127
	movl	16(%rsp), %esi
	imull	%r10d, %esi
	movl	%esi, %edi
	shrl	$31, %edi
	addl	%esi, %edi
	andl	$4294967294, %edi
	movslq	%edi, %rsi
	movsd	(%r9,%rsi,8), %xmm1
	orl	$1, %edi
	movslq	%edi, %rsi
	movapd	%xmm0, %xmm2
	mulsd	(%r9,%rsi,8), %xmm2
	movslq	%ecx, %rsi
	movsd	(%rdx,%rsi,8), %xmm3
	leal	1(%rcx), %edi
	movslq	%edi, %rdi
	movsd	(%rdx,%rdi,8), %xmm4
	cmpl	$131, 20(%rsp)
	jne	.LBB142_37	# bb130
.LBB142_36:	# bb128
	movapd	%xmm2, %xmm5
	mulsd	%xmm3, %xmm5
	movapd	%xmm1, %xmm6
	mulsd	%xmm4, %xmm6
	addsd	%xmm5, %xmm6
	mulsd	%xmm4, %xmm2
	mulsd	%xmm3, %xmm1
	subsd	%xmm2, %xmm1
	movapd	%xmm1, %xmm3
	movapd	%xmm6, %xmm4
.LBB142_37:	# bb130
	cmpl	$0, 88(%rsp)
	movl	$0, %r11d
	cmovle	8(%rsp), %r11d
	testl	%r10d, %r10d
	jle	.LBB142_40	# bb136
.LBB142_38:	# bb.nph232
	movl	88(%rsp), %r10d
	leal	(%r10,%r10), %r10d
	addl	%r11d, %r11d
	leal	1(,%r8,2), %ebx
	leal	-1(%rax), %r14d
	xorl	%r15d, %r15d
	movl	%r15d, %r12d
	.align	16
.LBB142_39:	# bb134
	leal	(%rbx,%r15), %r13d
	imull	%r12d, %r13d
	movl	%r13d, %ebp
	shrl	$31, %ebp
	addl	%r13d, %ebp
	sarl	%ebp
	leal	(%r14,%r15), %r13d
	addl	%ebp, %r13d
	leal	1(,%r13,2), %ebp
	movslq	%ebp, %rbp
	movapd	%xmm0, %xmm1
	mulsd	(%r9,%rbp,8), %xmm1
	movslq	%r11d, %rbp
	movsd	(%rdx,%rbp,8), %xmm2
	movapd	%xmm1, %xmm5
	mulsd	%xmm2, %xmm5
	addl	%r13d, %r13d
	movslq	%r13d, %r13
	movsd	(%r9,%r13,8), %xmm6
	leal	1(%r11), %r13d
	movslq	%r13d, %r13
	movsd	(%rdx,%r13,8), %xmm7
	movapd	%xmm6, %xmm8
	mulsd	%xmm7, %xmm8
	addsd	%xmm5, %xmm8
	addsd	%xmm8, %xmm4
	mulsd	%xmm7, %xmm1
	mulsd	%xmm2, %xmm6
	subsd	%xmm1, %xmm6
	addsd	%xmm6, %xmm3
	addl	%r10d, %r11d
	decl	%r15d
	incl	%r12d
	cmpl	%r14d, %r12d
	jne	.LBB142_39	# bb134
.LBB142_40:	# bb136
	movsd	%xmm3, (%rdx,%rsi,8)
	movsd	%xmm4, (%rdx,%rdi,8)
	subl	12(%rsp), %ecx
	decl	%eax
	incl	16(%rsp)
.LBB142_41:	# bb137
	testl	%eax, %eax
	jle	.LBB142_29	# bb105.thread
.LBB142_42:	# bb138
	leal	-1(%rax), %r10d
	testl	%eax, %eax
	jne	.LBB142_35	# bb127
	jmp	.LBB142_29	# bb105.thread
.LBB142_43:	# bb143
	cmpl	$111, %eax
	sete	%cl
	andb	%cl, %dil
	cmpl	$112, %eax
	sete	%al
	testb	%al, %r15b
	jne	.LBB142_45	# bb159
.LBB142_44:	# bb143
	notb	%dil
	testb	$1, %dil
	jne	.LBB142_59	# bb174
.LBB142_45:	# bb159
	cmpl	$0, 88(%rsp)
	jg	.LBB142_58	# bb159.bb173.preheader_crit_edge
.LBB142_46:	# bb160
	movl	$1, %eax
	subl	%r8d, %eax
	imull	88(%rsp), %eax
.LBB142_47:	# bb173.preheader
	testl	%r8d, %r8d
	jle	.LBB142_29	# bb105.thread
.LBB142_48:	# bb.nph226
	movl	$1, %ecx
	subl	%r8d, %ecx
	movl	88(%rsp), %esi
	imull	%esi, %ecx
	movl	%ecx, 12(%rsp)
	addl	%eax, %eax
	leal	-1(%r8), %ecx
	cvtsi2sd	%r10d, %xmm0
	leal	(%rsi,%rsi), %edi
	movl	%edi, 16(%rsp)
	xorl	%edi, %edi
	.align	16
.LBB142_49:	# bb163
	leal	1(%rdi), %r10d
	movl	%r10d, %r11d
	imull	%edi, %r11d
	movl	%r11d, %ebx
	shrl	$31, %ebx
	addl	%r11d, %ebx
	sarl	%ebx
	addl	%edi, %ebx
	leal	1(,%rbx,2), %r11d
	movslq	%r11d, %r11
	movapd	%xmm0, %xmm1
	mulsd	(%r9,%r11,8), %xmm1
	addl	%ebx, %ebx
	movslq	%ebx, %r11
	movsd	(%r9,%r11,8), %xmm2
	movslq	%eax, %r11
	movsd	(%rdx,%r11,8), %xmm3
	incl	%eax
	movslq	%eax, %rax
	movsd	(%rdx,%rax,8), %xmm4
	cmpl	$131, 20(%rsp)
	jne	.LBB142_51	# bb166
.LBB142_50:	# bb164
	movapd	%xmm1, %xmm5
	mulsd	%xmm3, %xmm5
	movapd	%xmm2, %xmm6
	mulsd	%xmm4, %xmm6
	addsd	%xmm5, %xmm6
	mulsd	%xmm4, %xmm1
	mulsd	%xmm3, %xmm2
	subsd	%xmm1, %xmm2
	movapd	%xmm2, %xmm3
	movapd	%xmm6, %xmm4
.LBB142_51:	# bb166
	cmpl	$0, 88(%rsp)
	movl	$0, %ebx
	cmovle	12(%rsp), %ebx
	cmpl	%r8d, %r10d
	jge	.LBB142_54	# bb172
.LBB142_52:	# bb.nph
	leal	1(%rdi), %r10d
	leal	2(%rdi), %r14d
	xorl	%r15d, %r15d
	movl	%esi, %r12d
	.align	16
.LBB142_53:	# bb170
	leal	(%r10,%r15), %r13d
	leal	(%r14,%r15), %ebp
	imull	%r13d, %ebp
	movl	%ebp, %r13d
	shrl	$31, %r13d
	addl	%ebp, %r13d
	sarl	%r13d
	addl	%edi, %r13d
	leal	1(,%r13,2), %ebp
	movslq	%ebp, %rbp
	movapd	%xmm0, %xmm1
	mulsd	(%r9,%rbp,8), %xmm1
	addl	%ebx, %r12d
	leal	1(,%r12,2), %ebx
	leal	(%r12,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%rdx,%rbp,8), %xmm2
	movapd	%xmm1, %xmm5
	mulsd	%xmm2, %xmm5
	addl	%r13d, %r13d
	movslq	%r13d, %r13
	movsd	(%r9,%r13,8), %xmm6
	movslq	%ebx, %rbx
	movsd	(%rdx,%rbx,8), %xmm7
	movapd	%xmm6, %xmm8
	mulsd	%xmm7, %xmm8
	addsd	%xmm5, %xmm8
	addsd	%xmm8, %xmm4
	mulsd	%xmm7, %xmm1
	mulsd	%xmm2, %xmm6
	subsd	%xmm1, %xmm6
	addsd	%xmm6, %xmm3
	incl	%r15d
	cmpl	%ecx, %r15d
	movl	88(%rsp), %ebx
	jne	.LBB142_53	# bb170
.LBB142_54:	# bb172
	movsd	%xmm3, (%rdx,%r11,8)
	movsd	%xmm4, (%rdx,%rax,8)
	movl	%r11d, %eax
	addl	16(%rsp), %eax
	addl	88(%rsp), %esi
	decl	%ecx
	incl	%edi
	cmpl	%r8d, %edi
	je	.LBB142_29	# bb105.thread
	jmp	.LBB142_49	# bb163
.LBB142_55:	# bb56.bb70.preheader_crit_edge
	xorl	%eax, %eax
	jmp	.LBB142_6	# bb70.preheader
.LBB142_56:	# bb87.bb90_crit_edge
	xorl	%eax, %eax
	jmp	.LBB142_20	# bb90
.LBB142_57:	# bb123.bb126_crit_edge
	xorl	%eax, %eax
	jmp	.LBB142_34	# bb126
.LBB142_58:	# bb159.bb173.preheader_crit_edge
	xorl	%eax, %eax
	jmp	.LBB142_47	# bb173.preheader
.LBB142_59:	# bb174
	xorl	%edi, %edi
	leaq	.str189, %rsi
	leaq	.str1190, %rdx
	xorb	%al, %al
	call	cblas_xerbla
	jmp	.LBB142_29	# bb105.thread
	.size	cblas_ztpmv, .-cblas_ztpmv
.Leh_func_end99:


	.align	16
	.globl	cblas_ztpsv
	.type	cblas_ztpsv,@function
cblas_ztpsv:
.Leh_func_begin100:
.Llabel100:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$88, %rsp
	cmpl	$113, %edx
	movl	$4294967295, %eax
	movl	$1, %r10d
	cmove	%eax, %r10d
	movl	%r10d, 56(%rsp)
	movl	$112, %eax
	cmovne	%edx, %eax
	testl	%r8d, %r8d
	movq	144(%rsp), %rbx
	movq	%r9, %r14
	movl	%r8d, 52(%rsp)
	movl	%ecx, 48(%rsp)
	je	.LBB143_18	# bb105.thread
.LBB143_1:	# bb73
	cmpl	$121, %esi
	sete	%cl
	setne	%dl
	cmpl	$101, %edi
	sete	%r8b
	setne	%r9b
	andb	%cl, %r8b
	orb	%dl, %r9b
	testb	%r9b, %r9b
	jne	.LBB143_3	# bb80
.LBB143_2:	# bb73
	cmpl	$111, %eax
	je	.LBB143_5	# bb88
.LBB143_3:	# bb80
	cmpl	$122, %esi
	sete	%cl
	setne	%dl
	cmpl	$102, %edi
	sete	%r9b
	setne	%r10b
	andb	%cl, %r9b
	orb	%dl, %r10b
	testb	%r10b, %r10b
	jne	.LBB143_19	# bb107
.LBB143_4:	# bb80
	cmpl	$112, %eax
	jne	.LBB143_19	# bb107
.LBB143_5:	# bb88
	cmpl	$0, 152(%rsp)
	jg	.LBB143_65	# bb88.bb91_crit_edge
.LBB143_6:	# bb89
	movl	$1, %r15d
	subl	52(%rsp), %r15d
	imull	152(%rsp), %r15d
.LBB143_7:	# bb91
	movl	52(%rsp), %eax
	leal	-1(%rax), %eax
	movl	%eax, %ecx
	imull	152(%rsp), %ecx
	addl	%r15d, %ecx
	cmpl	$131, 48(%rsp)
	jne	.LBB143_9	# bb93
.LBB143_8:	# bb92
	movl	52(%rsp), %edx
	leal	(%rdx,%rdx), %esi
	leal	-2(%rdx), %edx
	subl	%edx, %esi
	imull	%eax, %esi
	movl	%esi, %eax
	shrl	$31, %eax
	addl	%esi, %eax
	andl	$4294967294, %eax
	movslq	%eax, %rdx
	orl	$1, %eax
	movslq	%eax, %rax
	cvtsi2sd	56(%rsp), %xmm0
	mulsd	(%r14,%rax,8), %xmm0
	movsd	%xmm0, 72(%rsp)
	movsd	(%r14,%rdx,8), %xmm0
	movsd	%xmm0, 80(%rsp)
	leal	1(,%rcx,2), %eax
	movslq	%eax, %r12
	movsd	(%rbx,%r12,8), %xmm0
	movsd	%xmm0, 40(%rsp)
	addl	%ecx, %ecx
	movslq	%ecx, %r13
	movsd	(%rbx,%r13,8), %xmm0
	movsd	%xmm0, 64(%rsp)
	movsd	80(%rsp), %xmm0
	movsd	72(%rsp), %xmm1
	call	_ZL6xhypotdd193
	movsd	80(%rsp), %xmm1
	divsd	%xmm0, %xmm1
	movsd	%xmm1, 80(%rsp)
	movsd	72(%rsp), %xmm1
	divsd	%xmm0, %xmm1
	movsd	%xmm1, 72(%rsp)
	movsd	40(%rsp), %xmm1
	mulsd	72(%rsp), %xmm1
	movsd	72(%rsp), %xmm2
	mulsd	64(%rsp), %xmm2
	movsd	%xmm2, 72(%rsp)
	movsd	64(%rsp), %xmm2
	mulsd	80(%rsp), %xmm2
	addsd	%xmm1, %xmm2
	divsd	%xmm0, %xmm2
	movsd	%xmm2, (%rbx,%r13,8)
	movsd	80(%rsp), %xmm1
	mulsd	40(%rsp), %xmm1
	subsd	72(%rsp), %xmm1
	divsd	%xmm0, %xmm1
	movsd	%xmm1, (%rbx,%r12,8)
.LBB143_9:	# bb93
	movl	52(%rsp), %eax
	leal	-1(%rax), %ecx
	movl	152(%rsp), %edx
	imull	%edx, %ecx
	addl	%r15d, %ecx
	addl	%ecx, %ecx
	movl	%ecx, 16(%rsp)
	leal	-2(%rax), %ecx
	imull	%edx, %ecx
	addl	%r15d, %ecx
	leal	1(,%rcx,2), %esi
	movl	%esi, 24(%rsp)
	addl	%ecx, %ecx
	movl	%ecx, 12(%rsp)
	leal	3(%rax), %ecx
	movl	%ecx, 28(%rsp)
	cvtsi2sd	56(%rsp), %xmm0
	movsd	%xmm0, 64(%rsp)
	leal	(%rdx,%rdx), %ecx
	movl	%ecx, 20(%rsp)
	xorl	%r15d, %r15d
	movl	%eax, 40(%rsp)
	movl	%r15d, %r12d
	jmp	.LBB143_16	# bb101
.LBB143_10:	# bb94
	movl	12(%rsp), %edx
	leal	(%rdx,%r15), %edx
	movl	24(%rsp), %esi
	leal	(%rsi,%r15), %esi
	cmpl	52(%rsp), %eax
	movslq	%esi, %r13
	movsd	(%rbx,%r13,8), %xmm0
	movsd	%xmm0, 80(%rsp)
	movslq	%edx, %rbp
	movsd	(%rbx,%rbp,8), %xmm0
	movsd	%xmm0, 72(%rsp)
	jge	.LBB143_13	# bb97
.LBB143_11:	# bb.nph294
	movl	28(%rsp), %eax
	leal	(%rax,%r12), %eax
	imull	%ecx, %eax
	movl	%eax, %edx
	shrl	$31, %edx
	addl	%eax, %edx
	andl	$4294967294, %edx
	movl	16(%rsp), %eax
	leal	(%rax,%r15), %eax
	movl	152(%rsp), %esi
	leal	(%rsi,%rsi), %esi
	leal	1(%r12), %edi
	xorl	%r8d, %r8d
	.align	16
.LBB143_12:	# bb95
	leal	3(%rdx), %r9d
	movslq	%r9d, %r9
	movsd	64(%rsp), %xmm0
	mulsd	(%r14,%r9,8), %xmm0
	movslq	%eax, %r9
	movsd	(%rbx,%r9,8), %xmm1
	movapd	%xmm0, %xmm2
	mulsd	%xmm1, %xmm2
	leal	1(%rax), %r9d
	movslq	%r9d, %r9
	movsd	(%rbx,%r9,8), %xmm3
	addl	$2, %edx
	movslq	%edx, %r9
	movsd	(%r14,%r9,8), %xmm4
	movapd	%xmm4, %xmm5
	mulsd	%xmm3, %xmm5
	addsd	%xmm2, %xmm5
	movsd	80(%rsp), %xmm2
	subsd	%xmm5, %xmm2
	movsd	%xmm2, 80(%rsp)
	mulsd	%xmm3, %xmm0
	mulsd	%xmm1, %xmm4
	subsd	%xmm0, %xmm4
	movsd	72(%rsp), %xmm0
	subsd	%xmm4, %xmm0
	movsd	%xmm0, 72(%rsp)
	addl	%esi, %eax
	incl	%r8d
	cmpl	%edi, %r8d
	jne	.LBB143_12	# bb95
.LBB143_13:	# bb97
	cmpl	$131, 48(%rsp)
	je	.LBB143_66	# bb98
.LBB143_14:	# bb99
	movsd	72(%rsp), %xmm0
	movsd	%xmm0, (%rbx,%rbp,8)
	movsd	80(%rsp), %xmm0
	movsd	%xmm0, 32(%rsp)
.LBB143_15:	# bb101.backedge
	movsd	32(%rsp), %xmm0
	movsd	%xmm0, (%rbx,%r13,8)
	subl	20(%rsp), %r15d
	decl	40(%rsp)
	incl	%r12d
.LBB143_16:	# bb101
	movl	40(%rsp), %eax
	leal	-1(%rax), %eax
	testl	%eax, %eax
	jle	.LBB143_18	# bb105.thread
.LBB143_17:	# bb102
	movl	40(%rsp), %ecx
	leal	-2(%rcx), %ecx
	cmpl	$4294967295, %ecx
	jne	.LBB143_10	# bb94
.LBB143_18:	# bb105.thread
	addq	$88, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB143_19:	# bb107
	cmpl	$122, %esi
	sete	%cl
	setne	%dl
	cmpl	$101, %edi
	sete	%r10b
	setne	%r11b
	andb	%cl, %r10b
	orb	%dl, %r11b
	testb	%r11b, %r11b
	jne	.LBB143_21	# bb115
.LBB143_20:	# bb107
	cmpl	$111, %eax
	je	.LBB143_23	# bb123
.LBB143_21:	# bb115
	cmpl	$121, %esi
	sete	%cl
	setne	%dl
	cmpl	$102, %edi
	sete	%sil
	setne	%dil
	andb	%cl, %sil
	orb	%dl, %dil
	testb	%dil, %dil
	jne	.LBB143_35	# bb140
.LBB143_22:	# bb115
	cmpl	$112, %eax
	jne	.LBB143_35	# bb140
.LBB143_23:	# bb123
	cmpl	$0, 152(%rsp)
	jg	.LBB143_67	# bb123.bb126_crit_edge
.LBB143_24:	# bb124
	movl	$1, %r15d
	subl	52(%rsp), %r15d
	imull	152(%rsp), %r15d
.LBB143_25:	# bb126
	cmpl	$131, 48(%rsp)
	jne	.LBB143_27	# bb139.preheader
.LBB143_26:	# bb127
	cvtsi2sd	56(%rsp), %xmm0
	mulsd	8(%r14), %xmm0
	movsd	%xmm0, 72(%rsp)
	leal	1(,%r15,2), %eax
	movslq	%eax, %r12
	movsd	(%rbx,%r12,8), %xmm0
	movsd	%xmm0, 40(%rsp)
	leal	(%r15,%r15), %eax
	movslq	%eax, %r13
	movsd	(%rbx,%r13,8), %xmm0
	movsd	%xmm0, 64(%rsp)
	movsd	(%r14), %xmm0
	movsd	%xmm0, 80(%rsp)
	movsd	72(%rsp), %xmm1
	call	_ZL6xhypotdd193
	movsd	72(%rsp), %xmm1
	divsd	%xmm0, %xmm1
	movsd	%xmm1, 72(%rsp)
	movsd	40(%rsp), %xmm1
	mulsd	72(%rsp), %xmm1
	movsd	72(%rsp), %xmm2
	mulsd	64(%rsp), %xmm2
	movsd	%xmm2, 72(%rsp)
	movsd	80(%rsp), %xmm2
	divsd	%xmm0, %xmm2
	movsd	%xmm2, 80(%rsp)
	movsd	64(%rsp), %xmm2
	mulsd	80(%rsp), %xmm2
	addsd	%xmm1, %xmm2
	divsd	%xmm0, %xmm2
	movsd	%xmm2, (%rbx,%r13,8)
	movsd	80(%rsp), %xmm1
	mulsd	40(%rsp), %xmm1
	subsd	72(%rsp), %xmm1
	divsd	%xmm0, %xmm1
	movsd	%xmm1, (%rbx,%r12,8)
.LBB143_27:	# bb139.preheader
	cmpl	$2, 52(%rsp)
	jl	.LBB143_18	# bb105.thread
.LBB143_28:	# bb.nph285
	movl	152(%rsp), %eax
	addl	%eax, %r15d
	addl	%r15d, %r15d
	movl	$1, %r12d
	movl	52(%rsp), %ecx
	subl	%ecx, %r12d
	imull	%eax, %r12d
	decl	%ecx
	movl	%ecx, 52(%rsp)
	cvtsi2sd	56(%rsp), %xmm0
	movsd	%xmm0, 64(%rsp)
	leal	(%rax,%rax), %eax
	movl	%eax, 32(%rsp)
	xorl	%r13d, %r13d
	.align	16
.LBB143_29:	# bb129
	cmpl	$0, 152(%rsp)
	movl	$0, %eax
	cmovle	%r12d, %eax
	movslq	%r15d, %rbp
	movsd	(%rbx,%rbp,8), %xmm0
	movsd	%xmm0, 80(%rsp)
	incl	%r15d
	movslq	%r15d, %r15
	movsd	(%rbx,%r15,8), %xmm0
	movsd	%xmm0, 72(%rsp)
	leal	1(%r13), %ecx
	testl	%ecx, %ecx
	jle	.LBB143_32	# bb135
.LBB143_30:	# bb.nph278
	leal	2(%r13), %edx
	imull	%ecx, %edx
	movl	%edx, %esi
	shrl	$31, %esi
	addl	%edx, %esi
	andl	$4294967294, %esi
	movl	152(%rsp), %edx
	leal	(%rdx,%rdx), %edx
	addl	%eax, %eax
	xorl	%edi, %edi
	.align	16
.LBB143_31:	# bb133
	movslq	%esi, %r8
	leal	1(%rsi), %r9d
	movslq	%r9d, %r9
	movsd	64(%rsp), %xmm0
	mulsd	(%r14,%r9,8), %xmm0
	movslq	%eax, %r9
	movsd	(%rbx,%r9,8), %xmm1
	movapd	%xmm0, %xmm2
	mulsd	%xmm1, %xmm2
	movsd	(%r14,%r8,8), %xmm3
	leal	1(%rax), %r8d
	movslq	%r8d, %r8
	movsd	(%rbx,%r8,8), %xmm4
	movapd	%xmm3, %xmm5
	mulsd	%xmm4, %xmm5
	addsd	%xmm2, %xmm5
	movsd	72(%rsp), %xmm2
	subsd	%xmm5, %xmm2
	movsd	%xmm2, 72(%rsp)
	mulsd	%xmm1, %xmm3
	mulsd	%xmm4, %xmm0
	subsd	%xmm0, %xmm3
	movsd	80(%rsp), %xmm0
	subsd	%xmm3, %xmm0
	movsd	%xmm0, 80(%rsp)
	addl	%edx, %eax
	addl	$2, %esi
	incl	%edi
	cmpl	%ecx, %edi
	jne	.LBB143_31	# bb133
.LBB143_32:	# bb135
	cmpl	$131, 48(%rsp)
	je	.LBB143_68	# bb136
.LBB143_33:	# bb137
	movsd	80(%rsp), %xmm0
	movsd	%xmm0, (%rbx,%rbp,8)
	movsd	72(%rsp), %xmm0
	movsd	%xmm0, 40(%rsp)
.LBB143_34:	# bb138
	movsd	40(%rsp), %xmm0
	movsd	%xmm0, (%rbx,%r15,8)
	movl	%ebp, %r15d
	addl	32(%rsp), %r15d
	incl	%r13d
	cmpl	52(%rsp), %r13d
	je	.LBB143_18	# bb105.thread
	jmp	.LBB143_29	# bb129
.LBB143_35:	# bb140
	cmpl	$111, %eax
	sete	%cl
	andb	%cl, %r9b
	cmpl	$112, %eax
	sete	%cl
	testb	%cl, %r8b
	jne	.LBB143_37	# bb156
.LBB143_36:	# bb140
	notb	%r9b
	testb	$1, %r9b
	jne	.LBB143_49	# bb173
.LBB143_37:	# bb156
	cmpl	$0, 152(%rsp)
	jg	.LBB143_69	# bb156.bb159_crit_edge
.LBB143_38:	# bb157
	movl	$1, %r15d
	subl	52(%rsp), %r15d
	imull	152(%rsp), %r15d
.LBB143_39:	# bb159
	cmpl	$131, 48(%rsp)
	jne	.LBB143_41	# bb172.preheader
.LBB143_40:	# bb160
	cvtsi2sd	56(%rsp), %xmm0
	mulsd	8(%r14), %xmm0
	movsd	%xmm0, 72(%rsp)
	leal	1(,%r15,2), %eax
	movslq	%eax, %r12
	movsd	(%rbx,%r12,8), %xmm0
	movsd	%xmm0, 40(%rsp)
	leal	(%r15,%r15), %eax
	movslq	%eax, %r13
	movsd	(%rbx,%r13,8), %xmm0
	movsd	%xmm0, 64(%rsp)
	movsd	(%r14), %xmm0
	movsd	%xmm0, 80(%rsp)
	movsd	72(%rsp), %xmm1
	call	_ZL6xhypotdd193
	movsd	72(%rsp), %xmm1
	divsd	%xmm0, %xmm1
	movsd	%xmm1, 72(%rsp)
	movsd	40(%rsp), %xmm1
	mulsd	72(%rsp), %xmm1
	movsd	72(%rsp), %xmm2
	mulsd	64(%rsp), %xmm2
	movsd	%xmm2, 72(%rsp)
	movsd	80(%rsp), %xmm2
	divsd	%xmm0, %xmm2
	movsd	%xmm2, 80(%rsp)
	movsd	64(%rsp), %xmm2
	mulsd	80(%rsp), %xmm2
	addsd	%xmm1, %xmm2
	divsd	%xmm0, %xmm2
	movsd	%xmm2, (%rbx,%r13,8)
	movsd	80(%rsp), %xmm1
	mulsd	40(%rsp), %xmm1
	subsd	72(%rsp), %xmm1
	divsd	%xmm0, %xmm1
	movsd	%xmm1, (%rbx,%r12,8)
.LBB143_41:	# bb172.preheader
	cmpl	$2, 52(%rsp)
	jl	.LBB143_18	# bb105.thread
.LBB143_42:	# bb.nph273
	movl	152(%rsp), %eax
	addl	%eax, %r15d
	addl	%r15d, %r15d
	movl	$1, %edx
	movl	52(%rsp), %ecx
	subl	%ecx, %edx
	imull	%eax, %edx
	movl	%edx, 24(%rsp)
	leal	(%rcx,%rcx), %r12d
	leal	-1(%rcx), %ecx
	movl	%ecx, 32(%rsp)
	cvtsi2sd	56(%rsp), %xmm0
	movsd	%xmm0, 64(%rsp)
	leal	(%rax,%rax), %eax
	movl	%eax, 28(%rsp)
	xorl	%r13d, %r13d
	.align	16
.LBB143_43:	# bb162
	cmpl	$0, 152(%rsp)
	movl	$0, %eax
	cmovle	24(%rsp), %eax
	movslq	%r15d, %rbp
	movsd	(%rbx,%rbp,8), %xmm0
	movsd	%xmm0, 80(%rsp)
	incl	%r15d
	movslq	%r15d, %r15
	movsd	(%rbx,%r15,8), %xmm0
	movsd	%xmm0, 72(%rsp)
	leal	1(%r13), %ecx
	testl	%ecx, %ecx
	jle	.LBB143_46	# bb168
.LBB143_44:	# bb166.preheader
	movl	152(%rsp), %edx
	leal	(%rdx,%rdx), %edx
	addl	%eax, %eax
	movl	52(%rsp), %esi
	leal	(%rsi,%rsi), %esi
	xorl	%edi, %edi
	movl	$1, %r8d
	.align	16
.LBB143_45:	# bb166
	leal	(%rsi,%r8), %r9d
	imull	%edi, %r9d
	movl	%r9d, %r10d
	shrl	$31, %r10d
	addl	%r9d, %r10d
	sarl	%r10d
	leal	(%r13,%r8), %r9d
	addl	%r10d, %r9d
	leal	1(,%r9,2), %r10d
	movslq	%r10d, %r10
	movsd	64(%rsp), %xmm0
	mulsd	(%r14,%r10,8), %xmm0
	movslq	%eax, %r10
	movsd	(%rbx,%r10,8), %xmm1
	movapd	%xmm0, %xmm2
	mulsd	%xmm1, %xmm2
	addl	%r9d, %r9d
	movslq	%r9d, %r9
	movsd	(%r14,%r9,8), %xmm3
	leal	1(%rax), %r9d
	movslq	%r9d, %r9
	movsd	(%rbx,%r9,8), %xmm4
	movapd	%xmm3, %xmm5
	mulsd	%xmm4, %xmm5
	addsd	%xmm2, %xmm5
	movsd	72(%rsp), %xmm2
	subsd	%xmm5, %xmm2
	movsd	%xmm2, 72(%rsp)
	mulsd	%xmm4, %xmm0
	mulsd	%xmm1, %xmm3
	subsd	%xmm0, %xmm3
	movsd	80(%rsp), %xmm0
	subsd	%xmm3, %xmm0
	movsd	%xmm0, 80(%rsp)
	addl	%edx, %eax
	decl	%r8d
	incl	%edi
	cmpl	%ecx, %edi
	jne	.LBB143_45	# bb166
.LBB143_46:	# bb168
	cmpl	$131, 48(%rsp)
	je	.LBB143_70	# bb169
.LBB143_47:	# bb170
	movsd	80(%rsp), %xmm0
	movsd	%xmm0, (%rbx,%rbp,8)
	movsd	72(%rsp), %xmm0
	movsd	%xmm0, 40(%rsp)
.LBB143_48:	# bb171
	movsd	40(%rsp), %xmm0
	movsd	%xmm0, (%rbx,%r15,8)
	movl	%ebp, %r15d
	addl	28(%rsp), %r15d
	decl	%r12d
	incl	%r13d
	cmpl	32(%rsp), %r13d
	je	.LBB143_18	# bb105.thread
	jmp	.LBB143_43	# bb162
.LBB143_49:	# bb173
	cmpl	$111, %eax
	sete	%cl
	andb	%cl, %sil
	cmpl	$112, %eax
	sete	%al
	testb	%al, %r10b
	jne	.LBB143_51	# bb189
.LBB143_50:	# bb173
	notb	%sil
	testb	$1, %sil
	jne	.LBB143_64	# bb208
.LBB143_51:	# bb189
	cmpl	$0, 152(%rsp)
	jg	.LBB143_71	# bb189.bb192_crit_edge
.LBB143_52:	# bb190
	movl	$1, %r15d
	subl	52(%rsp), %r15d
	imull	152(%rsp), %r15d
.LBB143_53:	# bb192
	movl	52(%rsp), %eax
	leal	-1(%rax), %eax
	movl	%eax, %ecx
	imull	152(%rsp), %ecx
	addl	%r15d, %ecx
	cmpl	$131, 48(%rsp)
	jne	.LBB143_55	# bb194
.LBB143_54:	# bb193
	movl	52(%rsp), %edx
	imull	%edx, %eax
	movl	%eax, %esi
	shrl	$31, %esi
	addl	%eax, %esi
	sarl	%esi
	leal	2147483647(%rdx,%rsi), %eax
	leal	1(,%rax,2), %edx
	movslq	%edx, %rdx
	cvtsi2sd	56(%rsp), %xmm0
	mulsd	(%r14,%rdx,8), %xmm0
	movsd	%xmm0, 72(%rsp)
	addl	%eax, %eax
	movslq	%eax, %rax
	movsd	(%r14,%rax,8), %xmm0
	movsd	%xmm0, 80(%rsp)
	leal	1(,%rcx,2), %eax
	movslq	%eax, %r12
	movsd	(%rbx,%r12,8), %xmm0
	movsd	%xmm0, 40(%rsp)
	addl	%ecx, %ecx
	movslq	%ecx, %r13
	movsd	(%rbx,%r13,8), %xmm0
	movsd	%xmm0, 64(%rsp)
	movsd	80(%rsp), %xmm0
	movsd	72(%rsp), %xmm1
	call	_ZL6xhypotdd193
	movsd	80(%rsp), %xmm1
	divsd	%xmm0, %xmm1
	movsd	%xmm1, 80(%rsp)
	movsd	72(%rsp), %xmm1
	divsd	%xmm0, %xmm1
	movsd	%xmm1, 72(%rsp)
	movsd	40(%rsp), %xmm1
	mulsd	72(%rsp), %xmm1
	movsd	72(%rsp), %xmm2
	mulsd	64(%rsp), %xmm2
	movsd	%xmm2, 72(%rsp)
	movsd	64(%rsp), %xmm2
	mulsd	80(%rsp), %xmm2
	addsd	%xmm1, %xmm2
	divsd	%xmm0, %xmm2
	movsd	%xmm2, (%rbx,%r13,8)
	movsd	80(%rsp), %xmm1
	mulsd	40(%rsp), %xmm1
	subsd	72(%rsp), %xmm1
	divsd	%xmm0, %xmm1
	movsd	%xmm1, (%rbx,%r12,8)
.LBB143_55:	# bb194
	movl	52(%rsp), %eax
	leal	-1(%rax), %ecx
	movl	152(%rsp), %edx
	imull	%edx, %ecx
	addl	%r15d, %ecx
	addl	%ecx, %ecx
	movl	%ecx, 20(%rsp)
	leal	-2(%rax), %ecx
	imull	%edx, %ecx
	addl	%r15d, %ecx
	leal	1(,%rcx,2), %esi
	movl	%esi, 28(%rsp)
	addl	%ecx, %ecx
	movl	%ecx, 16(%rsp)
	cvtsi2sd	56(%rsp), %xmm0
	movsd	%xmm0, 64(%rsp)
	leal	(%rdx,%rdx), %ecx
	movl	%ecx, 24(%rsp)
	movl	$1, %r15d
	xorl	%r12d, %r12d
	movl	%eax, %r13d
	jmp	.LBB143_62	# bb202
.LBB143_56:	# bb195
	movl	16(%rsp), %edx
	leal	(%rdx,%r12), %edx
	movl	28(%rsp), %esi
	leal	(%rsi,%r12), %esi
	cmpl	52(%rsp), %ecx
	movslq	%esi, %rsi
	movq	%rsi, 32(%rsp)
	movsd	(%rbx,%rsi,8), %xmm0
	movsd	%xmm0, 80(%rsp)
	movslq	%edx, %rbp
	movsd	(%rbx,%rbp,8), %xmm0
	movsd	%xmm0, 72(%rsp)
	jge	.LBB143_59	# bb198
.LBB143_57:	# bb.nph
	movl	20(%rsp), %edx
	leal	(%rdx,%r12), %edx
	leal	-2(%r13), %esi
	movl	152(%rsp), %edi
	leal	(%rdi,%rdi), %edi
	leal	-1(%r13), %r8d
	xorl	%r9d, %r9d
	.align	16
.LBB143_58:	# bb196
	leal	(%r8,%r9), %r10d
	leal	(%r13,%r9), %r11d
	imull	%r10d, %r11d
	movl	%r11d, %r10d
	shrl	$31, %r10d
	addl	%r11d, %r10d
	sarl	%r10d
	addl	%esi, %r10d
	leal	1(,%r10,2), %r11d
	movslq	%r11d, %r11
	movsd	64(%rsp), %xmm0
	mulsd	(%r14,%r11,8), %xmm0
	movslq	%edx, %r11
	movsd	(%rbx,%r11,8), %xmm1
	movapd	%xmm0, %xmm2
	mulsd	%xmm1, %xmm2
	addl	%r10d, %r10d
	movslq	%r10d, %r10
	movsd	(%r14,%r10,8), %xmm3
	leal	1(%rdx), %r10d
	movslq	%r10d, %r10
	movsd	(%rbx,%r10,8), %xmm4
	movapd	%xmm3, %xmm5
	mulsd	%xmm4, %xmm5
	addsd	%xmm2, %xmm5
	movsd	80(%rsp), %xmm2
	subsd	%xmm5, %xmm2
	movsd	%xmm2, 80(%rsp)
	mulsd	%xmm4, %xmm0
	mulsd	%xmm1, %xmm3
	subsd	%xmm0, %xmm3
	movsd	72(%rsp), %xmm0
	subsd	%xmm3, %xmm0
	movsd	%xmm0, 72(%rsp)
	addl	%edi, %edx
	incl	%r9d
	cmpl	%r15d, %r9d
	jne	.LBB143_58	# bb196
.LBB143_59:	# bb198
	cmpl	$131, 48(%rsp)
	je	.LBB143_72	# bb199
.LBB143_60:	# bb200
	movsd	72(%rsp), %xmm0
	movsd	%xmm0, (%rbx,%rbp,8)
	movsd	80(%rsp), %xmm0
	movsd	%xmm0, 40(%rsp)
.LBB143_61:	# bb202.backedge
	movsd	40(%rsp), %xmm0
	movq	32(%rsp), %rax
	movsd	%xmm0, (%rbx,%rax,8)
	subl	24(%rsp), %r12d
	decl	%r13d
	incl	%r15d
.LBB143_62:	# bb202
	leal	-1(%r13), %ecx
	testl	%ecx, %ecx
	jle	.LBB143_18	# bb105.thread
.LBB143_63:	# bb203
	leal	-2(%r13), %eax
	cmpl	$4294967295, %eax
	jne	.LBB143_56	# bb195
	jmp	.LBB143_18	# bb105.thread
.LBB143_64:	# bb208
	xorl	%edi, %edi
	leaq	.str191, %rsi
	leaq	.str1192, %rdx
	xorb	%al, %al
	call	cblas_xerbla
	jmp	.LBB143_18	# bb105.thread
.LBB143_65:	# bb88.bb91_crit_edge
	xorl	%r15d, %r15d
	jmp	.LBB143_7	# bb91
.LBB143_66:	# bb98
	movl	28(%rsp), %eax
	leal	(%rax,%r12), %eax
	imull	%ecx, %eax
	movl	%eax, %ecx
	shrl	$31, %ecx
	addl	%eax, %ecx
	andl	$4294967294, %ecx
	movslq	%ecx, %rax
	movsd	(%r14,%rax,8), %xmm0
	movsd	%xmm0, 32(%rsp)
	orl	$1, %ecx
	movslq	%ecx, %rax
	movsd	64(%rsp), %xmm0
	mulsd	(%r14,%rax,8), %xmm0
	movsd	%xmm0, 56(%rsp)
	movsd	32(%rsp), %xmm0
	movsd	56(%rsp), %xmm1
	call	_ZL6xhypotdd193
	movsd	56(%rsp), %xmm1
	divsd	%xmm0, %xmm1
	movsd	%xmm1, 56(%rsp)
	movsd	80(%rsp), %xmm1
	movapd	%xmm1, %xmm2
	mulsd	56(%rsp), %xmm2
	movsd	32(%rsp), %xmm3
	divsd	%xmm0, %xmm3
	movsd	%xmm3, 32(%rsp)
	movsd	72(%rsp), %xmm3
	movapd	%xmm3, %xmm4
	mulsd	32(%rsp), %xmm4
	addsd	%xmm2, %xmm4
	divsd	%xmm0, %xmm4
	movsd	%xmm4, (%rbx,%rbp,8)
	movsd	56(%rsp), %xmm2
	mulsd	%xmm3, %xmm2
	movsd	%xmm2, 56(%rsp)
	movsd	32(%rsp), %xmm2
	mulsd	%xmm1, %xmm2
	subsd	56(%rsp), %xmm2
	divsd	%xmm0, %xmm2
	movsd	%xmm2, 32(%rsp)
	jmp	.LBB143_15	# bb101.backedge
.LBB143_67:	# bb123.bb126_crit_edge
	xorl	%r15d, %r15d
	jmp	.LBB143_25	# bb126
.LBB143_68:	# bb136
	leal	2(%r13), %eax
	imull	%ecx, %eax
	movl	%eax, %ecx
	shrl	$31, %ecx
	addl	%eax, %ecx
	sarl	%ecx
	addl	%r13d, %ecx
	leal	3(,%rcx,2), %eax
	movslq	%eax, %rax
	movsd	64(%rsp), %xmm0
	mulsd	(%r14,%rax,8), %xmm0
	movsd	%xmm0, 56(%rsp)
	leal	2(,%rcx,2), %eax
	movslq	%eax, %rax
	movsd	(%r14,%rax,8), %xmm0
	movsd	%xmm0, 40(%rsp)
	movsd	56(%rsp), %xmm1
	call	_ZL6xhypotdd193
	movsd	56(%rsp), %xmm1
	divsd	%xmm0, %xmm1
	movsd	%xmm1, 56(%rsp)
	movsd	72(%rsp), %xmm1
	movapd	%xmm1, %xmm2
	mulsd	56(%rsp), %xmm2
	movsd	40(%rsp), %xmm3
	divsd	%xmm0, %xmm3
	movsd	%xmm3, 40(%rsp)
	movsd	80(%rsp), %xmm3
	movapd	%xmm3, %xmm4
	mulsd	40(%rsp), %xmm4
	addsd	%xmm2, %xmm4
	divsd	%xmm0, %xmm4
	movsd	%xmm4, (%rbx,%rbp,8)
	movsd	56(%rsp), %xmm2
	mulsd	%xmm3, %xmm2
	movsd	%xmm2, 56(%rsp)
	movsd	40(%rsp), %xmm2
	mulsd	%xmm1, %xmm2
	subsd	56(%rsp), %xmm2
	divsd	%xmm0, %xmm2
	movsd	%xmm2, 40(%rsp)
	jmp	.LBB143_34	# bb138
.LBB143_69:	# bb156.bb159_crit_edge
	xorl	%r15d, %r15d
	jmp	.LBB143_39	# bb159
.LBB143_70:	# bb169
	imull	%r12d, %ecx
	movl	%ecx, %eax
	shrl	$31, %eax
	addl	%ecx, %eax
	andl	$4294967294, %eax
	movslq	%eax, %rcx
	movsd	(%r14,%rcx,8), %xmm0
	movsd	%xmm0, 40(%rsp)
	orl	$1, %eax
	movslq	%eax, %rax
	movsd	64(%rsp), %xmm0
	mulsd	(%r14,%rax,8), %xmm0
	movsd	%xmm0, 56(%rsp)
	movsd	40(%rsp), %xmm0
	movsd	56(%rsp), %xmm1
	call	_ZL6xhypotdd193
	movsd	56(%rsp), %xmm1
	divsd	%xmm0, %xmm1
	movsd	%xmm1, 56(%rsp)
	movsd	72(%rsp), %xmm1
	movapd	%xmm1, %xmm2
	mulsd	56(%rsp), %xmm2
	movsd	40(%rsp), %xmm3
	divsd	%xmm0, %xmm3
	movsd	%xmm3, 40(%rsp)
	movsd	80(%rsp), %xmm3
	movapd	%xmm3, %xmm4
	mulsd	40(%rsp), %xmm4
	addsd	%xmm2, %xmm4
	divsd	%xmm0, %xmm4
	movsd	%xmm4, (%rbx,%rbp,8)
	movsd	56(%rsp), %xmm2
	mulsd	%xmm3, %xmm2
	movsd	%xmm2, 56(%rsp)
	movsd	40(%rsp), %xmm2
	mulsd	%xmm1, %xmm2
	subsd	56(%rsp), %xmm2
	divsd	%xmm0, %xmm2
	movsd	%xmm2, 40(%rsp)
	jmp	.LBB143_48	# bb171
.LBB143_71:	# bb189.bb192_crit_edge
	xorl	%r15d, %r15d
	jmp	.LBB143_53	# bb192
.LBB143_72:	# bb199
	imull	%ecx, %eax
	movl	%eax, %ecx
	shrl	$31, %ecx
	addl	%eax, %ecx
	sarl	%ecx
	addl	%r13d, %ecx
	leal	-3(,%rcx,2), %eax
	movslq	%eax, %rax
	movsd	64(%rsp), %xmm0
	mulsd	(%r14,%rax,8), %xmm0
	movsd	%xmm0, 56(%rsp)
	leal	-4(,%rcx,2), %eax
	movslq	%eax, %rax
	movsd	(%r14,%rax,8), %xmm0
	movsd	%xmm0, 40(%rsp)
	movsd	56(%rsp), %xmm1
	call	_ZL6xhypotdd193
	movsd	56(%rsp), %xmm1
	divsd	%xmm0, %xmm1
	movsd	%xmm1, 56(%rsp)
	movsd	80(%rsp), %xmm1
	movapd	%xmm1, %xmm2
	mulsd	56(%rsp), %xmm2
	movsd	40(%rsp), %xmm3
	divsd	%xmm0, %xmm3
	movsd	%xmm3, 40(%rsp)
	movsd	72(%rsp), %xmm3
	movapd	%xmm3, %xmm4
	mulsd	40(%rsp), %xmm4
	addsd	%xmm2, %xmm4
	divsd	%xmm0, %xmm4
	movsd	%xmm4, (%rbx,%rbp,8)
	movsd	56(%rsp), %xmm2
	mulsd	%xmm3, %xmm2
	movsd	%xmm2, 56(%rsp)
	movsd	40(%rsp), %xmm2
	mulsd	%xmm1, %xmm2
	subsd	56(%rsp), %xmm2
	divsd	%xmm0, %xmm2
	movsd	%xmm2, 40(%rsp)
	jmp	.LBB143_61	# bb202.backedge
	.size	cblas_ztpsv, .-cblas_ztpsv
.Leh_func_end100:


	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI144_0:					
	.quad	9223372036854775807	# double value: nan
	.quad	9223372036854775807	# double value: nan
	.section	.rodata.cst8,"aM",@progbits,8
	.align	16
.LCPI144_1:					
	.quad	4607182418800017408	# double value: 1.000000e+00
	.text
	.align	16
	.type	_ZL6xhypotdd193,@function
_ZL6xhypotdd193:
	movsd	.LCPI144_0(%rip), %xmm2
	andpd	%xmm2, %xmm0
	andpd	%xmm2, %xmm1
	movapd	%xmm1, %xmm2
	maxsd	%xmm0, %xmm2
	minsd	%xmm1, %xmm0
	pxor	%xmm1, %xmm1
	ucomisd	%xmm1, %xmm0
	setnp	%al
	sete	%cl
	testb	%al, %cl
	jne	.LBB144_2	# bb5
.LBB144_1:	# bb4
	divsd	%xmm2, %xmm0
	mulsd	%xmm0, %xmm0
	addsd	.LCPI144_1(%rip), %xmm0
	sqrtsd	%xmm0, %xmm0
	mulsd	%xmm2, %xmm0
	ret
.LBB144_2:	# bb5
	movapd	%xmm2, %xmm0
	ret
	.size	_ZL6xhypotdd193, .-_ZL6xhypotdd193


	.align	16
	.globl	cblas_ztrmm
	.type	cblas_ztrmm,@function
cblas_ztrmm:
.Leh_func_begin101:
.Llabel101:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$56, %rsp
	cmpl	$113, %ecx
	movl	$4294967295, %eax
	movl	$1, %r10d
	cmove	%eax, %r10d
	cmpl	$101, %edi
	movq	120(%rsp), %rax
	movsd	8(%rax), %xmm0
	movsd	(%rax), %xmm1
	movq	144(%rsp), %rax
	movq	128(%rsp), %rdi
	movl	112(%rsp), %r11d
	movl	%r8d, 48(%rsp)
	je	.LBB145_112	# bb63
.LBB145_1:	# bb67
	cmpl	$111, %ecx
	movl	$111, %ecx
	movl	$112, %r8d
	cmove	%ecx, %r8d
	cmpl	$121, %edx
	movl	$122, %ecx
	movl	$121, %edx
	cmove	%ecx, %edx
	cmpl	$141, %esi
	movl	$142, %ecx
	movl	$141, %esi
	cmove	%ecx, %esi
	movl	%r9d, 52(%rsp)
	movl	%r11d, %r9d
.LBB145_2:	# bb77
	movl	%r9d, 44(%rsp)
	cmpl	$121, %edx
	sete	%cl
	setne	%r9b
	cmpl	$111, %r8d
	sete	%r11b
	setne	%bl
	andb	%cl, %r11b
	orb	%r9b, %bl
	testb	%bl, %bl
	jne	.LBB145_15	# bb96
.LBB145_3:	# bb77
	cmpl	$141, %esi
	jne	.LBB145_15	# bb96
.LBB145_4:	# bb95.preheader
	cmpl	$0, 44(%rsp)
	jle	.LBB145_111	# bb118.thread
.LBB145_5:	# bb.nph397
	cmpl	$0, 52(%rsp)
	cvtsi2sd	%r10d, %xmm2
	jle	.LBB145_111	# bb118.thread
.LBB145_6:	# bb93.preheader.preheader
	movl	136(%rsp), %r8d
	leal	2(,%r8,2), %r8d
	movl	%r8d, 24(%rsp)
	movl	152(%rsp), %r8d
	leal	(%r8,%r8), %r8d
	movl	%r8d, 20(%rsp)
	movl	44(%rsp), %r8d
	leal	-1(%r8), %edx
	xorl	%r9d, %r9d
	movl	%r9d, 40(%rsp)
	movl	%r9d, 32(%rsp)
	jmp	.LBB145_14	# bb93.preheader
	.align	16
.LBB145_7:	# bb86
	cmpl	$131, 48(%rsp)
	je	.LBB145_113	# bb87
.LBB145_8:	# bb88
	movslq	%r11d, %r14
	movsd	(%rax,%r14,8), %xmm7
	leal	1(%r11), %r14d
	movslq	%r14d, %r14
	movsd	(%rax,%r14,8), %xmm8
.LBB145_9:	# bb91.preheader
	cmpl	44(%rsp), %r8d
	jge	.LBB145_12	# bb92
.LBB145_10:	# bb.nph391
	leal	(%r10,%r11), %r14d
	movl	152(%rsp), %r15d
	leal	(%r15,%r15), %r15d
	xorl	%r12d, %r12d
	movl	%r9d, %r13d
	.align	16
.LBB145_11:	# bb90
	leal	3(%r13), %ebp
	movslq	%ebp, %rbp
	movapd	%xmm2, %xmm3
	mulsd	(%rdi,%rbp,8), %xmm3
	movslq	%r14d, %rbp
	movsd	(%rax,%rbp,8), %xmm4
	movapd	%xmm3, %xmm5
	mulsd	%xmm4, %xmm5
	leal	1(%r14), %ebp
	movslq	%ebp, %rbp
	movsd	(%rax,%rbp,8), %xmm6
	addl	$2, %r13d
	movslq	%r13d, %rbp
	movsd	(%rdi,%rbp,8), %xmm9
	movapd	%xmm9, %xmm10
	mulsd	%xmm6, %xmm10
	addsd	%xmm5, %xmm10
	addsd	%xmm10, %xmm8
	mulsd	%xmm6, %xmm3
	mulsd	%xmm4, %xmm9
	subsd	%xmm3, %xmm9
	addsd	%xmm9, %xmm7
	addl	%r15d, %r14d
	incl	%r12d
	cmpl	%edx, %r12d
	jne	.LBB145_11	# bb90
.LBB145_12:	# bb92
	movapd	%xmm0, %xmm3
	mulsd	%xmm8, %xmm3
	movapd	%xmm1, %xmm4
	mulsd	%xmm7, %xmm4
	subsd	%xmm3, %xmm4
	movslq	%r11d, %r14
	movsd	%xmm4, (%rax,%r14,8)
	mulsd	%xmm0, %xmm7
	mulsd	%xmm1, %xmm8
	addsd	%xmm7, %xmm8
	leal	1(%r11), %r14d
	movslq	%r14d, %r14
	movsd	%xmm8, (%rax,%r14,8)
	addl	$2, %r11d
	incl	%ecx
	cmpl	52(%rsp), %ecx
	jne	.LBB145_7	# bb86
.LBB145_13:	# bb94
	addl	24(%rsp), %r9d
	movl	40(%rsp), %r8d
	addl	20(%rsp), %r8d
	movl	%r8d, 40(%rsp)
	decl	%edx
	movl	32(%rsp), %r8d
	incl	%r8d
	movl	%r8d, 32(%rsp)
	cmpl	44(%rsp), %r8d
	je	.LBB145_111	# bb118.thread
.LBB145_14:	# bb93.preheader
	movslq	%r9d, %rsi
	leal	1(%r9), %r8d
	movslq	%r8d, %rbx
	movl	152(%rsp), %r8d
	leal	(%r8,%r8), %r10d
	movl	32(%rsp), %r8d
	leal	1(%r8), %r8d
	xorl	%ecx, %ecx
	movl	40(%rsp), %r11d
	jmp	.LBB145_7	# bb86
.LBB145_15:	# bb96
	cmpl	$121, %edx
	sete	%cl
	setne	%r9b
	cmpl	$112, %r8d
	sete	%bl
	setne	%r14b
	andb	%cl, %bl
	orb	%r9b, %r14b
	testb	%r14b, %r14b
	jne	.LBB145_30	# bb120
.LBB145_16:	# bb96
	cmpl	$141, %esi
	jne	.LBB145_30	# bb120
.LBB145_17:	# bb114.preheader
	movl	136(%rsp), %r8d
	leal	2(,%r8,2), %ecx
	leal	(%r8,%r8), %r8d
	movl	$4294967294, %edx
	subl	%r8d, %edx
	movl	%edx, 16(%rsp)
	movl	44(%rsp), %r8d
	leal	-1(%r8), %edx
	movl	152(%rsp), %esi
	movl	%esi, %r9d
	imull	%edx, %r9d
	imull	%edx, %ecx
	movl	%ecx, 24(%rsp)
	addl	%r9d, %r9d
	leal	(%r8,%r8), %r8d
	movl	%r8d, 40(%rsp)
	cvtsi2sd	%r10d, %xmm2
	leal	(%rsi,%rsi), %r8d
	movl	%r8d, 20(%rsp)
	.align	16
.LBB145_18:	# bb114
	cmpl	$0, 44(%rsp)
	jle	.LBB145_111	# bb118.thread
.LBB145_19:	# bb115
	movl	44(%rsp), %r8d
	leal	-1(%r8), %r10d
	movl	%r10d, 32(%rsp)
	testl	%r8d, %r8d
	je	.LBB145_111	# bb118.thread
.LBB145_20:	# bb113.preheader
	cmpl	$0, 52(%rsp)
	jle	.LBB145_28	# bb114.loopexit
.LBB145_21:	# bb.nph385
	movl	24(%rsp), %r8d
	movslq	%r8d, %rcx
	leal	1(%r8), %r8d
	movslq	%r8d, %rdx
	movl	44(%rsp), %r8d
	leal	-1(%r8), %r11d
	leal	1(%r9), %r8d
	xorl	%esi, %esi
	movl	%esi, %r10d
	.align	16
.LBB145_22:	# bb108.preheader
	cmpl	$0, 32(%rsp)
	jle	.LBB145_29	# bb108.preheader.bb109_crit_edge
.LBB145_23:	# bb.nph381
	movl	136(%rsp), %ebx
	leal	(%rbx,%rbx), %ebx
	movl	152(%rsp), %r14d
	leal	(%r14,%r14), %r14d
	pxor	%xmm3, %xmm3
	xorl	%r15d, %r15d
	movl	40(%rsp), %r12d
	movl	%esi, %r13d
	movapd	%xmm3, %xmm4
	.align	16
.LBB145_24:	# bb107
	leal	-1(%r12), %ebp
	movslq	%ebp, %rbp
	movapd	%xmm2, %xmm5
	mulsd	(%rdi,%rbp,8), %xmm5
	movslq	%r13d, %rbp
	movsd	(%rax,%rbp,8), %xmm6
	movapd	%xmm5, %xmm7
	mulsd	%xmm6, %xmm7
	leal	1(%r13), %ebp
	movslq	%ebp, %rbp
	movsd	(%rax,%rbp,8), %xmm8
	leal	-2(%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%rdi,%rbp,8), %xmm9
	movapd	%xmm9, %xmm10
	mulsd	%xmm8, %xmm10
	addsd	%xmm7, %xmm10
	addsd	%xmm10, %xmm4
	mulsd	%xmm8, %xmm5
	mulsd	%xmm6, %xmm9
	subsd	%xmm5, %xmm9
	addsd	%xmm9, %xmm3
	addl	%ebx, %r12d
	addl	%r14d, %r13d
	incl	%r15d
	cmpl	%r11d, %r15d
	jne	.LBB145_24	# bb107
.LBB145_25:	# bb109
	cmpl	$131, 48(%rsp)
	je	.LBB145_114	# bb110
.LBB145_26:	# bb111
	leal	(%r8,%rsi), %ebx
	movslq	%ebx, %rbx
	movsd	(%rax,%rbx,8), %xmm10
	leal	(%r9,%rsi), %ebx
	movslq	%ebx, %rbx
	movsd	(%rax,%rbx,8), %xmm9
.LBB145_27:	# bb112
	addsd	%xmm4, %xmm10
	movapd	%xmm0, %xmm4
	mulsd	%xmm10, %xmm4
	addsd	%xmm3, %xmm9
	movapd	%xmm1, %xmm3
	mulsd	%xmm9, %xmm3
	subsd	%xmm4, %xmm3
	leal	(%r9,%rsi), %ebx
	movslq	%ebx, %rbx
	movsd	%xmm3, (%rax,%rbx,8)
	mulsd	%xmm0, %xmm9
	mulsd	%xmm1, %xmm10
	addsd	%xmm9, %xmm10
	leal	(%r8,%rsi), %ebx
	movslq	%ebx, %rbx
	movsd	%xmm10, (%rax,%rbx,8)
	addl	$2, %esi
	incl	%r10d
	cmpl	52(%rsp), %r10d
	jne	.LBB145_22	# bb108.preheader
.LBB145_28:	# bb114.loopexit
	movl	24(%rsp), %r8d
	addl	16(%rsp), %r8d
	movl	%r8d, 24(%rsp)
	subl	20(%rsp), %r9d
	addl	$4294967294, 40(%rsp)
	decl	44(%rsp)
	jmp	.LBB145_18	# bb114
.LBB145_29:	# bb108.preheader.bb109_crit_edge
	pxor	%xmm3, %xmm3
	movapd	%xmm3, %xmm4
	jmp	.LBB145_25	# bb109
.LBB145_30:	# bb120
	cmpl	$122, %edx
	sete	%cl
	setne	%r9b
	cmpl	$111, %r8d
	sete	%r14b
	setne	%r15b
	andb	%cl, %r14b
	orb	%r9b, %r15b
	testb	%r15b, %r15b
	jne	.LBB145_45	# bb144
.LBB145_31:	# bb120
	cmpl	$141, %esi
	jne	.LBB145_45	# bb144
.LBB145_32:	# bb138.preheader
	movl	44(%rsp), %r8d
	leal	-1(%r8), %r8d
	movl	152(%rsp), %ecx
	movl	%ecx, %edx
	imull	%r8d, %edx
	movl	136(%rsp), %esi
	leal	2(,%rsi,2), %r9d
	movl	%esi, %r11d
	imull	%r8d, %r11d
	imull	%r8d, %r9d
	movl	%r9d, 24(%rsp)
	addl	%esi, %esi
	movl	%esi, 4(%rsp)
	movl	$4294967294, %r8d
	subl	%esi, %r8d
	movl	%r8d, 16(%rsp)
	addl	%r11d, %r11d
	movl	%r11d, 40(%rsp)
	addl	%edx, %edx
	cvtsi2sd	%r10d, %xmm2
	leal	(%rcx,%rcx), %r8d
	movl	%r8d, 20(%rsp)
	.align	16
.LBB145_33:	# bb138
	cmpl	$0, 44(%rsp)
	jle	.LBB145_111	# bb118.thread
.LBB145_34:	# bb139
	movl	44(%rsp), %r8d
	leal	-1(%r8), %r10d
	movl	%r10d, 32(%rsp)
	testl	%r8d, %r8d
	je	.LBB145_111	# bb118.thread
.LBB145_35:	# bb137.preheader
	cmpl	$0, 52(%rsp)
	jle	.LBB145_43	# bb138.loopexit
.LBB145_36:	# bb.nph377
	movl	24(%rsp), %r8d
	movslq	%r8d, %rcx
	leal	1(%r8), %r8d
	movslq	%r8d, %rsi
	movl	44(%rsp), %r8d
	leal	-1(%r8), %r11d
	leal	1(%rdx), %r9d
	xorl	%r8d, %r8d
	movl	%r8d, %r10d
	.align	16
.LBB145_37:	# bb132.preheader
	cmpl	$0, 32(%rsp)
	jle	.LBB145_44	# bb132.preheader.bb133_crit_edge
.LBB145_38:	# bb.nph373
	movl	152(%rsp), %ebx
	leal	(%rbx,%rbx), %ebx
	pxor	%xmm3, %xmm3
	xorl	%r14d, %r14d
	movl	%r8d, %r15d
	movl	40(%rsp), %r12d
	movapd	%xmm3, %xmm4
	.align	16
.LBB145_39:	# bb131
	movslq	%r12d, %r13
	leal	1(%r12), %ebp
	movslq	%ebp, %rbp
	movapd	%xmm2, %xmm5
	mulsd	(%rdi,%rbp,8), %xmm5
	movslq	%r15d, %rbp
	movsd	(%rax,%rbp,8), %xmm6
	movapd	%xmm5, %xmm7
	mulsd	%xmm6, %xmm7
	movsd	(%rdi,%r13,8), %xmm8
	leal	1(%r15), %r13d
	movslq	%r13d, %r13
	movsd	(%rax,%r13,8), %xmm9
	movapd	%xmm8, %xmm10
	mulsd	%xmm9, %xmm10
	addsd	%xmm7, %xmm10
	addsd	%xmm10, %xmm4
	mulsd	%xmm6, %xmm8
	mulsd	%xmm9, %xmm5
	subsd	%xmm5, %xmm8
	addsd	%xmm8, %xmm3
	addl	%ebx, %r15d
	addl	$2, %r12d
	incl	%r14d
	cmpl	%r11d, %r14d
	jne	.LBB145_39	# bb131
.LBB145_40:	# bb133
	cmpl	$131, 48(%rsp)
	je	.LBB145_115	# bb134
.LBB145_41:	# bb135
	leal	(%r9,%r8), %ebx
	movslq	%ebx, %rbx
	movsd	(%rax,%rbx,8), %xmm10
	leal	(%rdx,%r8), %ebx
	movslq	%ebx, %rbx
	movsd	(%rax,%rbx,8), %xmm9
.LBB145_42:	# bb136
	addsd	%xmm4, %xmm10
	movapd	%xmm0, %xmm4
	mulsd	%xmm10, %xmm4
	addsd	%xmm3, %xmm9
	movapd	%xmm1, %xmm3
	mulsd	%xmm9, %xmm3
	subsd	%xmm4, %xmm3
	leal	(%rdx,%r8), %ebx
	movslq	%ebx, %rbx
	movsd	%xmm3, (%rax,%rbx,8)
	mulsd	%xmm0, %xmm9
	mulsd	%xmm1, %xmm10
	addsd	%xmm9, %xmm10
	leal	(%r9,%r8), %ebx
	movslq	%ebx, %rbx
	movsd	%xmm10, (%rax,%rbx,8)
	addl	$2, %r8d
	incl	%r10d
	cmpl	52(%rsp), %r10d
	jne	.LBB145_37	# bb132.preheader
.LBB145_43:	# bb138.loopexit
	movl	24(%rsp), %r8d
	addl	16(%rsp), %r8d
	movl	%r8d, 24(%rsp)
	movl	40(%rsp), %r8d
	subl	4(%rsp), %r8d
	movl	%r8d, 40(%rsp)
	subl	20(%rsp), %edx
	decl	44(%rsp)
	jmp	.LBB145_33	# bb138
.LBB145_44:	# bb132.preheader.bb133_crit_edge
	pxor	%xmm3, %xmm3
	movapd	%xmm3, %xmm4
	jmp	.LBB145_40	# bb133
.LBB145_45:	# bb144
	cmpl	$122, %edx
	sete	%cl
	setne	%dl
	cmpl	$112, %r8d
	sete	%r8b
	setne	%r9b
	andb	%cl, %r8b
	orb	%dl, %r9b
	testb	%r9b, %r9b
	jne	.LBB145_58	# bb164
.LBB145_46:	# bb144
	cmpl	$141, %esi
	jne	.LBB145_58	# bb164
.LBB145_47:	# bb163.preheader
	cmpl	$0, 44(%rsp)
	jle	.LBB145_111	# bb118.thread
.LBB145_48:	# bb.nph369
	cmpl	$0, 52(%rsp)
	cvtsi2sd	%r10d, %xmm2
	jle	.LBB145_111	# bb118.thread
.LBB145_49:	# bb161.preheader.preheader
	movl	136(%rsp), %r10d
	leal	2(,%r10,2), %ecx
	movl	%ecx, 16(%rsp)
	leal	(%r10,%r10), %r10d
	movl	%r10d, 12(%rsp)
	movl	152(%rsp), %r10d
	leal	(%r10,%r10), %r10d
	movl	%r10d, 8(%rsp)
	movl	44(%rsp), %r10d
	leal	-1(%r10), %r9d
	xorl	%r10d, %r10d
	movl	%r10d, 24(%rsp)
	movl	%r10d, 20(%rsp)
	jmp	.LBB145_57	# bb161.preheader
	.align	16
.LBB145_50:	# bb154
	cmpl	$131, 48(%rsp)
	je	.LBB145_116	# bb155
.LBB145_51:	# bb156
	movslq	%r10d, %r11
	movsd	(%rax,%r11,8), %xmm7
	leal	1(%r10), %r11d
	movslq	%r11d, %r11
	movsd	(%rax,%r11,8), %xmm8
.LBB145_52:	# bb159.preheader
	cmpl	44(%rsp), %r8d
	jge	.LBB145_55	# bb160
.LBB145_53:	# bb.nph363
	leal	(%rcx,%r10), %r11d
	movl	136(%rsp), %ebx
	leal	(%rbx,%rbx), %ebx
	movl	152(%rsp), %r14d
	leal	(%r14,%r14), %r14d
	xorl	%r15d, %r15d
	movl	40(%rsp), %r12d
	.align	16
.LBB145_54:	# bb158
	movslq	%r12d, %r13
	leal	1(%r12), %ebp
	movslq	%ebp, %rbp
	movapd	%xmm2, %xmm3
	mulsd	(%rdi,%rbp,8), %xmm3
	movslq	%r11d, %rbp
	movsd	(%rax,%rbp,8), %xmm4
	movapd	%xmm3, %xmm5
	mulsd	%xmm4, %xmm5
	movsd	(%rdi,%r13,8), %xmm6
	leal	1(%r11), %r13d
	movslq	%r13d, %r13
	movsd	(%rax,%r13,8), %xmm9
	movapd	%xmm6, %xmm10
	mulsd	%xmm9, %xmm10
	addsd	%xmm5, %xmm10
	addsd	%xmm10, %xmm8
	mulsd	%xmm4, %xmm6
	mulsd	%xmm9, %xmm3
	subsd	%xmm3, %xmm6
	addsd	%xmm6, %xmm7
	addl	%ebx, %r12d
	addl	%r14d, %r11d
	incl	%r15d
	cmpl	%r9d, %r15d
	jne	.LBB145_54	# bb158
.LBB145_55:	# bb160
	movapd	%xmm0, %xmm3
	mulsd	%xmm8, %xmm3
	movapd	%xmm1, %xmm4
	mulsd	%xmm7, %xmm4
	subsd	%xmm3, %xmm4
	movslq	%r10d, %r11
	movsd	%xmm4, (%rax,%r11,8)
	mulsd	%xmm0, %xmm7
	mulsd	%xmm1, %xmm8
	addsd	%xmm7, %xmm8
	leal	1(%r10), %r11d
	movslq	%r11d, %r11
	movsd	%xmm8, (%rax,%r11,8)
	addl	$2, %r10d
	incl	%esi
	cmpl	52(%rsp), %esi
	jne	.LBB145_50	# bb154
.LBB145_56:	# bb162
	movl	%edx, %r10d
	addl	16(%rsp), %r10d
	movl	24(%rsp), %ecx
	addl	8(%rsp), %ecx
	movl	%ecx, 24(%rsp)
	decl	%r9d
	movl	20(%rsp), %ecx
	incl	%ecx
	movl	%ecx, 20(%rsp)
	cmpl	44(%rsp), %ecx
	je	.LBB145_111	# bb118.thread
.LBB145_57:	# bb161.preheader
	movl	12(%rsp), %ecx
	leal	(%rcx,%r10), %ecx
	movl	%ecx, 40(%rsp)
	movslq	%r10d, %rdx
	incl	%r10d
	movslq	%r10d, %rcx
	movq	%rcx, 32(%rsp)
	movl	152(%rsp), %r10d
	leal	(%r10,%r10), %ecx
	movl	20(%rsp), %r10d
	leal	1(%r10), %r8d
	xorl	%esi, %esi
	movl	24(%rsp), %r10d
	jmp	.LBB145_50	# bb154
.LBB145_58:	# bb164
	cmpl	$142, %esi
	setne	%cl
	notb	%r11b
	orb	%cl, %r11b
	testb	$1, %r11b
	jne	.LBB145_71	# bb189
.LBB145_59:	# bb188.preheader
	cmpl	$0, 44(%rsp)
	jle	.LBB145_111	# bb118.thread
.LBB145_60:	# bb.nph357
	movl	152(%rsp), %ecx
	addl	%ecx, %ecx
	movl	%ecx, 28(%rsp)
	cvtsi2sd	%r10d, %xmm2
	xorl	%r8d, %r8d
	movl	%r8d, 40(%rsp)
	.align	16
.LBB145_61:	# bb181.preheader
	movl	136(%rsp), %r10d
	leal	2(,%r10,2), %ecx
	leal	(%r10,%r10), %r10d
	movl	$4294967294, %r11d
	subl	%r10d, %r11d
	movl	52(%rsp), %edx
	leal	-1(%rdx), %r10d
	imull	%ecx, %r10d
	leal	-1(%r8), %r9d
	leal	-2(%r8), %esi
	leal	(%rdx,%rdx), %ecx
	jmp	.LBB145_67	# bb181
.LBB145_62:	# bb.nph353
	movl	136(%rsp), %ebx
	leal	(%rbx,%rbx), %ebx
	leal	-1(%rdx), %r14d
	pxor	%xmm3, %xmm3
	xorl	%r15d, %r15d
	movl	%ecx, %r12d
	movl	%r8d, %r13d
	movapd	%xmm3, %xmm4
	.align	16
.LBB145_63:	# bb175
	leal	-1(%r12), %ebp
	movslq	%ebp, %rbp
	movapd	%xmm2, %xmm5
	mulsd	(%rdi,%rbp,8), %xmm5
	movslq	%r13d, %rbp
	movsd	(%rax,%rbp,8), %xmm6
	movapd	%xmm5, %xmm7
	mulsd	%xmm6, %xmm7
	leal	1(%r13), %ebp
	movslq	%ebp, %rbp
	movsd	(%rax,%rbp,8), %xmm8
	leal	-2(%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%rdi,%rbp,8), %xmm9
	movapd	%xmm9, %xmm10
	mulsd	%xmm8, %xmm10
	addsd	%xmm7, %xmm10
	addsd	%xmm10, %xmm4
	mulsd	%xmm8, %xmm5
	mulsd	%xmm6, %xmm9
	subsd	%xmm5, %xmm9
	addsd	%xmm9, %xmm3
	addl	%ebx, %r12d
	addl	$2, %r13d
	incl	%r15d
	cmpl	%r14d, %r15d
	jne	.LBB145_63	# bb175
.LBB145_64:	# bb177
	cmpl	$131, 48(%rsp)
	je	.LBB145_117	# bb178
.LBB145_65:	# bb179
	leal	(%r9,%rcx), %ebx
	movslq	%ebx, %rbx
	movsd	(%rax,%rbx,8), %xmm10
	leal	(%rsi,%rcx), %ebx
	movslq	%ebx, %rbx
	movsd	(%rax,%rbx,8), %xmm8
.LBB145_66:	# bb180
	addsd	%xmm4, %xmm10
	movapd	%xmm0, %xmm4
	mulsd	%xmm10, %xmm4
	addsd	%xmm3, %xmm8
	movapd	%xmm1, %xmm3
	mulsd	%xmm8, %xmm3
	subsd	%xmm4, %xmm3
	leal	(%rsi,%rcx), %ebx
	movslq	%ebx, %rbx
	movsd	%xmm3, (%rax,%rbx,8)
	mulsd	%xmm0, %xmm8
	mulsd	%xmm1, %xmm10
	addsd	%xmm8, %xmm10
	leal	(%r9,%rcx), %ebx
	movslq	%ebx, %rbx
	movsd	%xmm10, (%rax,%rbx,8)
	addl	%r11d, %r10d
	addl	$4294967294, %ecx
	decl	%edx
.LBB145_67:	# bb181
	testl	%edx, %edx
	jle	.LBB145_110	# bb187
.LBB145_68:	# bb182
	leal	-1(%rdx), %ebx
	testl	%edx, %edx
	je	.LBB145_110	# bb187
.LBB145_69:	# bb176.preheader
	testl	%ebx, %ebx
	jg	.LBB145_62	# bb.nph353
.LBB145_70:	# bb176.preheader.bb177_crit_edge
	pxor	%xmm3, %xmm3
	movapd	%xmm3, %xmm4
	jmp	.LBB145_64	# bb177
.LBB145_71:	# bb189
	cmpl	$142, %esi
	setne	%cl
	notb	%bl
	orb	%cl, %bl
	testb	$1, %bl
	jne	.LBB145_83	# bb209
.LBB145_72:	# bb208.preheader
	cmpl	$0, 44(%rsp)
	jle	.LBB145_111	# bb118.thread
.LBB145_73:	# bb.nph349
	cmpl	$0, 52(%rsp)
	cvtsi2sd	%r10d, %xmm2
	jle	.LBB145_111	# bb118.thread
.LBB145_74:	# bb206.preheader.preheader
	movl	152(%rsp), %r10d
	addl	%r10d, %r10d
	movl	%r10d, 28(%rsp)
	xorl	%r8d, %r8d
	movl	%r8d, 40(%rsp)
	jmp	.LBB145_82	# bb206.preheader
	.align	16
.LBB145_75:	# bb199
	cmpl	$131, 48(%rsp)
	je	.LBB145_118	# bb200
.LBB145_76:	# bb201
	movslq	%edx, %r11
	movsd	(%rax,%r11,8), %xmm6
	leal	1(%rdx), %r11d
	movslq	%r11d, %r11
	movsd	(%rax,%r11,8), %xmm8
.LBB145_77:	# bb204.preheader
	leal	1(%r10), %r11d
	cmpl	52(%rsp), %r11d
	jge	.LBB145_80	# bb205
.LBB145_78:	# bb.nph343
	leal	3(%rdx), %r11d
	leal	2(%rdx), %ebx
	leal	3(%rsi), %r14d
	leal	2(%rsi), %r15d
	xorl	%r12d, %r12d
	movl	%r12d, %r13d
	.align	16
.LBB145_79:	# bb203
	leal	(%rbx,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%rax,%rbp,8), %xmm3
	leal	(%r14,%r12), %ebp
	movslq	%ebp, %rbp
	movapd	%xmm2, %xmm4
	mulsd	(%rdi,%rbp,8), %xmm4
	movapd	%xmm4, %xmm5
	mulsd	%xmm3, %xmm5
	leal	(%r11,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%rax,%rbp,8), %xmm7
	leal	(%r15,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%rdi,%rbp,8), %xmm9
	movapd	%xmm9, %xmm10
	mulsd	%xmm7, %xmm10
	addsd	%xmm5, %xmm10
	addsd	%xmm10, %xmm8
	mulsd	%xmm7, %xmm4
	mulsd	%xmm3, %xmm9
	subsd	%xmm4, %xmm9
	addsd	%xmm9, %xmm6
	addl	$2, %r12d
	incl	%r13d
	cmpl	%ecx, %r13d
	jne	.LBB145_79	# bb203
.LBB145_80:	# bb205
	movapd	%xmm0, %xmm3
	mulsd	%xmm8, %xmm3
	movapd	%xmm1, %xmm4
	mulsd	%xmm6, %xmm4
	subsd	%xmm3, %xmm4
	movslq	%edx, %r11
	movsd	%xmm4, (%rax,%r11,8)
	mulsd	%xmm0, %xmm6
	mulsd	%xmm1, %xmm8
	addsd	%xmm6, %xmm8
	leal	1(%rdx), %r11d
	movslq	%r11d, %r11
	movsd	%xmm8, (%rax,%r11,8)
	addl	%r9d, %esi
	addl	$2, %edx
	decl	%ecx
	incl	%r10d
	cmpl	52(%rsp), %r10d
	jne	.LBB145_75	# bb199
.LBB145_81:	# bb207
	addl	28(%rsp), %r8d
	movl	40(%rsp), %r10d
	incl	%r10d
	movl	%r10d, 40(%rsp)
	cmpl	44(%rsp), %r10d
	je	.LBB145_111	# bb118.thread
.LBB145_82:	# bb206.preheader
	movl	136(%rsp), %r10d
	leal	2(,%r10,2), %r9d
	movl	52(%rsp), %r10d
	leal	-1(%r10), %ecx
	xorl	%esi, %esi
	movl	%r8d, %edx
	movl	%esi, %r10d
	jmp	.LBB145_75	# bb199
.LBB145_83:	# bb209
	cmpl	$142, %esi
	setne	%cl
	notb	%r14b
	orb	%cl, %r14b
	testb	$1, %r14b
	jne	.LBB145_95	# bb229
.LBB145_84:	# bb228.preheader
	cmpl	$0, 44(%rsp)
	jle	.LBB145_111	# bb118.thread
.LBB145_85:	# bb.nph337
	cmpl	$0, 52(%rsp)
	cvtsi2sd	%r10d, %xmm2
	jle	.LBB145_111	# bb118.thread
.LBB145_86:	# bb226.preheader.preheader
	movl	152(%rsp), %r10d
	addl	%r10d, %r10d
	movl	%r10d, 28(%rsp)
	xorl	%r10d, %r10d
	movl	%r10d, 40(%rsp)
	movl	%r10d, 32(%rsp)
	jmp	.LBB145_94	# bb226.preheader
	.align	16
.LBB145_87:	# bb219
	cmpl	$131, 48(%rsp)
	je	.LBB145_119	# bb220
.LBB145_88:	# bb221
	movslq	%esi, %r11
	movsd	(%rax,%r11,8), %xmm6
	leal	1(%rsi), %r11d
	movslq	%r11d, %r11
	movsd	(%rax,%r11,8), %xmm8
.LBB145_89:	# bb224.preheader
	leal	1(%r8), %r11d
	cmpl	52(%rsp), %r11d
	jge	.LBB145_92	# bb225
.LBB145_90:	# bb.nph331
	leal	(%rcx,%r9), %r11d
	movl	136(%rsp), %ebx
	leal	(%rbx,%rbx), %ebx
	xorl	%r14d, %r14d
	movl	%esi, %r15d
	.align	16
.LBB145_91:	# bb223
	movslq	%r11d, %r12
	leal	1(%r11), %r13d
	movslq	%r13d, %r13
	movapd	%xmm2, %xmm3
	mulsd	(%rdi,%r13,8), %xmm3
	leal	3(%r15), %r13d
	leal	2(%r15), %r15d
	movslq	%r15d, %rbp
	movsd	(%rax,%rbp,8), %xmm4
	movapd	%xmm3, %xmm5
	mulsd	%xmm4, %xmm5
	movsd	(%rdi,%r12,8), %xmm7
	movslq	%r13d, %r12
	movsd	(%rax,%r12,8), %xmm9
	movapd	%xmm7, %xmm10
	mulsd	%xmm9, %xmm10
	addsd	%xmm5, %xmm10
	addsd	%xmm10, %xmm8
	mulsd	%xmm9, %xmm3
	mulsd	%xmm4, %xmm7
	subsd	%xmm3, %xmm7
	addsd	%xmm7, %xmm6
	addl	%ebx, %r11d
	incl	%r14d
	cmpl	%r10d, %r14d
	jne	.LBB145_91	# bb223
.LBB145_92:	# bb225
	movapd	%xmm0, %xmm3
	mulsd	%xmm8, %xmm3
	movapd	%xmm1, %xmm4
	mulsd	%xmm6, %xmm4
	subsd	%xmm3, %xmm4
	movslq	%esi, %r11
	movsd	%xmm4, (%rax,%r11,8)
	mulsd	%xmm0, %xmm6
	mulsd	%xmm1, %xmm8
	addsd	%xmm6, %xmm8
	leal	1(%rsi), %r11d
	movslq	%r11d, %r11
	movsd	%xmm8, (%rax,%r11,8)
	addl	%edx, %r9d
	addl	$2, %esi
	decl	%r10d
	incl	%r8d
	cmpl	52(%rsp), %r8d
	jne	.LBB145_87	# bb219
.LBB145_93:	# bb227
	movl	40(%rsp), %r10d
	addl	28(%rsp), %r10d
	movl	%r10d, 40(%rsp)
	movl	32(%rsp), %r10d
	incl	%r10d
	movl	%r10d, 32(%rsp)
	cmpl	44(%rsp), %r10d
	je	.LBB145_111	# bb118.thread
.LBB145_94:	# bb226.preheader
	movl	136(%rsp), %r10d
	leal	2(,%r10,2), %edx
	leal	(%r10,%r10), %ecx
	movl	52(%rsp), %r10d
	leal	-1(%r10), %r10d
	xorl	%r9d, %r9d
	movl	40(%rsp), %esi
	movl	%r9d, %r8d
	jmp	.LBB145_87	# bb219
.LBB145_95:	# bb229
	cmpl	$142, %esi
	setne	%cl
	notb	%r8b
	orb	%cl, %r8b
	testb	$1, %r8b
	jne	.LBB145_109	# bb254
.LBB145_96:	# bb253.preheader
	cmpl	$0, 44(%rsp)
	jle	.LBB145_111	# bb118.thread
.LBB145_97:	# bb.nph325
	movl	52(%rsp), %ecx
	leal	(%rcx,%rcx), %ecx
	movl	%ecx, 32(%rsp)
	movl	152(%rsp), %ecx
	addl	%ecx, %ecx
	movl	%ecx, 28(%rsp)
	cvtsi2sd	%r10d, %xmm2
	xorl	%esi, %esi
	movl	%esi, 40(%rsp)
	jmp	.LBB145_108	# bb246.preheader
.LBB145_98:	# bb.nph
	leal	1(%rdx), %r14d
	leal	-1(%r9), %r15d
	pxor	%xmm3, %xmm3
	xorl	%r12d, %r12d
	movl	%r12d, %r13d
	movapd	%xmm3, %xmm4
	.align	16
.LBB145_99:	# bb240
	leal	(%rsi,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%rax,%rbp,8), %xmm5
	leal	(%r14,%r12), %ebp
	movslq	%ebp, %rbp
	movapd	%xmm2, %xmm6
	mulsd	(%rdi,%rbp,8), %xmm6
	movapd	%xmm6, %xmm7
	mulsd	%xmm5, %xmm7
	leal	(%rbx,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%rax,%rbp,8), %xmm8
	leal	(%rdx,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%rdi,%rbp,8), %xmm9
	movapd	%xmm9, %xmm10
	mulsd	%xmm8, %xmm10
	addsd	%xmm7, %xmm10
	addsd	%xmm10, %xmm3
	mulsd	%xmm8, %xmm6
	mulsd	%xmm5, %xmm9
	subsd	%xmm6, %xmm9
	addsd	%xmm9, %xmm4
	addl	$2, %r12d
	incl	%r13d
	cmpl	%r15d, %r13d
	jne	.LBB145_99	# bb240
.LBB145_100:	# bb242
	cmpl	$131, 48(%rsp)
	je	.LBB145_120	# bb243
.LBB145_101:	# bb244
	leal	-1(%r8), %r14d
	movslq	%r14d, %r14
	movsd	(%rax,%r14,8), %xmm10
	leal	-2(%r8), %r14d
	movslq	%r14d, %r14
	movsd	(%rax,%r14,8), %xmm8
.LBB145_102:	# bb245
	addsd	%xmm3, %xmm10
	movapd	%xmm0, %xmm3
	mulsd	%xmm10, %xmm3
	addsd	%xmm4, %xmm8
	movapd	%xmm1, %xmm4
	mulsd	%xmm8, %xmm4
	subsd	%xmm3, %xmm4
	leal	-1(%r8), %r14d
	leal	-2(%r8), %r8d
	movslq	%r8d, %r15
	movsd	%xmm4, (%rax,%r15,8)
	mulsd	%xmm0, %xmm8
	mulsd	%xmm1, %xmm10
	addsd	%xmm8, %xmm10
	movslq	%r14d, %r14
	movsd	%xmm10, (%rax,%r14,8)
	addl	%r10d, %ecx
	subl	%r11d, %edx
	decl	%r9d
.LBB145_103:	# bb246
	testl	%r9d, %r9d
	jle	.LBB145_107	# bb252
.LBB145_104:	# bb247
	leal	-1(%r9), %r14d
	testl	%r9d, %r9d
	je	.LBB145_107	# bb252
.LBB145_105:	# bb241.preheader
	testl	%r14d, %r14d
	jg	.LBB145_98	# bb.nph
.LBB145_106:	# bb241.preheader.bb242_crit_edge
	pxor	%xmm3, %xmm3
	movapd	%xmm3, %xmm4
	jmp	.LBB145_100	# bb242
.LBB145_107:	# bb252
	addl	28(%rsp), %esi
	movl	40(%rsp), %r10d
	incl	%r10d
	movl	%r10d, 40(%rsp)
	cmpl	44(%rsp), %r10d
	je	.LBB145_111	# bb118.thread
.LBB145_108:	# bb246.preheader
	movl	52(%rsp), %r9d
	leal	-1(%r9), %r8d
	movl	136(%rsp), %r10d
	leal	2(,%r10,2), %ecx
	movl	%r10d, %edx
	imull	%r8d, %edx
	imull	%r8d, %ecx
	leal	(%r10,%r10), %r11d
	movl	$4294967294, %r10d
	subl	%r11d, %r10d
	addl	%edx, %edx
	movl	32(%rsp), %r8d
	leal	(%r8,%rsi), %r8d
	leal	1(%rsi), %ebx
	jmp	.LBB145_103	# bb246
.LBB145_109:	# bb254
	xorl	%edi, %edi
	leaq	.str195, %rsi
	leaq	.str1196, %rdx
	xorb	%al, %al
	call	cblas_xerbla
	jmp	.LBB145_111	# bb118.thread
.LBB145_110:	# bb187
	addl	28(%rsp), %r8d
	movl	40(%rsp), %r10d
	incl	%r10d
	movl	%r10d, 40(%rsp)
	cmpl	44(%rsp), %r10d
	jne	.LBB145_61	# bb181.preheader
.LBB145_111:	# bb118.thread
	addq	$56, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB145_112:	# bb63
	cmpl	$111, %ecx
	movl	$111, %ecx
	movl	$112, %r8d
	cmove	%ecx, %r8d
	movl	%r11d, 52(%rsp)
	jmp	.LBB145_2	# bb77
.LBB145_113:	# bb87
	movapd	%xmm2, %xmm3
	mulsd	(%rdi,%rbx,8), %xmm3
	movslq	%r11d, %r14
	movsd	(%rax,%r14,8), %xmm4
	movapd	%xmm3, %xmm5
	mulsd	%xmm4, %xmm5
	leal	1(%r11), %r14d
	movslq	%r14d, %r14
	movsd	(%rax,%r14,8), %xmm6
	movsd	(%rdi,%rsi,8), %xmm7
	movapd	%xmm7, %xmm8
	mulsd	%xmm6, %xmm8
	addsd	%xmm5, %xmm8
	mulsd	%xmm4, %xmm7
	mulsd	%xmm6, %xmm3
	subsd	%xmm3, %xmm7
	jmp	.LBB145_9	# bb91.preheader
.LBB145_114:	# bb110
	movapd	%xmm2, %xmm5
	mulsd	(%rdi,%rdx,8), %xmm5
	leal	(%r9,%rsi), %ebx
	movslq	%ebx, %rbx
	movsd	(%rax,%rbx,8), %xmm6
	movapd	%xmm5, %xmm7
	mulsd	%xmm6, %xmm7
	leal	(%r8,%rsi), %ebx
	movslq	%ebx, %rbx
	movsd	(%rax,%rbx,8), %xmm8
	movsd	(%rdi,%rcx,8), %xmm9
	movapd	%xmm9, %xmm10
	mulsd	%xmm8, %xmm10
	addsd	%xmm7, %xmm10
	mulsd	%xmm8, %xmm5
	mulsd	%xmm6, %xmm9
	subsd	%xmm5, %xmm9
	jmp	.LBB145_27	# bb112
.LBB145_115:	# bb134
	movapd	%xmm2, %xmm5
	mulsd	(%rdi,%rsi,8), %xmm5
	leal	(%rdx,%r8), %ebx
	movslq	%ebx, %rbx
	movsd	(%rax,%rbx,8), %xmm6
	movapd	%xmm5, %xmm7
	mulsd	%xmm6, %xmm7
	leal	(%r9,%r8), %ebx
	movslq	%ebx, %rbx
	movsd	(%rax,%rbx,8), %xmm8
	movsd	(%rdi,%rcx,8), %xmm9
	movapd	%xmm9, %xmm10
	mulsd	%xmm8, %xmm10
	addsd	%xmm7, %xmm10
	mulsd	%xmm8, %xmm5
	mulsd	%xmm6, %xmm9
	subsd	%xmm5, %xmm9
	jmp	.LBB145_42	# bb136
.LBB145_116:	# bb155
	movapd	%xmm2, %xmm3
	movq	32(%rsp), %r11
	mulsd	(%rdi,%r11,8), %xmm3
	movslq	%r10d, %r11
	movsd	(%rax,%r11,8), %xmm4
	movapd	%xmm3, %xmm5
	mulsd	%xmm4, %xmm5
	leal	1(%r10), %r11d
	movslq	%r11d, %r11
	movsd	(%rax,%r11,8), %xmm6
	movsd	(%rdi,%rdx,8), %xmm7
	movapd	%xmm7, %xmm8
	mulsd	%xmm6, %xmm8
	addsd	%xmm5, %xmm8
	mulsd	%xmm4, %xmm7
	mulsd	%xmm6, %xmm3
	subsd	%xmm3, %xmm7
	jmp	.LBB145_52	# bb159.preheader
.LBB145_117:	# bb178
	movslq	%r10d, %rbx
	leal	1(%r10), %r14d
	movslq	%r14d, %r14
	movapd	%xmm2, %xmm5
	mulsd	(%rdi,%r14,8), %xmm5
	leal	(%rsi,%rcx), %r14d
	movslq	%r14d, %r14
	movsd	(%rax,%r14,8), %xmm6
	movapd	%xmm5, %xmm7
	mulsd	%xmm6, %xmm7
	movsd	(%rdi,%rbx,8), %xmm8
	leal	(%r9,%rcx), %ebx
	movslq	%ebx, %rbx
	movsd	(%rax,%rbx,8), %xmm9
	movapd	%xmm8, %xmm10
	mulsd	%xmm9, %xmm10
	addsd	%xmm7, %xmm10
	mulsd	%xmm9, %xmm5
	mulsd	%xmm6, %xmm8
	subsd	%xmm5, %xmm8
	jmp	.LBB145_66	# bb180
.LBB145_118:	# bb200
	movslq	%esi, %r11
	leal	1(%rsi), %ebx
	movslq	%ebx, %rbx
	movapd	%xmm2, %xmm3
	mulsd	(%rdi,%rbx,8), %xmm3
	movslq	%edx, %rbx
	movsd	(%rax,%rbx,8), %xmm4
	movapd	%xmm3, %xmm5
	mulsd	%xmm4, %xmm5
	movsd	(%rdi,%r11,8), %xmm6
	leal	1(%rdx), %r11d
	movslq	%r11d, %r11
	movsd	(%rax,%r11,8), %xmm7
	movapd	%xmm6, %xmm8
	mulsd	%xmm7, %xmm8
	addsd	%xmm5, %xmm8
	mulsd	%xmm4, %xmm6
	mulsd	%xmm7, %xmm3
	subsd	%xmm3, %xmm6
	jmp	.LBB145_77	# bb204.preheader
.LBB145_119:	# bb220
	movslq	%r9d, %r11
	leal	1(%r9), %ebx
	movslq	%ebx, %rbx
	movapd	%xmm2, %xmm3
	mulsd	(%rdi,%rbx,8), %xmm3
	movslq	%esi, %rbx
	movsd	(%rax,%rbx,8), %xmm4
	movapd	%xmm3, %xmm5
	mulsd	%xmm4, %xmm5
	movsd	(%rdi,%r11,8), %xmm6
	leal	1(%rsi), %r11d
	movslq	%r11d, %r11
	movsd	(%rax,%r11,8), %xmm7
	movapd	%xmm6, %xmm8
	mulsd	%xmm7, %xmm8
	addsd	%xmm5, %xmm8
	mulsd	%xmm4, %xmm6
	mulsd	%xmm7, %xmm3
	subsd	%xmm3, %xmm6
	jmp	.LBB145_89	# bb224.preheader
.LBB145_120:	# bb243
	movslq	%ecx, %r14
	leal	1(%rcx), %r15d
	movslq	%r15d, %r15
	movapd	%xmm2, %xmm5
	mulsd	(%rdi,%r15,8), %xmm5
	leal	-2(%r8), %r15d
	movslq	%r15d, %r15
	movsd	(%rax,%r15,8), %xmm6
	movapd	%xmm5, %xmm7
	mulsd	%xmm6, %xmm7
	movsd	(%rdi,%r14,8), %xmm8
	leal	-1(%r8), %r14d
	movslq	%r14d, %r14
	movsd	(%rax,%r14,8), %xmm9
	movapd	%xmm8, %xmm10
	mulsd	%xmm9, %xmm10
	addsd	%xmm7, %xmm10
	mulsd	%xmm9, %xmm5
	mulsd	%xmm6, %xmm8
	subsd	%xmm5, %xmm8
	jmp	.LBB145_102	# bb245
	.size	cblas_ztrmm, .-cblas_ztrmm
.Leh_func_end101:


	.align	16
	.globl	cblas_ztrmv
	.type	cblas_ztrmv,@function
cblas_ztrmv:
.Leh_func_begin102:
.Llabel102:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	cmpl	$113, %edx
	movl	$4294967295, %eax
	movl	$1, %r10d
	cmove	%eax, %r10d
	movl	$112, %eax
	cmovne	%edx, %eax
	cmpl	$121, %esi
	sete	%dl
	setne	%r11b
	cmpl	$101, %edi
	sete	%bl
	setne	%r14b
	andb	%dl, %bl
	orb	%r11b, %r14b
	testb	%r14b, %r14b
	movl	112(%rsp), %edx
	movq	104(%rsp), %r11
	movl	%ecx, 36(%rsp)
	jne	.LBB146_2	# bb54
.LBB146_1:	# entry
	cmpl	$111, %eax
	je	.LBB146_4	# bb62
.LBB146_2:	# bb54
	cmpl	$122, %esi
	sete	%cl
	setne	%r14b
	cmpl	$102, %edi
	sete	%r15b
	setne	%r12b
	andb	%cl, %r15b
	orb	%r14b, %r12b
	testb	%r12b, %r12b
	jne	.LBB146_14	# bb77
.LBB146_3:	# bb54
	cmpl	$112, %eax
	jne	.LBB146_14	# bb77
.LBB146_4:	# bb62
	testl	%edx, %edx
	jg	.LBB146_55	# bb62.bb76.preheader_crit_edge
.LBB146_5:	# bb63
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
.LBB146_6:	# bb76.preheader
	testl	%r8d, %r8d
	jle	.LBB146_29	# bb111.thread
.LBB146_7:	# bb.nph246
	movl	$1, %ecx
	subl	%r8d, %ecx
	imull	%edx, %ecx
	addl	%eax, %eax
	movl	96(%rsp), %esi
	leal	2(,%rsi,2), %esi
	movl	%esi, 32(%rsp)
	leal	-1(%r8), %esi
	cvtsi2sd	%r10d, %xmm0
	leal	(%rdx,%rdx), %r10d
	movl	%r10d, 28(%rsp)
	xorl	%r10d, %r10d
	movl	%edx, %edi
	movl	%r10d, %ebx
	.align	16
.LBB146_8:	# bb66
	testl	%edx, %edx
	movl	$0, %r14d
	cmovle	%ecx, %r14d
	leal	1(%rbx), %r15d
	cmpl	%r8d, %r15d
	jge	.LBB146_56	# bb66.bb72_crit_edge
.LBB146_9:	# bb66.bb70_crit_edge
	pxor	%xmm1, %xmm1
	xorl	%r15d, %r15d
	movl	%r10d, %r12d
	movl	%edi, %r13d
	movapd	%xmm1, %xmm2
	.align	16
.LBB146_10:	# bb70
	leal	3(%r12), %ebp
	movslq	%ebp, %rbp
	movapd	%xmm0, %xmm3
	mulsd	(%r9,%rbp,8), %xmm3
	addl	%r14d, %r13d
	leal	1(,%r13,2), %r14d
	leal	(%r13,%r13), %ebp
	movslq	%ebp, %rbp
	movsd	(%r11,%rbp,8), %xmm4
	movapd	%xmm3, %xmm5
	mulsd	%xmm4, %xmm5
	addl	$2, %r12d
	movslq	%r12d, %rbp
	movsd	(%r9,%rbp,8), %xmm6
	movslq	%r14d, %r14
	movsd	(%r11,%r14,8), %xmm7
	movapd	%xmm6, %xmm8
	mulsd	%xmm7, %xmm8
	addsd	%xmm5, %xmm8
	addsd	%xmm8, %xmm2
	mulsd	%xmm7, %xmm3
	mulsd	%xmm4, %xmm6
	subsd	%xmm3, %xmm6
	addsd	%xmm6, %xmm1
	incl	%r15d
	cmpl	%esi, %r15d
	movl	%edx, %r14d
	jne	.LBB146_10	# bb70
.LBB146_11:	# bb72
	movslq	%eax, %r14
	movsd	(%r11,%r14,8), %xmm3
	cmpl	$131, 36(%rsp)
	je	.LBB146_57	# bb73
.LBB146_12:	# bb74
	addsd	%xmm1, %xmm3
	movsd	%xmm3, (%r11,%r14,8)
	leal	1(%rax), %r14d
	movslq	%r14d, %r14
	addsd	(%r11,%r14,8), %xmm2
	movsd	%xmm2, (%r11,%r14,8)
.LBB146_13:	# bb75
	addl	28(%rsp), %eax
	addl	%edx, %edi
	addl	32(%rsp), %r10d
	decl	%esi
	incl	%ebx
	cmpl	%r8d, %ebx
	jne	.LBB146_8	# bb66
	jmp	.LBB146_29	# bb111.thread
.LBB146_14:	# bb77
	cmpl	$122, %esi
	sete	%cl
	setne	%r14b
	cmpl	$101, %edi
	sete	%r12b
	setne	%r13b
	andb	%cl, %r12b
	orb	%r14b, %r13b
	testb	%r13b, %r13b
	jne	.LBB146_16	# bb85
.LBB146_15:	# bb77
	cmpl	$111, %eax
	je	.LBB146_18	# bb93
.LBB146_16:	# bb85
	cmpl	$121, %esi
	sete	%cl
	setne	%sil
	cmpl	$102, %edi
	sete	%dil
	setne	%r14b
	andb	%cl, %dil
	orb	%sil, %r14b
	testb	%r14b, %r14b
	jne	.LBB146_30	# bb113
.LBB146_17:	# bb85
	cmpl	$112, %eax
	jne	.LBB146_30	# bb113
.LBB146_18:	# bb93
	testl	%edx, %edx
	jg	.LBB146_58	# bb93.bb96_crit_edge
.LBB146_19:	# bb94
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
.LBB146_20:	# bb96
	leal	-1(%r8), %ecx
	movl	96(%rsp), %esi
	leal	2(,%rsi,2), %edi
	movl	%esi, %ebx
	imull	%ecx, %ebx
	movl	%edx, %r14d
	imull	%ecx, %r14d
	imull	%ecx, %edi
	addl	%esi, %esi
	movl	%esi, 16(%rsp)
	movl	$4294967294, %ecx
	subl	%esi, %ecx
	movl	%ecx, 28(%rsp)
	addl	%ebx, %ebx
	addl	%eax, %r14d
	addl	%r14d, %r14d
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
	cvtsi2sd	%r10d, %xmm0
	leal	(%rdx,%rdx), %r10d
	movl	%r10d, 32(%rsp)
	jmp	.LBB146_27	# bb107
.LBB146_21:	# bb97
	testl	%edx, %edx
	movl	$0, %ecx
	cmovle	%eax, %ecx
	testl	%r10d, %r10d
	jle	.LBB146_59	# bb97.bb103_crit_edge
.LBB146_22:	# bb.nph228
	leal	(%rdx,%rdx), %r10d
	addl	%ecx, %ecx
	leal	-1(%r8), %esi
	pxor	%xmm1, %xmm1
	xorl	%r15d, %r15d
	movl	%ebx, %r12d
	movapd	%xmm1, %xmm2
	.align	16
.LBB146_23:	# bb101
	movslq	%r12d, %r13
	leal	1(%r12), %ebp
	movslq	%ebp, %rbp
	movapd	%xmm0, %xmm3
	mulsd	(%r9,%rbp,8), %xmm3
	movslq	%ecx, %rbp
	movsd	(%r11,%rbp,8), %xmm4
	movapd	%xmm3, %xmm5
	mulsd	%xmm4, %xmm5
	movsd	(%r9,%r13,8), %xmm6
	leal	1(%rcx), %r13d
	movslq	%r13d, %r13
	movsd	(%r11,%r13,8), %xmm7
	movapd	%xmm6, %xmm8
	mulsd	%xmm7, %xmm8
	addsd	%xmm5, %xmm8
	addsd	%xmm8, %xmm2
	mulsd	%xmm4, %xmm6
	mulsd	%xmm7, %xmm3
	subsd	%xmm3, %xmm6
	addsd	%xmm6, %xmm1
	addl	%r10d, %ecx
	addl	$2, %r12d
	incl	%r15d
	cmpl	%esi, %r15d
	jne	.LBB146_23	# bb101
.LBB146_24:	# bb103
	movslq	%r14d, %rcx
	movsd	(%r11,%rcx,8), %xmm3
	cmpl	$131, 36(%rsp)
	je	.LBB146_60	# bb104
.LBB146_25:	# bb105
	addsd	%xmm1, %xmm3
	movsd	%xmm3, (%r11,%rcx,8)
	leal	1(%r14), %r10d
	movslq	%r10d, %rcx
	addsd	(%r11,%rcx,8), %xmm2
	movsd	%xmm2, (%r11,%rcx,8)
.LBB146_26:	# bb106
	addl	28(%rsp), %edi
	subl	32(%rsp), %r14d
	subl	16(%rsp), %ebx
	decl	%r8d
.LBB146_27:	# bb107
	testl	%r8d, %r8d
	jle	.LBB146_29	# bb111.thread
.LBB146_28:	# bb108
	leal	-1(%r8), %r10d
	testl	%r8d, %r8d
	jne	.LBB146_21	# bb97
.LBB146_29:	# bb111.thread
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB146_30:	# bb113
	cmpl	$111, %eax
	sete	%cl
	andb	%cl, %r15b
	cmpl	$112, %eax
	sete	%cl
	testb	%cl, %bl
	jne	.LBB146_32	# bb129
.LBB146_31:	# bb113
	notb	%r15b
	testb	$1, %r15b
	jne	.LBB146_43	# bb149
.LBB146_32:	# bb129
	testl	%edx, %edx
	jg	.LBB146_61	# bb129.bb132_crit_edge
.LBB146_33:	# bb130
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
.LBB146_34:	# bb132
	movl	96(%rsp), %ecx
	leal	2(,%rcx,2), %esi
	leal	(%rcx,%rcx), %ecx
	movl	$4294967294, %edi
	subl	%ecx, %edi
	movl	%edi, 28(%rsp)
	leal	-1(%r8), %ecx
	movl	%edx, %edi
	imull	%ecx, %edi
	imull	%ecx, %esi
	addl	%eax, %edi
	addl	%edi, %edi
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
	leal	(%r8,%r8), %ecx
	cvtsi2sd	%r10d, %xmm0
	leal	(%rdx,%rdx), %r10d
	movl	%r10d, 32(%rsp)
	jmp	.LBB146_41	# bb143
.LBB146_35:	# bb133
	testl	%edx, %edx
	movl	$0, %ebx
	cmovle	%eax, %ebx
	testl	%r10d, %r10d
	jle	.LBB146_62	# bb133.bb139_crit_edge
.LBB146_36:	# bb.nph219
	leal	(%rdx,%rdx), %r10d
	addl	%ebx, %ebx
	movl	96(%rsp), %r14d
	leal	(%r14,%r14), %r14d
	leal	-1(%r8), %r15d
	pxor	%xmm1, %xmm1
	xorl	%r12d, %r12d
	movl	%ecx, %r13d
	movapd	%xmm1, %xmm2
	.align	16
.LBB146_37:	# bb137
	leal	-1(%r13), %ebp
	movslq	%ebp, %rbp
	movapd	%xmm0, %xmm3
	mulsd	(%r9,%rbp,8), %xmm3
	movslq	%ebx, %rbp
	movsd	(%r11,%rbp,8), %xmm4
	movapd	%xmm3, %xmm5
	mulsd	%xmm4, %xmm5
	leal	1(%rbx), %ebp
	movslq	%ebp, %rbp
	movsd	(%r11,%rbp,8), %xmm6
	leal	-2(%r13), %ebp
	movslq	%ebp, %rbp
	movsd	(%r9,%rbp,8), %xmm7
	movapd	%xmm7, %xmm8
	mulsd	%xmm6, %xmm8
	addsd	%xmm5, %xmm8
	addsd	%xmm8, %xmm2
	mulsd	%xmm6, %xmm3
	mulsd	%xmm4, %xmm7
	subsd	%xmm3, %xmm7
	addsd	%xmm7, %xmm1
	addl	%r10d, %ebx
	addl	%r14d, %r13d
	incl	%r12d
	cmpl	%r15d, %r12d
	jne	.LBB146_37	# bb137
.LBB146_38:	# bb139
	movslq	%edi, %r10
	movsd	(%r11,%r10,8), %xmm3
	cmpl	$131, 36(%rsp)
	je	.LBB146_63	# bb140
.LBB146_39:	# bb141
	addsd	%xmm1, %xmm3
	movsd	%xmm3, (%r11,%r10,8)
	leal	1(%rdi), %r10d
	movslq	%r10d, %r10
	addsd	(%r11,%r10,8), %xmm2
	movsd	%xmm2, (%r11,%r10,8)
.LBB146_40:	# bb142
	addl	28(%rsp), %esi
	subl	32(%rsp), %edi
	addl	$4294967294, %ecx
	decl	%r8d
.LBB146_41:	# bb143
	testl	%r8d, %r8d
	jle	.LBB146_29	# bb111.thread
.LBB146_42:	# bb144
	leal	-1(%r8), %r10d
	testl	%r8d, %r8d
	jne	.LBB146_35	# bb133
	jmp	.LBB146_29	# bb111.thread
.LBB146_43:	# bb149
	cmpl	$111, %eax
	sete	%cl
	andb	%cl, %dil
	cmpl	$112, %eax
	sete	%al
	testb	%al, %r12b
	jne	.LBB146_45	# bb165
.LBB146_44:	# bb149
	notb	%dil
	testb	$1, %dil
	jne	.LBB146_67	# bb180
.LBB146_45:	# bb165
	testl	%edx, %edx
	jg	.LBB146_64	# bb165.bb179.preheader_crit_edge
.LBB146_46:	# bb166
	movl	$1, %eax
	subl	%r8d, %eax
	imull	%edx, %eax
.LBB146_47:	# bb179.preheader
	testl	%r8d, %r8d
	jle	.LBB146_29	# bb111.thread
.LBB146_48:	# bb.nph213
	movl	$1, %ecx
	subl	%r8d, %ecx
	imull	%edx, %ecx
	movl	%ecx, 12(%rsp)
	addl	%eax, %eax
	movl	96(%rsp), %ecx
	leal	2(,%rcx,2), %esi
	movl	%esi, 28(%rsp)
	leal	(%rcx,%rcx), %ecx
	movl	%ecx, 24(%rsp)
	leal	-1(%r8), %ecx
	cvtsi2sd	%r10d, %xmm0
	leal	(%rdx,%rdx), %esi
	movl	%esi, 20(%rsp)
	xorl	%esi, %esi
	movl	%edx, 32(%rsp)
	movl	%esi, %edi
	.align	16
.LBB146_49:	# bb169
	testl	%edx, %edx
	movl	$0, %r10d
	cmovle	12(%rsp), %r10d
	leal	1(%rdi), %ebx
	cmpl	%r8d, %ebx
	jge	.LBB146_65	# bb169.bb175_crit_edge
.LBB146_50:	# bb.nph
	movl	24(%rsp), %ebx
	leal	(%rbx,%rsi), %ebx
	movl	96(%rsp), %r14d
	leal	(%r14,%r14), %r14d
	pxor	%xmm1, %xmm1
	xorl	%r15d, %r15d
	movl	32(%rsp), %r12d
	movapd	%xmm1, %xmm2
	.align	16
.LBB146_51:	# bb173
	movslq	%ebx, %r13
	leal	1(%rbx), %ebp
	movslq	%ebp, %rbp
	movapd	%xmm0, %xmm3
	mulsd	(%r9,%rbp,8), %xmm3
	addl	%r10d, %r12d
	leal	1(,%r12,2), %r10d
	leal	(%r12,%r12), %ebp
	movslq	%ebp, %rbp
	movsd	(%r11,%rbp,8), %xmm4
	movapd	%xmm3, %xmm5
	mulsd	%xmm4, %xmm5
	movsd	(%r9,%r13,8), %xmm6
	movslq	%r10d, %r10
	movsd	(%r11,%r10,8), %xmm7
	movapd	%xmm6, %xmm8
	mulsd	%xmm7, %xmm8
	addsd	%xmm5, %xmm8
	addsd	%xmm8, %xmm1
	mulsd	%xmm7, %xmm3
	mulsd	%xmm4, %xmm6
	subsd	%xmm3, %xmm6
	addsd	%xmm6, %xmm2
	addl	%r14d, %ebx
	incl	%r15d
	cmpl	%ecx, %r15d
	movl	%edx, %r10d
	jne	.LBB146_51	# bb173
.LBB146_52:	# bb175
	movslq	%eax, %r10
	movsd	(%r11,%r10,8), %xmm3
	cmpl	$131, 36(%rsp)
	je	.LBB146_66	# bb176
.LBB146_53:	# bb177
	addsd	%xmm2, %xmm3
	movsd	%xmm3, (%r11,%r10,8)
	leal	1(%rax), %r10d
	movslq	%r10d, %r10
	addsd	(%r11,%r10,8), %xmm1
	movsd	%xmm1, (%r11,%r10,8)
.LBB146_54:	# bb178
	addl	20(%rsp), %eax
	addl	%edx, 32(%rsp)
	addl	28(%rsp), %esi
	decl	%ecx
	incl	%edi
	cmpl	%r8d, %edi
	je	.LBB146_29	# bb111.thread
	jmp	.LBB146_49	# bb169
.LBB146_55:	# bb62.bb76.preheader_crit_edge
	xorl	%eax, %eax
	jmp	.LBB146_6	# bb76.preheader
.LBB146_56:	# bb66.bb72_crit_edge
	pxor	%xmm1, %xmm1
	movapd	%xmm1, %xmm2
	jmp	.LBB146_11	# bb72
.LBB146_57:	# bb73
	movslq	%r10d, %r15
	leal	1(%r10), %r12d
	movslq	%r12d, %r12
	movapd	%xmm0, %xmm4
	mulsd	(%r9,%r12,8), %xmm4
	leal	1(%rax), %r12d
	movslq	%r12d, %r12
	movsd	(%r11,%r12,8), %xmm5
	movapd	%xmm4, %xmm6
	mulsd	%xmm5, %xmm6
	movsd	(%r9,%r15,8), %xmm7
	movapd	%xmm7, %xmm8
	mulsd	%xmm3, %xmm8
	subsd	%xmm6, %xmm8
	addsd	%xmm1, %xmm8
	movsd	%xmm8, (%r11,%r14,8)
	mulsd	%xmm5, %xmm7
	mulsd	%xmm3, %xmm4
	addsd	%xmm7, %xmm4
	addsd	%xmm2, %xmm4
	movsd	%xmm4, (%r11,%r12,8)
	jmp	.LBB146_13	# bb75
.LBB146_58:	# bb93.bb96_crit_edge
	xorl	%eax, %eax
	jmp	.LBB146_20	# bb96
.LBB146_59:	# bb97.bb103_crit_edge
	pxor	%xmm1, %xmm1
	movapd	%xmm1, %xmm2
	jmp	.LBB146_24	# bb103
.LBB146_60:	# bb104
	movslq	%edi, %rsi
	leal	1(%rdi), %r10d
	movslq	%r10d, %r10
	movapd	%xmm0, %xmm4
	mulsd	(%r9,%r10,8), %xmm4
	leal	1(%r14), %r10d
	movslq	%r10d, %r10
	movsd	(%r11,%r10,8), %xmm5
	movapd	%xmm4, %xmm6
	mulsd	%xmm5, %xmm6
	movsd	(%r9,%rsi,8), %xmm7
	movapd	%xmm7, %xmm8
	mulsd	%xmm3, %xmm8
	subsd	%xmm6, %xmm8
	addsd	%xmm1, %xmm8
	movsd	%xmm8, (%r11,%rcx,8)
	mulsd	%xmm5, %xmm7
	mulsd	%xmm3, %xmm4
	addsd	%xmm7, %xmm4
	addsd	%xmm2, %xmm4
	movsd	%xmm4, (%r11,%r10,8)
	jmp	.LBB146_26	# bb106
.LBB146_61:	# bb129.bb132_crit_edge
	xorl	%eax, %eax
	jmp	.LBB146_34	# bb132
.LBB146_62:	# bb133.bb139_crit_edge
	pxor	%xmm1, %xmm1
	movapd	%xmm1, %xmm2
	jmp	.LBB146_38	# bb139
.LBB146_63:	# bb140
	movslq	%esi, %rbx
	leal	1(%rsi), %r14d
	movslq	%r14d, %r14
	movapd	%xmm0, %xmm4
	mulsd	(%r9,%r14,8), %xmm4
	leal	1(%rdi), %r14d
	movslq	%r14d, %r14
	movsd	(%r11,%r14,8), %xmm5
	movapd	%xmm4, %xmm6
	mulsd	%xmm5, %xmm6
	movsd	(%r9,%rbx,8), %xmm7
	movapd	%xmm7, %xmm8
	mulsd	%xmm3, %xmm8
	subsd	%xmm6, %xmm8
	addsd	%xmm1, %xmm8
	movsd	%xmm8, (%r11,%r10,8)
	mulsd	%xmm5, %xmm7
	mulsd	%xmm3, %xmm4
	addsd	%xmm7, %xmm4
	addsd	%xmm2, %xmm4
	movsd	%xmm4, (%r11,%r14,8)
	jmp	.LBB146_40	# bb142
.LBB146_64:	# bb165.bb179.preheader_crit_edge
	xorl	%eax, %eax
	jmp	.LBB146_47	# bb179.preheader
.LBB146_65:	# bb169.bb175_crit_edge
	pxor	%xmm1, %xmm1
	movapd	%xmm1, %xmm2
	jmp	.LBB146_52	# bb175
.LBB146_66:	# bb176
	movslq	%esi, %rbx
	leal	1(%rsi), %r14d
	movslq	%r14d, %r14
	movapd	%xmm0, %xmm4
	mulsd	(%r9,%r14,8), %xmm4
	leal	1(%rax), %r14d
	movslq	%r14d, %r14
	movsd	(%r11,%r14,8), %xmm5
	movapd	%xmm4, %xmm6
	mulsd	%xmm5, %xmm6
	movsd	(%r9,%rbx,8), %xmm7
	movapd	%xmm7, %xmm8
	mulsd	%xmm3, %xmm8
	subsd	%xmm6, %xmm8
	addsd	%xmm2, %xmm8
	movsd	%xmm8, (%r11,%r10,8)
	mulsd	%xmm5, %xmm7
	mulsd	%xmm3, %xmm4
	addsd	%xmm7, %xmm4
	addsd	%xmm1, %xmm4
	movsd	%xmm4, (%r11,%r14,8)
	jmp	.LBB146_54	# bb178
.LBB146_67:	# bb180
	xorl	%edi, %edi
	leaq	.str197, %rsi
	leaq	.str1198, %rdx
	xorb	%al, %al
	call	cblas_xerbla
	jmp	.LBB146_29	# bb111.thread
	.size	cblas_ztrmv, .-cblas_ztrmv
.Leh_func_end102:


	.section	.rodata.cst8,"aM",@progbits,8
	.align	8
.LCPI147_0:					
	.quad	4607182418800017408	# double value: 1.000000e+00
	.text
	.align	16
	.globl	cblas_ztrsm
	.type	cblas_ztrsm,@function
cblas_ztrsm:
.Leh_func_begin103:
.Llabel103:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$72, %rsp
	cmpl	$113, %ecx
	movl	$4294967295, %eax
	movl	$1, %r10d
	cmove	%eax, %r10d
	cmpl	$101, %edi
	movq	136(%rsp), %rax
	movsd	8(%rax), %xmm0
	movsd	(%rax), %xmm1
	movq	160(%rsp), %rbx
	movl	128(%rsp), %eax
	movl	%r8d, 36(%rsp)
	je	.LBB147_170	# bb90
.LBB147_1:	# bb94
	cmpl	$111, %ecx
	movl	$111, %ecx
	movl	$112, %edi
	cmove	%ecx, %edi
	cmpl	$121, %edx
	movl	$122, %ecx
	movl	$121, %edx
	cmove	%ecx, %edx
	cmpl	$141, %esi
	movl	$142, %ecx
	movl	$141, %esi
	cmove	%ecx, %esi
	movl	%r9d, 68(%rsp)
	movl	%eax, %r9d
.LBB147_2:	# bb104
	movl	%r9d, 32(%rsp)
	cmpl	$121, %edx
	sete	%al
	setne	%cl
	cmpl	$111, %edi
	sete	%r8b
	setne	%r9b
	andb	%al, %r8b
	orb	%cl, %r9b
	testb	%r9b, %r9b
	jne	.LBB147_25	# bb136
.LBB147_3:	# bb104
	cmpl	$141, %esi
	jne	.LBB147_25	# bb136
.LBB147_4:	# bb111
	pxor	%xmm2, %xmm2
	ucomisd	%xmm2, %xmm0
	setnp	%r8b
	sete	%al
	andb	%r8b, %al
	ucomisd	.LCPI147_0(%rip), %xmm1
	setnp	%r8b
	sete	%cl
	andb	%r8b, %cl
	testb	%cl, %al
	jne	.LBB147_171	# bb130.preheader
.LBB147_5:	# bb111
	cmpl	$0, 32(%rsp)
	jle	.LBB147_171	# bb130.preheader
.LBB147_6:	# bb.nph497
	cmpl	$0, 68(%rsp)
	jle	.LBB147_171	# bb130.preheader
.LBB147_7:	# bb116.preheader.preheader
	movl	168(%rsp), %edi
	leal	(%rdi,%rdi), %edi
	xorl	%eax, %eax
	movl	%eax, %esi
	jmp	.LBB147_10	# bb116.preheader
	.align	16
.LBB147_8:	# bb115
	movslq	%ecx, %r8
	movsd	(%rbx,%r8,8), %xmm2
	movapd	%xmm1, %xmm3
	mulsd	%xmm2, %xmm3
	leal	1(%rcx), %r9d
	movslq	%r9d, %r9
	movsd	(%rbx,%r9,8), %xmm4
	movapd	%xmm0, %xmm5
	mulsd	%xmm4, %xmm5
	subsd	%xmm5, %xmm3
	movsd	%xmm3, (%rbx,%r8,8)
	mulsd	%xmm0, %xmm2
	mulsd	%xmm1, %xmm4
	addsd	%xmm2, %xmm4
	movsd	%xmm4, (%rbx,%r9,8)
	addl	$2, %ecx
	incl	%edx
	cmpl	68(%rsp), %edx
	jne	.LBB147_8	# bb115
.LBB147_9:	# bb117
	addl	%edi, %eax
	incl	%esi
	cmpl	32(%rsp), %esi
	je	.LBB147_171	# bb130.preheader
.LBB147_10:	# bb116.preheader
	xorl	%edx, %edx
	movl	%eax, %ecx
	jmp	.LBB147_8	# bb115
.LBB147_11:	# bb120
	cmpl	$131, 36(%rsp)
	jne	.LBB147_15	# bb129.preheader
.LBB147_12:	# bb121
	movl	28(%rsp), %edi
	movslq	%edi, %rax
	leal	1(%rdi), %edi
	movslq	%edi, %rcx
	movsd	56(%rsp), %xmm0
	movq	144(%rsp), %rdx
	mulsd	(%rdx,%rcx,8), %xmm0
	movsd	%xmm0, 48(%rsp)
	movsd	(%rdx,%rax,8), %xmm0
	movsd	%xmm0, 40(%rsp)
	movsd	48(%rsp), %xmm1
	call	_ZL6xhypotdd201
	movsd	48(%rsp), %xmm1
	divsd	%xmm0, %xmm1
	movsd	%xmm1, 48(%rsp)
	movsd	40(%rsp), %xmm1
	divsd	%xmm0, %xmm1
	movsd	%xmm1, 40(%rsp)
	cmpl	$0, 68(%rsp)
	jle	.LBB147_15	# bb129.preheader
.LBB147_13:	# bb121.bb122_crit_edge
	xorl	%edi, %edi
	movl	%r14d, %r10d
	.align	16
.LBB147_14:	# bb122
	movslq	%r10d, %rax
	movsd	(%rbx,%rax,8), %xmm1
	movapd	%xmm1, %xmm2
	movsd	40(%rsp), %xmm3
	mulsd	%xmm3, %xmm2
	leal	1(%r10), %ecx
	movslq	%ecx, %rcx
	movsd	(%rbx,%rcx,8), %xmm4
	movapd	%xmm4, %xmm5
	movsd	48(%rsp), %xmm6
	mulsd	%xmm6, %xmm5
	addsd	%xmm2, %xmm5
	divsd	%xmm0, %xmm5
	movsd	%xmm5, (%rbx,%rax,8)
	mulsd	%xmm6, %xmm1
	mulsd	%xmm3, %xmm4
	subsd	%xmm1, %xmm4
	divsd	%xmm0, %xmm4
	movsd	%xmm4, (%rbx,%rcx,8)
	addl	$2, %r10d
	incl	%edi
	cmpl	68(%rsp), %edi
	jne	.LBB147_14	# bb122
.LBB147_15:	# bb129.preheader
	testl	%r15d, %r15d
	jle	.LBB147_21	# bb130.loopexit404
.LBB147_16:	# bb.nph493
	cmpl	$0, 68(%rsp)
	jle	.LBB147_21	# bb130.loopexit404
.LBB147_17:	# bb.nph493.split
	leal	1(%r14), %edi
	movl	152(%rsp), %r10d
	leal	(%r10,%r10), %r10d
	movl	168(%rsp), %r15d
	leal	(%r15,%r15), %r15d
	movl	32(%rsp), %eax
	leal	-1(%rax), %eax
	xorl	%ecx, %ecx
	movl	24(%rsp), %edx
	movl	%ecx, %esi
	.align	16
.LBB147_18:	# bb125
	leal	-1(%rdx), %r8d
	movslq	%r8d, %r8
	movsd	56(%rsp), %xmm0
	movq	144(%rsp), %r9
	mulsd	(%r9,%r8,8), %xmm0
	leal	-2(%rdx), %r8d
	movslq	%r8d, %r8
	movsd	(%r9,%r8,8), %xmm1
	leal	1(%rcx), %r8d
	xorl	%r9d, %r9d
	movl	%r9d, %r11d
	.align	16
.LBB147_19:	# bb126
	leal	(%rdi,%r9), %r12d
	movslq	%r12d, %r12
	movsd	(%rbx,%r12,8), %xmm2
	movapd	%xmm0, %xmm3
	mulsd	%xmm2, %xmm3
	leal	(%r14,%r9), %r12d
	movslq	%r12d, %r12
	movsd	(%rbx,%r12,8), %xmm4
	movapd	%xmm1, %xmm5
	mulsd	%xmm4, %xmm5
	subsd	%xmm3, %xmm5
	leal	(%rcx,%r9), %r12d
	movslq	%r12d, %r12
	movsd	(%rbx,%r12,8), %xmm3
	subsd	%xmm5, %xmm3
	movsd	%xmm3, (%rbx,%r12,8)
	mulsd	%xmm0, %xmm4
	mulsd	%xmm1, %xmm2
	addsd	%xmm4, %xmm2
	leal	(%r8,%r9), %r12d
	movslq	%r12d, %r12
	movsd	(%rbx,%r12,8), %xmm3
	subsd	%xmm2, %xmm3
	movsd	%xmm3, (%rbx,%r12,8)
	addl	$2, %r9d
	incl	%r11d
	cmpl	68(%rsp), %r11d
	jne	.LBB147_19	# bb126
.LBB147_20:	# bb128
	addl	%r10d, %edx
	addl	%r15d, %ecx
	incl	%esi
	cmpl	%eax, %esi
	jne	.LBB147_18	# bb125
.LBB147_21:	# bb130.loopexit404
	movl	28(%rsp), %edi
	addl	16(%rsp), %edi
	movl	%edi, 28(%rsp)
	subl	20(%rsp), %r14d
	addl	$4294967294, 24(%rsp)
	decl	32(%rsp)
.LBB147_22:	# bb130
	cmpl	$0, 32(%rsp)
	jle	.LBB147_24	# bb134.thread
.LBB147_23:	# bb131
	movl	32(%rsp), %edi
	leal	-1(%rdi), %r15d
	testl	%edi, %edi
	jne	.LBB147_11	# bb120
.LBB147_24:	# bb134.thread
	addq	$72, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB147_25:	# bb136
	cmpl	$121, %edx
	sete	%al
	setne	%cl
	cmpl	$112, %edi
	sete	%r9b
	setne	%r11b
	andb	%al, %r9b
	orb	%cl, %r11b
	testb	%r11b, %r11b
	jne	.LBB147_47	# bb165
.LBB147_26:	# bb136
	cmpl	$141, %esi
	jne	.LBB147_47	# bb165
.LBB147_27:	# bb144
	pxor	%xmm2, %xmm2
	ucomisd	%xmm2, %xmm0
	setnp	%r8b
	sete	%r9b
	andb	%r8b, %r9b
	ucomisd	.LCPI147_0(%rip), %xmm1
	setnp	%r8b
	sete	%al
	andb	%r8b, %al
	testb	%al, %r9b
	jne	.LBB147_34	# bb164.preheader
.LBB147_28:	# bb144
	cmpl	$0, 32(%rsp)
	jle	.LBB147_34	# bb164.preheader
.LBB147_29:	# bb.nph487
	cmpl	$0, 68(%rsp)
	jle	.LBB147_34	# bb164.preheader
.LBB147_30:	# bb149.preheader.preheader
	movl	168(%rsp), %edi
	leal	(%rdi,%rdi), %ecx
	xorl	%esi, %esi
	movl	%esi, %edx
	.align	16
.LBB147_31:	# bb149.preheader
	xorl	%edi, %edi
	movl	%esi, %eax
	.align	16
.LBB147_32:	# bb148
	movslq	%eax, %r8
	movsd	(%rbx,%r8,8), %xmm2
	movapd	%xmm1, %xmm3
	mulsd	%xmm2, %xmm3
	leal	1(%rax), %r9d
	movslq	%r9d, %r9
	movsd	(%rbx,%r9,8), %xmm4
	movapd	%xmm0, %xmm5
	mulsd	%xmm4, %xmm5
	subsd	%xmm5, %xmm3
	movsd	%xmm3, (%rbx,%r8,8)
	mulsd	%xmm0, %xmm2
	mulsd	%xmm1, %xmm4
	addsd	%xmm2, %xmm4
	movsd	%xmm4, (%rbx,%r9,8)
	addl	$2, %eax
	incl	%edi
	cmpl	68(%rsp), %edi
	jne	.LBB147_32	# bb148
.LBB147_33:	# bb150
	addl	%ecx, %esi
	incl	%edx
	cmpl	32(%rsp), %edx
	jne	.LBB147_31	# bb149.preheader
.LBB147_34:	# bb164.preheader
	cmpl	$0, 32(%rsp)
	jle	.LBB147_24	# bb134.thread
.LBB147_35:	# bb.nph483
	movl	152(%rsp), %edi
	leal	2(,%rdi,2), %edi
	movl	%edi, 24(%rsp)
	movl	168(%rsp), %edi
	leal	(%rdi,%rdi), %edi
	movl	%edi, 20(%rsp)
	movl	32(%rsp), %edi
	leal	-1(%rdi), %r14d
	cvtsi2sd	%r10d, %xmm0
	movsd	%xmm0, 56(%rsp)
	xorl	%r15d, %r15d
	movl	%r15d, %r12d
	movl	%r15d, 28(%rsp)
	.align	16
.LBB147_36:	# bb153
	cmpl	$131, 36(%rsp)
	jne	.LBB147_40	# bb162.preheader
.LBB147_37:	# bb154
	movslq	%r15d, %rax
	leal	1(%r15), %edi
	movslq	%edi, %rcx
	movsd	56(%rsp), %xmm0
	movq	144(%rsp), %rdx
	mulsd	(%rdx,%rcx,8), %xmm0
	movsd	%xmm0, 48(%rsp)
	movsd	(%rdx,%rax,8), %xmm0
	movsd	%xmm0, 40(%rsp)
	movsd	48(%rsp), %xmm1
	call	_ZL6xhypotdd201
	movsd	48(%rsp), %xmm1
	divsd	%xmm0, %xmm1
	movsd	%xmm1, 48(%rsp)
	movsd	40(%rsp), %xmm1
	divsd	%xmm0, %xmm1
	movsd	%xmm1, 40(%rsp)
	cmpl	$0, 68(%rsp)
	jle	.LBB147_40	# bb162.preheader
.LBB147_38:	# bb154.bb155_crit_edge
	xorl	%edi, %edi
	movl	%r12d, %r10d
	.align	16
.LBB147_39:	# bb155
	movslq	%r10d, %rax
	movsd	(%rbx,%rax,8), %xmm1
	movapd	%xmm1, %xmm2
	movsd	40(%rsp), %xmm3
	mulsd	%xmm3, %xmm2
	leal	1(%r10), %ecx
	movslq	%ecx, %rcx
	movsd	(%rbx,%rcx,8), %xmm4
	movapd	%xmm4, %xmm5
	movsd	48(%rsp), %xmm6
	mulsd	%xmm6, %xmm5
	addsd	%xmm2, %xmm5
	divsd	%xmm0, %xmm5
	movsd	%xmm5, (%rbx,%rax,8)
	mulsd	%xmm6, %xmm1
	mulsd	%xmm3, %xmm4
	subsd	%xmm1, %xmm4
	divsd	%xmm0, %xmm4
	movsd	%xmm4, (%rbx,%rcx,8)
	addl	$2, %r10d
	incl	%edi
	cmpl	68(%rsp), %edi
	jne	.LBB147_39	# bb155
.LBB147_40:	# bb162.preheader
	movl	28(%rsp), %edi
	leal	1(%rdi), %edi
	cmpl	32(%rsp), %edi
	jge	.LBB147_46	# bb163
.LBB147_41:	# bb.nph481
	cmpl	$0, 68(%rsp)
	jle	.LBB147_46	# bb163
.LBB147_42:	# bb.nph481.split
	movl	168(%rsp), %edi
	leal	(%rdi,%rdi), %edi
	xorl	%r10d, %r10d
	movl	%edi, %eax
	movl	%r15d, %ecx
	.align	16
.LBB147_43:	# bb158
	leal	3(%rcx), %edx
	movslq	%edx, %rdx
	movsd	56(%rsp), %xmm0
	movq	144(%rsp), %rsi
	mulsd	(%rsi,%rdx,8), %xmm0
	leal	2(%rcx), %edx
	movslq	%edx, %rdx
	movsd	(%rsi,%rdx,8), %xmm1
	leal	1(%rax), %edx
	xorl	%esi, %esi
	movl	%r12d, %r8d
	.align	16
.LBB147_44:	# bb159
	movslq	%r8d, %r9
	movsd	(%rbx,%r9,8), %xmm2
	movapd	%xmm1, %xmm3
	mulsd	%xmm2, %xmm3
	leal	1(%r8), %r9d
	movslq	%r9d, %r9
	movsd	(%rbx,%r9,8), %xmm4
	movapd	%xmm0, %xmm5
	mulsd	%xmm4, %xmm5
	subsd	%xmm5, %xmm3
	leal	(%rax,%r8), %r9d
	movslq	%r9d, %r9
	movsd	(%rbx,%r9,8), %xmm5
	subsd	%xmm3, %xmm5
	movsd	%xmm5, (%rbx,%r9,8)
	mulsd	%xmm0, %xmm2
	mulsd	%xmm1, %xmm4
	addsd	%xmm2, %xmm4
	leal	(%rdx,%r8), %r9d
	movslq	%r9d, %r9
	movsd	(%rbx,%r9,8), %xmm2
	subsd	%xmm4, %xmm2
	movsd	%xmm2, (%rbx,%r9,8)
	addl	$2, %r8d
	incl	%esi
	cmpl	68(%rsp), %esi
	jne	.LBB147_44	# bb159
.LBB147_45:	# bb162.loopexit
	addl	%edi, %eax
	addl	$2, %ecx
	incl	%r10d
	cmpl	%r14d, %r10d
	jne	.LBB147_43	# bb158
.LBB147_46:	# bb163
	addl	24(%rsp), %r15d
	addl	20(%rsp), %r12d
	decl	%r14d
	movl	28(%rsp), %edi
	incl	%edi
	movl	%edi, 28(%rsp)
	cmpl	32(%rsp), %edi
	je	.LBB147_24	# bb134.thread
	jmp	.LBB147_36	# bb153
.LBB147_47:	# bb165
	cmpl	$122, %edx
	sete	%al
	setne	%cl
	cmpl	$111, %edi
	sete	%r11b
	setne	%r14b
	andb	%al, %r11b
	orb	%cl, %r14b
	testb	%r14b, %r14b
	jne	.LBB147_69	# bb194
.LBB147_48:	# bb165
	cmpl	$141, %esi
	jne	.LBB147_69	# bb194
.LBB147_49:	# bb173
	pxor	%xmm2, %xmm2
	ucomisd	%xmm2, %xmm0
	setnp	%r8b
	sete	%r9b
	andb	%r8b, %r9b
	ucomisd	.LCPI147_0(%rip), %xmm1
	setnp	%r8b
	sete	%r11b
	andb	%r8b, %r11b
	testb	%r11b, %r9b
	jne	.LBB147_56	# bb193.preheader
.LBB147_50:	# bb173
	cmpl	$0, 32(%rsp)
	jle	.LBB147_56	# bb193.preheader
.LBB147_51:	# bb.nph473
	cmpl	$0, 68(%rsp)
	jle	.LBB147_56	# bb193.preheader
.LBB147_52:	# bb178.preheader.preheader
	movl	168(%rsp), %edi
	leal	(%rdi,%rdi), %eax
	xorl	%edi, %edi
	movl	%edi, %ecx
	.align	16
.LBB147_53:	# bb178.preheader
	xorl	%edx, %edx
	movl	%edi, %esi
	.align	16
.LBB147_54:	# bb177
	movslq	%esi, %r8
	movsd	(%rbx,%r8,8), %xmm2
	movapd	%xmm1, %xmm3
	mulsd	%xmm2, %xmm3
	leal	1(%rsi), %r9d
	movslq	%r9d, %r9
	movsd	(%rbx,%r9,8), %xmm4
	movapd	%xmm0, %xmm5
	mulsd	%xmm4, %xmm5
	subsd	%xmm5, %xmm3
	movsd	%xmm3, (%rbx,%r8,8)
	mulsd	%xmm0, %xmm2
	mulsd	%xmm1, %xmm4
	addsd	%xmm2, %xmm4
	movsd	%xmm4, (%rbx,%r9,8)
	addl	$2, %esi
	incl	%edx
	cmpl	68(%rsp), %edx
	jne	.LBB147_54	# bb177
.LBB147_55:	# bb179
	addl	%eax, %edi
	incl	%ecx
	cmpl	32(%rsp), %ecx
	jne	.LBB147_53	# bb178.preheader
.LBB147_56:	# bb193.preheader
	cmpl	$0, 32(%rsp)
	jle	.LBB147_24	# bb134.thread
.LBB147_57:	# bb.nph469
	movl	152(%rsp), %edi
	leal	2(,%rdi,2), %eax
	movl	%eax, 24(%rsp)
	leal	(%rdi,%rdi), %edi
	movl	%edi, 20(%rsp)
	movl	168(%rsp), %edi
	leal	(%rdi,%rdi), %edi
	movl	%edi, 16(%rsp)
	movl	32(%rsp), %edi
	leal	-1(%rdi), %r14d
	cvtsi2sd	%r10d, %xmm0
	movsd	%xmm0, 56(%rsp)
	xorl	%r15d, %r15d
	movl	%r15d, %r12d
	movl	%r15d, 28(%rsp)
	.align	16
.LBB147_58:	# bb182
	cmpl	$131, 36(%rsp)
	jne	.LBB147_62	# bb191.preheader
.LBB147_59:	# bb183
	movslq	%r15d, %rax
	leal	1(%r15), %edi
	movslq	%edi, %rcx
	movsd	56(%rsp), %xmm0
	movq	144(%rsp), %rdx
	mulsd	(%rdx,%rcx,8), %xmm0
	movsd	%xmm0, 48(%rsp)
	movsd	(%rdx,%rax,8), %xmm0
	movsd	%xmm0, 40(%rsp)
	movsd	48(%rsp), %xmm1
	call	_ZL6xhypotdd201
	movsd	48(%rsp), %xmm1
	divsd	%xmm0, %xmm1
	movsd	%xmm1, 48(%rsp)
	movsd	40(%rsp), %xmm1
	divsd	%xmm0, %xmm1
	movsd	%xmm1, 40(%rsp)
	cmpl	$0, 68(%rsp)
	jle	.LBB147_62	# bb191.preheader
.LBB147_60:	# bb183.bb184_crit_edge
	xorl	%edi, %edi
	movl	%r12d, %r10d
	.align	16
.LBB147_61:	# bb184
	movslq	%r10d, %rax
	movsd	(%rbx,%rax,8), %xmm1
	movapd	%xmm1, %xmm2
	movsd	40(%rsp), %xmm3
	mulsd	%xmm3, %xmm2
	leal	1(%r10), %ecx
	movslq	%ecx, %rcx
	movsd	(%rbx,%rcx,8), %xmm4
	movapd	%xmm4, %xmm5
	movsd	48(%rsp), %xmm6
	mulsd	%xmm6, %xmm5
	addsd	%xmm2, %xmm5
	divsd	%xmm0, %xmm5
	movsd	%xmm5, (%rbx,%rax,8)
	mulsd	%xmm6, %xmm1
	mulsd	%xmm3, %xmm4
	subsd	%xmm1, %xmm4
	divsd	%xmm0, %xmm4
	movsd	%xmm4, (%rbx,%rcx,8)
	addl	$2, %r10d
	incl	%edi
	cmpl	68(%rsp), %edi
	jne	.LBB147_61	# bb184
.LBB147_62:	# bb191.preheader
	movl	28(%rsp), %edi
	leal	1(%rdi), %edi
	cmpl	32(%rsp), %edi
	jge	.LBB147_68	# bb192
.LBB147_63:	# bb.nph467
	cmpl	$0, 68(%rsp)
	jle	.LBB147_68	# bb192
.LBB147_64:	# bb.nph467.split
	movl	20(%rsp), %edi
	leal	(%rdi,%r15), %edi
	movl	152(%rsp), %r10d
	leal	(%r10,%r10), %r10d
	movl	168(%rsp), %eax
	leal	(%rax,%rax), %eax
	xorl	%ecx, %ecx
	movl	%eax, %edx
	.align	16
.LBB147_65:	# bb187
	movslq	%edi, %rsi
	leal	1(%rdi), %r8d
	movslq	%r8d, %r8
	movsd	56(%rsp), %xmm0
	movq	144(%rsp), %r9
	mulsd	(%r9,%r8,8), %xmm0
	movsd	(%r9,%rsi,8), %xmm1
	leal	1(%rdx), %esi
	xorl	%r8d, %r8d
	movl	%r12d, %r9d
	.align	16
.LBB147_66:	# bb188
	movslq	%r9d, %r11
	movsd	(%rbx,%r11,8), %xmm2
	movapd	%xmm1, %xmm3
	mulsd	%xmm2, %xmm3
	leal	1(%r9), %r11d
	movslq	%r11d, %r11
	movsd	(%rbx,%r11,8), %xmm4
	movapd	%xmm0, %xmm5
	mulsd	%xmm4, %xmm5
	subsd	%xmm5, %xmm3
	leal	(%rdx,%r9), %r11d
	movslq	%r11d, %r11
	movsd	(%rbx,%r11,8), %xmm5
	subsd	%xmm3, %xmm5
	movsd	%xmm5, (%rbx,%r11,8)
	mulsd	%xmm0, %xmm2
	mulsd	%xmm1, %xmm4
	addsd	%xmm2, %xmm4
	leal	(%rsi,%r9), %r11d
	movslq	%r11d, %r11
	movsd	(%rbx,%r11,8), %xmm2
	subsd	%xmm4, %xmm2
	movsd	%xmm2, (%rbx,%r11,8)
	addl	$2, %r9d
	incl	%r8d
	cmpl	68(%rsp), %r8d
	jne	.LBB147_66	# bb188
.LBB147_67:	# bb191.loopexit
	addl	%r10d, %edi
	addl	%eax, %edx
	incl	%ecx
	cmpl	%r14d, %ecx
	jne	.LBB147_65	# bb187
.LBB147_68:	# bb192
	addl	24(%rsp), %r15d
	addl	16(%rsp), %r12d
	decl	%r14d
	movl	28(%rsp), %edi
	incl	%edi
	movl	%edi, 28(%rsp)
	cmpl	32(%rsp), %edi
	je	.LBB147_24	# bb134.thread
	jmp	.LBB147_58	# bb182
.LBB147_69:	# bb194
	cmpl	$122, %edx
	sete	%al
	setne	%cl
	cmpl	$112, %edi
	sete	%dl
	setne	%dil
	andb	%al, %dl
	orb	%cl, %dil
	testb	%dil, %dil
	jne	.LBB147_91	# bb227
.LBB147_70:	# bb194
	cmpl	$141, %esi
	jne	.LBB147_91	# bb227
.LBB147_71:	# bb202
	pxor	%xmm2, %xmm2
	ucomisd	%xmm2, %xmm0
	setnp	%dl
	sete	%r8b
	andb	%dl, %r8b
	ucomisd	.LCPI147_0(%rip), %xmm1
	setnp	%dl
	sete	%r9b
	andb	%dl, %r9b
	testb	%r9b, %r8b
	jne	.LBB147_172	# bb221.preheader
.LBB147_72:	# bb202
	cmpl	$0, 32(%rsp)
	jle	.LBB147_172	# bb221.preheader
.LBB147_73:	# bb.nph459
	cmpl	$0, 68(%rsp)
	jle	.LBB147_172	# bb221.preheader
.LBB147_74:	# bb207.preheader.preheader
	movl	168(%rsp), %eax
	leal	(%rax,%rax), %esi
	xorl	%eax, %eax
	movl	%eax, %edi
	jmp	.LBB147_77	# bb207.preheader
	.align	16
.LBB147_75:	# bb206
	movslq	%edx, %r8
	movsd	(%rbx,%r8,8), %xmm2
	movapd	%xmm1, %xmm3
	mulsd	%xmm2, %xmm3
	leal	1(%rdx), %r9d
	movslq	%r9d, %r9
	movsd	(%rbx,%r9,8), %xmm4
	movapd	%xmm0, %xmm5
	mulsd	%xmm4, %xmm5
	subsd	%xmm5, %xmm3
	movsd	%xmm3, (%rbx,%r8,8)
	mulsd	%xmm0, %xmm2
	mulsd	%xmm1, %xmm4
	addsd	%xmm2, %xmm4
	movsd	%xmm4, (%rbx,%r9,8)
	addl	$2, %edx
	incl	%ecx
	cmpl	68(%rsp), %ecx
	jne	.LBB147_75	# bb206
.LBB147_76:	# bb208
	addl	%esi, %eax
	incl	%edi
	cmpl	32(%rsp), %edi
	je	.LBB147_172	# bb221.preheader
.LBB147_77:	# bb207.preheader
	xorl	%ecx, %ecx
	movl	%eax, %edx
	jmp	.LBB147_75	# bb206
.LBB147_78:	# bb211
	cmpl	$131, 36(%rsp)
	jne	.LBB147_82	# bb220.preheader
.LBB147_79:	# bb212
	movslq	%r12d, %rax
	leal	1(%r12), %r10d
	movslq	%r10d, %rcx
	movsd	56(%rsp), %xmm0
	movq	144(%rsp), %rdx
	mulsd	(%rdx,%rcx,8), %xmm0
	movsd	%xmm0, 48(%rsp)
	movsd	(%rdx,%rax,8), %xmm0
	movsd	%xmm0, 40(%rsp)
	movsd	48(%rsp), %xmm1
	call	_ZL6xhypotdd201
	movsd	48(%rsp), %xmm1
	divsd	%xmm0, %xmm1
	movsd	%xmm1, 48(%rsp)
	movsd	40(%rsp), %xmm1
	divsd	%xmm0, %xmm1
	movsd	%xmm1, 40(%rsp)
	cmpl	$0, 68(%rsp)
	jle	.LBB147_82	# bb220.preheader
.LBB147_80:	# bb212.bb213_crit_edge
	xorl	%r10d, %r10d
	movl	%r15d, %eax
	.align	16
.LBB147_81:	# bb213
	movslq	%eax, %rcx
	movsd	(%rbx,%rcx,8), %xmm1
	movapd	%xmm1, %xmm2
	movsd	40(%rsp), %xmm3
	mulsd	%xmm3, %xmm2
	leal	1(%rax), %edx
	movslq	%edx, %rdx
	movsd	(%rbx,%rdx,8), %xmm4
	movapd	%xmm4, %xmm5
	movsd	48(%rsp), %xmm6
	mulsd	%xmm6, %xmm5
	addsd	%xmm2, %xmm5
	divsd	%xmm0, %xmm5
	movsd	%xmm5, (%rbx,%rcx,8)
	mulsd	%xmm6, %xmm1
	mulsd	%xmm3, %xmm4
	subsd	%xmm1, %xmm4
	divsd	%xmm0, %xmm4
	movsd	%xmm4, (%rbx,%rdx,8)
	addl	$2, %eax
	incl	%r10d
	cmpl	68(%rsp), %r10d
	jne	.LBB147_81	# bb213
.LBB147_82:	# bb220.preheader
	testl	%r14d, %r14d
	jle	.LBB147_88	# bb221.loopexit406
.LBB147_83:	# bb.nph455
	cmpl	$0, 68(%rsp)
	jle	.LBB147_88	# bb221.loopexit406
.LBB147_84:	# bb.nph455.split
	leal	1(%r15), %r10d
	movl	168(%rsp), %r14d
	leal	(%r14,%r14), %r14d
	movl	32(%rsp), %eax
	leal	-1(%rax), %eax
	xorl	%ecx, %ecx
	movl	28(%rsp), %edx
	movl	%ecx, %esi
	.align	16
.LBB147_85:	# bb216
	movslq	%edx, %rdi
	leal	1(%rdx), %r8d
	movslq	%r8d, %r8
	movsd	56(%rsp), %xmm0
	movq	144(%rsp), %r9
	mulsd	(%r9,%r8,8), %xmm0
	movsd	(%r9,%rdi,8), %xmm1
	leal	1(%rcx), %edi
	xorl	%r8d, %r8d
	movl	%r8d, %r9d
	.align	16
.LBB147_86:	# bb217
	leal	(%r10,%r8), %r11d
	movslq	%r11d, %r11
	movsd	(%rbx,%r11,8), %xmm2
	movapd	%xmm0, %xmm3
	mulsd	%xmm2, %xmm3
	leal	(%r15,%r8), %r11d
	movslq	%r11d, %r11
	movsd	(%rbx,%r11,8), %xmm4
	movapd	%xmm1, %xmm5
	mulsd	%xmm4, %xmm5
	subsd	%xmm3, %xmm5
	leal	(%rcx,%r8), %r11d
	movslq	%r11d, %r11
	movsd	(%rbx,%r11,8), %xmm3
	subsd	%xmm5, %xmm3
	movsd	%xmm3, (%rbx,%r11,8)
	mulsd	%xmm0, %xmm4
	mulsd	%xmm1, %xmm2
	addsd	%xmm4, %xmm2
	leal	(%rdi,%r8), %r11d
	movslq	%r11d, %r11
	movsd	(%rbx,%r11,8), %xmm3
	subsd	%xmm2, %xmm3
	movsd	%xmm3, (%rbx,%r11,8)
	addl	$2, %r8d
	incl	%r9d
	cmpl	68(%rsp), %r9d
	jne	.LBB147_86	# bb217
.LBB147_87:	# bb219
	addl	%r14d, %ecx
	addl	$2, %edx
	incl	%esi
	cmpl	%eax, %esi
	jne	.LBB147_85	# bb216
.LBB147_88:	# bb221.loopexit406
	addl	20(%rsp), %r12d
	movl	28(%rsp), %r10d
	subl	(%rsp), %r10d
	movl	%r10d, 28(%rsp)
	subl	24(%rsp), %r15d
	decl	32(%rsp)
.LBB147_89:	# bb221
	cmpl	$0, 32(%rsp)
	jle	.LBB147_24	# bb134.thread
.LBB147_90:	# bb222
	movl	32(%rsp), %r10d
	leal	-1(%r10), %r14d
	testl	%r10d, %r10d
	jne	.LBB147_78	# bb211
	jmp	.LBB147_24	# bb134.thread
.LBB147_91:	# bb227
	cmpl	$142, %esi
	setne	%al
	notb	%r8b
	orb	%al, %r8b
	testb	$1, %r8b
	jne	.LBB147_110	# bb254
.LBB147_92:	# bb235
	pxor	%xmm2, %xmm2
	ucomisd	%xmm2, %xmm0
	setnp	%dl
	sete	%r9b
	andb	%dl, %r9b
	ucomisd	.LCPI147_0(%rip), %xmm1
	setnp	%dl
	sete	%r11b
	andb	%dl, %r11b
	testb	%r11b, %r9b
	jne	.LBB147_106	# bb253.preheader
.LBB147_93:	# bb235
	cmpl	$0, 32(%rsp)
	jle	.LBB147_106	# bb253.preheader
.LBB147_94:	# bb.nph449
	cmpl	$0, 68(%rsp)
	jle	.LBB147_106	# bb253.preheader
.LBB147_95:	# bb240.preheader.preheader
	movl	168(%rsp), %eax
	leal	(%rax,%rax), %esi
	xorl	%edx, %edx
	movl	%edx, %eax
	jmp	.LBB147_98	# bb240.preheader
	.align	16
.LBB147_96:	# bb239
	movslq	%ecx, %r8
	movsd	(%rbx,%r8,8), %xmm2
	movapd	%xmm1, %xmm3
	mulsd	%xmm2, %xmm3
	leal	1(%rcx), %r9d
	movslq	%r9d, %r9
	movsd	(%rbx,%r9,8), %xmm4
	movapd	%xmm0, %xmm5
	mulsd	%xmm4, %xmm5
	subsd	%xmm5, %xmm3
	movsd	%xmm3, (%rbx,%r8,8)
	mulsd	%xmm0, %xmm2
	mulsd	%xmm1, %xmm4
	addsd	%xmm2, %xmm4
	movsd	%xmm4, (%rbx,%r9,8)
	addl	$2, %ecx
	incl	%edi
	cmpl	68(%rsp), %edi
	jne	.LBB147_96	# bb239
.LBB147_97:	# bb241
	addl	%esi, %edx
	incl	%eax
	cmpl	32(%rsp), %eax
	je	.LBB147_106	# bb253.preheader
.LBB147_98:	# bb240.preheader
	xorl	%edi, %edi
	movl	%edx, %ecx
	jmp	.LBB147_96	# bb239
	.align	16
.LBB147_99:	# bb245
	cmpl	$131, 36(%rsp)
	jne	.LBB147_101	# bb247
.LBB147_100:	# bb246
	movslq	%r15d, %rax
	leal	1(%r15), %r10d
	movslq	%r10d, %rcx
	movsd	48(%rsp), %xmm0
	movq	144(%rsp), %rdx
	mulsd	(%rdx,%rcx,8), %xmm0
	movsd	%xmm0, 40(%rsp)
	movsd	(%rdx,%rax,8), %xmm0
	movsd	%xmm0, 56(%rsp)
	movsd	40(%rsp), %xmm1
	call	_ZL6xhypotdd201
	movsd	40(%rsp), %xmm1
	divsd	%xmm0, %xmm1
	movsd	%xmm1, 40(%rsp)
	movslq	%r14d, %rax
	leal	1(%r14), %r10d
	movslq	%r10d, %rcx
	movsd	(%rbx,%rcx,8), %xmm1
	movapd	%xmm1, %xmm2
	mulsd	40(%rsp), %xmm2
	movsd	56(%rsp), %xmm3
	divsd	%xmm0, %xmm3
	movsd	%xmm3, 56(%rsp)
	movsd	(%rbx,%rax,8), %xmm3
	movapd	%xmm3, %xmm4
	mulsd	56(%rsp), %xmm4
	addsd	%xmm2, %xmm4
	divsd	%xmm0, %xmm4
	movsd	%xmm4, (%rbx,%rax,8)
	movsd	40(%rsp), %xmm2
	mulsd	%xmm3, %xmm2
	movsd	%xmm2, 40(%rsp)
	movsd	56(%rsp), %xmm2
	mulsd	%xmm1, %xmm2
	subsd	40(%rsp), %xmm2
	divsd	%xmm0, %xmm2
	movsd	%xmm2, (%rbx,%rcx,8)
.LBB147_101:	# bb247
	movslq	%r14d, %rax
	leal	1(%r14), %r10d
	leal	1(%r13), %ecx
	cmpl	68(%rsp), %ecx
	movsd	(%rbx,%rax,8), %xmm0
	movslq	%r10d, %rax
	movsd	(%rbx,%rax,8), %xmm1
	jge	.LBB147_104	# bb250
.LBB147_102:	# bb.nph441
	leal	3(%r14), %r10d
	leal	2(%r14), %eax
	leal	3(%r15), %ecx
	leal	2(%r15), %edx
	xorl	%esi, %esi
	movl	%esi, %edi
	.align	16
.LBB147_103:	# bb248
	leal	(%rcx,%rsi), %r8d
	movslq	%r8d, %r8
	movsd	48(%rsp), %xmm2
	movq	144(%rsp), %r9
	mulsd	(%r9,%r8,8), %xmm2
	movapd	%xmm2, %xmm3
	mulsd	%xmm1, %xmm3
	leal	(%rdx,%rsi), %r8d
	movslq	%r8d, %r8
	movsd	(%r9,%r8,8), %xmm4
	movapd	%xmm4, %xmm5
	mulsd	%xmm0, %xmm5
	subsd	%xmm3, %xmm5
	leal	(%rax,%rsi), %r8d
	movslq	%r8d, %r8
	movsd	(%rbx,%r8,8), %xmm3
	subsd	%xmm5, %xmm3
	movsd	%xmm3, (%rbx,%r8,8)
	mulsd	%xmm0, %xmm2
	mulsd	%xmm1, %xmm4
	addsd	%xmm2, %xmm4
	leal	(%r10,%rsi), %r8d
	movslq	%r8d, %r8
	movsd	(%rbx,%r8,8), %xmm2
	subsd	%xmm4, %xmm2
	movsd	%xmm2, (%rbx,%r8,8)
	addl	$2, %esi
	incl	%edi
	cmpl	%r12d, %edi
	jne	.LBB147_103	# bb248
.LBB147_104:	# bb250
	addl	28(%rsp), %r15d
	addl	$2, %r14d
	decl	%r12d
	incl	%r13d
	cmpl	68(%rsp), %r13d
	jne	.LBB147_99	# bb245
.LBB147_105:	# bb252
	movl	24(%rsp), %r10d
	addl	12(%rsp), %r10d
	movl	%r10d, 24(%rsp)
	movl	20(%rsp), %r10d
	incl	%r10d
	movl	%r10d, 20(%rsp)
	cmpl	32(%rsp), %r10d
	je	.LBB147_24	# bb134.thread
	jmp	.LBB147_109	# bb251.preheader
.LBB147_106:	# bb253.preheader
	cmpl	$0, 32(%rsp)
	jle	.LBB147_24	# bb134.thread
.LBB147_107:	# bb.nph445
	cmpl	$0, 68(%rsp)
	cvtsi2sd	%r10d, %xmm0
	movsd	%xmm0, 48(%rsp)
	jle	.LBB147_24	# bb134.thread
.LBB147_108:	# bb251.preheader.preheader
	movl	168(%rsp), %r10d
	addl	%r10d, %r10d
	movl	%r10d, 12(%rsp)
	xorl	%r10d, %r10d
	movl	%r10d, 24(%rsp)
	movl	%r10d, 20(%rsp)
	.align	16
.LBB147_109:	# bb251.preheader
	movl	152(%rsp), %r10d
	leal	2(,%r10,2), %r10d
	movl	%r10d, 28(%rsp)
	movl	68(%rsp), %r10d
	leal	-1(%r10), %r12d
	xorl	%r15d, %r15d
	movl	24(%rsp), %r14d
	movl	%r15d, %r13d
	jmp	.LBB147_99	# bb245
.LBB147_110:	# bb254
	cmpl	$142, %esi
	setne	%al
	notb	%r9b
	orb	%al, %r9b
	testb	$1, %r9b
	jne	.LBB147_130	# bb285
.LBB147_111:	# bb262
	pxor	%xmm2, %xmm2
	ucomisd	%xmm2, %xmm0
	setnp	%dl
	sete	%r11b
	andb	%dl, %r11b
	ucomisd	.LCPI147_0(%rip), %xmm1
	setnp	%dl
	sete	%al
	andb	%dl, %al
	testb	%al, %r11b
	jne	.LBB147_127	# bb284.preheader
.LBB147_112:	# bb262
	cmpl	$0, 32(%rsp)
	jle	.LBB147_127	# bb284.preheader
.LBB147_113:	# bb.nph437
	cmpl	$0, 68(%rsp)
	jle	.LBB147_127	# bb284.preheader
.LBB147_114:	# bb267.preheader.preheader
	movl	168(%rsp), %eax
	leal	(%rax,%rax), %edi
	xorl	%ecx, %ecx
	movl	%ecx, %eax
	jmp	.LBB147_117	# bb267.preheader
	.align	16
.LBB147_115:	# bb266
	movslq	%esi, %r8
	movsd	(%rbx,%r8,8), %xmm2
	movapd	%xmm1, %xmm3
	mulsd	%xmm2, %xmm3
	leal	1(%rsi), %r9d
	movslq	%r9d, %r9
	movsd	(%rbx,%r9,8), %xmm4
	movapd	%xmm0, %xmm5
	mulsd	%xmm4, %xmm5
	subsd	%xmm5, %xmm3
	movsd	%xmm3, (%rbx,%r8,8)
	mulsd	%xmm0, %xmm2
	mulsd	%xmm1, %xmm4
	addsd	%xmm2, %xmm4
	movsd	%xmm4, (%rbx,%r9,8)
	addl	$2, %esi
	incl	%edx
	cmpl	68(%rsp), %edx
	jne	.LBB147_115	# bb266
.LBB147_116:	# bb268
	addl	%edi, %ecx
	incl	%eax
	cmpl	32(%rsp), %eax
	je	.LBB147_127	# bb284.preheader
.LBB147_117:	# bb267.preheader
	xorl	%edx, %edx
	movl	%ecx, %esi
	jmp	.LBB147_115	# bb266
.LBB147_118:	# bb272
	cmpl	$131, 36(%rsp)
	jne	.LBB147_120	# bb274
.LBB147_119:	# bb273
	movslq	%r13d, %rax
	leal	1(%r13), %r10d
	movslq	%r10d, %rcx
	movsd	48(%rsp), %xmm0
	movq	144(%rsp), %rdx
	mulsd	(%rdx,%rcx,8), %xmm0
	movsd	%xmm0, 40(%rsp)
	movsd	(%rdx,%rax,8), %xmm0
	movsd	%xmm0, 56(%rsp)
	movsd	40(%rsp), %xmm1
	call	_ZL6xhypotdd201
	movsd	40(%rsp), %xmm1
	divsd	%xmm0, %xmm1
	movsd	%xmm1, 40(%rsp)
	movl	24(%rsp), %r10d
	leal	(%r10,%r14), %r10d
	movslq	%r10d, %rax
	movsd	(%rbx,%rax,8), %xmm1
	movapd	%xmm1, %xmm2
	mulsd	40(%rsp), %xmm2
	movsd	56(%rsp), %xmm3
	divsd	%xmm0, %xmm3
	movsd	%xmm3, 56(%rsp)
	movl	28(%rsp), %r10d
	leal	(%r10,%r14), %r10d
	movslq	%r10d, %rcx
	movsd	(%rbx,%rcx,8), %xmm3
	movapd	%xmm3, %xmm4
	mulsd	56(%rsp), %xmm4
	addsd	%xmm2, %xmm4
	divsd	%xmm0, %xmm4
	movsd	%xmm4, (%rbx,%rcx,8)
	mulsd	40(%rsp), %xmm3
	movsd	56(%rsp), %xmm2
	mulsd	%xmm1, %xmm2
	subsd	%xmm3, %xmm2
	divsd	%xmm0, %xmm2
	movsd	%xmm2, (%rbx,%rax,8)
.LBB147_120:	# bb274
	movl	24(%rsp), %r10d
	leal	(%r10,%r14), %r10d
	movslq	%r10d, %rax
	movsd	(%rbx,%rax,8), %xmm0
	movl	28(%rsp), %r10d
	leal	(%r10,%r14), %r10d
	movslq	%r10d, %rax
	movsd	(%rbx,%rax,8), %xmm1
	testl	%r12d, %r12d
	jle	.LBB147_123	# bb277.loopexit
.LBB147_121:	# bb.nph431
	movl	152(%rsp), %r10d
	leal	(%r10,%r10), %r10d
	leal	-1(%r15), %r12d
	xorl	%eax, %eax
	movl	%r14d, %ecx
	movl	20(%rsp), %edx
	.align	16
.LBB147_122:	# bb275
	leal	-1(%rcx), %esi
	movslq	%esi, %rsi
	movsd	48(%rsp), %xmm2
	movq	144(%rsp), %rdi
	mulsd	(%rdi,%rsi,8), %xmm2
	movapd	%xmm2, %xmm3
	mulsd	%xmm0, %xmm3
	leal	-2(%rcx), %esi
	movslq	%esi, %rsi
	movsd	(%rdi,%rsi,8), %xmm4
	movapd	%xmm4, %xmm5
	mulsd	%xmm1, %xmm5
	subsd	%xmm3, %xmm5
	movslq	%edx, %rsi
	movsd	(%rbx,%rsi,8), %xmm3
	subsd	%xmm5, %xmm3
	movsd	%xmm3, (%rbx,%rsi,8)
	mulsd	%xmm1, %xmm2
	mulsd	%xmm0, %xmm4
	addsd	%xmm2, %xmm4
	leal	1(%rdx), %esi
	movslq	%esi, %rsi
	movsd	(%rbx,%rsi,8), %xmm2
	subsd	%xmm4, %xmm2
	movsd	%xmm2, (%rbx,%rsi,8)
	addl	%r10d, %ecx
	addl	$2, %edx
	incl	%eax
	cmpl	%r12d, %eax
	jne	.LBB147_122	# bb275
.LBB147_123:	# bb277.loopexit
	addl	16(%rsp), %r13d
	addl	$4294967294, %r14d
	decl	%r15d
.LBB147_124:	# bb277
	testl	%r15d, %r15d
	jle	.LBB147_126	# bb283
.LBB147_125:	# bb278
	leal	-1(%r15), %r12d
	testl	%r15d, %r15d
	jne	.LBB147_118	# bb272
.LBB147_126:	# bb283
	movl	20(%rsp), %r10d
	addl	12(%rsp), %r10d
	movl	%r10d, 20(%rsp)
	movl	8(%rsp), %r10d
	incl	%r10d
	movl	%r10d, 8(%rsp)
	cmpl	32(%rsp), %r10d
	je	.LBB147_24	# bb134.thread
	jmp	.LBB147_129	# bb277.preheader
.LBB147_127:	# bb284.preheader
	cmpl	$0, 32(%rsp)
	jle	.LBB147_24	# bb134.thread
.LBB147_128:	# bb.nph433
	movl	168(%rsp), %r14d
	addl	%r14d, %r14d
	movl	%r14d, 12(%rsp)
	cvtsi2sd	%r10d, %xmm0
	movsd	%xmm0, 48(%rsp)
	xorl	%r10d, %r10d
	movl	%r10d, 20(%rsp)
	movl	%r10d, 8(%rsp)
	.align	16
.LBB147_129:	# bb277.preheader
	movl	152(%rsp), %r10d
	leal	2(,%r10,2), %r14d
	leal	(%r10,%r10), %r10d
	movl	$4294967294, %r15d
	subl	%r10d, %r15d
	movl	%r15d, 16(%rsp)
	movl	68(%rsp), %r10d
	leal	-1(%r10), %r13d
	imull	%r14d, %r13d
	movl	20(%rsp), %r14d
	leal	-1(%r14), %r15d
	movl	%r15d, 24(%rsp)
	leal	-2(%r14), %r14d
	movl	%r14d, 28(%rsp)
	leal	(%r10,%r10), %r14d
	movl	%r10d, %r15d
	jmp	.LBB147_124	# bb277
.LBB147_130:	# bb285
	cmpl	$142, %esi
	setne	%al
	notb	%r11b
	orb	%al, %r11b
	testb	$1, %r11b
	jne	.LBB147_150	# bb316
.LBB147_131:	# bb293
	pxor	%xmm2, %xmm2
	ucomisd	%xmm2, %xmm0
	setnp	%dl
	sete	%al
	andb	%dl, %al
	ucomisd	.LCPI147_0(%rip), %xmm1
	setnp	%dl
	sete	%cl
	andb	%dl, %cl
	testb	%cl, %al
	jne	.LBB147_147	# bb315.preheader
.LBB147_132:	# bb293
	cmpl	$0, 32(%rsp)
	jle	.LBB147_147	# bb315.preheader
.LBB147_133:	# bb.nph429
	cmpl	$0, 68(%rsp)
	jle	.LBB147_147	# bb315.preheader
.LBB147_134:	# bb298.preheader.preheader
	movl	168(%rsp), %eax
	leal	(%rax,%rax), %ecx
	xorl	%eax, %eax
	movl	%eax, %edi
	jmp	.LBB147_137	# bb298.preheader
	.align	16
.LBB147_135:	# bb297
	movslq	%edx, %r8
	movsd	(%rbx,%r8,8), %xmm2
	movapd	%xmm1, %xmm3
	mulsd	%xmm2, %xmm3
	leal	1(%rdx), %r9d
	movslq	%r9d, %r9
	movsd	(%rbx,%r9,8), %xmm4
	movapd	%xmm0, %xmm5
	mulsd	%xmm4, %xmm5
	subsd	%xmm5, %xmm3
	movsd	%xmm3, (%rbx,%r8,8)
	mulsd	%xmm0, %xmm2
	mulsd	%xmm1, %xmm4
	addsd	%xmm2, %xmm4
	movsd	%xmm4, (%rbx,%r9,8)
	addl	$2, %edx
	incl	%esi
	cmpl	68(%rsp), %esi
	jne	.LBB147_135	# bb297
.LBB147_136:	# bb299
	addl	%ecx, %eax
	incl	%edi
	cmpl	32(%rsp), %edi
	je	.LBB147_147	# bb315.preheader
.LBB147_137:	# bb298.preheader
	xorl	%esi, %esi
	movl	%eax, %edx
	jmp	.LBB147_135	# bb297
.LBB147_138:	# bb303
	cmpl	$131, 36(%rsp)
	jne	.LBB147_140	# bb305
.LBB147_139:	# bb304
	movl	28(%rsp), %r10d
	movslq	%r10d, %rax
	leal	1(%r10), %r10d
	movslq	%r10d, %rcx
	movsd	48(%rsp), %xmm0
	movq	144(%rsp), %rdx
	mulsd	(%rdx,%rcx,8), %xmm0
	movsd	%xmm0, 40(%rsp)
	movsd	(%rdx,%rax,8), %xmm0
	movsd	%xmm0, 56(%rsp)
	movsd	40(%rsp), %xmm1
	call	_ZL6xhypotdd201
	movsd	40(%rsp), %xmm1
	divsd	%xmm0, %xmm1
	movsd	%xmm1, 40(%rsp)
	leal	-1(%r12), %r10d
	movslq	%r10d, %rax
	movsd	(%rbx,%rax,8), %xmm1
	movapd	%xmm1, %xmm2
	mulsd	40(%rsp), %xmm2
	movsd	56(%rsp), %xmm3
	divsd	%xmm0, %xmm3
	movsd	%xmm3, 56(%rsp)
	leal	-2(%r12), %r10d
	movslq	%r10d, %rcx
	movsd	(%rbx,%rcx,8), %xmm3
	movapd	%xmm3, %xmm4
	mulsd	56(%rsp), %xmm4
	addsd	%xmm2, %xmm4
	divsd	%xmm0, %xmm4
	movsd	%xmm4, (%rbx,%rcx,8)
	mulsd	40(%rsp), %xmm3
	movsd	56(%rsp), %xmm2
	mulsd	%xmm1, %xmm2
	subsd	%xmm3, %xmm2
	divsd	%xmm0, %xmm2
	movsd	%xmm2, (%rbx,%rax,8)
.LBB147_140:	# bb305
	leal	-1(%r12), %r10d
	movslq	%r10d, %rax
	movsd	(%rbx,%rax,8), %xmm0
	leal	-2(%r12), %r10d
	movslq	%r10d, %rax
	movsd	(%rbx,%rax,8), %xmm1
	cmpl	$0, 24(%rsp)
	jle	.LBB147_143	# bb308.loopexit
.LBB147_141:	# bb.nph423
	leal	1(%r13), %r10d
	leal	-1(%r15), %eax
	xorl	%ecx, %ecx
	movl	%ecx, %edx
	.align	16
.LBB147_142:	# bb306
	leal	(%r10,%rcx), %esi
	movslq	%esi, %rsi
	movsd	48(%rsp), %xmm2
	movq	144(%rsp), %rdi
	mulsd	(%rdi,%rsi,8), %xmm2
	movapd	%xmm2, %xmm3
	mulsd	%xmm0, %xmm3
	leal	(%r13,%rcx), %esi
	movslq	%esi, %rsi
	movsd	(%rdi,%rsi,8), %xmm4
	movapd	%xmm4, %xmm5
	mulsd	%xmm1, %xmm5
	subsd	%xmm3, %xmm5
	leal	(%r14,%rcx), %esi
	movslq	%esi, %rsi
	movsd	(%rbx,%rsi,8), %xmm3
	subsd	%xmm5, %xmm3
	movsd	%xmm3, (%rbx,%rsi,8)
	mulsd	%xmm1, %xmm2
	mulsd	%xmm0, %xmm4
	addsd	%xmm2, %xmm4
	leal	(%rbp,%rcx), %esi
	movslq	%esi, %rsi
	movsd	(%rbx,%rsi,8), %xmm2
	subsd	%xmm4, %xmm2
	movsd	%xmm2, (%rbx,%rsi,8)
	addl	$2, %ecx
	incl	%edx
	cmpl	%eax, %edx
	jne	.LBB147_142	# bb306
.LBB147_143:	# bb308.loopexit
	movl	16(%rsp), %r10d
	addl	%r10d, 28(%rsp)
	subl	20(%rsp), %r13d
	addl	$4294967294, %r12d
	decl	%r15d
.LBB147_144:	# bb308
	testl	%r15d, %r15d
	jle	.LBB147_146	# bb314
.LBB147_145:	# bb309
	leal	-1(%r15), %r10d
	movl	%r10d, 24(%rsp)
	testl	%r15d, %r15d
	jne	.LBB147_138	# bb303
.LBB147_146:	# bb314
	addl	12(%rsp), %r14d
	movl	8(%rsp), %r10d
	incl	%r10d
	movl	%r10d, 8(%rsp)
	cmpl	32(%rsp), %r10d
	je	.LBB147_24	# bb134.thread
	jmp	.LBB147_149	# bb308.preheader
.LBB147_147:	# bb315.preheader
	cmpl	$0, 32(%rsp)
	jle	.LBB147_24	# bb134.thread
.LBB147_148:	# bb.nph425
	movl	68(%rsp), %r14d
	leal	(%r14,%r14), %r14d
	movl	%r14d, 4(%rsp)
	movl	168(%rsp), %r14d
	addl	%r14d, %r14d
	movl	%r14d, 12(%rsp)
	cvtsi2sd	%r10d, %xmm0
	movsd	%xmm0, 48(%rsp)
	xorl	%r14d, %r14d
	movl	%r14d, 8(%rsp)
	.align	16
.LBB147_149:	# bb308.preheader
	movl	68(%rsp), %r10d
	leal	-1(%r10), %r15d
	movl	152(%rsp), %r12d
	leal	2(,%r12,2), %ebp
	movl	%r12d, %r13d
	imull	%r15d, %r13d
	imull	%r15d, %ebp
	movl	%ebp, 28(%rsp)
	leal	(%r12,%r12), %r15d
	movl	%r15d, 20(%rsp)
	movl	$4294967294, %r12d
	subl	%r15d, %r12d
	movl	%r12d, 16(%rsp)
	addl	%r13d, %r13d
	movl	4(%rsp), %r15d
	leal	(%r15,%r14), %r12d
	leal	1(%r14), %ebp
	movl	%r10d, %r15d
	jmp	.LBB147_144	# bb308
.LBB147_150:	# bb316
	cmpl	$142, %esi
	setne	%al
	notb	%dl
	orb	%al, %dl
	testb	$1, %dl
	jne	.LBB147_169	# bb343
.LBB147_151:	# bb324
	pxor	%xmm2, %xmm2
	ucomisd	%xmm2, %xmm0
	setnp	%al
	sete	%cl
	andb	%al, %cl
	ucomisd	.LCPI147_0(%rip), %xmm1
	setnp	%al
	sete	%dl
	andb	%al, %dl
	testb	%dl, %cl
	jne	.LBB147_165	# bb342.preheader
.LBB147_152:	# bb324
	cmpl	$0, 32(%rsp)
	jle	.LBB147_165	# bb342.preheader
.LBB147_153:	# bb.nph421
	cmpl	$0, 68(%rsp)
	jle	.LBB147_165	# bb342.preheader
.LBB147_154:	# bb329.preheader.preheader
	movl	168(%rsp), %eax
	leal	(%rax,%rax), %esi
	xorl	%eax, %eax
	movl	%eax, %edx
	jmp	.LBB147_157	# bb329.preheader
	.align	16
.LBB147_155:	# bb328
	movslq	%edi, %r8
	movsd	(%rbx,%r8,8), %xmm2
	movapd	%xmm1, %xmm3
	mulsd	%xmm2, %xmm3
	leal	1(%rdi), %r9d
	movslq	%r9d, %r9
	movsd	(%rbx,%r9,8), %xmm4
	movapd	%xmm0, %xmm5
	mulsd	%xmm4, %xmm5
	subsd	%xmm5, %xmm3
	movsd	%xmm3, (%rbx,%r8,8)
	mulsd	%xmm0, %xmm2
	mulsd	%xmm1, %xmm4
	addsd	%xmm2, %xmm4
	movsd	%xmm4, (%rbx,%r9,8)
	addl	$2, %edi
	incl	%ecx
	cmpl	68(%rsp), %ecx
	jne	.LBB147_155	# bb328
.LBB147_156:	# bb330
	addl	%esi, %eax
	incl	%edx
	cmpl	32(%rsp), %edx
	je	.LBB147_165	# bb342.preheader
.LBB147_157:	# bb329.preheader
	xorl	%ecx, %ecx
	movl	%eax, %edi
	jmp	.LBB147_155	# bb328
	.align	16
.LBB147_158:	# bb334
	cmpl	$131, 36(%rsp)
	jne	.LBB147_160	# bb336
.LBB147_159:	# bb335
	movslq	%r12d, %rax
	leal	1(%r12), %r10d
	movslq	%r10d, %rcx
	movsd	48(%rsp), %xmm0
	movq	144(%rsp), %rdx
	mulsd	(%rdx,%rcx,8), %xmm0
	movsd	%xmm0, 40(%rsp)
	movsd	(%rdx,%rax,8), %xmm0
	movsd	%xmm0, 56(%rsp)
	movsd	40(%rsp), %xmm1
	call	_ZL6xhypotdd201
	movsd	40(%rsp), %xmm1
	divsd	%xmm0, %xmm1
	movsd	%xmm1, 40(%rsp)
	movslq	%ebp, %rax
	leal	1(%rbp), %r10d
	movslq	%r10d, %rcx
	movsd	(%rbx,%rcx,8), %xmm1
	movapd	%xmm1, %xmm2
	mulsd	40(%rsp), %xmm2
	movsd	56(%rsp), %xmm3
	divsd	%xmm0, %xmm3
	movsd	%xmm3, 56(%rsp)
	movsd	(%rbx,%rax,8), %xmm3
	movapd	%xmm3, %xmm4
	mulsd	56(%rsp), %xmm4
	addsd	%xmm2, %xmm4
	divsd	%xmm0, %xmm4
	movsd	%xmm4, (%rbx,%rax,8)
	movsd	40(%rsp), %xmm2
	mulsd	%xmm3, %xmm2
	movsd	%xmm2, 40(%rsp)
	movsd	56(%rsp), %xmm2
	mulsd	%xmm1, %xmm2
	subsd	40(%rsp), %xmm2
	divsd	%xmm0, %xmm2
	movsd	%xmm2, (%rbx,%rcx,8)
.LBB147_160:	# bb336
	movslq	%ebp, %rax
	leal	1(%rbp), %r10d
	leal	1(%r14), %ecx
	cmpl	68(%rsp), %ecx
	movsd	(%rbx,%rax,8), %xmm0
	movslq	%r10d, %rax
	movsd	(%rbx,%rax,8), %xmm1
	jge	.LBB147_163	# bb339
.LBB147_161:	# bb.nph
	leal	(%r15,%r12), %r10d
	movl	152(%rsp), %eax
	leal	(%rax,%rax), %eax
	xorl	%ecx, %ecx
	movl	%ebp, %edx
	.align	16
.LBB147_162:	# bb337
	movslq	%r10d, %rsi
	leal	1(%r10), %edi
	movslq	%edi, %rdi
	movsd	48(%rsp), %xmm2
	movq	144(%rsp), %r8
	mulsd	(%r8,%rdi,8), %xmm2
	movapd	%xmm2, %xmm3
	mulsd	%xmm1, %xmm3
	movsd	(%r8,%rsi,8), %xmm4
	movapd	%xmm4, %xmm5
	mulsd	%xmm0, %xmm5
	subsd	%xmm3, %xmm5
	leal	2(%rdx), %esi
	movslq	%esi, %rdi
	movsd	(%rbx,%rdi,8), %xmm3
	subsd	%xmm5, %xmm3
	movsd	%xmm3, (%rbx,%rdi,8)
	mulsd	%xmm0, %xmm2
	mulsd	%xmm1, %xmm4
	addsd	%xmm2, %xmm4
	addl	$3, %edx
	movslq	%edx, %rdx
	movsd	(%rbx,%rdx,8), %xmm2
	subsd	%xmm4, %xmm2
	movsd	%xmm2, (%rbx,%rdx,8)
	addl	%eax, %r10d
	incl	%ecx
	cmpl	%r13d, %ecx
	movl	%esi, %edx
	jne	.LBB147_162	# bb337
.LBB147_163:	# bb339
	addl	28(%rsp), %r12d
	addl	$2, %ebp
	decl	%r13d
	incl	%r14d
	cmpl	68(%rsp), %r14d
	jne	.LBB147_158	# bb334
.LBB147_164:	# bb341
	movl	24(%rsp), %r10d
	addl	12(%rsp), %r10d
	movl	%r10d, 24(%rsp)
	movl	20(%rsp), %r10d
	incl	%r10d
	movl	%r10d, 20(%rsp)
	cmpl	32(%rsp), %r10d
	je	.LBB147_24	# bb134.thread
	jmp	.LBB147_168	# bb340.preheader
.LBB147_165:	# bb342.preheader
	cmpl	$0, 32(%rsp)
	jle	.LBB147_24	# bb134.thread
.LBB147_166:	# bb.nph417
	cmpl	$0, 68(%rsp)
	cvtsi2sd	%r10d, %xmm0
	movsd	%xmm0, 48(%rsp)
	jle	.LBB147_24	# bb134.thread
.LBB147_167:	# bb340.preheader.preheader
	movl	168(%rsp), %r14d
	addl	%r14d, %r14d
	movl	%r14d, 12(%rsp)
	xorl	%r14d, %r14d
	movl	%r14d, 24(%rsp)
	movl	%r14d, 20(%rsp)
	.align	16
.LBB147_168:	# bb340.preheader
	movl	152(%rsp), %r14d
	leal	2(,%r14,2), %r15d
	movl	%r15d, 28(%rsp)
	leal	(%r14,%r14), %r15d
	movl	68(%rsp), %r14d
	leal	-1(%r14), %r13d
	xorl	%r12d, %r12d
	movl	24(%rsp), %ebp
	movl	%r12d, %r14d
	jmp	.LBB147_158	# bb334
.LBB147_169:	# bb343
	xorl	%edi, %edi
	leaq	.str199, %rsi
	leaq	.str1200, %rdx
	xorb	%al, %al
	call	cblas_xerbla
	jmp	.LBB147_24	# bb134.thread
.LBB147_170:	# bb90
	cmpl	$111, %ecx
	movl	$111, %ecx
	movl	$112, %edi
	cmove	%ecx, %edi
	movl	%eax, 68(%rsp)
	jmp	.LBB147_2	# bb104
.LBB147_171:	# bb130.preheader
	movl	152(%rsp), %edi
	leal	2(,%rdi,2), %r15d
	leal	(%rdi,%rdi), %edi
	movl	$4294967294, %r14d
	subl	%edi, %r14d
	movl	%r14d, 16(%rsp)
	movl	32(%rsp), %edi
	leal	-1(%rdi), %eax
	movl	168(%rsp), %ecx
	movl	%ecx, %r14d
	imull	%eax, %r14d
	imull	%eax, %r15d
	movl	%r15d, 28(%rsp)
	addl	%r14d, %r14d
	leal	(%rdi,%rdi), %edi
	movl	%edi, 24(%rsp)
	cvtsi2sd	%r10d, %xmm0
	movsd	%xmm0, 56(%rsp)
	leal	(%rcx,%rcx), %edi
	movl	%edi, 20(%rsp)
	jmp	.LBB147_22	# bb130
.LBB147_172:	# bb221.preheader
	movl	32(%rsp), %r14d
	leal	-1(%r14), %r14d
	movl	168(%rsp), %eax
	movl	%eax, %r15d
	imull	%r14d, %r15d
	movl	152(%rsp), %ecx
	leal	2(,%rcx,2), %r12d
	movl	%ecx, %edx
	imull	%r14d, %edx
	imull	%r14d, %r12d
	addl	%ecx, %ecx
	movl	%ecx, (%rsp)
	movl	$4294967294, %r14d
	subl	%ecx, %r14d
	movl	%r14d, 20(%rsp)
	addl	%edx, %edx
	movl	%edx, 28(%rsp)
	addl	%r15d, %r15d
	cvtsi2sd	%r10d, %xmm0
	movsd	%xmm0, 56(%rsp)
	leal	(%rax,%rax), %r10d
	movl	%r10d, 24(%rsp)
	jmp	.LBB147_89	# bb221
	.size	cblas_ztrsm, .-cblas_ztrsm
.Leh_func_end103:


	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI148_0:					
	.quad	9223372036854775807	# double value: nan
	.quad	9223372036854775807	# double value: nan
	.section	.rodata.cst8,"aM",@progbits,8
	.align	16
.LCPI148_1:					
	.quad	4607182418800017408	# double value: 1.000000e+00
	.text
	.align	16
	.type	_ZL6xhypotdd201,@function
_ZL6xhypotdd201:
	movsd	.LCPI148_0(%rip), %xmm2
	andpd	%xmm2, %xmm0
	andpd	%xmm2, %xmm1
	movapd	%xmm1, %xmm2
	maxsd	%xmm0, %xmm2
	minsd	%xmm1, %xmm0
	pxor	%xmm1, %xmm1
	ucomisd	%xmm1, %xmm0
	setnp	%al
	sete	%cl
	testb	%al, %cl
	jne	.LBB148_2	# bb5
.LBB148_1:	# bb4
	divsd	%xmm2, %xmm0
	mulsd	%xmm0, %xmm0
	addsd	.LCPI148_1(%rip), %xmm0
	sqrtsd	%xmm0, %xmm0
	mulsd	%xmm2, %xmm0
	ret
.LBB148_2:	# bb5
	movapd	%xmm2, %xmm0
	ret
	.size	_ZL6xhypotdd201, .-_ZL6xhypotdd201


	.align	16
	.globl	cblas_ztrsv
	.type	cblas_ztrsv,@function
cblas_ztrsv:
.Leh_func_begin104:
.Llabel104:
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$104, %rsp
	cmpl	$113, %edx
	movl	$4294967295, %eax
	movl	$1, %r10d
	cmove	%eax, %r10d
	movl	%r10d, 72(%rsp)
	movl	$112, %eax
	cmovne	%edx, %eax
	testl	%r8d, %r8d
	movq	168(%rsp), %rbx
	movq	%r9, %r14
	movl	%r8d, 68(%rsp)
	movl	%ecx, 64(%rsp)
	je	.LBB149_18	# bb99.thread
.LBB149_1:	# bb67
	cmpl	$121, %esi
	sete	%cl
	setne	%dl
	cmpl	$101, %edi
	sete	%r8b
	setne	%r9b
	andb	%cl, %r8b
	orb	%dl, %r9b
	testb	%r9b, %r9b
	jne	.LBB149_3	# bb74
.LBB149_2:	# bb67
	cmpl	$111, %eax
	je	.LBB149_5	# bb82
.LBB149_3:	# bb74
	cmpl	$122, %esi
	sete	%cl
	setne	%dl
	cmpl	$102, %edi
	sete	%r9b
	setne	%r10b
	andb	%cl, %r9b
	orb	%dl, %r10b
	testb	%r10b, %r10b
	jne	.LBB149_19	# bb101
.LBB149_4:	# bb74
	cmpl	$112, %eax
	jne	.LBB149_19	# bb101
.LBB149_5:	# bb82
	cmpl	$0, 176(%rsp)
	jg	.LBB149_65	# bb82.bb85_crit_edge
.LBB149_6:	# bb83
	movl	$1, %r15d
	subl	68(%rsp), %r15d
	imull	176(%rsp), %r15d
.LBB149_7:	# bb85
	movl	68(%rsp), %eax
	leal	-1(%rax), %eax
	movl	%eax, %ecx
	imull	176(%rsp), %ecx
	addl	%r15d, %ecx
	cmpl	$131, 64(%rsp)
	jne	.LBB149_9	# bb87
.LBB149_8:	# bb86
	movl	160(%rsp), %edx
	leal	2(,%rdx,2), %edx
	imull	%eax, %edx
	movslq	%edx, %rax
	orl	$1, %edx
	movslq	%edx, %rdx
	cvtsi2sd	72(%rsp), %xmm0
	mulsd	(%r14,%rdx,8), %xmm0
	movsd	%xmm0, 88(%rsp)
	leal	1(,%rcx,2), %edx
	movslq	%edx, %r12
	movsd	(%rbx,%r12,8), %xmm0
	movsd	%xmm0, 56(%rsp)
	addl	%ecx, %ecx
	movslq	%ecx, %r13
	movsd	(%rbx,%r13,8), %xmm0
	movsd	%xmm0, 80(%rsp)
	movsd	(%r14,%rax,8), %xmm0
	movsd	%xmm0, 96(%rsp)
	movsd	88(%rsp), %xmm1
	call	_ZL6xhypotdd204
	movsd	96(%rsp), %xmm1
	divsd	%xmm0, %xmm1
	movsd	%xmm1, 96(%rsp)
	movsd	88(%rsp), %xmm1
	divsd	%xmm0, %xmm1
	movsd	%xmm1, 88(%rsp)
	movsd	56(%rsp), %xmm1
	mulsd	88(%rsp), %xmm1
	movsd	88(%rsp), %xmm2
	mulsd	80(%rsp), %xmm2
	movsd	%xmm2, 88(%rsp)
	movsd	80(%rsp), %xmm2
	mulsd	96(%rsp), %xmm2
	addsd	%xmm1, %xmm2
	divsd	%xmm0, %xmm2
	movsd	%xmm2, (%rbx,%r13,8)
	movsd	96(%rsp), %xmm1
	mulsd	56(%rsp), %xmm1
	subsd	88(%rsp), %xmm1
	divsd	%xmm0, %xmm1
	movsd	%xmm1, (%rbx,%r12,8)
.LBB149_9:	# bb87
	movl	160(%rsp), %eax
	leal	2(,%rax,2), %ecx
	leal	(%rax,%rax), %edx
	movl	$4294967294, %esi
	subl	%edx, %esi
	movl	%esi, 4(%rsp)
	movl	68(%rsp), %edx
	leal	-2(%rdx), %esi
	movl	176(%rsp), %edi
	movl	%edi, %r8d
	imull	%esi, %r8d
	imull	%esi, %eax
	imull	%esi, %ecx
	movl	%ecx, 24(%rsp)
	orl	$1, %ecx
	movl	%ecx, 32(%rsp)
	addl	%edx, %eax
	leal	-2(,%rax,2), %eax
	movl	%eax, 36(%rsp)
	leal	-1(%rdx), %eax
	imull	%edi, %eax
	addl	%r15d, %eax
	addl	%eax, %eax
	movl	%eax, 12(%rsp)
	addl	%r15d, %r8d
	leal	1(,%r8,2), %eax
	movl	%eax, 28(%rsp)
	addl	%r8d, %r8d
	movl	%r8d, 16(%rsp)
	cvtsi2sd	72(%rsp), %xmm0
	movsd	%xmm0, 80(%rsp)
	leal	(%rdi,%rdi), %eax
	movl	%eax, 20(%rsp)
	movl	$1, %r15d
	xorl	%r12d, %r12d
	movl	%r12d, %r13d
	movl	%edx, 56(%rsp)
	jmp	.LBB149_16	# bb95
.LBB149_10:	# bb88
	movl	16(%rsp), %ecx
	leal	(%rcx,%r13), %ecx
	movl	28(%rsp), %edx
	leal	(%rdx,%r13), %edx
	cmpl	68(%rsp), %eax
	movslq	%edx, %rax
	movq	%rax, 40(%rsp)
	movsd	(%rbx,%rax,8), %xmm0
	movsd	%xmm0, 96(%rsp)
	movslq	%ecx, %rbp
	movsd	(%rbx,%rbp,8), %xmm0
	movsd	%xmm0, 88(%rsp)
	jge	.LBB149_13	# bb91
.LBB149_11:	# bb.nph267
	movl	36(%rsp), %eax
	leal	(%rax,%r12), %eax
	movl	12(%rsp), %ecx
	leal	(%rcx,%r13), %ecx
	movl	176(%rsp), %edx
	leal	(%rdx,%rdx), %edx
	xorl	%esi, %esi
	.align	16
.LBB149_12:	# bb89
	movslq	%eax, %rdi
	leal	1(%rax), %r8d
	movslq	%r8d, %r8
	movsd	80(%rsp), %xmm0
	mulsd	(%r14,%r8,8), %xmm0
	movslq	%ecx, %r8
	movsd	(%rbx,%r8,8), %xmm1
	movapd	%xmm0, %xmm2
	mulsd	%xmm1, %xmm2
	movsd	(%r14,%rdi,8), %xmm3
	leal	1(%rcx), %edi
	movslq	%edi, %rdi
	movsd	(%rbx,%rdi,8), %xmm4
	movapd	%xmm3, %xmm5
	mulsd	%xmm4, %xmm5
	addsd	%xmm2, %xmm5
	movsd	96(%rsp), %xmm2
	subsd	%xmm5, %xmm2
	movsd	%xmm2, 96(%rsp)
	mulsd	%xmm1, %xmm3
	mulsd	%xmm4, %xmm0
	subsd	%xmm0, %xmm3
	movsd	88(%rsp), %xmm0
	subsd	%xmm3, %xmm0
	movsd	%xmm0, 88(%rsp)
	addl	%edx, %ecx
	addl	$2, %eax
	incl	%esi
	cmpl	%r15d, %esi
	jne	.LBB149_12	# bb89
.LBB149_13:	# bb91
	cmpl	$131, 64(%rsp)
	je	.LBB149_66	# bb92
.LBB149_14:	# bb93
	movsd	88(%rsp), %xmm0
	movsd	%xmm0, (%rbx,%rbp,8)
	movsd	96(%rsp), %xmm0
	movsd	%xmm0, 48(%rsp)
.LBB149_15:	# bb95.backedge
	movsd	48(%rsp), %xmm0
	movq	40(%rsp), %rax
	movsd	%xmm0, (%rbx,%rax,8)
	addl	4(%rsp), %r12d
	subl	20(%rsp), %r13d
	decl	56(%rsp)
	incl	%r15d
.LBB149_16:	# bb95
	movl	56(%rsp), %eax
	leal	-1(%rax), %eax
	testl	%eax, %eax
	jle	.LBB149_18	# bb99.thread
.LBB149_17:	# bb96
	cmpl	$1, 56(%rsp)
	jne	.LBB149_10	# bb88
.LBB149_18:	# bb99.thread
	addq	$104, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret
.LBB149_19:	# bb101
	cmpl	$122, %esi
	sete	%cl
	setne	%dl
	cmpl	$101, %edi
	sete	%r10b
	setne	%r11b
	andb	%cl, %r10b
	orb	%dl, %r11b
	testb	%r11b, %r11b
	jne	.LBB149_21	# bb109
.LBB149_20:	# bb101
	cmpl	$111, %eax
	je	.LBB149_23	# bb117
.LBB149_21:	# bb109
	cmpl	$121, %esi
	sete	%cl
	setne	%dl
	cmpl	$102, %edi
	sete	%sil
	setne	%dil
	andb	%cl, %sil
	orb	%dl, %dil
	testb	%dil, %dil
	jne	.LBB149_35	# bb134
.LBB149_22:	# bb109
	cmpl	$112, %eax
	jne	.LBB149_35	# bb134
.LBB149_23:	# bb117
	cmpl	$0, 176(%rsp)
	jg	.LBB149_67	# bb117.bb120_crit_edge
.LBB149_24:	# bb118
	movl	$1, %r15d
	subl	68(%rsp), %r15d
	imull	176(%rsp), %r15d
.LBB149_25:	# bb120
	cmpl	$131, 64(%rsp)
	jne	.LBB149_27	# bb133.preheader
.LBB149_26:	# bb121
	cvtsi2sd	72(%rsp), %xmm0
	mulsd	8(%r14), %xmm0
	movsd	%xmm0, 88(%rsp)
	leal	1(,%r15,2), %eax
	movslq	%eax, %r12
	movsd	(%rbx,%r12,8), %xmm0
	movsd	%xmm0, 56(%rsp)
	leal	(%r15,%r15), %eax
	movslq	%eax, %r13
	movsd	(%rbx,%r13,8), %xmm0
	movsd	%xmm0, 80(%rsp)
	movsd	(%r14), %xmm0
	movsd	%xmm0, 96(%rsp)
	movsd	88(%rsp), %xmm1
	call	_ZL6xhypotdd204
	movsd	88(%rsp), %xmm1
	divsd	%xmm0, %xmm1
	movsd	%xmm1, 88(%rsp)
	movsd	56(%rsp), %xmm1
	mulsd	88(%rsp), %xmm1
	movsd	88(%rsp), %xmm2
	mulsd	80(%rsp), %xmm2
	movsd	%xmm2, 88(%rsp)
	movsd	96(%rsp), %xmm2
	divsd	%xmm0, %xmm2
	movsd	%xmm2, 96(%rsp)
	movsd	80(%rsp), %xmm2
	mulsd	96(%rsp), %xmm2
	addsd	%xmm1, %xmm2
	divsd	%xmm0, %xmm2
	movsd	%xmm2, (%rbx,%r13,8)
	movsd	96(%rsp), %xmm1
	mulsd	56(%rsp), %xmm1
	subsd	88(%rsp), %xmm1
	divsd	%xmm0, %xmm1
	movsd	%xmm1, (%rbx,%r12,8)
.LBB149_27:	# bb133.preheader
	cmpl	$2, 68(%rsp)
	jl	.LBB149_18	# bb99.thread
.LBB149_28:	# bb.nph258
	movl	176(%rsp), %eax
	addl	%eax, %r15d
	addl	%r15d, %r15d
	movl	$1, %edx
	movl	68(%rsp), %ecx
	subl	%ecx, %edx
	imull	%eax, %edx
	movl	%edx, 32(%rsp)
	movl	160(%rsp), %edx
	leal	2(,%rdx,2), %esi
	movl	%esi, 40(%rsp)
	addl	%edx, %edx
	movl	%edx, 8(%rsp)
	decl	%ecx
	movl	%ecx, 68(%rsp)
	cvtsi2sd	72(%rsp), %xmm0
	movsd	%xmm0, 80(%rsp)
	leal	(%rax,%rax), %eax
	movl	%eax, 36(%rsp)
	xorl	%r12d, %r12d
	movl	%edx, %r13d
	movl	%edx, 56(%rsp)
	.align	16
.LBB149_29:	# bb123
	cmpl	$0, 176(%rsp)
	movl	$0, %eax
	cmovle	32(%rsp), %eax
	movslq	%r15d, %rbp
	movsd	(%rbx,%rbp,8), %xmm0
	movsd	%xmm0, 96(%rsp)
	incl	%r15d
	movslq	%r15d, %r15
	movsd	(%rbx,%r15,8), %xmm0
	movsd	%xmm0, 88(%rsp)
	leal	1(%r12), %ecx
	testl	%ecx, %ecx
	jle	.LBB149_32	# bb129
.LBB149_30:	# bb.nph251
	movl	176(%rsp), %edx
	leal	(%rdx,%rdx), %edx
	addl	%eax, %eax
	xorl	%esi, %esi
	movl	56(%rsp), %edi
	.align	16
.LBB149_31:	# bb127
	movslq	%edi, %r8
	leal	1(%rdi), %r9d
	movslq	%r9d, %r9
	movsd	80(%rsp), %xmm0
	mulsd	(%r14,%r9,8), %xmm0
	movslq	%eax, %r9
	movsd	(%rbx,%r9,8), %xmm1
	movapd	%xmm0, %xmm2
	mulsd	%xmm1, %xmm2
	movsd	(%r14,%r8,8), %xmm3
	leal	1(%rax), %r8d
	movslq	%r8d, %r8
	movsd	(%rbx,%r8,8), %xmm4
	movapd	%xmm3, %xmm5
	mulsd	%xmm4, %xmm5
	addsd	%xmm2, %xmm5
	movsd	88(%rsp), %xmm2
	subsd	%xmm5, %xmm2
	movsd	%xmm2, 88(%rsp)
	mulsd	%xmm1, %xmm3
	mulsd	%xmm4, %xmm0
	subsd	%xmm0, %xmm3
	movsd	96(%rsp), %xmm0
	subsd	%xmm3, %xmm0
	movsd	%xmm0, 96(%rsp)
	addl	%edx, %eax
	addl	$2, %edi
	incl	%esi
	cmpl	%ecx, %esi
	jne	.LBB149_31	# bb127
.LBB149_32:	# bb129
	cmpl	$131, 64(%rsp)
	je	.LBB149_68	# bb130
.LBB149_33:	# bb131
	movsd	96(%rsp), %xmm0
	movsd	%xmm0, (%rbx,%rbp,8)
	movsd	88(%rsp), %xmm0
	movsd	%xmm0, 48(%rsp)
.LBB149_34:	# bb132
	movsd	48(%rsp), %xmm0
	movsd	%xmm0, (%rbx,%r15,8)
	movl	%ebp, %r15d
	addl	36(%rsp), %r15d
	addl	40(%rsp), %r13d
	movl	56(%rsp), %eax
	addl	8(%rsp), %eax
	movl	%eax, 56(%rsp)
	incl	%r12d
	cmpl	68(%rsp), %r12d
	je	.LBB149_18	# bb99.thread
	jmp	.LBB149_29	# bb123
.LBB149_35:	# bb134
	cmpl	$111, %eax
	sete	%cl
	andb	%cl, %r9b
	cmpl	$112, %eax
	sete	%cl
	testb	%cl, %r8b
	jne	.LBB149_37	# bb150
.LBB149_36:	# bb134
	notb	%r9b
	testb	$1, %r9b
	jne	.LBB149_49	# bb167
.LBB149_37:	# bb150
	cmpl	$0, 176(%rsp)
	jg	.LBB149_69	# bb150.bb153_crit_edge
.LBB149_38:	# bb151
	movl	$1, %r15d
	subl	68(%rsp), %r15d
	imull	176(%rsp), %r15d
.LBB149_39:	# bb153
	cmpl	$131, 64(%rsp)
	jne	.LBB149_41	# bb166.preheader
.LBB149_40:	# bb154
	cvtsi2sd	72(%rsp), %xmm0
	mulsd	8(%r14), %xmm0
	movsd	%xmm0, 88(%rsp)
	leal	1(,%r15,2), %eax
	movslq	%eax, %r12
	movsd	(%rbx,%r12,8), %xmm0
	movsd	%xmm0, 56(%rsp)
	leal	(%r15,%r15), %eax
	movslq	%eax, %r13
	movsd	(%rbx,%r13,8), %xmm0
	movsd	%xmm0, 80(%rsp)
	movsd	(%r14), %xmm0
	movsd	%xmm0, 96(%rsp)
	movsd	88(%rsp), %xmm1
	call	_ZL6xhypotdd204
	movsd	88(%rsp), %xmm1
	divsd	%xmm0, %xmm1
	movsd	%xmm1, 88(%rsp)
	movsd	56(%rsp), %xmm1
	mulsd	88(%rsp), %xmm1
	movsd	88(%rsp), %xmm2
	mulsd	80(%rsp), %xmm2
	movsd	%xmm2, 88(%rsp)
	movsd	96(%rsp), %xmm2
	divsd	%xmm0, %xmm2
	movsd	%xmm2, 96(%rsp)
	movsd	80(%rsp), %xmm2
	mulsd	96(%rsp), %xmm2
	addsd	%xmm1, %xmm2
	divsd	%xmm0, %xmm2
	movsd	%xmm2, (%rbx,%r13,8)
	movsd	96(%rsp), %xmm1
	mulsd	56(%rsp), %xmm1
	subsd	88(%rsp), %xmm1
	divsd	%xmm0, %xmm1
	movsd	%xmm1, (%rbx,%r12,8)
.LBB149_41:	# bb166.preheader
	cmpl	$2, 68(%rsp)
	jl	.LBB149_18	# bb99.thread
.LBB149_42:	# bb.nph246
	movl	176(%rsp), %eax
	addl	%eax, %r15d
	addl	%r15d, %r15d
	movl	$1, %edx
	movl	68(%rsp), %ecx
	subl	%ecx, %edx
	imull	%eax, %edx
	movl	%edx, 36(%rsp)
	movl	160(%rsp), %edx
	leal	2(,%rdx,2), %esi
	movl	%esi, 48(%rsp)
	leal	(%rdx,%rdx), %r12d
	decl	%ecx
	movl	%ecx, 68(%rsp)
	cvtsi2sd	72(%rsp), %xmm0
	movsd	%xmm0, 80(%rsp)
	leal	(%rax,%rax), %eax
	movl	%eax, 40(%rsp)
	xorl	%r13d, %r13d
	.align	16
.LBB149_43:	# bb156
	cmpl	$0, 176(%rsp)
	movl	$0, %eax
	cmovle	36(%rsp), %eax
	movslq	%r15d, %rbp
	movsd	(%rbx,%rbp,8), %xmm0
	movsd	%xmm0, 96(%rsp)
	incl	%r15d
	movslq	%r15d, %r15
	movsd	(%rbx,%r15,8), %xmm0
	movsd	%xmm0, 88(%rsp)
	leal	1(%r13), %ecx
	testl	%ecx, %ecx
	jle	.LBB149_46	# bb162
.LBB149_44:	# bb160.preheader
	leal	(%r13,%r13), %edx
	movl	176(%rsp), %esi
	leal	(%rsi,%rsi), %esi
	addl	%eax, %eax
	movl	160(%rsp), %edi
	leal	(%rdi,%rdi), %edi
	xorl	%r8d, %r8d
	.align	16
.LBB149_45:	# bb160
	leal	3(%rdx), %r9d
	movslq	%r9d, %r9
	movsd	80(%rsp), %xmm0
	mulsd	(%r14,%r9,8), %xmm0
	movslq	%eax, %r9
	movsd	(%rbx,%r9,8), %xmm1
	movapd	%xmm0, %xmm2
	mulsd	%xmm1, %xmm2
	leal	1(%rax), %r9d
	movslq	%r9d, %r9
	movsd	(%rbx,%r9,8), %xmm3
	leal	2(%rdx), %r9d
	movslq	%r9d, %r9
	movsd	(%r14,%r9,8), %xmm4
	movapd	%xmm4, %xmm5
	mulsd	%xmm3, %xmm5
	addsd	%xmm2, %xmm5
	movsd	88(%rsp), %xmm2
	subsd	%xmm5, %xmm2
	movsd	%xmm2, 88(%rsp)
	mulsd	%xmm3, %xmm0
	mulsd	%xmm1, %xmm4
	subsd	%xmm0, %xmm4
	movsd	96(%rsp), %xmm0
	subsd	%xmm4, %xmm0
	movsd	%xmm0, 96(%rsp)
	addl	%esi, %eax
	addl	%edi, %edx
	incl	%r8d
	cmpl	%ecx, %r8d
	jne	.LBB149_45	# bb160
.LBB149_46:	# bb162
	cmpl	$131, 64(%rsp)
	je	.LBB149_70	# bb163
.LBB149_47:	# bb164
	movsd	96(%rsp), %xmm0
	movsd	%xmm0, (%rbx,%rbp,8)
	movsd	88(%rsp), %xmm0
	movsd	%xmm0, 56(%rsp)
.LBB149_48:	# bb165
	movsd	56(%rsp), %xmm0
	movsd	%xmm0, (%rbx,%r15,8)
	movl	%ebp, %r15d
	addl	40(%rsp), %r15d
	addl	48(%rsp), %r12d
	incl	%r13d
	cmpl	68(%rsp), %r13d
	je	.LBB149_18	# bb99.thread
	jmp	.LBB149_43	# bb156
.LBB149_49:	# bb167
	cmpl	$111, %eax
	sete	%cl
	andb	%cl, %sil
	cmpl	$112, %eax
	sete	%al
	testb	%al, %r10b
	jne	.LBB149_51	# bb183
.LBB149_50:	# bb167
	notb	%sil
	testb	$1, %sil
	jne	.LBB149_64	# bb202
.LBB149_51:	# bb183
	cmpl	$0, 176(%rsp)
	jg	.LBB149_71	# bb183.bb186_crit_edge
.LBB149_52:	# bb184
	movl	$1, %r15d
	subl	68(%rsp), %r15d
	imull	176(%rsp), %r15d
.LBB149_53:	# bb186
	movl	68(%rsp), %eax
	leal	-1(%rax), %eax
	movl	%eax, %ecx
	imull	176(%rsp), %ecx
	addl	%r15d, %ecx
	cmpl	$131, 64(%rsp)
	jne	.LBB149_55	# bb188
.LBB149_54:	# bb187
	movl	160(%rsp), %edx
	leal	2(,%rdx,2), %edx
	imull	%eax, %edx
	movslq	%edx, %rax
	orl	$1, %edx
	movslq	%edx, %rdx
	cvtsi2sd	72(%rsp), %xmm0
	mulsd	(%r14,%rdx,8), %xmm0
	movsd	%xmm0, 88(%rsp)
	leal	1(,%rcx,2), %edx
	movslq	%edx, %r12
	movsd	(%rbx,%r12,8), %xmm0
	movsd	%xmm0, 56(%rsp)
	addl	%ecx, %ecx
	movslq	%ecx, %r13
	movsd	(%rbx,%r13,8), %xmm0
	movsd	%xmm0, 80(%rsp)
	movsd	(%r14,%rax,8), %xmm0
	movsd	%xmm0, 96(%rsp)
	movsd	88(%rsp), %xmm1
	call	_ZL6xhypotdd204
	movsd	96(%rsp), %xmm1
	divsd	%xmm0, %xmm1
	movsd	%xmm1, 96(%rsp)
	movsd	88(%rsp), %xmm1
	divsd	%xmm0, %xmm1
	movsd	%xmm1, 88(%rsp)
	movsd	56(%rsp), %xmm1
	mulsd	88(%rsp), %xmm1
	movsd	88(%rsp), %xmm2
	mulsd	80(%rsp), %xmm2
	movsd	%xmm2, 88(%rsp)
	movsd	80(%rsp), %xmm2
	mulsd	96(%rsp), %xmm2
	addsd	%xmm1, %xmm2
	divsd	%xmm0, %xmm2
	movsd	%xmm2, (%rbx,%r13,8)
	movsd	96(%rsp), %xmm1
	mulsd	56(%rsp), %xmm1
	subsd	88(%rsp), %xmm1
	divsd	%xmm0, %xmm1
	movsd	%xmm1, (%rbx,%r12,8)
.LBB149_55:	# bb188
	movl	160(%rsp), %eax
	leal	2(,%rax,2), %ecx
	leal	(%rax,%rax), %edx
	movl	$4294967294, %esi
	subl	%edx, %esi
	movl	%esi, 4(%rsp)
	movl	68(%rsp), %edx
	leal	-2(%rdx), %esi
	movl	176(%rsp), %edi
	movl	%edi, %r8d
	imull	%esi, %r8d
	imull	%esi, %ecx
	movl	%ecx, 24(%rsp)
	orl	$1, %ecx
	movl	%ecx, 36(%rsp)
	leal	-1(%rdx), %ecx
	imull	%ecx, %eax
	addl	%edx, %eax
	leal	-4(,%rax,2), %eax
	movl	%eax, 32(%rsp)
	imull	%edi, %ecx
	addl	%r15d, %ecx
	addl	%ecx, %ecx
	movl	%ecx, 12(%rsp)
	addl	%r15d, %r8d
	leal	1(,%r8,2), %eax
	movl	%eax, 28(%rsp)
	addl	%r8d, %r8d
	movl	%r8d, 16(%rsp)
	cvtsi2sd	72(%rsp), %xmm0
	movsd	%xmm0, 80(%rsp)
	leal	(%rdi,%rdi), %eax
	movl	%eax, 20(%rsp)
	movl	$1, %r15d
	xorl	%r12d, %r12d
	movl	%r12d, %r13d
	movl	%edx, 56(%rsp)
	jmp	.LBB149_62	# bb196
.LBB149_56:	# bb189
	movl	16(%rsp), %ecx
	leal	(%rcx,%r13), %ecx
	movl	28(%rsp), %edx
	leal	(%rdx,%r13), %edx
	cmpl	68(%rsp), %eax
	movslq	%edx, %rax
	movq	%rax, 40(%rsp)
	movsd	(%rbx,%rax,8), %xmm0
	movsd	%xmm0, 96(%rsp)
	movslq	%ecx, %rbp
	movsd	(%rbx,%rbp,8), %xmm0
	movsd	%xmm0, 88(%rsp)
	jge	.LBB149_59	# bb192
.LBB149_57:	# bb.nph
	movl	32(%rsp), %eax
	leal	(%rax,%r12), %eax
	movl	12(%rsp), %ecx
	leal	(%rcx,%r13), %ecx
	movl	176(%rsp), %edx
	leal	(%rdx,%rdx), %edx
	movl	160(%rsp), %esi
	leal	(%rsi,%rsi), %esi
	xorl	%edi, %edi
	.align	16
.LBB149_58:	# bb190
	movslq	%eax, %r8
	leal	1(%rax), %r9d
	movslq	%r9d, %r9
	movsd	80(%rsp), %xmm0
	mulsd	(%r14,%r9,8), %xmm0
	movslq	%ecx, %r9
	movsd	(%rbx,%r9,8), %xmm1
	movapd	%xmm0, %xmm2
	mulsd	%xmm1, %xmm2
	movsd	(%r14,%r8,8), %xmm3
	leal	1(%rcx), %r8d
	movslq	%r8d, %r8
	movsd	(%rbx,%r8,8), %xmm4
	movapd	%xmm3, %xmm5
	mulsd	%xmm4, %xmm5
	addsd	%xmm2, %xmm5
	movsd	96(%rsp), %xmm2
	subsd	%xmm5, %xmm2
	movsd	%xmm2, 96(%rsp)
	mulsd	%xmm1, %xmm3
	mulsd	%xmm4, %xmm0
	subsd	%xmm0, %xmm3
	movsd	88(%rsp), %xmm0
	subsd	%xmm3, %xmm0
	movsd	%xmm0, 88(%rsp)
	addl	%edx, %ecx
	addl	%esi, %eax
	incl	%edi
	cmpl	%r15d, %edi
	jne	.LBB149_58	# bb190
.LBB149_59:	# bb192
	cmpl	$131, 64(%rsp)
	je	.LBB149_72	# bb193
.LBB149_60:	# bb194
	movsd	88(%rsp), %xmm0
	movsd	%xmm0, (%rbx,%rbp,8)
	movsd	96(%rsp), %xmm0
	movsd	%xmm0, 48(%rsp)
.LBB149_61:	# bb196.backedge
	movsd	48(%rsp), %xmm0
	movq	40(%rsp), %rax
	movsd	%xmm0, (%rbx,%rax,8)
	addl	4(%rsp), %r12d
	subl	20(%rsp), %r13d
	decl	56(%rsp)
	incl	%r15d
.LBB149_62:	# bb196
	movl	56(%rsp), %eax
	leal	-1(%rax), %eax
	testl	%eax, %eax
	jle	.LBB149_18	# bb99.thread
.LBB149_63:	# bb197
	cmpl	$1, 56(%rsp)
	jne	.LBB149_56	# bb189
	jmp	.LBB149_18	# bb99.thread
.LBB149_64:	# bb202
	xorl	%edi, %edi
	leaq	.str202, %rsi
	leaq	.str1203, %rdx
	xorb	%al, %al
	call	cblas_xerbla
	jmp	.LBB149_18	# bb99.thread
.LBB149_65:	# bb82.bb85_crit_edge
	xorl	%r15d, %r15d
	jmp	.LBB149_7	# bb85
.LBB149_66:	# bb92
	movl	32(%rsp), %eax
	leal	(%rax,%r12), %eax
	movslq	%eax, %rax
	movsd	80(%rsp), %xmm0
	mulsd	(%r14,%rax,8), %xmm0
	movsd	%xmm0, 72(%rsp)
	movl	24(%rsp), %eax
	leal	(%rax,%r12), %eax
	movslq	%eax, %rax
	movsd	(%r14,%rax,8), %xmm0
	movsd	%xmm0, 48(%rsp)
	movsd	72(%rsp), %xmm1
	call	_ZL6xhypotdd204
	movsd	72(%rsp), %xmm1
	divsd	%xmm0, %xmm1
	movsd	%xmm1, 72(%rsp)
	movsd	96(%rsp), %xmm1
	movapd	%xmm1, %xmm2
	mulsd	72(%rsp), %xmm2
	movsd	48(%rsp), %xmm3
	divsd	%xmm0, %xmm3
	movsd	%xmm3, 48(%rsp)
	movsd	88(%rsp), %xmm3
	movapd	%xmm3, %xmm4
	mulsd	48(%rsp), %xmm4
	addsd	%xmm2, %xmm4
	divsd	%xmm0, %xmm4
	movsd	%xmm4, (%rbx,%rbp,8)
	movsd	72(%rsp), %xmm2
	mulsd	%xmm3, %xmm2
	movsd	%xmm2, 72(%rsp)
	movsd	48(%rsp), %xmm2
	mulsd	%xmm1, %xmm2
	subsd	72(%rsp), %xmm2
	divsd	%xmm0, %xmm2
	movsd	%xmm2, 48(%rsp)
	jmp	.LBB149_15	# bb95.backedge
.LBB149_67:	# bb117.bb120_crit_edge
	xorl	%r15d, %r15d
	jmp	.LBB149_25	# bb120
.LBB149_68:	# bb130
	leal	3(%r13), %eax
	movslq	%eax, %rax
	movsd	80(%rsp), %xmm0
	mulsd	(%r14,%rax,8), %xmm0
	movsd	%xmm0, 72(%rsp)
	leal	2(%r13), %eax
	movslq	%eax, %rax
	movsd	(%r14,%rax,8), %xmm0
	movsd	%xmm0, 48(%rsp)
	movsd	72(%rsp), %xmm1
	call	_ZL6xhypotdd204
	movsd	72(%rsp), %xmm1
	divsd	%xmm0, %xmm1
	movsd	%xmm1, 72(%rsp)
	movsd	88(%rsp), %xmm1
	movapd	%xmm1, %xmm2
	mulsd	72(%rsp), %xmm2
	movsd	48(%rsp), %xmm3
	divsd	%xmm0, %xmm3
	movsd	%xmm3, 48(%rsp)
	movsd	96(%rsp), %xmm3
	movapd	%xmm3, %xmm4
	mulsd	48(%rsp), %xmm4
	addsd	%xmm2, %xmm4
	divsd	%xmm0, %xmm4
	movsd	%xmm4, (%rbx,%rbp,8)
	movsd	72(%rsp), %xmm2
	mulsd	%xmm3, %xmm2
	movsd	%xmm2, 72(%rsp)
	movsd	48(%rsp), %xmm2
	mulsd	%xmm1, %xmm2
	subsd	72(%rsp), %xmm2
	divsd	%xmm0, %xmm2
	movsd	%xmm2, 48(%rsp)
	jmp	.LBB149_34	# bb132
.LBB149_69:	# bb150.bb153_crit_edge
	xorl	%r15d, %r15d
	jmp	.LBB149_39	# bb153
.LBB149_70:	# bb163
	leal	3(%r12), %eax
	movslq	%eax, %rax
	movsd	80(%rsp), %xmm0
	mulsd	(%r14,%rax,8), %xmm0
	movsd	%xmm0, 72(%rsp)
	leal	2(%r12), %eax
	movslq	%eax, %rax
	movsd	(%r14,%rax,8), %xmm0
	movsd	%xmm0, 56(%rsp)
	movsd	72(%rsp), %xmm1
	call	_ZL6xhypotdd204
	movsd	72(%rsp), %xmm1
	divsd	%xmm0, %xmm1
	movsd	%xmm1, 72(%rsp)
	movsd	88(%rsp), %xmm1
	movapd	%xmm1, %xmm2
	mulsd	72(%rsp), %xmm2
	movsd	56(%rsp), %xmm3
	divsd	%xmm0, %xmm3
	movsd	%xmm3, 56(%rsp)
	movsd	96(%rsp), %xmm3
	movapd	%xmm3, %xmm4
	mulsd	56(%rsp), %xmm4
	addsd	%xmm2, %xmm4
	divsd	%xmm0, %xmm4
	movsd	%xmm4, (%rbx,%rbp,8)
	movsd	72(%rsp), %xmm2
	mulsd	%xmm3, %xmm2
	movsd	%xmm2, 72(%rsp)
	movsd	56(%rsp), %xmm2
	mulsd	%xmm1, %xmm2
	subsd	72(%rsp), %xmm2
	divsd	%xmm0, %xmm2
	movsd	%xmm2, 56(%rsp)
	jmp	.LBB149_48	# bb165
.LBB149_71:	# bb183.bb186_crit_edge
	xorl	%r15d, %r15d
	jmp	.LBB149_53	# bb186
.LBB149_72:	# bb193
	movl	36(%rsp), %eax
	leal	(%rax,%r12), %eax
	movslq	%eax, %rax
	movsd	80(%rsp), %xmm0
	mulsd	(%r14,%rax,8), %xmm0
	movsd	%xmm0, 72(%rsp)
	movl	24(%rsp), %eax
	leal	(%rax,%r12), %eax
	movslq	%eax, %rax
	movsd	(%r14,%rax,8), %xmm0
	movsd	%xmm0, 48(%rsp)
	movsd	72(%rsp), %xmm1
	call	_ZL6xhypotdd204
	movsd	72(%rsp), %xmm1
	divsd	%xmm0, %xmm1
	movsd	%xmm1, 72(%rsp)
	movsd	96(%rsp), %xmm1
	movapd	%xmm1, %xmm2
	mulsd	72(%rsp), %xmm2
	movsd	48(%rsp), %xmm3
	divsd	%xmm0, %xmm3
	movsd	%xmm3, 48(%rsp)
	movsd	88(%rsp), %xmm3
	movapd	%xmm3, %xmm4
	mulsd	48(%rsp), %xmm4
	addsd	%xmm2, %xmm4
	divsd	%xmm0, %xmm4
	movsd	%xmm4, (%rbx,%rbp,8)
	movsd	72(%rsp), %xmm2
	mulsd	%xmm3, %xmm2
	movsd	%xmm2, 72(%rsp)
	movsd	48(%rsp), %xmm2
	mulsd	%xmm1, %xmm2
	subsd	72(%rsp), %xmm2
	divsd	%xmm0, %xmm2
	movsd	%xmm2, 48(%rsp)
	jmp	.LBB149_61	# bb196.backedge
	.size	cblas_ztrsv, .-cblas_ztrsv
.Leh_func_end104:


	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI150_0:					
	.quad	9223372036854775807	# double value: nan
	.quad	9223372036854775807	# double value: nan
	.section	.rodata.cst8,"aM",@progbits,8
	.align	16
.LCPI150_1:					
	.quad	4607182418800017408	# double value: 1.000000e+00
	.text
	.align	16
	.type	_ZL6xhypotdd204,@function
_ZL6xhypotdd204:
	movsd	.LCPI150_0(%rip), %xmm2
	andpd	%xmm2, %xmm0
	andpd	%xmm2, %xmm1
	movapd	%xmm1, %xmm2
	maxsd	%xmm0, %xmm2
	minsd	%xmm1, %xmm0
	pxor	%xmm1, %xmm1
	ucomisd	%xmm1, %xmm0
	setnp	%al
	sete	%cl
	testb	%al, %cl
	jne	.LBB150_2	# bb5
.LBB150_1:	# bb4
	divsd	%xmm2, %xmm0
	mulsd	%xmm0, %xmm0
	addsd	.LCPI150_1(%rip), %xmm0
	sqrtsd	%xmm0, %xmm0
	mulsd	%xmm2, %xmm0
	ret
.LBB150_2:	# bb5
	movapd	%xmm2, %xmm0
	ret
	.size	_ZL6xhypotdd204, .-_ZL6xhypotdd204
	.type	.str,@object
	.section	.rodata.str1.1,"aMS",@progbits,1
.str:				# .str
	.size	.str, 16
	.asciz	"source_gbmv_c.h"
	.type	.str1,@object
	.align	16
.str1:				# .str1
	.size	.str1, 23
	.asciz	"unrecognized operation"
	.type	.str2,@object
.str2:				# .str2
	.size	.str2, 16
	.asciz	"source_gemm_c.h"
	.type	.str13,@object
	.align	16
.str13:				# .str13
	.size	.str13, 23
	.asciz	"unrecognized operation"
	.type	.str4,@object
.str4:				# .str4
	.size	.str4, 16
	.asciz	"source_gemv_c.h"
	.type	.str15,@object
	.align	16
.str15:				# .str15
	.size	.str15, 23
	.asciz	"unrecognized operation"
	.type	.str6,@object
.str6:				# .str6
	.size	.str6, 14
	.asciz	"source_gerc.h"
	.type	.str17,@object
	.align	16
.str17:				# .str17
	.size	.str17, 23
	.asciz	"unrecognized operation"
	.type	.str8,@object
.str8:				# .str8
	.size	.str8, 14
	.asciz	"source_geru.h"
	.type	.str19,@object
	.align	16
.str19:				# .str19
	.size	.str19, 23
	.asciz	"unrecognized operation"
	.type	.str10,@object
.str10:				# .str10
	.size	.str10, 14
	.asciz	"source_hbmv.h"
	.type	.str111,@object
	.align	16
.str111:				# .str111
	.size	.str111, 23
	.asciz	"unrecognized operation"
	.type	.str12,@object
.str12:				# .str12
	.size	.str12, 14
	.asciz	"source_hemm.h"
	.type	.str113,@object
	.align	16
.str113:				# .str113
	.size	.str113, 23
	.asciz	"unrecognized operation"
	.type	.str14,@object
.str14:				# .str14
	.size	.str14, 14
	.asciz	"source_hemv.h"
	.type	.str115,@object
	.align	16
.str115:				# .str115
	.size	.str115, 23
	.asciz	"unrecognized operation"
	.type	.str16,@object
.str16:				# .str16
	.size	.str16, 14
	.asciz	"source_her2.h"
	.type	.str117,@object
	.align	16
.str117:				# .str117
	.size	.str117, 23
	.asciz	"unrecognized operation"
	.type	.str18,@object
.str18:				# .str18
	.size	.str18, 15
	.asciz	"source_her2k.h"
	.type	.str119,@object
	.align	16
.str119:				# .str119
	.size	.str119, 23
	.asciz	"unrecognized operation"
	.type	.str20,@object
.str20:				# .str20
	.size	.str20, 13
	.asciz	"source_her.h"
	.type	.str121,@object
	.align	16
.str121:				# .str121
	.size	.str121, 23
	.asciz	"unrecognized operation"
	.type	.str22,@object
.str22:				# .str22
	.size	.str22, 14
	.asciz	"source_herk.h"
	.type	.str123,@object
	.align	16
.str123:				# .str123
	.size	.str123, 23
	.asciz	"unrecognized operation"
	.type	.str24,@object
.str24:				# .str24
	.size	.str24, 14
	.asciz	"source_hpmv.h"
	.type	.str125,@object
	.align	16
.str125:				# .str125
	.size	.str125, 23
	.asciz	"unrecognized operation"
	.type	.str26,@object
.str26:				# .str26
	.size	.str26, 14
	.asciz	"source_hpr2.h"
	.type	.str127,@object
	.align	16
.str127:				# .str127
	.size	.str127, 23
	.asciz	"unrecognized operation"
	.type	.str28,@object
.str28:				# .str28
	.size	.str28, 13
	.asciz	"source_hpr.h"
	.type	.str129,@object
	.align	16
.str129:				# .str129
	.size	.str129, 23
	.asciz	"unrecognized operation"
	.type	.str30,@object
.str30:				# .str30
	.size	.str30, 16
	.asciz	"source_symm_c.h"
	.type	.str131,@object
	.align	16
.str131:				# .str131
	.size	.str131, 23
	.asciz	"unrecognized operation"
	.type	.str32,@object
	.align	16
.str32:				# .str32
	.size	.str32, 17
	.asciz	"source_syr2k_c.h"
	.type	.str133,@object
	.align	16
.str133:				# .str133
	.size	.str133, 23
	.asciz	"unrecognized operation"
	.type	.str34,@object
.str34:				# .str34
	.size	.str34, 16
	.asciz	"source_syrk_c.h"
	.type	.str135,@object
	.align	16
.str135:				# .str135
	.size	.str135, 23
	.asciz	"unrecognized operation"
	.type	.str36,@object
.str36:				# .str36
	.size	.str36, 16
	.asciz	"source_tbmv_c.h"
	.type	.str137,@object
	.align	16
.str137:				# .str137
	.size	.str137, 23
	.asciz	"unrecognized operation"
	.type	.str38,@object
.str38:				# .str38
	.size	.str38, 16
	.asciz	"source_tbsv_c.h"
	.type	.str139,@object
	.align	16
.str139:				# .str139
	.size	.str139, 23
	.asciz	"unrecognized operation"
	.type	.str40,@object
.str40:				# .str40
	.size	.str40, 16
	.asciz	"source_tpmv_c.h"
	.type	.str141,@object
	.align	16
.str141:				# .str141
	.size	.str141, 23
	.asciz	"unrecognized operation"
	.type	.str42,@object
.str42:				# .str42
	.size	.str42, 16
	.asciz	"source_tpsv_c.h"
	.type	.str143,@object
	.align	16
.str143:				# .str143
	.size	.str143, 23
	.asciz	"unrecognized operation"
	.type	.str45,@object
.str45:				# .str45
	.size	.str45, 16
	.asciz	"source_trmm_c.h"
	.type	.str146,@object
	.align	16
.str146:				# .str146
	.size	.str146, 23
	.asciz	"unrecognized operation"
	.type	.str47,@object
.str47:				# .str47
	.size	.str47, 16
	.asciz	"source_trmv_c.h"
	.type	.str148,@object
	.align	16
.str148:				# .str148
	.size	.str148, 23
	.asciz	"unrecognized operation"
	.type	.str49,@object
.str49:				# .str49
	.size	.str49, 16
	.asciz	"source_trsm_c.h"
	.type	.str150,@object
	.align	16
.str150:				# .str150
	.size	.str150, 23
	.asciz	"unrecognized operation"
	.type	.str52,@object
.str52:				# .str52
	.size	.str52, 16
	.asciz	"source_trsv_c.h"
	.type	.str153,@object
	.align	16
.str153:				# .str153
	.size	.str153, 23
	.asciz	"unrecognized operation"
	.type	.str55,@object
.str55:				# .str55
	.size	.str55, 16
	.asciz	"source_gbmv_r.h"
	.type	.str156,@object
	.align	16
.str156:				# .str156
	.size	.str156, 23
	.asciz	"unrecognized operation"
	.type	.str57,@object
.str57:				# .str57
	.size	.str57, 16
	.asciz	"source_gemm_r.h"
	.type	.str158,@object
	.align	16
.str158:				# .str158
	.size	.str158, 23
	.asciz	"unrecognized operation"
	.type	.str59,@object
.str59:				# .str59
	.size	.str59, 16
	.asciz	"source_gemv_r.h"
	.type	.str160,@object
	.align	16
.str160:				# .str160
	.size	.str160, 23
	.asciz	"unrecognized operation"
	.type	.str61,@object
.str61:				# .str61
	.size	.str61, 14
	.asciz	"source_rotm.h"
	.type	.str162,@object
	.align	16
.str162:				# .str162
	.size	.str162, 27
	.asciz	"unrecognized value of P[0]"
	.type	.str63,@object
.str63:				# .str63
	.size	.str63, 14
	.asciz	"source_sbmv.h"
	.type	.str164,@object
	.align	16
.str164:				# .str164
	.size	.str164, 23
	.asciz	"unrecognized operation"
	.type	.str65,@object
.str65:				# .str65
	.size	.str65, 14
	.asciz	"source_spmv.h"
	.type	.str166,@object
	.align	16
.str166:				# .str166
	.size	.str166, 23
	.asciz	"unrecognized operation"
	.type	.str67,@object
.str67:				# .str67
	.size	.str67, 14
	.asciz	"source_spr2.h"
	.type	.str168,@object
	.align	16
.str168:				# .str168
	.size	.str168, 23
	.asciz	"unrecognized operation"
	.type	.str69,@object
.str69:				# .str69
	.size	.str69, 13
	.asciz	"source_spr.h"
	.type	.str170,@object
	.align	16
.str170:				# .str170
	.size	.str170, 23
	.asciz	"unrecognized operation"
	.type	.str71,@object
.str71:				# .str71
	.size	.str71, 16
	.asciz	"source_symm_r.h"
	.type	.str172,@object
	.align	16
.str172:				# .str172
	.size	.str172, 23
	.asciz	"unrecognized operation"
	.type	.str73,@object
.str73:				# .str73
	.size	.str73, 14
	.asciz	"source_symv.h"
	.type	.str174,@object
	.align	16
.str174:				# .str174
	.size	.str174, 23
	.asciz	"unrecognized operation"
	.type	.str75,@object
.str75:				# .str75
	.size	.str75, 14
	.asciz	"source_syr2.h"
	.type	.str176,@object
	.align	16
.str176:				# .str176
	.size	.str176, 23
	.asciz	"unrecognized operation"
	.type	.str77,@object
	.align	16
.str77:				# .str77
	.size	.str77, 17
	.asciz	"source_syr2k_r.h"
	.type	.str178,@object
	.align	16
.str178:				# .str178
	.size	.str178, 23
	.asciz	"unrecognized operation"
	.type	.str79,@object
.str79:				# .str79
	.size	.str79, 13
	.asciz	"source_syr.h"
	.type	.str180,@object
	.align	16
.str180:				# .str180
	.size	.str180, 23
	.asciz	"unrecognized operation"
	.type	.str81,@object
.str81:				# .str81
	.size	.str81, 16
	.asciz	"source_syrk_r.h"
	.type	.str182,@object
	.align	16
.str182:				# .str182
	.size	.str182, 23
	.asciz	"unrecognized operation"
	.type	.str83,@object
.str83:				# .str83
	.size	.str83, 16
	.asciz	"source_tbsv_r.h"
	.type	.str184,@object
	.align	16
.str184:				# .str184
	.size	.str184, 23
	.asciz	"unrecognized operation"
	.type	.str85,@object
.str85:				# .str85
	.size	.str85, 16
	.asciz	"source_tpmv_r.h"
	.type	.str186,@object
	.align	16
.str186:				# .str186
	.size	.str186, 23
	.asciz	"unrecognized operation"
	.type	.str87,@object
.str87:				# .str87
	.size	.str87, 16
	.asciz	"source_tpsv_r.h"
	.type	.str188,@object
	.align	16
.str188:				# .str188
	.size	.str188, 23
	.asciz	"unrecognized operation"
	.type	.str89,@object
.str89:				# .str89
	.size	.str89, 16
	.asciz	"source_trmm_r.h"
	.type	.str190,@object
	.align	16
.str190:				# .str190
	.size	.str190, 23
	.asciz	"unrecognized operation"
	.type	.str91,@object
.str91:				# .str91
	.size	.str91, 16
	.asciz	"source_trmv_r.h"
	.type	.str192,@object
	.align	16
.str192:				# .str192
	.size	.str192, 23
	.asciz	"unrecognized operation"
	.type	.str93,@object
.str93:				# .str93
	.size	.str93, 16
	.asciz	"source_trsm_r.h"
	.type	.str194,@object
	.align	16
.str194:				# .str194
	.size	.str194, 23
	.asciz	"unrecognized operation"
	.type	.str95,@object
.str95:				# .str95
	.size	.str95, 16
	.asciz	"source_trsv_r.h"
	.type	.str196,@object
	.align	16
.str196:				# .str196
	.size	.str196, 23
	.asciz	"unrecognized operation"
	.type	.str97,@object
.str97:				# .str97
	.size	.str97, 16
	.asciz	"source_gbmv_r.h"
	.type	.str198,@object
	.align	16
.str198:				# .str198
	.size	.str198, 23
	.asciz	"unrecognized operation"
	.type	.str99,@object
.str99:				# .str99
	.size	.str99, 16
	.asciz	"source_gemm_r.h"
	.type	.str1100,@object
	.align	16
.str1100:				# .str1100
	.size	.str1100, 23
	.asciz	"unrecognized operation"
	.type	.str101,@object
.str101:				# .str101
	.size	.str101, 16
	.asciz	"source_gemv_r.h"
	.type	.str1102,@object
	.align	16
.str1102:				# .str1102
	.size	.str1102, 23
	.asciz	"unrecognized operation"
	.type	.str103,@object
.str103:				# .str103
	.size	.str103, 13
	.asciz	"source_ger.h"
	.type	.str1104,@object
	.align	16
.str1104:				# .str1104
	.size	.str1104, 23
	.asciz	"unrecognized operation"
	.type	.str105,@object
.str105:				# .str105
	.size	.str105, 14
	.asciz	"source_rotm.h"
	.type	.str1106,@object
	.align	16
.str1106:				# .str1106
	.size	.str1106, 27
	.asciz	"unrecognized value of P[0]"
	.type	.str107,@object
.str107:				# .str107
	.size	.str107, 14
	.asciz	"source_sbmv.h"
	.type	.str1108,@object
	.align	16
.str1108:				# .str1108
	.size	.str1108, 23
	.asciz	"unrecognized operation"
	.type	.str109,@object
.str109:				# .str109
	.size	.str109, 14
	.asciz	"source_spmv.h"
	.type	.str1110,@object
	.align	16
.str1110:				# .str1110
	.size	.str1110, 23
	.asciz	"unrecognized operation"
	.type	.str112,@object
.str112:				# .str112
	.size	.str112, 14
	.asciz	"source_spr2.h"
	.type	.str1113,@object
	.align	16
.str1113:				# .str1113
	.size	.str1113, 23
	.asciz	"unrecognized operation"
	.type	.str114,@object
.str114:				# .str114
	.size	.str114, 13
	.asciz	"source_spr.h"
	.type	.str1115,@object
	.align	16
.str1115:				# .str1115
	.size	.str1115, 23
	.asciz	"unrecognized operation"
	.type	.str116,@object
.str116:				# .str116
	.size	.str116, 16
	.asciz	"source_symm_r.h"
	.type	.str1117,@object
	.align	16
.str1117:				# .str1117
	.size	.str1117, 23
	.asciz	"unrecognized operation"
	.type	.str118,@object
.str118:				# .str118
	.size	.str118, 14
	.asciz	"source_symv.h"
	.type	.str1119,@object
	.align	16
.str1119:				# .str1119
	.size	.str1119, 23
	.asciz	"unrecognized operation"
	.type	.str120,@object
.str120:				# .str120
	.size	.str120, 14
	.asciz	"source_syr2.h"
	.type	.str1121,@object
	.align	16
.str1121:				# .str1121
	.size	.str1121, 23
	.asciz	"unrecognized operation"
	.type	.str122,@object
	.align	16
.str122:				# .str122
	.size	.str122, 17
	.asciz	"source_syr2k_r.h"
	.type	.str1123,@object
	.align	16
.str1123:				# .str1123
	.size	.str1123, 23
	.asciz	"unrecognized operation"
	.type	.str124,@object
.str124:				# .str124
	.size	.str124, 13
	.asciz	"source_syr.h"
	.type	.str1125,@object
	.align	16
.str1125:				# .str1125
	.size	.str1125, 23
	.asciz	"unrecognized operation"
	.type	.str126,@object
.str126:				# .str126
	.size	.str126, 16
	.asciz	"source_syrk_r.h"
	.type	.str1127,@object
	.align	16
.str1127:				# .str1127
	.size	.str1127, 23
	.asciz	"unrecognized operation"
	.type	.str128,@object
.str128:				# .str128
	.size	.str128, 16
	.asciz	"source_tbsv_r.h"
	.type	.str1129,@object
	.align	16
.str1129:				# .str1129
	.size	.str1129, 23
	.asciz	"unrecognized operation"
	.type	.str130,@object
.str130:				# .str130
	.size	.str130, 16
	.asciz	"source_tpmv_r.h"
	.type	.str1131,@object
	.align	16
.str1131:				# .str1131
	.size	.str1131, 23
	.asciz	"unrecognized operation"
	.type	.str132,@object
.str132:				# .str132
	.size	.str132, 16
	.asciz	"source_tpsv_r.h"
	.type	.str1133,@object
	.align	16
.str1133:				# .str1133
	.size	.str1133, 23
	.asciz	"unrecognized operation"
	.type	.str134,@object
.str134:				# .str134
	.size	.str134, 16
	.asciz	"source_trmm_r.h"
	.type	.str1135,@object
	.align	16
.str1135:				# .str1135
	.size	.str1135, 23
	.asciz	"unrecognized operation"
	.type	.str136,@object
.str136:				# .str136
	.size	.str136, 16
	.asciz	"source_trmv_r.h"
	.type	.str1137,@object
	.align	16
.str1137:				# .str1137
	.size	.str1137, 23
	.asciz	"unrecognized operation"
	.type	.str138,@object
.str138:				# .str138
	.size	.str138, 16
	.asciz	"source_trsm_r.h"
	.type	.str1139,@object
	.align	16
.str1139:				# .str1139
	.size	.str1139, 23
	.asciz	"unrecognized operation"
	.type	.str140,@object
.str140:				# .str140
	.size	.str140, 16
	.asciz	"source_trsv_r.h"
	.type	.str1141,@object
	.align	16
.str1141:				# .str1141
	.size	.str1141, 23
	.asciz	"unrecognized operation"
	.type	.str142,@object
	.align	16
.str142:				# .str142
	.size	.str142, 42
	.asciz	"Parameter %d to routine %s was incorrect\n"
	.type	.str144,@object
.str144:				# .str144
	.size	.str144, 16
	.asciz	"source_gbmv_c.h"
	.type	.str1145,@object
	.align	16
.str1145:				# .str1145
	.size	.str1145, 23
	.asciz	"unrecognized operation"
	.type	.str147,@object
.str147:				# .str147
	.size	.str147, 16
	.asciz	"source_gemm_c.h"
	.type	.str1148,@object
	.align	16
.str1148:				# .str1148
	.size	.str1148, 23
	.asciz	"unrecognized operation"
	.type	.str149,@object
.str149:				# .str149
	.size	.str149, 16
	.asciz	"source_gemv_c.h"
	.type	.str1150,@object
	.align	16
.str1150:				# .str1150
	.size	.str1150, 23
	.asciz	"unrecognized operation"
	.type	.str151,@object
.str151:				# .str151
	.size	.str151, 14
	.asciz	"source_gerc.h"
	.type	.str1152,@object
	.align	16
.str1152:				# .str1152
	.size	.str1152, 23
	.asciz	"unrecognized operation"
	.type	.str154,@object
.str154:				# .str154
	.size	.str154, 14
	.asciz	"source_geru.h"
	.type	.str1155,@object
	.align	16
.str1155:				# .str1155
	.size	.str1155, 23
	.asciz	"unrecognized operation"
	.type	.str157,@object
.str157:				# .str157
	.size	.str157, 14
	.asciz	"source_hbmv.h"
	.type	.str1158,@object
	.align	16
.str1158:				# .str1158
	.size	.str1158, 23
	.asciz	"unrecognized operation"
	.type	.str159,@object
.str159:				# .str159
	.size	.str159, 14
	.asciz	"source_hemm.h"
	.type	.str1160,@object
	.align	16
.str1160:				# .str1160
	.size	.str1160, 23
	.asciz	"unrecognized operation"
	.type	.str161,@object
.str161:				# .str161
	.size	.str161, 14
	.asciz	"source_hemv.h"
	.type	.str1162,@object
	.align	16
.str1162:				# .str1162
	.size	.str1162, 23
	.asciz	"unrecognized operation"
	.type	.str163,@object
.str163:				# .str163
	.size	.str163, 14
	.asciz	"source_her2.h"
	.type	.str1164,@object
	.align	16
.str1164:				# .str1164
	.size	.str1164, 23
	.asciz	"unrecognized operation"
	.type	.str165,@object
.str165:				# .str165
	.size	.str165, 15
	.asciz	"source_her2k.h"
	.type	.str1166,@object
	.align	16
.str1166:				# .str1166
	.size	.str1166, 23
	.asciz	"unrecognized operation"
	.type	.str167,@object
.str167:				# .str167
	.size	.str167, 13
	.asciz	"source_her.h"
	.type	.str1168,@object
	.align	16
.str1168:				# .str1168
	.size	.str1168, 23
	.asciz	"unrecognized operation"
	.type	.str169,@object
.str169:				# .str169
	.size	.str169, 14
	.asciz	"source_herk.h"
	.type	.str1170,@object
	.align	16
.str1170:				# .str1170
	.size	.str1170, 23
	.asciz	"unrecognized operation"
	.type	.str171,@object
.str171:				# .str171
	.size	.str171, 14
	.asciz	"source_hpmv.h"
	.type	.str1172,@object
	.align	16
.str1172:				# .str1172
	.size	.str1172, 23
	.asciz	"unrecognized operation"
	.type	.str173,@object
.str173:				# .str173
	.size	.str173, 14
	.asciz	"source_hpr2.h"
	.type	.str1174,@object
	.align	16
.str1174:				# .str1174
	.size	.str1174, 23
	.asciz	"unrecognized operation"
	.type	.str175,@object
.str175:				# .str175
	.size	.str175, 13
	.asciz	"source_hpr.h"
	.type	.str1176,@object
	.align	16
.str1176:				# .str1176
	.size	.str1176, 23
	.asciz	"unrecognized operation"
	.type	.str177,@object
.str177:				# .str177
	.size	.str177, 16
	.asciz	"source_symm_c.h"
	.type	.str1178,@object
	.align	16
.str1178:				# .str1178
	.size	.str1178, 23
	.asciz	"unrecognized operation"
	.type	.str179,@object
	.align	16
.str179:				# .str179
	.size	.str179, 17
	.asciz	"source_syr2k_c.h"
	.type	.str1180,@object
	.align	16
.str1180:				# .str1180
	.size	.str1180, 23
	.asciz	"unrecognized operation"
	.type	.str181,@object
.str181:				# .str181
	.size	.str181, 16
	.asciz	"source_syrk_c.h"
	.type	.str1182,@object
	.align	16
.str1182:				# .str1182
	.size	.str1182, 23
	.asciz	"unrecognized operation"
	.type	.str183,@object
.str183:				# .str183
	.size	.str183, 16
	.asciz	"source_tbmv_c.h"
	.type	.str1184,@object
	.align	16
.str1184:				# .str1184
	.size	.str1184, 23
	.asciz	"unrecognized operation"
	.type	.str185,@object
.str185:				# .str185
	.size	.str185, 16
	.asciz	"source_tbsv_c.h"
	.type	.str1186,@object
	.align	16
.str1186:				# .str1186
	.size	.str1186, 23
	.asciz	"unrecognized operation"
	.type	.str189,@object
.str189:				# .str189
	.size	.str189, 16
	.asciz	"source_tpmv_c.h"
	.type	.str1190,@object
	.align	16
.str1190:				# .str1190
	.size	.str1190, 23
	.asciz	"unrecognized operation"
	.type	.str191,@object
.str191:				# .str191
	.size	.str191, 16
	.asciz	"source_tpsv_c.h"
	.type	.str1192,@object
	.align	16
.str1192:				# .str1192
	.size	.str1192, 23
	.asciz	"unrecognized operation"
	.type	.str195,@object
.str195:				# .str195
	.size	.str195, 16
	.asciz	"source_trmm_c.h"
	.type	.str1196,@object
	.align	16
.str1196:				# .str1196
	.size	.str1196, 23
	.asciz	"unrecognized operation"
	.type	.str197,@object
.str197:				# .str197
	.size	.str197, 16
	.asciz	"source_trmv_c.h"
	.type	.str1198,@object
	.align	16
.str1198:				# .str1198
	.size	.str1198, 23
	.asciz	"unrecognized operation"
	.type	.str199,@object
.str199:				# .str199
	.size	.str199, 16
	.asciz	"source_trsm_c.h"
	.type	.str1200,@object
	.align	16
.str1200:				# .str1200
	.size	.str1200, 23
	.asciz	"unrecognized operation"
	.type	.str202,@object
.str202:				# .str202
	.size	.str202, 16
	.asciz	"source_trsv_c.h"
	.type	.str1203,@object
	.align	16
.str1203:				# .str1203
	.size	.str1203, 23
	.asciz	"unrecognized operation"
	.section	.eh_frame,"aw",@progbits
.LEH_frame0:
.Lsection_eh_frame:
.Leh_frame_common:
	.long	.Leh_frame_common_end-.Leh_frame_common_begin
.Leh_frame_common_begin:
	.long	0x0
	.byte	0x1
	.asciz	"zR"
	.uleb128	1
	.sleb128	-8
	.byte	0x10
	.uleb128	1
	.byte	0x1B
	.byte	0xC
	.uleb128	7
	.uleb128	8
	.byte	0x90
	.uleb128	1
	.align	8
.Leh_frame_common_end:

.Lcblas_cgbmv.eh:
	.long	.Leh_frame_end1-.Leh_frame_begin1
.Leh_frame_begin1:
	.long	.Leh_frame_begin1-.Leh_frame_common
	.long	.Leh_func_begin1-.
	.long	.Leh_func_end1-.Leh_func_begin1
	.uleb128	0
	.byte	0xE
	.uleb128	96
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel1-.Leh_func_begin1
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end1:
.Lcblas_cgemm.eh:
	.long	.Leh_frame_end2-.Leh_frame_begin2
.Leh_frame_begin2:
	.long	.Leh_frame_begin2-.Leh_frame_common
	.long	.Leh_func_begin2-.
	.long	.Leh_func_end2-.Leh_func_begin2
	.uleb128	0
	.byte	0xE
	.uleb128	96
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel2-.Leh_func_begin2
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end2:
.Lcblas_cgemv.eh:
	.long	.Leh_frame_end3-.Leh_frame_begin3
.Leh_frame_begin3:
	.long	.Leh_frame_begin3-.Leh_frame_common
	.long	.Leh_func_begin3-.
	.long	.Leh_func_end3-.Leh_func_begin3
	.uleb128	0
	.byte	0xE
	.uleb128	80
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel3-.Leh_func_begin3
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end3:
.Lcblas_cgerc.eh:
	.long	.Leh_frame_end4-.Leh_frame_begin4
.Leh_frame_begin4:
	.long	.Leh_frame_begin4-.Leh_frame_common
	.long	.Leh_func_begin4-.
	.long	.Leh_func_end4-.Leh_func_begin4
	.uleb128	0
	.byte	0xE
	.uleb128	64
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel4-.Leh_func_begin4
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end4:
.Lcblas_cgeru.eh:
	.long	.Leh_frame_end5-.Leh_frame_begin5
.Leh_frame_begin5:
	.long	.Leh_frame_begin5-.Leh_frame_common
	.long	.Leh_func_begin5-.
	.long	.Leh_func_end5-.Leh_func_begin5
	.uleb128	0
	.byte	0xE
	.uleb128	64
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel5-.Leh_func_begin5
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end5:
.Lcblas_chbmv.eh:
	.long	.Leh_frame_end6-.Leh_frame_begin6
.Leh_frame_begin6:
	.long	.Leh_frame_begin6-.Leh_frame_common
	.long	.Leh_func_begin6-.
	.long	.Leh_func_end6-.Leh_func_begin6
	.uleb128	0
	.byte	0xE
	.uleb128	112
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel6-.Leh_func_begin6
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end6:
.Lcblas_chemm.eh:
	.long	.Leh_frame_end7-.Leh_frame_begin7
.Leh_frame_begin7:
	.long	.Leh_frame_begin7-.Leh_frame_common
	.long	.Leh_func_begin7-.
	.long	.Leh_func_end7-.Leh_func_begin7
	.uleb128	0
	.byte	0xE
	.uleb128	144
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel7-.Leh_func_begin7
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end7:
.Lcblas_chemv.eh:
	.long	.Leh_frame_end8-.Leh_frame_begin8
.Leh_frame_begin8:
	.long	.Leh_frame_begin8-.Leh_frame_common
	.long	.Leh_func_begin8-.
	.long	.Leh_func_end8-.Leh_func_begin8
	.uleb128	0
	.byte	0xE
	.uleb128	96
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel8-.Leh_func_begin8
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end8:
.Lcblas_cher2.eh:
	.long	.Leh_frame_end9-.Leh_frame_begin9
.Leh_frame_begin9:
	.long	.Leh_frame_begin9-.Leh_frame_common
	.long	.Leh_func_begin9-.
	.long	.Leh_func_end9-.Leh_func_begin9
	.uleb128	0
	.byte	0xE
	.uleb128	112
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel9-.Leh_func_begin9
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end9:
.Lcblas_cher2k.eh:
	.long	.Leh_frame_end10-.Leh_frame_begin10
.Leh_frame_begin10:
	.long	.Leh_frame_begin10-.Leh_frame_common
	.long	.Leh_func_begin10-.
	.long	.Leh_func_end10-.Leh_func_begin10
	.uleb128	0
	.byte	0xE
	.uleb128	128
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel10-.Leh_func_begin10
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end10:
.Lcblas_cher.eh:
	.long	.Leh_frame_end11-.Leh_frame_begin11
.Leh_frame_begin11:
	.long	.Leh_frame_begin11-.Leh_frame_common
	.long	.Leh_func_begin11-.
	.long	.Leh_func_end11-.Leh_func_begin11
	.uleb128	0
	.byte	0xE
	.uleb128	80
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel11-.Leh_func_begin11
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end11:
.Lcblas_cherk.eh:
	.long	.Leh_frame_end12-.Leh_frame_begin12
.Leh_frame_begin12:
	.long	.Leh_frame_begin12-.Leh_frame_common
	.long	.Leh_func_begin12-.
	.long	.Leh_func_end12-.Leh_func_begin12
	.uleb128	0
	.byte	0xE
	.uleb128	80
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel12-.Leh_func_begin12
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end12:
.Lcblas_chpmv.eh:
	.long	.Leh_frame_end13-.Leh_frame_begin13
.Leh_frame_begin13:
	.long	.Leh_frame_begin13-.Leh_frame_common
	.long	.Leh_func_begin13-.
	.long	.Leh_func_end13-.Leh_func_begin13
	.uleb128	0
	.byte	0xE
	.uleb128	96
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel13-.Leh_func_begin13
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end13:
.Lcblas_chpr2.eh:
	.long	.Leh_frame_end14-.Leh_frame_begin14
.Leh_frame_begin14:
	.long	.Leh_frame_begin14-.Leh_frame_common
	.long	.Leh_func_begin14-.
	.long	.Leh_func_end14-.Leh_func_begin14
	.uleb128	0
	.byte	0xE
	.uleb128	112
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel14-.Leh_func_begin14
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end14:
.Lcblas_chpr.eh:
	.long	.Leh_frame_end15-.Leh_frame_begin15
.Leh_frame_begin15:
	.long	.Leh_frame_begin15-.Leh_frame_common
	.long	.Leh_func_begin15-.
	.long	.Leh_func_end15-.Leh_func_begin15
	.uleb128	0
	.byte	0xE
	.uleb128	80
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel15-.Leh_func_begin15
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end15:
.Lcblas_csymm.eh:
	.long	.Leh_frame_end16-.Leh_frame_begin16
.Leh_frame_begin16:
	.long	.Leh_frame_begin16-.Leh_frame_common
	.long	.Leh_func_begin16-.
	.long	.Leh_func_end16-.Leh_func_begin16
	.uleb128	0
	.byte	0xE
	.uleb128	160
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel16-.Leh_func_begin16
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end16:
.Lcblas_csyr2k.eh:
	.long	.Leh_frame_end17-.Leh_frame_begin17
.Leh_frame_begin17:
	.long	.Leh_frame_begin17-.Leh_frame_common
	.long	.Leh_func_begin17-.
	.long	.Leh_func_end17-.Leh_func_begin17
	.uleb128	0
	.byte	0xE
	.uleb128	112
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel17-.Leh_func_begin17
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end17:
.Lcblas_csyrk.eh:
	.long	.Leh_frame_end18-.Leh_frame_begin18
.Leh_frame_begin18:
	.long	.Leh_frame_begin18-.Leh_frame_common
	.long	.Leh_func_begin18-.
	.long	.Leh_func_end18-.Leh_func_begin18
	.uleb128	0
	.byte	0xE
	.uleb128	80
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel18-.Leh_func_begin18
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end18:
.Lcblas_ctbmv.eh:
	.long	.Leh_frame_end19-.Leh_frame_begin19
.Leh_frame_begin19:
	.long	.Leh_frame_begin19-.Leh_frame_common
	.long	.Leh_func_begin19-.
	.long	.Leh_func_end19-.Leh_func_begin19
	.uleb128	0
	.byte	0xE
	.uleb128	112
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel19-.Leh_func_begin19
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end19:
.Lcblas_ctbsv.eh:
	.long	.Leh_frame_end20-.Leh_frame_begin20
.Leh_frame_begin20:
	.long	.Leh_frame_begin20-.Leh_frame_common
	.long	.Leh_func_begin20-.
	.long	.Leh_func_end20-.Leh_func_begin20
	.uleb128	0
	.byte	0xE
	.uleb128	144
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel20-.Leh_func_begin20
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end20:
.Lcblas_ctpmv.eh:
	.long	.Leh_frame_end21-.Leh_frame_begin21
.Leh_frame_begin21:
	.long	.Leh_frame_begin21-.Leh_frame_common
	.long	.Leh_func_begin21-.
	.long	.Leh_func_end21-.Leh_func_begin21
	.uleb128	0
	.byte	0xE
	.uleb128	80
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel21-.Leh_func_begin21
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end21:
.Lcblas_ctpsv.eh:
	.long	.Leh_frame_end22-.Leh_frame_begin22
.Leh_frame_begin22:
	.long	.Leh_frame_begin22-.Leh_frame_common
	.long	.Leh_func_begin22-.
	.long	.Leh_func_end22-.Leh_func_begin22
	.uleb128	0
	.byte	0xE
	.uleb128	128
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel22-.Leh_func_begin22
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end22:
.Lcblas_ctrmm.eh:
	.long	.Leh_frame_end23-.Leh_frame_begin23
.Leh_frame_begin23:
	.long	.Leh_frame_begin23-.Leh_frame_common
	.long	.Leh_func_begin23-.
	.long	.Leh_func_end23-.Leh_func_begin23
	.uleb128	0
	.byte	0xE
	.uleb128	112
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel23-.Leh_func_begin23
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end23:
.Lcblas_ctrmv.eh:
	.long	.Leh_frame_end24-.Leh_frame_begin24
.Leh_frame_begin24:
	.long	.Leh_frame_begin24-.Leh_frame_common
	.long	.Leh_func_begin24-.
	.long	.Leh_func_end24-.Leh_func_begin24
	.uleb128	0
	.byte	0xE
	.uleb128	96
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel24-.Leh_func_begin24
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end24:
.Lcblas_ctrsm.eh:
	.long	.Leh_frame_end25-.Leh_frame_begin25
.Leh_frame_begin25:
	.long	.Leh_frame_begin25-.Leh_frame_common
	.long	.Leh_func_begin25-.
	.long	.Leh_func_end25-.Leh_func_begin25
	.uleb128	0
	.byte	0xE
	.uleb128	112
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel25-.Leh_func_begin25
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end25:
.Lcblas_ctrsv.eh:
	.long	.Leh_frame_end26-.Leh_frame_begin26
.Leh_frame_begin26:
	.long	.Leh_frame_begin26-.Leh_frame_common
	.long	.Leh_func_begin26-.
	.long	.Leh_func_end26-.Leh_func_begin26
	.uleb128	0
	.byte	0xE
	.uleb128	144
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel26-.Leh_func_begin26
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end26:
.Lcblas_daxpy.eh:
	.long	.Leh_frame_end27-.Leh_frame_begin27
.Leh_frame_begin27:
	.long	.Leh_frame_begin27-.Leh_frame_common
	.long	.Leh_func_begin27-.
	.long	.Leh_func_end27-.Leh_func_begin27
	.uleb128	0
	.byte	0xE
	.uleb128	64
	.byte	0x83
	.uleb128	6
	.byte	0x8C
	.uleb128	5
	.byte	0x8D
	.uleb128	4
	.byte	0x8E
	.uleb128	3
	.byte	0x8F
	.uleb128	2
	.byte	0x4
	.long	.Llabel27-.Leh_func_begin27
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end27:
.Lcblas_ddot.eh:
	.long	.Leh_frame_end28-.Leh_frame_begin28
.Leh_frame_begin28:
	.long	.Leh_frame_begin28-.Leh_frame_common
	.long	.Leh_func_begin28-.
	.long	.Leh_func_end28-.Leh_func_begin28
	.uleb128	0
	.byte	0xE
	.uleb128	48
	.byte	0x83
	.uleb128	6
	.byte	0x8C
	.uleb128	5
	.byte	0x8D
	.uleb128	4
	.byte	0x8E
	.uleb128	3
	.byte	0x8F
	.uleb128	2
	.byte	0x4
	.long	.Llabel28-.Leh_func_begin28
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end28:
.Lcblas_dgbmv.eh:
	.long	.Leh_frame_end29-.Leh_frame_begin29
.Leh_frame_begin29:
	.long	.Leh_frame_begin29-.Leh_frame_common
	.long	.Leh_func_begin29-.
	.long	.Leh_func_end29-.Leh_func_begin29
	.uleb128	0
	.byte	0xE
	.uleb128	80
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel29-.Leh_func_begin29
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end29:
.Lcblas_dgemm.eh:
	.long	.Leh_frame_end30-.Leh_frame_begin30
.Leh_frame_begin30:
	.long	.Leh_frame_begin30-.Leh_frame_common
	.long	.Leh_func_begin30-.
	.long	.Leh_func_end30-.Leh_func_begin30
	.uleb128	0
	.byte	0xE
	.uleb128	96
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel30-.Leh_func_begin30
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end30:
.Lcblas_dgemv.eh:
	.long	.Leh_frame_end31-.Leh_frame_begin31
.Leh_frame_begin31:
	.long	.Leh_frame_begin31-.Leh_frame_common
	.long	.Leh_func_begin31-.
	.long	.Leh_func_end31-.Leh_func_begin31
	.uleb128	0
	.byte	0xE
	.uleb128	96
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel31-.Leh_func_begin31
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end31:
.Lcblas_drot.eh:
	.long	.Leh_frame_end32-.Leh_frame_begin32
.Leh_frame_begin32:
	.long	.Leh_frame_begin32-.Leh_frame_common
	.long	.Leh_func_begin32-.
	.long	.Leh_func_end32-.Leh_func_begin32
	.uleb128	0
	.byte	0xE
	.uleb128	64
	.byte	0x83
	.uleb128	6
	.byte	0x8C
	.uleb128	5
	.byte	0x8D
	.uleb128	4
	.byte	0x8E
	.uleb128	3
	.byte	0x8F
	.uleb128	2
	.byte	0x4
	.long	.Llabel32-.Leh_func_begin32
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end32:
.Lcblas_drotm.eh:
	.long	.Leh_frame_end33-.Leh_frame_begin33
.Leh_frame_begin33:
	.long	.Leh_frame_begin33-.Leh_frame_common
	.long	.Leh_func_begin33-.
	.long	.Leh_func_end33-.Leh_func_begin33
	.uleb128	0
	.byte	0xE
	.uleb128	16
	.byte	0x83
	.uleb128	2
	.byte	0x4
	.long	.Llabel33-.Leh_func_begin33
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end33:
.Lcblas_dsbmv.eh:
	.long	.Leh_frame_end34-.Leh_frame_begin34
.Leh_frame_begin34:
	.long	.Leh_frame_begin34-.Leh_frame_common
	.long	.Leh_func_begin34-.
	.long	.Leh_func_end34-.Leh_func_begin34
	.uleb128	0
	.byte	0xE
	.uleb128	112
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel34-.Leh_func_begin34
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end34:
.Lcblas_dsdot.eh:
	.long	.Leh_frame_end35-.Leh_frame_begin35
.Leh_frame_begin35:
	.long	.Leh_frame_begin35-.Leh_frame_common
	.long	.Leh_func_begin35-.
	.long	.Leh_func_end35-.Leh_func_begin35
	.uleb128	0
	.byte	0xE
	.uleb128	48
	.byte	0x83
	.uleb128	6
	.byte	0x8C
	.uleb128	5
	.byte	0x8D
	.uleb128	4
	.byte	0x8E
	.uleb128	3
	.byte	0x8F
	.uleb128	2
	.byte	0x4
	.long	.Llabel35-.Leh_func_begin35
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end35:
.Lcblas_dspmv.eh:
	.long	.Leh_frame_end36-.Leh_frame_begin36
.Leh_frame_begin36:
	.long	.Leh_frame_begin36-.Leh_frame_common
	.long	.Leh_func_begin36-.
	.long	.Leh_func_end36-.Leh_func_begin36
	.uleb128	0
	.byte	0xE
	.uleb128	96
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel36-.Leh_func_begin36
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end36:
.Lcblas_dspr2.eh:
	.long	.Leh_frame_end37-.Leh_frame_begin37
.Leh_frame_begin37:
	.long	.Leh_frame_begin37-.Leh_frame_common
	.long	.Leh_func_begin37-.
	.long	.Leh_func_end37-.Leh_func_begin37
	.uleb128	0
	.byte	0xE
	.uleb128	64
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel37-.Leh_func_begin37
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end37:
.Lcblas_dspr.eh:
	.long	.Leh_frame_end38-.Leh_frame_begin38
.Leh_frame_begin38:
	.long	.Leh_frame_begin38-.Leh_frame_common
	.long	.Leh_func_begin38-.
	.long	.Leh_func_end38-.Leh_func_begin38
	.uleb128	0
	.byte	0xE
	.uleb128	48
	.byte	0x83
	.uleb128	5
	.byte	0x8C
	.uleb128	4
	.byte	0x8E
	.uleb128	3
	.byte	0x8F
	.uleb128	2
	.byte	0x4
	.long	.Llabel38-.Leh_func_begin38
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end38:
.Lcblas_dsymm.eh:
	.long	.Leh_frame_end39-.Leh_frame_begin39
.Leh_frame_begin39:
	.long	.Leh_frame_begin39-.Leh_frame_common
	.long	.Leh_func_begin39-.
	.long	.Leh_func_end39-.Leh_func_begin39
	.uleb128	0
	.byte	0xE
	.uleb128	96
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel39-.Leh_func_begin39
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end39:
.Lcblas_dsymv.eh:
	.long	.Leh_frame_end40-.Leh_frame_begin40
.Leh_frame_begin40:
	.long	.Leh_frame_begin40-.Leh_frame_common
	.long	.Leh_func_begin40-.
	.long	.Leh_func_end40-.Leh_func_begin40
	.uleb128	0
	.byte	0xE
	.uleb128	96
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel40-.Leh_func_begin40
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end40:
.Lcblas_dsyr2.eh:
	.long	.Leh_frame_end41-.Leh_frame_begin41
.Leh_frame_begin41:
	.long	.Leh_frame_begin41-.Leh_frame_common
	.long	.Leh_func_begin41-.
	.long	.Leh_func_end41-.Leh_func_begin41
	.uleb128	0
	.byte	0xE
	.uleb128	64
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel41-.Leh_func_begin41
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end41:
.Lcblas_dsyr2k.eh:
	.long	.Leh_frame_end42-.Leh_frame_begin42
.Leh_frame_begin42:
	.long	.Leh_frame_begin42-.Leh_frame_common
	.long	.Leh_func_begin42-.
	.long	.Leh_func_end42-.Leh_func_begin42
	.uleb128	0
	.byte	0xE
	.uleb128	64
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel42-.Leh_func_begin42
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end42:
.Lcblas_dsyr.eh:
	.long	.Leh_frame_end43-.Leh_frame_begin43
.Leh_frame_begin43:
	.long	.Leh_frame_begin43-.Leh_frame_common
	.long	.Leh_func_begin43-.
	.long	.Leh_func_end43-.Leh_func_begin43
	.uleb128	0
	.byte	0xE
	.uleb128	32
	.byte	0x83
	.uleb128	4
	.byte	0x8E
	.uleb128	3
	.byte	0x8F
	.uleb128	2
	.byte	0x4
	.long	.Llabel43-.Leh_func_begin43
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end43:
.Lcblas_dsyrk.eh:
	.long	.Leh_frame_end44-.Leh_frame_begin44
.Leh_frame_begin44:
	.long	.Leh_frame_begin44-.Leh_frame_common
	.long	.Leh_func_begin44-.
	.long	.Leh_func_end44-.Leh_func_begin44
	.uleb128	0
	.byte	0xE
	.uleb128	80
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel44-.Leh_func_begin44
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end44:
.Lcblas_dtbsv.eh:
	.long	.Leh_frame_end45-.Leh_frame_begin45
.Leh_frame_begin45:
	.long	.Leh_frame_begin45-.Leh_frame_common
	.long	.Leh_func_begin45-.
	.long	.Leh_func_end45-.Leh_func_begin45
	.uleb128	0
	.byte	0xE
	.uleb128	96
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel45-.Leh_func_begin45
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end45:
.Lcblas_dtpmv.eh:
	.long	.Leh_frame_end46-.Leh_frame_begin46
.Leh_frame_begin46:
	.long	.Leh_frame_begin46-.Leh_frame_common
	.long	.Leh_func_begin46-.
	.long	.Leh_func_end46-.Leh_func_begin46
	.uleb128	0
	.byte	0xE
	.uleb128	80
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel46-.Leh_func_begin46
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end46:
.Lcblas_dtpsv.eh:
	.long	.Leh_frame_end47-.Leh_frame_begin47
.Leh_frame_begin47:
	.long	.Leh_frame_begin47-.Leh_frame_common
	.long	.Leh_func_begin47-.
	.long	.Leh_func_end47-.Leh_func_begin47
	.uleb128	0
	.byte	0xE
	.uleb128	80
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel47-.Leh_func_begin47
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end47:
.Lcblas_dtrmm.eh:
	.long	.Leh_frame_end48-.Leh_frame_begin48
.Leh_frame_begin48:
	.long	.Leh_frame_begin48-.Leh_frame_common
	.long	.Leh_func_begin48-.
	.long	.Leh_func_end48-.Leh_func_begin48
	.uleb128	0
	.byte	0xE
	.uleb128	80
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel48-.Leh_func_begin48
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end48:
.Lcblas_dtrmv.eh:
	.long	.Leh_frame_end49-.Leh_frame_begin49
.Leh_frame_begin49:
	.long	.Leh_frame_begin49-.Leh_frame_common
	.long	.Leh_func_begin49-.
	.long	.Leh_func_end49-.Leh_func_begin49
	.uleb128	0
	.byte	0xE
	.uleb128	80
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel49-.Leh_func_begin49
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end49:
.Lcblas_dtrsm.eh:
	.long	.Leh_frame_end50-.Leh_frame_begin50
.Leh_frame_begin50:
	.long	.Leh_frame_begin50-.Leh_frame_common
	.long	.Leh_func_begin50-.
	.long	.Leh_func_end50-.Leh_func_begin50
	.uleb128	0
	.byte	0xE
	.uleb128	64
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel50-.Leh_func_begin50
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end50:
.Lcblas_dtrsv.eh:
	.long	.Leh_frame_end51-.Leh_frame_begin51
.Leh_frame_begin51:
	.long	.Leh_frame_begin51-.Leh_frame_common
	.long	.Leh_func_begin51-.
	.long	.Leh_func_end51-.Leh_func_begin51
	.uleb128	0
	.byte	0xE
	.uleb128	80
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel51-.Leh_func_begin51
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end51:
.Lcblas_saxpy.eh:
	.long	.Leh_frame_end52-.Leh_frame_begin52
.Leh_frame_begin52:
	.long	.Leh_frame_begin52-.Leh_frame_common
	.long	.Leh_func_begin52-.
	.long	.Leh_func_end52-.Leh_func_begin52
	.uleb128	0
	.byte	0xE
	.uleb128	64
	.byte	0x83
	.uleb128	6
	.byte	0x8C
	.uleb128	5
	.byte	0x8D
	.uleb128	4
	.byte	0x8E
	.uleb128	3
	.byte	0x8F
	.uleb128	2
	.byte	0x4
	.long	.Llabel52-.Leh_func_begin52
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end52:
.Lcblas_sdot.eh:
	.long	.Leh_frame_end53-.Leh_frame_begin53
.Leh_frame_begin53:
	.long	.Leh_frame_begin53-.Leh_frame_common
	.long	.Leh_func_begin53-.
	.long	.Leh_func_end53-.Leh_func_begin53
	.uleb128	0
	.byte	0xE
	.uleb128	48
	.byte	0x83
	.uleb128	6
	.byte	0x8C
	.uleb128	5
	.byte	0x8D
	.uleb128	4
	.byte	0x8E
	.uleb128	3
	.byte	0x8F
	.uleb128	2
	.byte	0x4
	.long	.Llabel53-.Leh_func_begin53
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end53:
.Lcblas_sdsdot.eh:
	.long	.Leh_frame_end54-.Leh_frame_begin54
.Leh_frame_begin54:
	.long	.Leh_frame_begin54-.Leh_frame_common
	.long	.Leh_func_begin54-.
	.long	.Leh_func_end54-.Leh_func_begin54
	.uleb128	0
	.byte	0xE
	.uleb128	64
	.byte	0x83
	.uleb128	6
	.byte	0x8C
	.uleb128	5
	.byte	0x8D
	.uleb128	4
	.byte	0x8E
	.uleb128	3
	.byte	0x8F
	.uleb128	2
	.byte	0x4
	.long	.Llabel54-.Leh_func_begin54
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end54:
.Lcblas_sgbmv.eh:
	.long	.Leh_frame_end55-.Leh_frame_begin55
.Leh_frame_begin55:
	.long	.Leh_frame_begin55-.Leh_frame_common
	.long	.Leh_func_begin55-.
	.long	.Leh_func_end55-.Leh_func_begin55
	.uleb128	0
	.byte	0xE
	.uleb128	80
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel55-.Leh_func_begin55
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end55:
.Lcblas_sgemm.eh:
	.long	.Leh_frame_end56-.Leh_frame_begin56
.Leh_frame_begin56:
	.long	.Leh_frame_begin56-.Leh_frame_common
	.long	.Leh_func_begin56-.
	.long	.Leh_func_end56-.Leh_func_begin56
	.uleb128	0
	.byte	0xE
	.uleb128	96
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel56-.Leh_func_begin56
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end56:
.Lcblas_sgemv.eh:
	.long	.Leh_frame_end57-.Leh_frame_begin57
.Leh_frame_begin57:
	.long	.Leh_frame_begin57-.Leh_frame_common
	.long	.Leh_func_begin57-.
	.long	.Leh_func_end57-.Leh_func_begin57
	.uleb128	0
	.byte	0xE
	.uleb128	80
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel57-.Leh_func_begin57
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end57:
.Lcblas_sger.eh:
	.long	.Leh_frame_end58-.Leh_frame_begin58
.Leh_frame_begin58:
	.long	.Leh_frame_begin58-.Leh_frame_common
	.long	.Leh_func_begin58-.
	.long	.Leh_func_end58-.Leh_func_begin58
	.uleb128	0
	.byte	0xE
	.uleb128	64
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel58-.Leh_func_begin58
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end58:
.Lcblas_srot.eh:
	.long	.Leh_frame_end59-.Leh_frame_begin59
.Leh_frame_begin59:
	.long	.Leh_frame_begin59-.Leh_frame_common
	.long	.Leh_func_begin59-.
	.long	.Leh_func_end59-.Leh_func_begin59
	.uleb128	0
	.byte	0xE
	.uleb128	64
	.byte	0x83
	.uleb128	6
	.byte	0x8C
	.uleb128	5
	.byte	0x8D
	.uleb128	4
	.byte	0x8E
	.uleb128	3
	.byte	0x8F
	.uleb128	2
	.byte	0x4
	.long	.Llabel59-.Leh_func_begin59
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end59:
.Lcblas_srotm.eh:
	.long	.Leh_frame_end60-.Leh_frame_begin60
.Leh_frame_begin60:
	.long	.Leh_frame_begin60-.Leh_frame_common
	.long	.Leh_func_begin60-.
	.long	.Leh_func_end60-.Leh_func_begin60
	.uleb128	0
	.byte	0xE
	.uleb128	16
	.byte	0x83
	.uleb128	2
	.byte	0x4
	.long	.Llabel60-.Leh_func_begin60
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end60:
.Lcblas_ssbmv.eh:
	.long	.Leh_frame_end61-.Leh_frame_begin61
.Leh_frame_begin61:
	.long	.Leh_frame_begin61-.Leh_frame_common
	.long	.Leh_func_begin61-.
	.long	.Leh_func_end61-.Leh_func_begin61
	.uleb128	0
	.byte	0xE
	.uleb128	112
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel61-.Leh_func_begin61
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end61:
.Lcblas_sspmv.eh:
	.long	.Leh_frame_end62-.Leh_frame_begin62
.Leh_frame_begin62:
	.long	.Leh_frame_begin62-.Leh_frame_common
	.long	.Leh_func_begin62-.
	.long	.Leh_func_end62-.Leh_func_begin62
	.uleb128	0
	.byte	0xE
	.uleb128	96
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel62-.Leh_func_begin62
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end62:
.Lcblas_sspr2.eh:
	.long	.Leh_frame_end63-.Leh_frame_begin63
.Leh_frame_begin63:
	.long	.Leh_frame_begin63-.Leh_frame_common
	.long	.Leh_func_begin63-.
	.long	.Leh_func_end63-.Leh_func_begin63
	.uleb128	0
	.byte	0xE
	.uleb128	64
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel63-.Leh_func_begin63
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end63:
.Lcblas_sspr.eh:
	.long	.Leh_frame_end64-.Leh_frame_begin64
.Leh_frame_begin64:
	.long	.Leh_frame_begin64-.Leh_frame_common
	.long	.Leh_func_begin64-.
	.long	.Leh_func_end64-.Leh_func_begin64
	.uleb128	0
	.byte	0xE
	.uleb128	48
	.byte	0x83
	.uleb128	5
	.byte	0x8C
	.uleb128	4
	.byte	0x8E
	.uleb128	3
	.byte	0x8F
	.uleb128	2
	.byte	0x4
	.long	.Llabel64-.Leh_func_begin64
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end64:
.Lcblas_ssymm.eh:
	.long	.Leh_frame_end65-.Leh_frame_begin65
.Leh_frame_begin65:
	.long	.Leh_frame_begin65-.Leh_frame_common
	.long	.Leh_func_begin65-.
	.long	.Leh_func_end65-.Leh_func_begin65
	.uleb128	0
	.byte	0xE
	.uleb128	96
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel65-.Leh_func_begin65
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end65:
.Lcblas_ssymv.eh:
	.long	.Leh_frame_end66-.Leh_frame_begin66
.Leh_frame_begin66:
	.long	.Leh_frame_begin66-.Leh_frame_common
	.long	.Leh_func_begin66-.
	.long	.Leh_func_end66-.Leh_func_begin66
	.uleb128	0
	.byte	0xE
	.uleb128	96
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel66-.Leh_func_begin66
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end66:
.Lcblas_ssyr2.eh:
	.long	.Leh_frame_end67-.Leh_frame_begin67
.Leh_frame_begin67:
	.long	.Leh_frame_begin67-.Leh_frame_common
	.long	.Leh_func_begin67-.
	.long	.Leh_func_end67-.Leh_func_begin67
	.uleb128	0
	.byte	0xE
	.uleb128	64
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel67-.Leh_func_begin67
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end67:
.Lcblas_ssyr2k.eh:
	.long	.Leh_frame_end68-.Leh_frame_begin68
.Leh_frame_begin68:
	.long	.Leh_frame_begin68-.Leh_frame_common
	.long	.Leh_func_begin68-.
	.long	.Leh_func_end68-.Leh_func_begin68
	.uleb128	0
	.byte	0xE
	.uleb128	64
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel68-.Leh_func_begin68
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end68:
.Lcblas_ssyr.eh:
	.long	.Leh_frame_end69-.Leh_frame_begin69
.Leh_frame_begin69:
	.long	.Leh_frame_begin69-.Leh_frame_common
	.long	.Leh_func_begin69-.
	.long	.Leh_func_end69-.Leh_func_begin69
	.uleb128	0
	.byte	0xE
	.uleb128	32
	.byte	0x83
	.uleb128	4
	.byte	0x8E
	.uleb128	3
	.byte	0x8F
	.uleb128	2
	.byte	0x4
	.long	.Llabel69-.Leh_func_begin69
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end69:
.Lcblas_ssyrk.eh:
	.long	.Leh_frame_end70-.Leh_frame_begin70
.Leh_frame_begin70:
	.long	.Leh_frame_begin70-.Leh_frame_common
	.long	.Leh_func_begin70-.
	.long	.Leh_func_end70-.Leh_func_begin70
	.uleb128	0
	.byte	0xE
	.uleb128	80
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel70-.Leh_func_begin70
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end70:
.Lcblas_stbsv.eh:
	.long	.Leh_frame_end71-.Leh_frame_begin71
.Leh_frame_begin71:
	.long	.Leh_frame_begin71-.Leh_frame_common
	.long	.Leh_func_begin71-.
	.long	.Leh_func_end71-.Leh_func_begin71
	.uleb128	0
	.byte	0xE
	.uleb128	96
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel71-.Leh_func_begin71
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end71:
.Lcblas_stpmv.eh:
	.long	.Leh_frame_end72-.Leh_frame_begin72
.Leh_frame_begin72:
	.long	.Leh_frame_begin72-.Leh_frame_common
	.long	.Leh_func_begin72-.
	.long	.Leh_func_end72-.Leh_func_begin72
	.uleb128	0
	.byte	0xE
	.uleb128	80
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel72-.Leh_func_begin72
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end72:
.Lcblas_stpsv.eh:
	.long	.Leh_frame_end73-.Leh_frame_begin73
.Leh_frame_begin73:
	.long	.Leh_frame_begin73-.Leh_frame_common
	.long	.Leh_func_begin73-.
	.long	.Leh_func_end73-.Leh_func_begin73
	.uleb128	0
	.byte	0xE
	.uleb128	80
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel73-.Leh_func_begin73
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end73:
.Lcblas_strmm.eh:
	.long	.Leh_frame_end74-.Leh_frame_begin74
.Leh_frame_begin74:
	.long	.Leh_frame_begin74-.Leh_frame_common
	.long	.Leh_func_begin74-.
	.long	.Leh_func_end74-.Leh_func_begin74
	.uleb128	0
	.byte	0xE
	.uleb128	80
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel74-.Leh_func_begin74
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end74:
.Lcblas_strmv.eh:
	.long	.Leh_frame_end75-.Leh_frame_begin75
.Leh_frame_begin75:
	.long	.Leh_frame_begin75-.Leh_frame_common
	.long	.Leh_func_begin75-.
	.long	.Leh_func_end75-.Leh_func_begin75
	.uleb128	0
	.byte	0xE
	.uleb128	80
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel75-.Leh_func_begin75
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end75:
.Lcblas_strsm.eh:
	.long	.Leh_frame_end76-.Leh_frame_begin76
.Leh_frame_begin76:
	.long	.Leh_frame_begin76-.Leh_frame_common
	.long	.Leh_func_begin76-.
	.long	.Leh_func_end76-.Leh_func_begin76
	.uleb128	0
	.byte	0xE
	.uleb128	64
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel76-.Leh_func_begin76
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end76:
.Lcblas_strsv.eh:
	.long	.Leh_frame_end77-.Leh_frame_begin77
.Leh_frame_begin77:
	.long	.Leh_frame_begin77-.Leh_frame_common
	.long	.Leh_func_begin77-.
	.long	.Leh_func_end77-.Leh_func_begin77
	.uleb128	0
	.byte	0xE
	.uleb128	80
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel77-.Leh_func_begin77
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end77:
.Lcblas_xerbla.eh:
	.long	.Leh_frame_end78-.Leh_frame_begin78
.Leh_frame_begin78:
	.long	.Leh_frame_begin78-.Leh_frame_common
	.long	.Leh_func_begin78-.
	.long	.Leh_func_end78-.Leh_func_begin78
	.uleb128	0
	.byte	0xE
	.uleb128	224
	.byte	0x83
	.uleb128	2
	.byte	0x4
	.long	.Llabel78-.Leh_func_begin78
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end78:
.Lcblas_zgbmv.eh:
	.long	.Leh_frame_end79-.Leh_frame_begin79
.Leh_frame_begin79:
	.long	.Leh_frame_begin79-.Leh_frame_common
	.long	.Leh_func_begin79-.
	.long	.Leh_func_end79-.Leh_func_begin79
	.uleb128	0
	.byte	0xE
	.uleb128	96
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel79-.Leh_func_begin79
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end79:
.Lcblas_zgemm.eh:
	.long	.Leh_frame_end80-.Leh_frame_begin80
.Leh_frame_begin80:
	.long	.Leh_frame_begin80-.Leh_frame_common
	.long	.Leh_func_begin80-.
	.long	.Leh_func_end80-.Leh_func_begin80
	.uleb128	0
	.byte	0xE
	.uleb128	96
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel80-.Leh_func_begin80
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end80:
.Lcblas_zgemv.eh:
	.long	.Leh_frame_end81-.Leh_frame_begin81
.Leh_frame_begin81:
	.long	.Leh_frame_begin81-.Leh_frame_common
	.long	.Leh_func_begin81-.
	.long	.Leh_func_end81-.Leh_func_begin81
	.uleb128	0
	.byte	0xE
	.uleb128	80
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel81-.Leh_func_begin81
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end81:
.Lcblas_zgerc.eh:
	.long	.Leh_frame_end82-.Leh_frame_begin82
.Leh_frame_begin82:
	.long	.Leh_frame_begin82-.Leh_frame_common
	.long	.Leh_func_begin82-.
	.long	.Leh_func_end82-.Leh_func_begin82
	.uleb128	0
	.byte	0xE
	.uleb128	64
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel82-.Leh_func_begin82
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end82:
.Lcblas_zgeru.eh:
	.long	.Leh_frame_end83-.Leh_frame_begin83
.Leh_frame_begin83:
	.long	.Leh_frame_begin83-.Leh_frame_common
	.long	.Leh_func_begin83-.
	.long	.Leh_func_end83-.Leh_func_begin83
	.uleb128	0
	.byte	0xE
	.uleb128	64
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel83-.Leh_func_begin83
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end83:
.Lcblas_zhbmv.eh:
	.long	.Leh_frame_end84-.Leh_frame_begin84
.Leh_frame_begin84:
	.long	.Leh_frame_begin84-.Leh_frame_common
	.long	.Leh_func_begin84-.
	.long	.Leh_func_end84-.Leh_func_begin84
	.uleb128	0
	.byte	0xE
	.uleb128	112
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel84-.Leh_func_begin84
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end84:
.Lcblas_zhemm.eh:
	.long	.Leh_frame_end85-.Leh_frame_begin85
.Leh_frame_begin85:
	.long	.Leh_frame_begin85-.Leh_frame_common
	.long	.Leh_func_begin85-.
	.long	.Leh_func_end85-.Leh_func_begin85
	.uleb128	0
	.byte	0xE
	.uleb128	144
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel85-.Leh_func_begin85
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end85:
.Lcblas_zhemv.eh:
	.long	.Leh_frame_end86-.Leh_frame_begin86
.Leh_frame_begin86:
	.long	.Leh_frame_begin86-.Leh_frame_common
	.long	.Leh_func_begin86-.
	.long	.Leh_func_end86-.Leh_func_begin86
	.uleb128	0
	.byte	0xE
	.uleb128	96
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel86-.Leh_func_begin86
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end86:
.Lcblas_zher2.eh:
	.long	.Leh_frame_end87-.Leh_frame_begin87
.Leh_frame_begin87:
	.long	.Leh_frame_begin87-.Leh_frame_common
	.long	.Leh_func_begin87-.
	.long	.Leh_func_end87-.Leh_func_begin87
	.uleb128	0
	.byte	0xE
	.uleb128	112
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel87-.Leh_func_begin87
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end87:
.Lcblas_zher2k.eh:
	.long	.Leh_frame_end88-.Leh_frame_begin88
.Leh_frame_begin88:
	.long	.Leh_frame_begin88-.Leh_frame_common
	.long	.Leh_func_begin88-.
	.long	.Leh_func_end88-.Leh_func_begin88
	.uleb128	0
	.byte	0xE
	.uleb128	128
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel88-.Leh_func_begin88
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end88:
.Lcblas_zher.eh:
	.long	.Leh_frame_end89-.Leh_frame_begin89
.Leh_frame_begin89:
	.long	.Leh_frame_begin89-.Leh_frame_common
	.long	.Leh_func_begin89-.
	.long	.Leh_func_end89-.Leh_func_begin89
	.uleb128	0
	.byte	0xE
	.uleb128	80
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel89-.Leh_func_begin89
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end89:
.Lcblas_zherk.eh:
	.long	.Leh_frame_end90-.Leh_frame_begin90
.Leh_frame_begin90:
	.long	.Leh_frame_begin90-.Leh_frame_common
	.long	.Leh_func_begin90-.
	.long	.Leh_func_end90-.Leh_func_begin90
	.uleb128	0
	.byte	0xE
	.uleb128	80
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel90-.Leh_func_begin90
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end90:
.Lcblas_zhpmv.eh:
	.long	.Leh_frame_end91-.Leh_frame_begin91
.Leh_frame_begin91:
	.long	.Leh_frame_begin91-.Leh_frame_common
	.long	.Leh_func_begin91-.
	.long	.Leh_func_end91-.Leh_func_begin91
	.uleb128	0
	.byte	0xE
	.uleb128	96
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel91-.Leh_func_begin91
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end91:
.Lcblas_zhpr2.eh:
	.long	.Leh_frame_end92-.Leh_frame_begin92
.Leh_frame_begin92:
	.long	.Leh_frame_begin92-.Leh_frame_common
	.long	.Leh_func_begin92-.
	.long	.Leh_func_end92-.Leh_func_begin92
	.uleb128	0
	.byte	0xE
	.uleb128	112
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel92-.Leh_func_begin92
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end92:
.Lcblas_zhpr.eh:
	.long	.Leh_frame_end93-.Leh_frame_begin93
.Leh_frame_begin93:
	.long	.Leh_frame_begin93-.Leh_frame_common
	.long	.Leh_func_begin93-.
	.long	.Leh_func_end93-.Leh_func_begin93
	.uleb128	0
	.byte	0xE
	.uleb128	80
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel93-.Leh_func_begin93
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end93:
.Lcblas_zsymm.eh:
	.long	.Leh_frame_end94-.Leh_frame_begin94
.Leh_frame_begin94:
	.long	.Leh_frame_begin94-.Leh_frame_common
	.long	.Leh_func_begin94-.
	.long	.Leh_func_end94-.Leh_func_begin94
	.uleb128	0
	.byte	0xE
	.uleb128	160
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel94-.Leh_func_begin94
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end94:
.Lcblas_zsyr2k.eh:
	.long	.Leh_frame_end95-.Leh_frame_begin95
.Leh_frame_begin95:
	.long	.Leh_frame_begin95-.Leh_frame_common
	.long	.Leh_func_begin95-.
	.long	.Leh_func_end95-.Leh_func_begin95
	.uleb128	0
	.byte	0xE
	.uleb128	112
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel95-.Leh_func_begin95
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end95:
.Lcblas_zsyrk.eh:
	.long	.Leh_frame_end96-.Leh_frame_begin96
.Leh_frame_begin96:
	.long	.Leh_frame_begin96-.Leh_frame_common
	.long	.Leh_func_begin96-.
	.long	.Leh_func_end96-.Leh_func_begin96
	.uleb128	0
	.byte	0xE
	.uleb128	80
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel96-.Leh_func_begin96
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end96:
.Lcblas_ztbmv.eh:
	.long	.Leh_frame_end97-.Leh_frame_begin97
.Leh_frame_begin97:
	.long	.Leh_frame_begin97-.Leh_frame_common
	.long	.Leh_func_begin97-.
	.long	.Leh_func_end97-.Leh_func_begin97
	.uleb128	0
	.byte	0xE
	.uleb128	112
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel97-.Leh_func_begin97
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end97:
.Lcblas_ztbsv.eh:
	.long	.Leh_frame_end98-.Leh_frame_begin98
.Leh_frame_begin98:
	.long	.Leh_frame_begin98-.Leh_frame_common
	.long	.Leh_func_begin98-.
	.long	.Leh_func_end98-.Leh_func_begin98
	.uleb128	0
	.byte	0xE
	.uleb128	176
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel98-.Leh_func_begin98
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end98:
.Lcblas_ztpmv.eh:
	.long	.Leh_frame_end99-.Leh_frame_begin99
.Leh_frame_begin99:
	.long	.Leh_frame_begin99-.Leh_frame_common
	.long	.Leh_func_begin99-.
	.long	.Leh_func_end99-.Leh_func_begin99
	.uleb128	0
	.byte	0xE
	.uleb128	80
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel99-.Leh_func_begin99
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end99:
.Lcblas_ztpsv.eh:
	.long	.Leh_frame_end100-.Leh_frame_begin100
.Leh_frame_begin100:
	.long	.Leh_frame_begin100-.Leh_frame_common
	.long	.Leh_func_begin100-.
	.long	.Leh_func_end100-.Leh_func_begin100
	.uleb128	0
	.byte	0xE
	.uleb128	144
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel100-.Leh_func_begin100
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end100:
.Lcblas_ztrmm.eh:
	.long	.Leh_frame_end101-.Leh_frame_begin101
.Leh_frame_begin101:
	.long	.Leh_frame_begin101-.Leh_frame_common
	.long	.Leh_func_begin101-.
	.long	.Leh_func_end101-.Leh_func_begin101
	.uleb128	0
	.byte	0xE
	.uleb128	112
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel101-.Leh_func_begin101
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end101:
.Lcblas_ztrmv.eh:
	.long	.Leh_frame_end102-.Leh_frame_begin102
.Leh_frame_begin102:
	.long	.Leh_frame_begin102-.Leh_frame_common
	.long	.Leh_func_begin102-.
	.long	.Leh_func_end102-.Leh_func_begin102
	.uleb128	0
	.byte	0xE
	.uleb128	96
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel102-.Leh_func_begin102
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end102:
.Lcblas_ztrsm.eh:
	.long	.Leh_frame_end103-.Leh_frame_begin103
.Leh_frame_begin103:
	.long	.Leh_frame_begin103-.Leh_frame_common
	.long	.Leh_func_begin103-.
	.long	.Leh_func_end103-.Leh_func_begin103
	.uleb128	0
	.byte	0xE
	.uleb128	128
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel103-.Leh_func_begin103
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end103:
.Lcblas_ztrsv.eh:
	.long	.Leh_frame_end104-.Leh_frame_begin104
.Leh_frame_begin104:
	.long	.Leh_frame_begin104-.Leh_frame_common
	.long	.Leh_func_begin104-.
	.long	.Leh_func_end104-.Leh_func_begin104
	.uleb128	0
	.byte	0xE
	.uleb128	160
	.byte	0x83
	.uleb128	7
	.byte	0x8C
	.uleb128	6
	.byte	0x8D
	.uleb128	5
	.byte	0x8E
	.uleb128	4
	.byte	0x8F
	.uleb128	3
	.byte	0x86
	.uleb128	2
	.byte	0x4
	.long	.Llabel104-.Leh_func_begin104
	.byte	0xD
	.uleb128	7
	.align	8
.Leh_frame_end104:

	.section	.note.GNU-stack,"",@progbits
